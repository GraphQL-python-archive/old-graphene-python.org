{
  "modules": {
    "BaseHTTPServer": {
      "file": "BaseHTTPServer.py", 
      "imports": [
        "sys", 
        "time", 
        "mimetools", 
        "socket", 
        "SocketServer", 
        "warnings"
      ]
    }, 
    "Bastion": {
      "file": "Bastion.py", 
      "imports": [
        "rexec", 
        "types", 
        "warnings"
      ]
    }, 
    "CGIHTTPServer": {
      "file": "CGIHTTPServer.py", 
      "imports": [
        "BaseHTTPServer", 
        "base64", 
        "binascii", 
        "select", 
        "subprocess", 
        "sys", 
        "copy", 
        "os", 
        "SimpleHTTPServer", 
        "urllib", 
        "pwd"
      ]
    }, 
    "ConfigParser": {
      "file": "ConfigParser.py", 
      "imports": [
        "collections", 
        "re", 
        "UserDict"
      ]
    }, 
    "Cookie": {
      "file": "Cookie.py", 
      "imports": [
        "Cookie", 
        "time.gmtime", 
        "time.time", 
        "doctest", 
        "pickle", 
        "re", 
        "string", 
        "warnings", 
        "cPickle"
      ]
    }, 
    "DocXMLRPCServer": {
      "file": "DocXMLRPCServer.py", 
      "imports": [
        "sys", 
        "inspect", 
        "pydoc", 
        "re", 
        "SimpleXMLRPCServer"
      ]
    }, 
    "HTMLParser": {
      "file": "HTMLParser.py", 
      "imports": [
        "htmlentitydefs", 
        "markupbase", 
        "re"
      ]
    }, 
    "MimeWriter": {
      "file": "MimeWriter.py", 
      "imports": [
        "mimetools", 
        "test.test_MimeWriter", 
        "warnings"
      ]
    }, 
    "Queue": {
      "file": "Queue.py", 
      "imports": [
        "collections", 
        "dummy_threading", 
        "heapq", 
        "threading", 
        "time.time"
      ]
    }, 
    "SimpleHTTPServer": {
      "file": "SimpleHTTPServer.py", 
      "imports": [
        "BaseHTTPServer", 
        "cStringIO.StringIO", 
        "cgi", 
        "mimetypes", 
        "os", 
        "posixpath", 
        "shutil", 
        "sys", 
        "StringIO", 
        "urllib"
      ]
    }, 
    "SimpleXMLRPCServer": {
      "file": "SimpleXMLRPCServer.py", 
      "imports": [
        "BaseHTTPServer", 
        "fcntl", 
        "os", 
        "pydoc", 
        "re", 
        "sys", 
        "SocketServer", 
        "traceback", 
        "xmlrpclib"
      ]
    }, 
    "SocketServer": {
      "file": "SocketServer.py", 
      "imports": [
        "cStringIO.StringIO", 
        "dummy_threading", 
        "errno", 
        "os", 
        "select", 
        "socket", 
        "sys", 
        "threading", 
        "StringIO", 
        "traceback"
      ]
    }, 
    "StringIO": {
      "file": "StringIO.py", 
      "imports": [
        "errno.EINVAL", 
        "sys"
      ]
    }, 
    "UserDict": {
      "file": "UserDict.py", 
      "imports": [
        "_abcoll", 
        "copy"
      ]
    }, 
    "UserList": {
      "file": "UserList.py", 
      "imports": [
        "collections"
      ]
    }, 
    "UserString": {
      "file": "UserString.py", 
      "imports": [
        "collections", 
        "os", 
        "sys", 
        "test.test_support", 
        "warnings"
      ]
    }, 
    "_LWPCookieJar": {
      "file": "_LWPCookieJar.py", 
      "imports": [
        "time", 
        "cookielib", 
        "re"
      ]
    }, 
    "_MozillaCookieJar": {
      "file": "_MozillaCookieJar.py", 
      "imports": [
        "time", 
        "cookielib", 
        "re"
      ]
    }, 
    "__future__": {
      "file": "__future__.py", 
      "imports": []
    }, 
    "__init__": {
      "file": "__init__.py", 
      "imports": []
    }, 
    "__phello__.foo": {
      "file": "__phello__.foo.py", 
      "imports": []
    }, 
    "_abcoll": {
      "file": "_abcoll.py", 
      "imports": [
        "sys", 
        "abc"
      ]
    }, 
    "_audioop_build": {
      "file": "_audioop_build.py", 
      "imports": [
        "cffi.FFI"
      ]
    }, 
    "_codecs_cn": {
      "file": "_codecs_cn.py", 
      "imports": [
        "_multibytecodec.__getcodec"
      ]
    }, 
    "_codecs_hk": {
      "file": "_codecs_hk.py", 
      "imports": [
        "_multibytecodec.__getcodec"
      ]
    }, 
    "_codecs_iso2022": {
      "file": "_codecs_iso2022.py", 
      "imports": [
        "_multibytecodec.__getcodec"
      ]
    }, 
    "_codecs_jp": {
      "file": "_codecs_jp.py", 
      "imports": [
        "_multibytecodec.__getcodec"
      ]
    }, 
    "_codecs_kr": {
      "file": "_codecs_kr.py", 
      "imports": [
        "_multibytecodec.__getcodec"
      ]
    }, 
    "_codecs_tw": {
      "file": "_codecs_tw.py", 
      "imports": [
        "_multibytecodec.__getcodec"
      ]
    }, 
    "_collections": {
      "file": "_collections.py", 
      "imports": [
        "threading._get_ident"
      ]
    }, 
    "_csv": {
      "file": "_csv.py", 
      "imports": []
    }, 
    "_ctypes_test": {
      "file": "_ctypes_test.py", 
      "imports": [
        "_ctypes", 
        "cpyext", 
        "imp", 
        "os", 
        "_pypy_testcapi"
      ]
    }, 
    "_curses": {
      "file": "_curses.py", 
      "imports": [
        "_curses_cffi.ffi", 
        "_curses_cffi.lib", 
        "functools", 
        "sys"
      ]
    }, 
    "_curses_build": {
      "file": "_curses_build.py", 
      "imports": [
        "cffi.FFI"
      ]
    }, 
    "_curses_panel": {
      "file": "_curses_panel.py", 
      "imports": [
        "_curses"
      ]
    }, 
    "_elementtree": {
      "file": "_elementtree.py", 
      "imports": [
        "xml.etree.ElementTree"
      ]
    }, 
    "_functools": {
      "file": "_functools.py", 
      "imports": []
    }, 
    "_gdbm_build": {
      "file": "_gdbm_build.py", 
      "imports": [
        "cffi", 
        "os", 
        "sys"
      ]
    }, 
    "_marshal": {
      "file": "_marshal.py", 
      "imports": [
        "__pypy__.builtinify", 
        "sys.intern", 
        "types"
      ]
    }, 
    "_md5": {
      "file": "_md5.py", 
      "imports": [
        "copy", 
        "struct"
      ]
    }, 
    "_osx_support": {
      "file": "_osx_support.py", 
      "imports": [
        "sys", 
        "contextlib", 
        "distutils.log", 
        "os", 
        "re", 
        "tempfile"
      ]
    }, 
    "_pwdgrp_build": {
      "file": "_pwdgrp_build.py", 
      "imports": [
        "cffi.FFI"
      ]
    }, 
    "_pyio": {
      "file": "_pyio.py", 
      "imports": [
        "__future__", 
        "_io.FileIO", 
        "array", 
        "errno", 
        "errno.EINTR", 
        "thread.allocate_lock", 
        "abc", 
        "codecs", 
        "dummy_thread", 
        "io", 
        "locale", 
        "os", 
        "warnings"
      ]
    }, 
    "_pypy_interact": {
      "file": "_pypy_interact.py", 
      "imports": [
        "__main__", 
        "code", 
        "os", 
        "sys", 
        "_pypy_irc_topic", 
        "pyrepl.simple_interact"
      ]
    }, 
    "_pypy_irc_topic": {
      "file": "_pypy_irc_topic.py", 
      "imports": [
        "string", 
        "time"
      ]
    }, 
    "_pypy_testcapi": {
      "file": "_pypy_testcapi.py", 
      "imports": [
        "binascii", 
        "distutils.ccompiler", 
        "imp", 
        "os", 
        "sys", 
        "tempfile"
      ]
    }, 
    "_pypy_wait": {
      "file": "_pypy_wait.py", 
      "imports": [
        "ctypes.CDLL", 
        "ctypes.POINTER", 
        "ctypes.byref", 
        "ctypes.c_int", 
        "ctypes.util.find_library", 
        "resource"
      ]
    }, 
    "_scproxy": {
      "file": "_scproxy.py", 
      "imports": [
        "ctypes.c_char_p", 
        "ctypes.c_int", 
        "ctypes.c_int32", 
        "ctypes.c_int64", 
        "ctypes.c_void_p", 
        "ctypes.cdll", 
        "ctypes.create_string_buffer", 
        "ctypes.pointer", 
        "ctypes.util.find_library", 
        "sys"
      ]
    }, 
    "_sha": {
      "file": "_sha.py", 
      "imports": [
        "copy", 
        "struct"
      ]
    }, 
    "_sha256": {
      "file": "_sha256.py", 
      "imports": [
        "struct"
      ]
    }, 
    "_sha512": {
      "file": "_sha512.py", 
      "imports": [
        "_sha512", 
        "struct"
      ]
    }, 
    "_sqlite3": {
      "file": "_sqlite3.py", 
      "imports": [
        "__pypy__.newlist_hint", 
        "_sqlite3_cffi.ffi", 
        "_sqlite3_cffi.lib", 
        "collections", 
        "functools", 
        "sqlite3.dump._iterdump", 
        "string", 
        "sys", 
        "threading._get_ident", 
        "weakref", 
        "datetime"
      ]
    }, 
    "_sqlite3_build": {
      "file": "_sqlite3_build.py", 
      "imports": [
        "cffi.FFI", 
        "os", 
        "sys"
      ]
    }, 
    "_strptime": {
      "file": "_strptime.py", 
      "imports": [
        "thread.allocate_lock", 
        "time", 
        "calendar", 
        "dummy_thread", 
        "locale", 
        "re", 
        "datetime"
      ]
    }, 
    "_structseq": {
      "file": "_structseq.py", 
      "imports": []
    }, 
    "_syslog_build": {
      "file": "_syslog_build.py", 
      "imports": [
        "cffi.FFI"
      ]
    }, 
    "_testcapi": {
      "file": "_testcapi.py", 
      "imports": [
        "_pypy_testcapi", 
        "cpyext", 
        "imp", 
        "os"
      ]
    }, 
    "_threading_local": {
      "file": "_threading_local.py", 
      "imports": [
        "threading", 
        "threading.RLock", 
        "threading.current_thread"
      ]
    }, 
    "_weakrefset": {
      "file": "_weakrefset.py", 
      "imports": [
        "_weakref.ref"
      ]
    }, 
    "abc": {
      "file": "abc.py", 
      "imports": [
        "_weakrefset", 
        "types"
      ]
    }, 
    "aifc": {
      "file": "aifc.py", 
      "imports": [
        "__builtin__", 
        "audioop", 
        "cl", 
        "math", 
        "sys", 
        "chunk", 
        "struct"
      ]
    }, 
    "antigravity": {
      "file": "antigravity.py", 
      "imports": [
        "webbrowser"
      ]
    }, 
    "anydbm": {
      "file": "anydbm.py", 
      "imports": [
        "whichdb"
      ]
    }, 
    "argparse": {
      "file": "argparse.py", 
      "imports": [
        "sys", 
        "collections", 
        "copy", 
        "gettext", 
        "os", 
        "re", 
        "textwrap", 
        "warnings"
      ]
    }, 
    "ast": {
      "file": "ast.py", 
      "imports": [
        "_ast.*", 
        "_ast.__version__", 
        "collections", 
        "inspect"
      ]
    }, 
    "asynchat": {
      "file": "asynchat.py", 
      "imports": [
        "errno", 
        "sys.py3kwarning", 
        "asyncore", 
        "collections", 
        "socket", 
        "warnings"
      ]
    }, 
    "asyncore": {
      "file": "asyncore.py", 
      "imports": [
        "errno.EAGAIN", 
        "errno.EALREADY", 
        "errno.EBADF", 
        "errno.ECONNABORTED", 
        "errno.ECONNRESET", 
        "errno.EINPROGRESS", 
        "errno.EINTR", 
        "errno.EINVAL", 
        "errno.EISCONN", 
        "errno.ENOTCONN", 
        "errno.EPIPE", 
        "errno.ESHUTDOWN", 
        "errno.EWOULDBLOCK", 
        "errno.errorcode", 
        "fcntl", 
        "select", 
        "sys", 
        "time", 
        "os", 
        "socket", 
        "warnings"
      ]
    }, 
    "atexit": {
      "file": "atexit.py", 
      "imports": [
        "sys", 
        "traceback"
      ]
    }, 
    "base64": {
      "file": "base64.py", 
      "imports": [
        "binascii", 
        "sys", 
        "getopt", 
        "re", 
        "struct"
      ]
    }, 
    "bdb": {
      "file": "bdb.py", 
      "imports": [
        "__main__", 
        "sys", 
        "fnmatch", 
        "linecache", 
        "os", 
        "repr", 
        "types"
      ]
    }, 
    "binhex": {
      "file": "binhex.py", 
      "imports": [
        "Carbon.File.FInfo", 
        "Carbon.File.FSSpec", 
        "MacOS.openrf", 
        "binascii", 
        "sys", 
        "os", 
        "struct"
      ]
    }, 
    "bisect": {
      "file": "bisect.py", 
      "imports": [
        "_bisect.*"
      ]
    }, 
    "cPickle": {
      "file": "cPickle.py", 
      "imports": [
        "__pypy__.builtinify", 
        "copy_reg", 
        "marshal", 
        "pickle", 
        "struct", 
        "sys", 
        "types"
      ]
    }, 
    "cProfile": {
      "file": "cProfile.py", 
      "imports": [
        "__main__", 
        "_lsprof", 
        "marshal", 
        "sys", 
        "optparse", 
        "os", 
        "pstats", 
        "types"
      ]
    }, 
    "cStringIO": {
      "file": "cStringIO.py", 
      "imports": [
        "StringIO"
      ]
    }, 
    "calendar": {
      "file": "calendar.py", 
      "imports": [
        "sys", 
        "locale", 
        "optparse", 
        "datetime"
      ]
    }, 
    "cgi": {
      "file": "cgi.py", 
      "imports": [
        "cStringIO.StringIO", 
        "operator.attrgetter", 
        "sys", 
        "mimetools", 
        "os", 
        "re", 
        "rfc822", 
        "StringIO", 
        "tempfile", 
        "traceback", 
        "urlparse", 
        "UserDict", 
        "warnings"
      ]
    }, 
    "cgitb": {
      "file": "cgitb.py", 
      "imports": [
        "sys", 
        "time", 
        "inspect", 
        "keyword", 
        "linecache", 
        "os", 
        "pydoc", 
        "tempfile", 
        "tokenize", 
        "traceback", 
        "types"
      ]
    }, 
    "chunk": {
      "file": "chunk.py", 
      "imports": [
        "struct"
      ]
    }, 
    "cmd": {
      "file": "cmd.py", 
      "imports": [
        "readline", 
        "sys", 
        "string"
      ]
    }, 
    "code": {
      "file": "code.py", 
      "imports": [
        "readline", 
        "sys", 
        "codeop", 
        "traceback"
      ]
    }, 
    "codecs": {
      "file": "codecs.py", 
      "imports": [
        "__builtin__", 
        "_codecs.*", 
        "sys", 
        "encodings"
      ]
    }, 
    "codeop": {
      "file": "codeop.py", 
      "imports": [
        "__future__"
      ]
    }, 
    "collections": {
      "file": "collections.py", 
      "imports": [
        "__pypy__.newdict", 
        "__pypy__.reversed_dict", 
        "_abcoll", 
        "_collections.defaultdict", 
        "_collections.deque", 
        "itertools.chain", 
        "itertools.imap", 
        "itertools.repeat", 
        "itertools.starmap", 
        "operator.eq", 
        "operator.itemgetter", 
        "sys", 
        "thread.get_ident", 
        "doctest", 
        "dummy_thread", 
        "heapq", 
        "keyword", 
        "cPickle"
      ]
    }, 
    "colorsys": {
      "file": "colorsys.py", 
      "imports": []
    }, 
    "commands": {
      "file": "commands.py", 
      "imports": [
        "os", 
        "warnings"
      ]
    }, 
    "compileall": {
      "file": "compileall.py", 
      "imports": [
        "imp", 
        "sys", 
        "getopt", 
        "os", 
        "py_compile", 
        "re", 
        "struct"
      ]
    }, 
    "compiler": {
      "dir": "compiler"
    }, 
    "compiler.__init__": {
      "file": "compiler/__init__.py", 
      "imports": [
        "compiler.pycodegen", 
        "compiler.transformer", 
        "compiler.visitor", 
        "warnings"
      ]
    }, 
    "compiler.ast": {
      "file": "compiler/ast.py", 
      "imports": [
        "compiler.consts"
      ]
    }, 
    "compiler.consts": {
      "file": "compiler/consts.py", 
      "imports": []
    }, 
    "compiler.future": {
      "file": "compiler/future.py", 
      "imports": [
        "compiler", 
        "compiler.ast", 
        "sys"
      ]
    }, 
    "compiler.misc": {
      "file": "compiler/misc.py", 
      "imports": []
    }, 
    "compiler.pyassem": {
      "file": "compiler/pyassem.py", 
      "imports": [
        "compiler.consts", 
        "compiler.misc", 
        "sys", 
        "dis", 
        "types"
      ]
    }, 
    "compiler.pycodegen": {
      "file": "compiler/pycodegen.py", 
      "imports": [
        "cStringIO.StringIO", 
        "compiler", 
        "compiler.ast", 
        "compiler.consts", 
        "compiler.future", 
        "compiler.misc", 
        "compiler.pyassem", 
        "compiler.symbols", 
        "compiler.syntax", 
        "imp", 
        "marshal", 
        "sys", 
        "os", 
        "pprint", 
        "struct"
      ]
    }, 
    "compiler.symbols": {
      "file": "compiler/symbols.py", 
      "imports": [
        "compiler", 
        "compiler.ast", 
        "compiler.consts", 
        "compiler.misc", 
        "symtable", 
        "sys", 
        "types"
      ]
    }, 
    "compiler.syntax": {
      "file": "compiler/syntax.py", 
      "imports": [
        "compiler", 
        "compiler.ast"
      ]
    }, 
    "compiler.transformer": {
      "file": "compiler/transformer.py", 
      "imports": [
        "compiler.ast", 
        "compiler.consts", 
        "parser", 
        "symbol", 
        "token"
      ]
    }, 
    "compiler.visitor": {
      "file": "compiler/visitor.py", 
      "imports": [
        "compiler.ast"
      ]
    }, 
    "contextlib": {
      "file": "contextlib.py", 
      "imports": [
        "sys", 
        "functools", 
        "warnings"
      ]
    }, 
    "cookielib": {
      "file": "cookielib.py", 
      "imports": [
        "_LWPCookieJar", 
        "_MozillaCookieJar", 
        "calendar", 
        "threading", 
        "time", 
        "copy", 
        "dummy_threading", 
        "httplib", 
        "logging", 
        "re", 
        "StringIO", 
        "traceback", 
        "urllib", 
        "urlparse", 
        "warnings"
      ]
    }, 
    "copy": {
      "file": "copy.py", 
      "imports": [
        "org.python.core.PyStringMap", 
        "sys", 
        "copy_reg", 
        "repr", 
        "types", 
        "weakref"
      ]
    }, 
    "copy_reg": {
      "file": "copy_reg.py", 
      "imports": [
        "types"
      ]
    }, 
    "csv": {
      "file": "csv.py", 
      "imports": [
        "_csv.Dialect", 
        "_csv.Error", 
        "_csv.QUOTE_ALL", 
        "_csv.QUOTE_MINIMAL", 
        "_csv.QUOTE_NONE", 
        "_csv.QUOTE_NONNUMERIC", 
        "_csv.__doc__", 
        "_csv.__version__", 
        "_csv.field_size_limit", 
        "_csv.get_dialect", 
        "_csv.list_dialects", 
        "_csv.reader", 
        "_csv.register_dialect", 
        "_csv.unregister_dialect", 
        "_csv.writer", 
        "cStringIO.StringIO", 
        "functools", 
        "re", 
        "StringIO"
      ]
    }, 
    "ctypes_config_cache": {
      "dir": "ctypes_config_cache"
    }, 
    "ctypes_config_cache.__init__": {
      "file": "ctypes_config_cache/__init__.py", 
      "imports": []
    }, 
    "ctypes_config_cache._locale_32_": {
      "file": "ctypes_config_cache/_locale_32_.py", 
      "imports": [
        "ctypes"
      ]
    }, 
    "ctypes_config_cache._locale_cache": {
      "file": "ctypes_config_cache/_locale_cache.py", 
      "imports": [
        "sys"
      ]
    }, 
    "ctypes_config_cache._resource_32_": {
      "file": "ctypes_config_cache/_resource_32_.py", 
      "imports": [
        "ctypes"
      ]
    }, 
    "ctypes_config_cache._resource_cache": {
      "file": "ctypes_config_cache/_resource_cache.py", 
      "imports": [
        "sys"
      ]
    }, 
    "ctypes_config_cache.dumpcache": {
      "file": "ctypes_config_cache/dumpcache.py", 
      "imports": [
        "ctypes_configure.dumpcache", 
        "os", 
        "sys"
      ]
    }, 
    "ctypes_config_cache.locale.ctc": {
      "file": "ctypes_config_cache/locale.ctc.py", 
      "imports": [
        "ctypes_config_cache.dumpcache", 
        "ctypes_configure.configure.ConstantInteger", 
        "ctypes_configure.configure.DefinedConstantInteger", 
        "ctypes_configure.configure.ExternalCompilationInfo", 
        "ctypes_configure.configure.SimpleType", 
        "ctypes_configure.configure.check_eci", 
        "ctypes_configure.configure.configure"
      ]
    }, 
    "ctypes_config_cache.rebuild": {
      "file": "ctypes_config_cache/rebuild.py", 
      "imports": [
        "os", 
        "py", 
        "rpython.tool.ansi_print.ansi_log", 
        "sys"
      ]
    }, 
    "ctypes_config_cache.resource.ctc": {
      "file": "ctypes_config_cache/resource.ctc.py", 
      "imports": [
        "ctypes.sizeof", 
        "ctypes_config_cache.dumpcache", 
        "ctypes_configure.configure.ConstantInteger", 
        "ctypes_configure.configure.DefinedConstantInteger", 
        "ctypes_configure.configure.ExternalCompilationInfo", 
        "ctypes_configure.configure.SimpleType", 
        "ctypes_configure.configure.configure"
      ]
    }, 
    "datetime": {
      "file": "datetime.py", 
      "imports": [
        "__future__", 
        "_strptime", 
        "math", 
        "struct", 
        "time"
      ]
    }, 
    "dbhash": {
      "file": "dbhash.py", 
      "imports": [
        "bsddb", 
        "sys", 
        "warnings"
      ]
    }, 
    "dbm": {
      "file": "dbm.py", 
      "imports": [
        "ctypes.CDLL", 
        "ctypes.POINTER", 
        "ctypes.Structure", 
        "ctypes.c_char", 
        "ctypes.c_char_p", 
        "ctypes.c_int", 
        "ctypes.c_void_p", 
        "ctypes.util", 
        "os", 
        "sys"
      ]
    }, 
    "decimal": {
      "file": "decimal.py", 
      "imports": [
        "collections", 
        "itertools.chain", 
        "itertools.repeat", 
        "math", 
        "sys", 
        "threading", 
        "doctest", 
        "locale", 
        "numbers", 
        "re"
      ]
    }, 
    "difflib": {
      "file": "difflib.py", 
      "imports": [
        "collections", 
        "difflib", 
        "doctest", 
        "functools", 
        "heapq", 
        "re"
      ]
    }, 
    "dircache": {
      "file": "dircache.py", 
      "imports": [
        "os", 
        "warnings"
      ]
    }, 
    "dis": {
      "file": "dis.py", 
      "imports": [
        "sys", 
        "opcode", 
        "types"
      ]
    }, 
    "distutils": {
      "dir": "distutils"
    }, 
    "distutils.__init__": {
      "file": "distutils/__init__.py", 
      "imports": []
    }, 
    "distutils.archive_util": {
      "file": "distutils/archive_util.py", 
      "imports": [
        "distutils.dir_util", 
        "distutils.errors", 
        "distutils.log", 
        "distutils.spawn", 
        "sys", 
        "os", 
        "tarfile", 
        "warnings", 
        "zipfile", 
        "grp", 
        "pwd"
      ]
    }, 
    "distutils.bcppcompiler": {
      "file": "distutils/bcppcompiler.py", 
      "imports": [
        "distutils.ccompiler", 
        "distutils.dep_util", 
        "distutils.errors", 
        "distutils.file_util", 
        "distutils.log", 
        "os"
      ]
    }, 
    "distutils.ccompiler": {
      "file": "distutils/ccompiler.py", 
      "imports": [
        "distutils.debug", 
        "distutils.dep_util", 
        "distutils.dir_util", 
        "distutils.errors", 
        "distutils.fancy_getopt", 
        "distutils.file_util", 
        "distutils.log", 
        "distutils.spawn", 
        "distutils.sysconfig", 
        "distutils.util", 
        "sys", 
        "os", 
        "re", 
        "tempfile"
      ]
    }, 
    "distutils.cmd": {
      "file": "distutils/cmd.py", 
      "imports": [
        "distutils.archive_util", 
        "distutils.debug", 
        "distutils.dep_util", 
        "distutils.dir_util", 
        "distutils.dist", 
        "distutils.errors", 
        "distutils.fancy_getopt", 
        "distutils.file_util", 
        "distutils.log", 
        "distutils.spawn", 
        "distutils.util", 
        "sys", 
        "os", 
        "re"
      ]
    }, 
    "distutils.command": {
      "dir": "distutils/command"
    }, 
    "distutils.command.__init__": {
      "file": "distutils/command/__init__.py", 
      "imports": []
    }, 
    "distutils.command.bdist": {
      "file": "distutils/command/bdist.py", 
      "imports": [
        "distutils.core", 
        "distutils.errors", 
        "distutils.fancy_getopt", 
        "distutils.util", 
        "os"
      ]
    }, 
    "distutils.command.bdist_dumb": {
      "file": "distutils/command/bdist_dumb.py", 
      "imports": [
        "distutils.core", 
        "distutils.dir_util", 
        "distutils.errors", 
        "distutils.log", 
        "distutils.sysconfig", 
        "distutils.util", 
        "os"
      ]
    }, 
    "distutils.command.bdist_msi": {
      "file": "distutils/command/bdist_msi.py", 
      "imports": [
        "distutils.core", 
        "distutils.dir_util", 
        "distutils.errors", 
        "distutils.log", 
        "distutils.sysconfig", 
        "distutils.util", 
        "distutils.version", 
        "msilib", 
        "msilib.Dialog", 
        "msilib.Directory", 
        "msilib.Feature", 
        "msilib.add_data", 
        "msilib.schema", 
        "msilib.sequence", 
        "msilib.text", 
        "sys", 
        "os"
      ]
    }, 
    "distutils.command.bdist_rpm": {
      "file": "distutils/command/bdist_rpm.py", 
      "imports": [
        "distutils.core", 
        "distutils.debug", 
        "distutils.errors", 
        "distutils.file_util", 
        "distutils.log", 
        "distutils.sysconfig", 
        "sys", 
        "os", 
        "string"
      ]
    }, 
    "distutils.command.bdist_wininst": {
      "file": "distutils/command/bdist_wininst.py", 
      "imports": [
        "distutils", 
        "distutils.core", 
        "distutils.dir_util", 
        "distutils.errors", 
        "distutils.log", 
        "distutils.msvccompiler", 
        "distutils.sysconfig", 
        "distutils.util", 
        "sys", 
        "time", 
        "os", 
        "string", 
        "struct", 
        "tempfile"
      ]
    }, 
    "distutils.command.build": {
      "file": "distutils/command/build.py", 
      "imports": [
        "distutils.ccompiler", 
        "distutils.core", 
        "distutils.errors", 
        "distutils.util", 
        "sys", 
        "os"
      ]
    }, 
    "distutils.command.build_clib": {
      "file": "distutils/command/build_clib.py", 
      "imports": [
        "distutils.ccompiler", 
        "distutils.core", 
        "distutils.errors", 
        "distutils.log", 
        "distutils.sysconfig", 
        "os"
      ]
    }, 
    "distutils.command.build_ext": {
      "file": "distutils/command/build_ext.py", 
      "imports": [
        "distutils.ccompiler", 
        "distutils.core", 
        "distutils.dep_util", 
        "distutils.errors", 
        "distutils.extension", 
        "distutils.log", 
        "distutils.msvccompiler", 
        "distutils.sysconfig", 
        "distutils.util", 
        "imp", 
        "sys", 
        "os", 
        "re", 
        "site", 
        "string", 
        "types"
      ]
    }, 
    "distutils.command.build_py": {
      "file": "distutils/command/build_py.py", 
      "imports": [
        "distutils.core", 
        "distutils.errors", 
        "distutils.log", 
        "distutils.util", 
        "sys", 
        "glob", 
        "os"
      ]
    }, 
    "distutils.command.build_scripts": {
      "file": "distutils/command/build_scripts.py", 
      "imports": [
        "distutils.core", 
        "distutils.dep_util", 
        "distutils.log", 
        "distutils.util", 
        "os", 
        "re", 
        "stat"
      ]
    }, 
    "distutils.command.check": {
      "file": "distutils/command/check.py", 
      "imports": [
        "distutils.core", 
        "distutils.dist", 
        "distutils.errors", 
        "docutils.frontend", 
        "docutils.nodes", 
        "docutils.parsers.rst.Parser", 
        "docutils.utils.Reporter", 
        "StringIO"
      ]
    }, 
    "distutils.command.clean": {
      "file": "distutils/command/clean.py", 
      "imports": [
        "distutils.core", 
        "distutils.dir_util", 
        "distutils.log", 
        "os"
      ]
    }, 
    "distutils.command.config": {
      "file": "distutils/command/config.py", 
      "imports": [
        "distutils.ccompiler", 
        "distutils.core", 
        "distutils.errors", 
        "distutils.log", 
        "distutils.sysconfig", 
        "os", 
        "re"
      ]
    }, 
    "distutils.command.install": {
      "file": "distutils/command/install.py", 
      "imports": [
        "distutils.core", 
        "distutils.debug", 
        "distutils.errors", 
        "distutils.fancy_getopt", 
        "distutils.file_util", 
        "distutils.log", 
        "distutils.sysconfig", 
        "distutils.util", 
        "sys", 
        "os", 
        "pprint", 
        "site", 
        "string", 
        "types"
      ]
    }, 
    "distutils.command.install_data": {
      "file": "distutils/command/install_data.py", 
      "imports": [
        "distutils.core", 
        "distutils.util", 
        "os"
      ]
    }, 
    "distutils.command.install_egg_info": {
      "file": "distutils/command/install_egg_info.py", 
      "imports": [
        "distutils.cmd", 
        "distutils.dir_util", 
        "distutils.log", 
        "sys", 
        "os", 
        "re"
      ]
    }, 
    "distutils.command.install_headers": {
      "file": "distutils/command/install_headers.py", 
      "imports": [
        "distutils.core"
      ]
    }, 
    "distutils.command.install_lib": {
      "file": "distutils/command/install_lib.py", 
      "imports": [
        "distutils.core", 
        "distutils.errors", 
        "distutils.util", 
        "sys", 
        "os"
      ]
    }, 
    "distutils.command.install_scripts": {
      "file": "distutils/command/install_scripts.py", 
      "imports": [
        "distutils.core", 
        "distutils.log", 
        "os", 
        "stat"
      ]
    }, 
    "distutils.command.register": {
      "file": "distutils/command/register.py", 
      "imports": [
        "distutils.core", 
        "distutils.log", 
        "getpass", 
        "urllib2", 
        "urlparse", 
        "warnings"
      ]
    }, 
    "distutils.command.sdist": {
      "file": "distutils/command/sdist.py", 
      "imports": [
        "distutils.archive_util", 
        "distutils.core", 
        "distutils.dep_util", 
        "distutils.dir_util", 
        "distutils.errors", 
        "distutils.fancy_getopt", 
        "distutils.file_util", 
        "distutils.filelist", 
        "distutils.log", 
        "distutils.text_file", 
        "distutils.util", 
        "sys", 
        "glob", 
        "os", 
        "string", 
        "warnings"
      ]
    }, 
    "distutils.command.upload": {
      "file": "distutils/command/upload.py", 
      "imports": [
        "base64", 
        "cStringIO", 
        "distutils.core", 
        "distutils.errors", 
        "distutils.log", 
        "distutils.spawn", 
        "hashlib", 
        "os", 
        "platform", 
        "socket", 
        "urllib2", 
        "urlparse"
      ]
    }, 
    "distutils.config": {
      "file": "distutils/config.py", 
      "imports": [
        "ConfigParser", 
        "distutils.cmd", 
        "os"
      ]
    }, 
    "distutils.core": {
      "file": "distutils/core.py", 
      "imports": [
        "distutils.cmd", 
        "distutils.config", 
        "distutils.debug", 
        "distutils.dist", 
        "distutils.errors", 
        "distutils.extension", 
        "sys", 
        "os"
      ]
    }, 
    "distutils.cygwinccompiler": {
      "file": "distutils/cygwinccompiler.py", 
      "imports": [
        "copy", 
        "distutils.ccompiler", 
        "distutils.errors", 
        "distutils.file_util", 
        "distutils.log", 
        "distutils.spawn", 
        "distutils.sysconfig", 
        "distutils.unixccompiler", 
        "distutils.version", 
        "sys", 
        "os", 
        "re", 
        "string"
      ]
    }, 
    "distutils.debug": {
      "file": "distutils/debug.py", 
      "imports": [
        "os"
      ]
    }, 
    "distutils.dep_util": {
      "file": "distutils/dep_util.py", 
      "imports": [
        "distutils.errors", 
        "os", 
        "stat"
      ]
    }, 
    "distutils.dir_util": {
      "file": "distutils/dir_util.py", 
      "imports": [
        "distutils.errors", 
        "distutils.file_util", 
        "distutils.log", 
        "errno", 
        "os"
      ]
    }, 
    "distutils.dist": {
      "file": "distutils/dist.py", 
      "imports": [
        "ConfigParser", 
        "distutils.cmd", 
        "distutils.command", 
        "distutils.core", 
        "distutils.debug", 
        "distutils.errors", 
        "distutils.fancy_getopt", 
        "distutils.log", 
        "distutils.util", 
        "distutils.versionpredicate", 
        "sys", 
        "email", 
        "os", 
        "pprint", 
        "re", 
        "warnings"
      ]
    }, 
    "distutils.emxccompiler": {
      "file": "distutils/emxccompiler.py", 
      "imports": [
        "copy", 
        "distutils.ccompiler", 
        "distutils.errors", 
        "distutils.file_util", 
        "distutils.log", 
        "distutils.spawn", 
        "distutils.sysconfig", 
        "distutils.unixccompiler", 
        "distutils.version", 
        "sys", 
        "os", 
        "re", 
        "string"
      ]
    }, 
    "distutils.errors": {
      "file": "distutils/errors.py", 
      "imports": []
    }, 
    "distutils.extension": {
      "file": "distutils/extension.py", 
      "imports": [
        "distutils.sysconfig", 
        "distutils.text_file", 
        "distutils.util", 
        "sys", 
        "os", 
        "string", 
        "types", 
        "warnings"
      ]
    }, 
    "distutils.fancy_getopt": {
      "file": "distutils/fancy_getopt.py", 
      "imports": [
        "distutils.errors", 
        "sys", 
        "getopt", 
        "re", 
        "string"
      ]
    }, 
    "distutils.file_util": {
      "file": "distutils/file_util.py", 
      "imports": [
        "distutils.dep_util", 
        "distutils.errors", 
        "distutils.log", 
        "errno", 
        "os", 
        "stat"
      ]
    }, 
    "distutils.filelist": {
      "file": "distutils/filelist.py", 
      "imports": [
        "distutils.debug", 
        "distutils.errors", 
        "distutils.log", 
        "distutils.util", 
        "fnmatch", 
        "os", 
        "re", 
        "stat"
      ]
    }, 
    "distutils.log": {
      "file": "distutils/log.py", 
      "imports": [
        "sys"
      ]
    }, 
    "distutils.msvc9compiler": {
      "file": "distutils/msvc9compiler.py", 
      "imports": [
        "_winreg", 
        "distutils.ccompiler", 
        "distutils.errors", 
        "distutils.log", 
        "distutils.util", 
        "subprocess", 
        "sys", 
        "os", 
        "re"
      ]
    }, 
    "distutils.msvccompiler": {
      "file": "distutils/msvccompiler.py", 
      "imports": [
        "_winreg", 
        "distutils.ccompiler", 
        "distutils.errors", 
        "distutils.log", 
        "distutils.msvc9compiler", 
        "sys", 
        "win32api", 
        "win32con", 
        "os", 
        "string"
      ]
    }, 
    "distutils.spawn": {
      "file": "distutils/spawn.py", 
      "imports": [
        "distutils.debug", 
        "distutils.errors", 
        "distutils.log", 
        "distutils.sysconfig", 
        "errno", 
        "subprocess", 
        "sys", 
        "os"
      ]
    }, 
    "distutils.sysconfig": {
      "file": "distutils/sysconfig.py", 
      "imports": [
        "distutils.sysconfig_cpython", 
        "distutils.sysconfig_pypy", 
        "sys"
      ]
    }, 
    "distutils.sysconfig_cpython": {
      "file": "distutils/sysconfig_cpython.py", 
      "imports": [
        "_osx_support", 
        "_sysconfigdata.build_time_vars", 
        "distutils.errors", 
        "distutils.text_file", 
        "sys", 
        "os", 
        "re", 
        "string"
      ]
    }, 
    "distutils.sysconfig_pypy": {
      "file": "distutils/sysconfig_pypy.py", 
      "imports": [
        "distutils.errors", 
        "distutils.sysconfig_cpython", 
        "sys", 
        "os", 
        "shlex"
      ]
    }, 
    "distutils.tests": {
      "dir": "distutils/tests"
    }, 
    "distutils.tests.__init__": {
      "file": "distutils/tests/__init__.py", 
      "imports": [
        "sys", 
        "os", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "distutils.tests.setuptools_build_ext": {
      "file": "distutils/tests/setuptools_build_ext.py", 
      "imports": [
        "Pyrex.Distutils.build_ext.build_ext", 
        "distutils.ccompiler", 
        "distutils.command.build_ext", 
        "distutils.errors", 
        "distutils.file_util", 
        "distutils.log", 
        "distutils.sysconfig", 
        "distutils.tests.setuptools_extension", 
        "distutils.util", 
        "dl.RTLD_NOW", 
        "sys", 
        "os"
      ]
    }, 
    "distutils.tests.setuptools_extension": {
      "file": "distutils/tests/setuptools_extension.py", 
      "imports": [
        "Pyrex.Distutils.build_ext.build_ext", 
        "distutils.core", 
        "distutils.extension", 
        "sys"
      ]
    }, 
    "distutils.tests.support": {
      "file": "distutils/tests/support.py", 
      "imports": [
        "copy", 
        "distutils.core", 
        "distutils.log", 
        "distutils.sysconfig", 
        "sys", 
        "os", 
        "shutil", 
        "tempfile", 
        "unittest", 
        "warnings"
      ]
    }, 
    "distutils.tests.test_archive_util": {
      "file": "distutils/tests/test_archive_util.py", 
      "imports": [
        "distutils.archive_util", 
        "distutils.spawn", 
        "distutils.tests.support", 
        "sys", 
        "zlib", 
        "os", 
        "tarfile", 
        "test.test_support", 
        "unittest", 
        "warnings", 
        "zipfile", 
        "grp", 
        "pwd"
      ]
    }, 
    "distutils.tests.test_bdist": {
      "file": "distutils/tests/test_bdist.py", 
      "imports": [
        "distutils.command.bdist", 
        "distutils.tests.support", 
        "os", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "distutils.tests.test_bdist_dumb": {
      "file": "distutils/tests/test_bdist_dumb.py", 
      "imports": [
        "distutils.command.bdist_dumb", 
        "distutils.core", 
        "distutils.tests.support", 
        "sys", 
        "zlib", 
        "os", 
        "test.test_support", 
        "unittest", 
        "zipfile"
      ]
    }, 
    "distutils.tests.test_bdist_msi": {
      "file": "distutils/tests/test_bdist_msi.py", 
      "imports": [
        "distutils.command.bdist_msi", 
        "distutils.tests.support", 
        "sys", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "distutils.tests.test_bdist_rpm": {
      "file": "distutils/tests/test_bdist_rpm.py", 
      "imports": [
        "distutils.command.bdist_rpm", 
        "distutils.core", 
        "distutils.errors", 
        "distutils.spawn", 
        "distutils.tests.support", 
        "sys", 
        "os", 
        "shutil", 
        "tempfile", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "distutils.tests.test_bdist_wininst": {
      "file": "distutils/tests/test_bdist_wininst.py", 
      "imports": [
        "distutils.command.bdist_wininst", 
        "distutils.tests.support", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "distutils.tests.test_build": {
      "file": "distutils/tests/test_build.py", 
      "imports": [
        "distutils.command.build", 
        "distutils.sysconfig", 
        "distutils.tests.support", 
        "sys", 
        "os", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "distutils.tests.test_build_clib": {
      "file": "distutils/tests/test_build_clib.py", 
      "imports": [
        "distutils.ccompiler", 
        "distutils.command.build_clib", 
        "distutils.errors", 
        "distutils.spawn", 
        "distutils.sysconfig", 
        "distutils.tests.support", 
        "sys", 
        "os", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "distutils.tests.test_build_ext": {
      "file": "distutils/tests/test_build_ext.py", 
      "imports": [
        "distutils.command.build_ext", 
        "distutils.core", 
        "distutils.errors", 
        "distutils.extension", 
        "distutils.sysconfig", 
        "distutils.tests.setuptools_build_ext", 
        "distutils.tests.setuptools_extension", 
        "distutils.tests.support", 
        "sys", 
        "xx", 
        "os", 
        "site", 
        "StringIO", 
        "test.test_support", 
        "textwrap", 
        "unittest"
      ]
    }, 
    "distutils.tests.test_build_py": {
      "file": "distutils/tests/test_build_py.py", 
      "imports": [
        "distutils.command.build_py", 
        "distutils.core", 
        "distutils.errors", 
        "distutils.tests.support", 
        "sys", 
        "os", 
        "StringIO", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "distutils.tests.test_build_scripts": {
      "file": "distutils/tests/test_build_scripts.py", 
      "imports": [
        "distutils.command.build_scripts", 
        "distutils.core", 
        "distutils.sysconfig", 
        "distutils.tests.support", 
        "sys", 
        "os", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "distutils.tests.test_ccompiler": {
      "file": "distutils/tests/test_ccompiler.py", 
      "imports": [
        "distutils.ccompiler", 
        "distutils.debug", 
        "distutils.sysconfig", 
        "distutils.tests.support", 
        "os", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "distutils.tests.test_check": {
      "file": "distutils/tests/test_check.py", 
      "imports": [
        "distutils.command.check", 
        "distutils.errors", 
        "distutils.tests.support", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "distutils.tests.test_clean": {
      "file": "distutils/tests/test_clean.py", 
      "imports": [
        "distutils.command.clean", 
        "distutils.tests.support", 
        "sys", 
        "getpass", 
        "os", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "distutils.tests.test_cmd": {
      "file": "distutils/tests/test_cmd.py", 
      "imports": [
        "distutils.cmd", 
        "distutils.debug", 
        "distutils.dist", 
        "distutils.errors", 
        "os", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "distutils.tests.test_config": {
      "file": "distutils/tests/test_config.py", 
      "imports": [
        "distutils.core", 
        "distutils.log", 
        "distutils.tests.support", 
        "sys", 
        "os", 
        "shutil", 
        "tempfile", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "distutils.tests.test_config_cmd": {
      "file": "distutils/tests/test_config_cmd.py", 
      "imports": [
        "distutils.command.config", 
        "distutils.log", 
        "distutils.tests.support", 
        "sys", 
        "os", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "distutils.tests.test_core": {
      "file": "distutils/tests/test_core.py", 
      "imports": [
        "distutils.core", 
        "distutils.tests.support", 
        "sys", 
        "os", 
        "shutil", 
        "StringIO", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "distutils.tests.test_dep_util": {
      "file": "distutils/tests/test_dep_util.py", 
      "imports": [
        "distutils.dep_util", 
        "distutils.errors", 
        "distutils.tests.support", 
        "time", 
        "os", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "distutils.tests.test_dir_util": {
      "file": "distutils/tests/test_dir_util.py", 
      "imports": [
        "distutils.dir_util", 
        "distutils.log", 
        "distutils.tests.support", 
        "sys", 
        "os", 
        "shutil", 
        "stat", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "distutils.tests.test_dist": {
      "file": "distutils/tests/test_dist.py", 
      "imports": [
        "distutils.cmd", 
        "distutils.dist", 
        "distutils.tests.support", 
        "distutils.tests.test_dist", 
        "sys", 
        "os", 
        "StringIO", 
        "test.test_support", 
        "textwrap", 
        "unittest", 
        "warnings"
      ]
    }, 
    "distutils.tests.test_file_util": {
      "file": "distutils/tests/test_file_util.py", 
      "imports": [
        "distutils.file_util", 
        "distutils.log", 
        "distutils.tests.support", 
        "os", 
        "shutil", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "distutils.tests.test_filelist": {
      "file": "distutils/tests/test_filelist.py", 
      "imports": [
        "distutils.debug", 
        "distutils.errors", 
        "distutils.filelist", 
        "distutils.log", 
        "distutils.tests.support", 
        "os", 
        "re", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "distutils.tests.test_install": {
      "file": "distutils/tests/test_install.py", 
      "imports": [
        "distutils.command.build_ext", 
        "distutils.command.install", 
        "distutils.core", 
        "distutils.errors", 
        "distutils.extension", 
        "distutils.sysconfig", 
        "distutils.tests.support", 
        "sys", 
        "os", 
        "site", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "distutils.tests.test_install_data": {
      "file": "distutils/tests/test_install_data.py", 
      "imports": [
        "distutils.command.install_data", 
        "distutils.tests.support", 
        "sys", 
        "getpass", 
        "os", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "distutils.tests.test_install_headers": {
      "file": "distutils/tests/test_install_headers.py", 
      "imports": [
        "distutils.command.install_headers", 
        "distutils.tests.support", 
        "sys", 
        "getpass", 
        "os", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "distutils.tests.test_install_lib": {
      "file": "distutils/tests/test_install_lib.py", 
      "imports": [
        "distutils.command.install_lib", 
        "distutils.errors", 
        "distutils.extension", 
        "distutils.tests.support", 
        "sys", 
        "os", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "distutils.tests.test_install_scripts": {
      "file": "distutils/tests/test_install_scripts.py", 
      "imports": [
        "distutils.command.install_scripts", 
        "distutils.core", 
        "distutils.tests.support", 
        "os", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "distutils.tests.test_msvc9compiler": {
      "file": "distutils/tests/test_msvc9compiler.py", 
      "imports": [
        "_winreg", 
        "distutils.errors", 
        "distutils.msvc9compiler", 
        "distutils.msvccompiler", 
        "distutils.tests.support", 
        "sys", 
        "os", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "distutils.tests.test_register": {
      "file": "distutils/tests/test_register.py", 
      "imports": [
        "distutils.command.register", 
        "distutils.errors", 
        "distutils.tests.test_config", 
        "docutils", 
        "getpass", 
        "os", 
        "test.test_support", 
        "unittest", 
        "urllib2", 
        "warnings"
      ]
    }, 
    "distutils.tests.test_sdist": {
      "file": "distutils/tests/test_sdist.py", 
      "imports": [
        "distutils.archive_util", 
        "distutils.command.sdist", 
        "distutils.core", 
        "distutils.errors", 
        "distutils.filelist", 
        "distutils.log", 
        "distutils.spawn", 
        "distutils.tests.test_config", 
        "zlib", 
        "os", 
        "tarfile", 
        "test.test_support", 
        "textwrap", 
        "unittest", 
        "warnings", 
        "zipfile", 
        "grp", 
        "pwd"
      ]
    }, 
    "distutils.tests.test_spawn": {
      "file": "distutils/tests/test_spawn.py", 
      "imports": [
        "distutils.errors", 
        "distutils.spawn", 
        "distutils.tests.support", 
        "time", 
        "os", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "distutils.tests.test_sysconfig": {
      "file": "distutils/tests/test_sysconfig.py", 
      "imports": [
        "distutils.sysconfig", 
        "distutils.tests.support", 
        "subprocess", 
        "sys", 
        "os", 
        "shutil", 
        "test.test_support", 
        "test", 
        "textwrap", 
        "unittest"
      ]
    }, 
    "distutils.tests.test_text_file": {
      "file": "distutils/tests/test_text_file.py", 
      "imports": [
        "distutils.tests.support", 
        "distutils.text_file", 
        "os", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "distutils.tests.test_unixccompiler": {
      "file": "distutils/tests/test_unixccompiler.py", 
      "imports": [
        "distutils.sysconfig", 
        "distutils.unixccompiler", 
        "sys", 
        "os", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "distutils.tests.test_upload": {
      "file": "distutils/tests/test_upload.py", 
      "imports": [
        "distutils.command.upload", 
        "distutils.core", 
        "distutils.errors", 
        "distutils.tests.test_config", 
        "os", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "distutils.tests.test_util": {
      "file": "distutils/tests/test_util.py", 
      "imports": [
        "distutils.errors", 
        "distutils.util", 
        "sys", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "distutils.tests.test_version": {
      "file": "distutils/tests/test_version.py", 
      "imports": [
        "distutils.version", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "distutils.tests.test_versionpredicate": {
      "file": "distutils/tests/test_versionpredicate.py", 
      "imports": [
        "distutils.versionpredicate", 
        "doctest", 
        "test.test_support"
      ]
    }, 
    "distutils.text_file": {
      "file": "distutils/text_file.py", 
      "imports": [
        "sys"
      ]
    }, 
    "distutils.unixccompiler": {
      "file": "distutils/unixccompiler.py", 
      "imports": [
        "_osx_support", 
        "distutils.ccompiler", 
        "distutils.dep_util", 
        "distutils.errors", 
        "distutils.log", 
        "distutils.sysconfig", 
        "sys", 
        "os", 
        "platform", 
        "re", 
        "types"
      ]
    }, 
    "distutils.util": {
      "file": "distutils/util.py", 
      "imports": [
        "_osx_support", 
        "distutils.dep_util", 
        "distutils.errors", 
        "distutils.log", 
        "distutils.spawn", 
        "distutils.sysconfig", 
        "sys", 
        "os", 
        "py_compile", 
        "re", 
        "string", 
        "tempfile", 
        "pwd"
      ]
    }, 
    "distutils.version": {
      "file": "distutils/version.py", 
      "imports": [
        "re", 
        "string", 
        "types"
      ]
    }, 
    "distutils.versionpredicate": {
      "file": "distutils/versionpredicate.py", 
      "imports": [
        "distutils.version", 
        "operator", 
        "re"
      ]
    }, 
    "doctest": {
      "file": "doctest.py", 
      "imports": [
        "__future__", 
        "collections", 
        "difflib", 
        "sys", 
        "inspect", 
        "linecache", 
        "os", 
        "pdb", 
        "re", 
        "StringIO", 
        "tempfile", 
        "traceback", 
        "types", 
        "unittest", 
        "warnings"
      ]
    }, 
    "dumbdbm": {
      "file": "dumbdbm.py", 
      "imports": [
        "__builtin__", 
        "os", 
        "UserDict"
      ]
    }, 
    "dummy_thread": {
      "file": "dummy_thread.py", 
      "imports": [
        "traceback"
      ]
    }, 
    "dummy_threading": {
      "file": "dummy_threading.py", 
      "imports": [
        "_dummy_threading.*", 
        "_dummy_threading.__all__", 
        "dummy_thread", 
        "sys.modules", 
        "threading"
      ]
    }, 
    "email": {
      "dir": "email"
    }, 
    "email.__init__": {
      "file": "email/__init__.py", 
      "imports": [
        "email.mime", 
        "email.parser", 
        "sys"
      ]
    }, 
    "email._parseaddr": {
      "file": "email/_parseaddr.py", 
      "imports": [
        "calendar", 
        "time"
      ]
    }, 
    "email.base64mime": {
      "file": "email/base64mime.py", 
      "imports": [
        "binascii.a2b_base64", 
        "binascii.b2a_base64", 
        "email.utils"
      ]
    }, 
    "email.charset": {
      "file": "email/charset.py", 
      "imports": [
        "codecs", 
        "email.base64mime", 
        "email.encoders", 
        "email.errors", 
        "email.quoprimime"
      ]
    }, 
    "email.encoders": {
      "file": "email/encoders.py", 
      "imports": [
        "base64", 
        "quopri"
      ]
    }, 
    "email.errors": {
      "file": "email/errors.py", 
      "imports": []
    }, 
    "email.feedparser": {
      "file": "email/feedparser.py", 
      "imports": [
        "email.errors", 
        "email.message", 
        "re"
      ]
    }, 
    "email.generator": {
      "file": "email/generator.py", 
      "imports": [
        "cStringIO.StringIO", 
        "email.header", 
        "sys", 
        "time", 
        "random", 
        "re", 
        "warnings"
      ]
    }, 
    "email.header": {
      "file": "email/header.py", 
      "imports": [
        "binascii", 
        "email.base64mime", 
        "email.charset", 
        "email.errors", 
        "email.quoprimime", 
        "re"
      ]
    }, 
    "email.iterators": {
      "file": "email/iterators.py", 
      "imports": [
        "cStringIO.StringIO", 
        "sys"
      ]
    }, 
    "email.message": {
      "file": "email/message.py", 
      "imports": [
        "binascii", 
        "cStringIO.StringIO", 
        "email.charset", 
        "email.errors", 
        "email.generator", 
        "email.iterators", 
        "email.utils", 
        "re", 
        "uu", 
        "warnings"
      ]
    }, 
    "email.mime": {
      "dir": "email/mime"
    }, 
    "email.mime.__init__": {
      "file": "email/mime/__init__.py", 
      "imports": []
    }, 
    "email.mime.application": {
      "file": "email/mime/application.py", 
      "imports": [
        "email.encoders", 
        "email.mime.nonmultipart"
      ]
    }, 
    "email.mime.audio": {
      "file": "email/mime/audio.py", 
      "imports": [
        "cStringIO.StringIO", 
        "email.encoders", 
        "email.mime.nonmultipart", 
        "sndhdr"
      ]
    }, 
    "email.mime.base": {
      "file": "email/mime/base.py", 
      "imports": [
        "email.message"
      ]
    }, 
    "email.mime.image": {
      "file": "email/mime/image.py", 
      "imports": [
        "email.encoders", 
        "email.mime.nonmultipart", 
        "imghdr"
      ]
    }, 
    "email.mime.message": {
      "file": "email/mime/message.py", 
      "imports": [
        "email.message", 
        "email.mime.nonmultipart"
      ]
    }, 
    "email.mime.multipart": {
      "file": "email/mime/multipart.py", 
      "imports": [
        "email.mime.base"
      ]
    }, 
    "email.mime.nonmultipart": {
      "file": "email/mime/nonmultipart.py", 
      "imports": [
        "email.errors", 
        "email.mime.base"
      ]
    }, 
    "email.mime.text": {
      "file": "email/mime/text.py", 
      "imports": [
        "email.encoders", 
        "email.mime.nonmultipart"
      ]
    }, 
    "email.parser": {
      "file": "email/parser.py", 
      "imports": [
        "cStringIO.StringIO", 
        "email.feedparser", 
        "email.message", 
        "warnings"
      ]
    }, 
    "email.quoprimime": {
      "file": "email/quoprimime.py", 
      "imports": [
        "email.utils", 
        "re", 
        "string"
      ]
    }, 
    "email.test": {
      "dir": "email/test"
    }, 
    "email.test.__init__": {
      "file": "email/test/__init__.py", 
      "imports": []
    }, 
    "email.test.test_email": {
      "file": "email/test/test_email.py", 
      "imports": [
        "base64", 
        "cStringIO.StringIO", 
        "difflib", 
        "email", 
        "email.feedparser", 
        "email.test", 
        "sys", 
        "time", 
        "os", 
        "random", 
        "re", 
        "test.test_support", 
        "textwrap", 
        "unittest", 
        "warnings"
      ]
    }, 
    "email.test.test_email_codecs": {
      "file": "email/test/test_email_codecs.py", 
      "imports": [
        "email.charset", 
        "email.header", 
        "email.message", 
        "email.test.test_email", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "email.test.test_email_codecs_renamed": {
      "file": "email/test/test_email_codecs_renamed.py", 
      "imports": [
        "email.charset", 
        "email.header", 
        "email.message", 
        "email.test.test_email", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "email.test.test_email_renamed": {
      "file": "email/test/test_email_renamed.py", 
      "imports": [
        "base64", 
        "cStringIO.StringIO", 
        "difflib", 
        "email", 
        "email.base64mime", 
        "email.charset", 
        "email.encoders", 
        "email.errors", 
        "email.generator", 
        "email.header", 
        "email.iterators", 
        "email.message", 
        "email.mime.application", 
        "email.mime.audio", 
        "email.mime.base", 
        "email.mime.image", 
        "email.mime.message", 
        "email.mime.multipart", 
        "email.mime.text", 
        "email.parser", 
        "email.quoprimime", 
        "email.test", 
        "email.utils", 
        "sys", 
        "time", 
        "os", 
        "test.test_support", 
        "unittest", 
        "warnings"
      ]
    }, 
    "email.test.test_email_torture": {
      "file": "email/test/test_email_torture.py", 
      "imports": [
        "cStringIO.StringIO", 
        "email", 
        "email.iterators", 
        "email.test.test_email", 
        "sys", 
        "os", 
        "test.test_support", 
        "types", 
        "unittest"
      ]
    }, 
    "email.utils": {
      "file": "email/utils.py", 
      "imports": [
        "base64", 
        "email._parseaddr", 
        "email.encoders", 
        "time", 
        "os", 
        "quopri", 
        "random", 
        "re", 
        "socket", 
        "urllib", 
        "warnings"
      ]
    }, 
    "encodings": {
      "dir": "encodings"
    }, 
    "encodings.__init__": {
      "file": "encodings/__init__.py", 
      "imports": [
        "__builtin__", 
        "codecs", 
        "encodings.aliases"
      ]
    }, 
    "encodings.aliases": {
      "file": "encodings/aliases.py", 
      "imports": []
    }, 
    "encodings.ascii": {
      "file": "encodings/ascii.py", 
      "imports": [
        "codecs"
      ]
    }, 
    "encodings.base64_codec": {
      "file": "encodings/base64_codec.py", 
      "imports": [
        "base64", 
        "codecs"
      ]
    }, 
    "encodings.big5": {
      "file": "encodings/big5.py", 
      "imports": [
        "_multibytecodec", 
        "codecs", 
        "_codecs_tw"
      ]
    }, 
    "encodings.big5hkscs": {
      "file": "encodings/big5hkscs.py", 
      "imports": [
        "_multibytecodec", 
        "codecs", 
        "_codecs_hk"
      ]
    }, 
    "encodings.bz2_codec": {
      "file": "encodings/bz2_codec.py", 
      "imports": [
        "bz2", 
        "codecs"
      ]
    }, 
    "encodings.charmap": {
      "file": "encodings/charmap.py", 
      "imports": [
        "codecs"
      ]
    }, 
    "encodings.cp037": {
      "file": "encodings/cp037.py", 
      "imports": [
        "codecs"
      ]
    }, 
    "encodings.cp1006": {
      "file": "encodings/cp1006.py", 
      "imports": [
        "codecs"
      ]
    }, 
    "encodings.cp1026": {
      "file": "encodings/cp1026.py", 
      "imports": [
        "codecs"
      ]
    }, 
    "encodings.cp1140": {
      "file": "encodings/cp1140.py", 
      "imports": [
        "codecs"
      ]
    }, 
    "encodings.cp1250": {
      "file": "encodings/cp1250.py", 
      "imports": [
        "codecs"
      ]
    }, 
    "encodings.cp1251": {
      "file": "encodings/cp1251.py", 
      "imports": [
        "codecs"
      ]
    }, 
    "encodings.cp1252": {
      "file": "encodings/cp1252.py", 
      "imports": [
        "codecs"
      ]
    }, 
    "encodings.cp1253": {
      "file": "encodings/cp1253.py", 
      "imports": [
        "codecs"
      ]
    }, 
    "encodings.cp1254": {
      "file": "encodings/cp1254.py", 
      "imports": [
        "codecs"
      ]
    }, 
    "encodings.cp1255": {
      "file": "encodings/cp1255.py", 
      "imports": [
        "codecs"
      ]
    }, 
    "encodings.cp1256": {
      "file": "encodings/cp1256.py", 
      "imports": [
        "codecs"
      ]
    }, 
    "encodings.cp1257": {
      "file": "encodings/cp1257.py", 
      "imports": [
        "codecs"
      ]
    }, 
    "encodings.cp1258": {
      "file": "encodings/cp1258.py", 
      "imports": [
        "codecs"
      ]
    }, 
    "encodings.cp424": {
      "file": "encodings/cp424.py", 
      "imports": [
        "codecs"
      ]
    }, 
    "encodings.cp437": {
      "file": "encodings/cp437.py", 
      "imports": [
        "codecs"
      ]
    }, 
    "encodings.cp500": {
      "file": "encodings/cp500.py", 
      "imports": [
        "codecs"
      ]
    }, 
    "encodings.cp720": {
      "file": "encodings/cp720.py", 
      "imports": [
        "codecs"
      ]
    }, 
    "encodings.cp737": {
      "file": "encodings/cp737.py", 
      "imports": [
        "codecs"
      ]
    }, 
    "encodings.cp775": {
      "file": "encodings/cp775.py", 
      "imports": [
        "codecs"
      ]
    }, 
    "encodings.cp850": {
      "file": "encodings/cp850.py", 
      "imports": [
        "codecs"
      ]
    }, 
    "encodings.cp852": {
      "file": "encodings/cp852.py", 
      "imports": [
        "codecs"
      ]
    }, 
    "encodings.cp855": {
      "file": "encodings/cp855.py", 
      "imports": [
        "codecs"
      ]
    }, 
    "encodings.cp856": {
      "file": "encodings/cp856.py", 
      "imports": [
        "codecs"
      ]
    }, 
    "encodings.cp857": {
      "file": "encodings/cp857.py", 
      "imports": [
        "codecs"
      ]
    }, 
    "encodings.cp858": {
      "file": "encodings/cp858.py", 
      "imports": [
        "codecs"
      ]
    }, 
    "encodings.cp860": {
      "file": "encodings/cp860.py", 
      "imports": [
        "codecs"
      ]
    }, 
    "encodings.cp861": {
      "file": "encodings/cp861.py", 
      "imports": [
        "codecs"
      ]
    }, 
    "encodings.cp862": {
      "file": "encodings/cp862.py", 
      "imports": [
        "codecs"
      ]
    }, 
    "encodings.cp863": {
      "file": "encodings/cp863.py", 
      "imports": [
        "codecs"
      ]
    }, 
    "encodings.cp864": {
      "file": "encodings/cp864.py", 
      "imports": [
        "codecs"
      ]
    }, 
    "encodings.cp865": {
      "file": "encodings/cp865.py", 
      "imports": [
        "codecs"
      ]
    }, 
    "encodings.cp866": {
      "file": "encodings/cp866.py", 
      "imports": [
        "codecs"
      ]
    }, 
    "encodings.cp869": {
      "file": "encodings/cp869.py", 
      "imports": [
        "codecs"
      ]
    }, 
    "encodings.cp874": {
      "file": "encodings/cp874.py", 
      "imports": [
        "codecs"
      ]
    }, 
    "encodings.cp875": {
      "file": "encodings/cp875.py", 
      "imports": [
        "codecs"
      ]
    }, 
    "encodings.cp932": {
      "file": "encodings/cp932.py", 
      "imports": [
        "_multibytecodec", 
        "codecs", 
        "_codecs_jp"
      ]
    }, 
    "encodings.cp949": {
      "file": "encodings/cp949.py", 
      "imports": [
        "_multibytecodec", 
        "codecs", 
        "_codecs_kr"
      ]
    }, 
    "encodings.cp950": {
      "file": "encodings/cp950.py", 
      "imports": [
        "_multibytecodec", 
        "codecs", 
        "_codecs_tw"
      ]
    }, 
    "encodings.euc_jis_2004": {
      "file": "encodings/euc_jis_2004.py", 
      "imports": [
        "_multibytecodec", 
        "codecs", 
        "_codecs_jp"
      ]
    }, 
    "encodings.euc_jisx0213": {
      "file": "encodings/euc_jisx0213.py", 
      "imports": [
        "_multibytecodec", 
        "codecs", 
        "_codecs_jp"
      ]
    }, 
    "encodings.euc_jp": {
      "file": "encodings/euc_jp.py", 
      "imports": [
        "_multibytecodec", 
        "codecs", 
        "_codecs_jp"
      ]
    }, 
    "encodings.euc_kr": {
      "file": "encodings/euc_kr.py", 
      "imports": [
        "_multibytecodec", 
        "codecs", 
        "_codecs_kr"
      ]
    }, 
    "encodings.gb18030": {
      "file": "encodings/gb18030.py", 
      "imports": [
        "_multibytecodec", 
        "codecs", 
        "_codecs_cn"
      ]
    }, 
    "encodings.gb2312": {
      "file": "encodings/gb2312.py", 
      "imports": [
        "_multibytecodec", 
        "codecs", 
        "_codecs_cn"
      ]
    }, 
    "encodings.gbk": {
      "file": "encodings/gbk.py", 
      "imports": [
        "_multibytecodec", 
        "codecs", 
        "_codecs_cn"
      ]
    }, 
    "encodings.hex_codec": {
      "file": "encodings/hex_codec.py", 
      "imports": [
        "binascii", 
        "codecs"
      ]
    }, 
    "encodings.hp_roman8": {
      "file": "encodings/hp_roman8.py", 
      "imports": [
        "codecs"
      ]
    }, 
    "encodings.hz": {
      "file": "encodings/hz.py", 
      "imports": [
        "_multibytecodec", 
        "codecs", 
        "_codecs_cn"
      ]
    }, 
    "encodings.idna": {
      "file": "encodings/idna.py", 
      "imports": [
        "codecs", 
        "unicodedata.ucd_3_2_0", 
        "re", 
        "stringprep"
      ]
    }, 
    "encodings.iso2022_jp": {
      "file": "encodings/iso2022_jp.py", 
      "imports": [
        "_multibytecodec", 
        "codecs", 
        "_codecs_iso2022"
      ]
    }, 
    "encodings.iso2022_jp_1": {
      "file": "encodings/iso2022_jp_1.py", 
      "imports": [
        "_multibytecodec", 
        "codecs", 
        "_codecs_iso2022"
      ]
    }, 
    "encodings.iso2022_jp_2": {
      "file": "encodings/iso2022_jp_2.py", 
      "imports": [
        "_multibytecodec", 
        "codecs", 
        "_codecs_iso2022"
      ]
    }, 
    "encodings.iso2022_jp_2004": {
      "file": "encodings/iso2022_jp_2004.py", 
      "imports": [
        "_multibytecodec", 
        "codecs", 
        "_codecs_iso2022"
      ]
    }, 
    "encodings.iso2022_jp_3": {
      "file": "encodings/iso2022_jp_3.py", 
      "imports": [
        "_multibytecodec", 
        "codecs", 
        "_codecs_iso2022"
      ]
    }, 
    "encodings.iso2022_jp_ext": {
      "file": "encodings/iso2022_jp_ext.py", 
      "imports": [
        "_multibytecodec", 
        "codecs", 
        "_codecs_iso2022"
      ]
    }, 
    "encodings.iso2022_kr": {
      "file": "encodings/iso2022_kr.py", 
      "imports": [
        "_multibytecodec", 
        "codecs", 
        "_codecs_iso2022"
      ]
    }, 
    "encodings.iso8859_1": {
      "file": "encodings/iso8859_1.py", 
      "imports": [
        "codecs"
      ]
    }, 
    "encodings.iso8859_10": {
      "file": "encodings/iso8859_10.py", 
      "imports": [
        "codecs"
      ]
    }, 
    "encodings.iso8859_11": {
      "file": "encodings/iso8859_11.py", 
      "imports": [
        "codecs"
      ]
    }, 
    "encodings.iso8859_13": {
      "file": "encodings/iso8859_13.py", 
      "imports": [
        "codecs"
      ]
    }, 
    "encodings.iso8859_14": {
      "file": "encodings/iso8859_14.py", 
      "imports": [
        "codecs"
      ]
    }, 
    "encodings.iso8859_15": {
      "file": "encodings/iso8859_15.py", 
      "imports": [
        "codecs"
      ]
    }, 
    "encodings.iso8859_16": {
      "file": "encodings/iso8859_16.py", 
      "imports": [
        "codecs"
      ]
    }, 
    "encodings.iso8859_2": {
      "file": "encodings/iso8859_2.py", 
      "imports": [
        "codecs"
      ]
    }, 
    "encodings.iso8859_3": {
      "file": "encodings/iso8859_3.py", 
      "imports": [
        "codecs"
      ]
    }, 
    "encodings.iso8859_4": {
      "file": "encodings/iso8859_4.py", 
      "imports": [
        "codecs"
      ]
    }, 
    "encodings.iso8859_5": {
      "file": "encodings/iso8859_5.py", 
      "imports": [
        "codecs"
      ]
    }, 
    "encodings.iso8859_6": {
      "file": "encodings/iso8859_6.py", 
      "imports": [
        "codecs"
      ]
    }, 
    "encodings.iso8859_7": {
      "file": "encodings/iso8859_7.py", 
      "imports": [
        "codecs"
      ]
    }, 
    "encodings.iso8859_8": {
      "file": "encodings/iso8859_8.py", 
      "imports": [
        "codecs"
      ]
    }, 
    "encodings.iso8859_9": {
      "file": "encodings/iso8859_9.py", 
      "imports": [
        "codecs"
      ]
    }, 
    "encodings.johab": {
      "file": "encodings/johab.py", 
      "imports": [
        "_multibytecodec", 
        "codecs", 
        "_codecs_kr"
      ]
    }, 
    "encodings.koi8_r": {
      "file": "encodings/koi8_r.py", 
      "imports": [
        "codecs"
      ]
    }, 
    "encodings.koi8_u": {
      "file": "encodings/koi8_u.py", 
      "imports": [
        "codecs"
      ]
    }, 
    "encodings.latin_1": {
      "file": "encodings/latin_1.py", 
      "imports": [
        "codecs"
      ]
    }, 
    "encodings.mac_arabic": {
      "file": "encodings/mac_arabic.py", 
      "imports": [
        "codecs"
      ]
    }, 
    "encodings.mac_centeuro": {
      "file": "encodings/mac_centeuro.py", 
      "imports": [
        "codecs"
      ]
    }, 
    "encodings.mac_croatian": {
      "file": "encodings/mac_croatian.py", 
      "imports": [
        "codecs"
      ]
    }, 
    "encodings.mac_cyrillic": {
      "file": "encodings/mac_cyrillic.py", 
      "imports": [
        "codecs"
      ]
    }, 
    "encodings.mac_farsi": {
      "file": "encodings/mac_farsi.py", 
      "imports": [
        "codecs"
      ]
    }, 
    "encodings.mac_greek": {
      "file": "encodings/mac_greek.py", 
      "imports": [
        "codecs"
      ]
    }, 
    "encodings.mac_iceland": {
      "file": "encodings/mac_iceland.py", 
      "imports": [
        "codecs"
      ]
    }, 
    "encodings.mac_latin2": {
      "file": "encodings/mac_latin2.py", 
      "imports": [
        "codecs"
      ]
    }, 
    "encodings.mac_roman": {
      "file": "encodings/mac_roman.py", 
      "imports": [
        "codecs"
      ]
    }, 
    "encodings.mac_romanian": {
      "file": "encodings/mac_romanian.py", 
      "imports": [
        "codecs"
      ]
    }, 
    "encodings.mac_turkish": {
      "file": "encodings/mac_turkish.py", 
      "imports": [
        "codecs"
      ]
    }, 
    "encodings.mbcs": {
      "file": "encodings/mbcs.py", 
      "imports": [
        "codecs"
      ]
    }, 
    "encodings.palmos": {
      "file": "encodings/palmos.py", 
      "imports": [
        "codecs"
      ]
    }, 
    "encodings.ptcp154": {
      "file": "encodings/ptcp154.py", 
      "imports": [
        "codecs"
      ]
    }, 
    "encodings.punycode": {
      "file": "encodings/punycode.py", 
      "imports": [
        "codecs"
      ]
    }, 
    "encodings.quopri_codec": {
      "file": "encodings/quopri_codec.py", 
      "imports": [
        "cStringIO.StringIO", 
        "codecs", 
        "quopri", 
        "StringIO"
      ]
    }, 
    "encodings.raw_unicode_escape": {
      "file": "encodings/raw_unicode_escape.py", 
      "imports": [
        "codecs"
      ]
    }, 
    "encodings.rot_13": {
      "file": "encodings/rot_13.py", 
      "imports": [
        "codecs", 
        "sys"
      ]
    }, 
    "encodings.shift_jis": {
      "file": "encodings/shift_jis.py", 
      "imports": [
        "_multibytecodec", 
        "codecs", 
        "_codecs_jp"
      ]
    }, 
    "encodings.shift_jis_2004": {
      "file": "encodings/shift_jis_2004.py", 
      "imports": [
        "_multibytecodec", 
        "codecs", 
        "_codecs_jp"
      ]
    }, 
    "encodings.shift_jisx0213": {
      "file": "encodings/shift_jisx0213.py", 
      "imports": [
        "_multibytecodec", 
        "codecs", 
        "_codecs_jp"
      ]
    }, 
    "encodings.string_escape": {
      "file": "encodings/string_escape.py", 
      "imports": [
        "codecs"
      ]
    }, 
    "encodings.tis_620": {
      "file": "encodings/tis_620.py", 
      "imports": [
        "codecs"
      ]
    }, 
    "encodings.undefined": {
      "file": "encodings/undefined.py", 
      "imports": [
        "codecs"
      ]
    }, 
    "encodings.unicode_escape": {
      "file": "encodings/unicode_escape.py", 
      "imports": [
        "codecs"
      ]
    }, 
    "encodings.unicode_internal": {
      "file": "encodings/unicode_internal.py", 
      "imports": [
        "codecs"
      ]
    }, 
    "encodings.utf_16": {
      "file": "encodings/utf_16.py", 
      "imports": [
        "codecs", 
        "sys"
      ]
    }, 
    "encodings.utf_16_be": {
      "file": "encodings/utf_16_be.py", 
      "imports": [
        "codecs"
      ]
    }, 
    "encodings.utf_16_le": {
      "file": "encodings/utf_16_le.py", 
      "imports": [
        "codecs"
      ]
    }, 
    "encodings.utf_32": {
      "file": "encodings/utf_32.py", 
      "imports": [
        "codecs", 
        "sys"
      ]
    }, 
    "encodings.utf_32_be": {
      "file": "encodings/utf_32_be.py", 
      "imports": [
        "codecs"
      ]
    }, 
    "encodings.utf_32_le": {
      "file": "encodings/utf_32_le.py", 
      "imports": [
        "codecs"
      ]
    }, 
    "encodings.utf_7": {
      "file": "encodings/utf_7.py", 
      "imports": [
        "codecs"
      ]
    }, 
    "encodings.utf_8": {
      "file": "encodings/utf_8.py", 
      "imports": [
        "codecs"
      ]
    }, 
    "encodings.utf_8_sig": {
      "file": "encodings/utf_8_sig.py", 
      "imports": [
        "codecs"
      ]
    }, 
    "encodings.uu_codec": {
      "file": "encodings/uu_codec.py", 
      "imports": [
        "binascii", 
        "binascii.a2b_uu", 
        "binascii.b2a_uu", 
        "cStringIO.StringIO", 
        "codecs"
      ]
    }, 
    "encodings.zlib_codec": {
      "file": "encodings/zlib_codec.py", 
      "imports": [
        "codecs", 
        "zlib"
      ]
    }, 
    "ensurepip": {
      "dir": "ensurepip"
    }, 
    "ensurepip.__init__": {
      "file": "ensurepip/__init__.py", 
      "imports": [
        "__future__", 
        "argparse", 
        "pip", 
        "ssl", 
        "sys", 
        "os", 
        "pkgutil", 
        "shutil", 
        "tempfile"
      ]
    }, 
    "ensurepip.__main__": {
      "file": "ensurepip/__main__.py", 
      "imports": [
        "ensurepip"
      ]
    }, 
    "ensurepip._uninstall": {
      "file": "ensurepip/_uninstall.py", 
      "imports": [
        "argparse", 
        "ensurepip"
      ]
    }, 
    "filecmp": {
      "file": "filecmp.py", 
      "imports": [
        "itertools.ifilter", 
        "itertools.ifilterfalse", 
        "itertools.imap", 
        "itertools.izip", 
        "sys", 
        "getopt", 
        "os", 
        "stat"
      ]
    }, 
    "fileinput": {
      "file": "fileinput.py", 
      "imports": [
        "bz2", 
        "sys", 
        "getopt", 
        "gzip", 
        "io", 
        "os"
      ]
    }, 
    "fnmatch": {
      "file": "fnmatch.py", 
      "imports": [
        "os", 
        "posixpath", 
        "re"
      ]
    }, 
    "formatter": {
      "file": "formatter.py", 
      "imports": [
        "sys"
      ]
    }, 
    "fpformat": {
      "file": "fpformat.py", 
      "imports": [
        "re", 
        "warnings"
      ]
    }, 
    "fractions": {
      "file": "fractions.py", 
      "imports": [
        "__future__", 
        "decimal", 
        "math", 
        "operator", 
        "numbers", 
        "re"
      ]
    }, 
    "ftplib": {
      "file": "ftplib.py", 
      "imports": [
        "SOCKS", 
        "ssl", 
        "sys", 
        "os", 
        "re", 
        "socket"
      ]
    }, 
    "functools": {
      "file": "functools.py", 
      "imports": [
        "_functools"
      ]
    }, 
    "future_builtins": {
      "file": "future_builtins.py", 
      "imports": [
        "itertools.ifilter", 
        "itertools.imap", 
        "itertools.izip"
      ]
    }, 
    "gdbm": {
      "file": "gdbm.py", 
      "imports": [
        "_gdbm_cffi.ffi", 
        "_gdbm_cffi.lib", 
        "os", 
        "thread"
      ]
    }, 
    "genericpath": {
      "file": "genericpath.py", 
      "imports": [
        "os", 
        "stat"
      ]
    }, 
    "getopt": {
      "file": "getopt.py", 
      "imports": [
        "sys", 
        "os"
      ]
    }, 
    "getpass": {
      "file": "getpass.py", 
      "imports": [
        "EasyDialogs.AskPassword", 
        "msvcrt", 
        "sys", 
        "termios", 
        "os", 
        "warnings", 
        "pwd"
      ]
    }, 
    "gettext": {
      "file": "gettext.py", 
      "imports": [
        "__builtin__", 
        "cStringIO.StringIO", 
        "copy", 
        "errno.ENOENT", 
        "sys", 
        "token", 
        "locale", 
        "os", 
        "re", 
        "StringIO", 
        "struct", 
        "tokenize"
      ]
    }, 
    "glob": {
      "file": "glob.py", 
      "imports": [
        "fnmatch", 
        "sys", 
        "os", 
        "re"
      ]
    }, 
    "greenlet": {
      "file": "greenlet.py", 
      "imports": [
        "_continuation", 
        "sys", 
        "threading.local"
      ]
    }, 
    "grp": {
      "file": "grp.py", 
      "imports": [
        "__pypy__.builtinify", 
        "_pwdgrp_cffi.ffi", 
        "_pwdgrp_cffi.lib", 
        "_structseq", 
        "os"
      ]
    }, 
    "gzip": {
      "file": "gzip.py", 
      "imports": [
        "__builtin__", 
        "errno", 
        "sys", 
        "time", 
        "zlib", 
        "io", 
        "os", 
        "struct", 
        "warnings"
      ]
    }, 
    "hashlib": {
      "file": "hashlib.py", 
      "imports": [
        "_hashlib", 
        "_hashlib.pbkdf2_hmac", 
        "_md5", 
        "_sha", 
        "binascii", 
        "logging", 
        "struct", 
        "_sha256", 
        "_sha512"
      ]
    }, 
    "heapq": {
      "file": "heapq.py", 
      "imports": [
        "_heapq.*", 
        "doctest", 
        "itertools.chain", 
        "itertools.count", 
        "itertools.imap", 
        "itertools.islice", 
        "itertools.izip", 
        "itertools.tee", 
        "operator.itemgetter"
      ]
    }, 
    "hmac": {
      "file": "hmac.py", 
      "imports": [
        "hashlib", 
        "operator._compare_digest", 
        "warnings"
      ]
    }, 
    "htmlentitydefs": {
      "file": "htmlentitydefs.py", 
      "imports": []
    }, 
    "htmllib": {
      "file": "htmllib.py", 
      "imports": [
        "formatter", 
        "htmlentitydefs", 
        "sys", 
        "sgmllib", 
        "warnings"
      ]
    }, 
    "httplib": {
      "file": "httplib.py", 
      "imports": [
        "array.array", 
        "cStringIO.StringIO", 
        "ssl", 
        "sys.py3kwarning", 
        "mimetools", 
        "os", 
        "socket", 
        "StringIO", 
        "urlparse", 
        "warnings"
      ]
    }, 
    "identity_dict": {
      "file": "identity_dict.py", 
      "imports": [
        "UserDict", 
        "__pypy__.identity_dict"
      ]
    }, 
    "ihooks": {
      "file": "ihooks.py", 
      "imports": [
        "__builtin__", 
        "imp", 
        "imp.C_BUILTIN", 
        "imp.C_EXTENSION", 
        "imp.PKG_DIRECTORY", 
        "imp.PY_COMPILED", 
        "imp.PY_FROZEN", 
        "imp.PY_SOURCE", 
        "marshal", 
        "sys", 
        "os", 
        "warnings"
      ]
    }, 
    "imaplib": {
      "file": "imaplib.py", 
      "imports": [
        "binascii", 
        "errno", 
        "getopt", 
        "getpass", 
        "hmac", 
        "ssl", 
        "subprocess", 
        "sys", 
        "time", 
        "random", 
        "re", 
        "socket"
      ]
    }, 
    "imghdr": {
      "file": "imghdr.py", 
      "imports": [
        "glob", 
        "sys", 
        "os"
      ]
    }, 
    "importlib": {
      "dir": "importlib"
    }, 
    "importlib.__init__": {
      "file": "importlib/__init__.py", 
      "imports": [
        "sys"
      ]
    }, 
    "imputil": {
      "file": "imputil.py", 
      "imports": [
        "__builtin__", 
        "dos.stat", 
        "imp", 
        "marshal", 
        "nt.stat", 
        "os2.stat", 
        "posix.stat", 
        "sys", 
        "struct", 
        "warnings"
      ]
    }, 
    "inspect": {
      "file": "inspect.py", 
      "imports": [
        "collections", 
        "dis", 
        "imp", 
        "operator.attrgetter", 
        "sys", 
        "linecache", 
        "os", 
        "re", 
        "string", 
        "tokenize", 
        "types"
      ]
    }, 
    "io": {
      "file": "io.py", 
      "imports": [
        "_io", 
        "_io.BlockingIOError", 
        "_io.BufferedRWPair", 
        "_io.BufferedRandom", 
        "_io.BufferedReader", 
        "_io.BufferedWriter", 
        "_io.BytesIO", 
        "_io.DEFAULT_BUFFER_SIZE", 
        "_io.FileIO", 
        "_io.IncrementalNewlineDecoder", 
        "_io.StringIO", 
        "_io.TextIOWrapper", 
        "_io.UnsupportedOperation", 
        "_io.open", 
        "abc"
      ]
    }, 
    "json": {
      "dir": "json"
    }, 
    "json.__init__": {
      "file": "json/__init__.py", 
      "imports": [
        "_pypyjson", 
        "json.decoder", 
        "json.encoder"
      ]
    }, 
    "json.decoder": {
      "file": "json/decoder.py", 
      "imports": [
        "_json.scanstring", 
        "json.scanner", 
        "sys", 
        "re", 
        "struct"
      ]
    }, 
    "json.encoder": {
      "file": "json/encoder.py", 
      "imports": [
        "__pypy__.builders.StringBuilder", 
        "__pypy__.builders.UnicodeBuilder", 
        "_pypyjson.raw_encode_basestring_ascii", 
        "re"
      ]
    }, 
    "json.scanner": {
      "file": "json/scanner.py", 
      "imports": [
        "_json.make_scanner", 
        "re"
      ]
    }, 
    "json.tests": {
      "dir": "json/tests"
    }, 
    "json.tests.__init__": {
      "file": "json/tests/__init__.py", 
      "imports": [
        "doctest", 
        "json", 
        "sys", 
        "os", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "json.tests.test_check_circular": {
      "file": "json/tests/test_check_circular.py", 
      "imports": [
        "json.tests"
      ]
    }, 
    "json.tests.test_decode": {
      "file": "json/tests/test_decode.py", 
      "imports": [
        "collections", 
        "decimal", 
        "json.tests", 
        "StringIO"
      ]
    }, 
    "json.tests.test_default": {
      "file": "json/tests/test_default.py", 
      "imports": [
        "json.tests"
      ]
    }, 
    "json.tests.test_dump": {
      "file": "json/tests/test_dump.py", 
      "imports": [
        "cStringIO.StringIO", 
        "json.tests"
      ]
    }, 
    "json.tests.test_encode_basestring_ascii": {
      "file": "json/tests/test_encode_basestring_ascii.py", 
      "imports": [
        "collections", 
        "json.tests"
      ]
    }, 
    "json.tests.test_fail": {
      "file": "json/tests/test_fail.py", 
      "imports": [
        "json.tests"
      ]
    }, 
    "json.tests.test_float": {
      "file": "json/tests/test_float.py", 
      "imports": [
        "json.tests", 
        "math"
      ]
    }, 
    "json.tests.test_indent": {
      "file": "json/tests/test_indent.py", 
      "imports": [
        "json.tests", 
        "StringIO", 
        "textwrap"
      ]
    }, 
    "json.tests.test_pass1": {
      "file": "json/tests/test_pass1.py", 
      "imports": [
        "json.tests"
      ]
    }, 
    "json.tests.test_pass2": {
      "file": "json/tests/test_pass2.py", 
      "imports": [
        "json.tests"
      ]
    }, 
    "json.tests.test_pass3": {
      "file": "json/tests/test_pass3.py", 
      "imports": [
        "json.tests"
      ]
    }, 
    "json.tests.test_recursion": {
      "file": "json/tests/test_recursion.py", 
      "imports": [
        "json.tests"
      ]
    }, 
    "json.tests.test_scanstring": {
      "file": "json/tests/test_scanstring.py", 
      "imports": [
        "json.tests", 
        "sys"
      ]
    }, 
    "json.tests.test_separators": {
      "file": "json/tests/test_separators.py", 
      "imports": [
        "json.tests", 
        "textwrap"
      ]
    }, 
    "json.tests.test_speedups": {
      "file": "json/tests/test_speedups.py", 
      "imports": [
        "json.tests"
      ]
    }, 
    "json.tests.test_tool": {
      "file": "json/tests/test_tool.py", 
      "imports": [
        "subprocess", 
        "sys", 
        "os", 
        "test.test_support", 
        "test.script_helper", 
        "textwrap", 
        "unittest"
      ]
    }, 
    "json.tests.test_unicode": {
      "file": "json/tests/test_unicode.py", 
      "imports": [
        "collections", 
        "json.tests"
      ]
    }, 
    "json.tool": {
      "file": "json/tool.py", 
      "imports": [
        "json", 
        "sys"
      ]
    }, 
    "keyword": {
      "file": "keyword.py", 
      "imports": [
        "sys", 
        "re"
      ]
    }, 
    "lib2to3": {
      "dir": "lib2to3"
    }, 
    "lib2to3.__init__": {
      "file": "lib2to3/__init__.py", 
      "imports": []
    }, 
    "lib2to3.__main__": {
      "file": "lib2to3/__main__.py", 
      "imports": [
        "lib2to3.main", 
        "sys"
      ]
    }, 
    "lib2to3.btm_matcher": {
      "file": "lib2to3/btm_matcher.py", 
      "imports": [
        "collections", 
        "itertools", 
        "lib2to3.btm_utils", 
        "lib2to3.pygram", 
        "lib2to3.pytree", 
        "logging"
      ]
    }, 
    "lib2to3.btm_utils": {
      "file": "lib2to3/btm_utils.py", 
      "imports": [
        "lib2to3.pgen2.grammar", 
        "lib2to3.pgen2.token", 
        "lib2to3.pygram", 
        "lib2to3.pytree"
      ]
    }, 
    "lib2to3.fixer_base": {
      "file": "lib2to3/fixer_base.py", 
      "imports": [
        "itertools", 
        "lib2to3.fixer_util", 
        "lib2to3.patcomp", 
        "lib2to3.pygram", 
        "logging"
      ]
    }, 
    "lib2to3.fixer_util": {
      "file": "lib2to3/fixer_util.py", 
      "imports": [
        "itertools.islice", 
        "lib2to3.patcomp", 
        "lib2to3.pgen2.token", 
        "lib2to3.pygram", 
        "lib2to3.pytree"
      ]
    }, 
    "lib2to3.fixes": {
      "dir": "lib2to3/fixes"
    }, 
    "lib2to3.fixes.__init__": {
      "file": "lib2to3/fixes/__init__.py", 
      "imports": []
    }, 
    "lib2to3.fixes.fix_apply": {
      "file": "lib2to3/fixes/fix_apply.py", 
      "imports": [
        "lib2to3.fixer_base", 
        "lib2to3.fixer_util", 
        "lib2to3.pgen2.token", 
        "lib2to3.pytree"
      ]
    }, 
    "lib2to3.fixes.fix_asserts": {
      "file": "lib2to3/fixes/fix_asserts.py", 
      "imports": [
        "lib2to3.fixer_base", 
        "lib2to3.fixer_util"
      ]
    }, 
    "lib2to3.fixes.fix_basestring": {
      "file": "lib2to3/fixes/fix_basestring.py", 
      "imports": [
        "lib2to3.fixer_base", 
        "lib2to3.fixer_util"
      ]
    }, 
    "lib2to3.fixes.fix_buffer": {
      "file": "lib2to3/fixes/fix_buffer.py", 
      "imports": [
        "lib2to3.fixer_base", 
        "lib2to3.fixer_util"
      ]
    }, 
    "lib2to3.fixes.fix_callable": {
      "file": "lib2to3/fixes/fix_callable.py", 
      "imports": [
        "lib2to3.fixer_base", 
        "lib2to3.fixer_util"
      ]
    }, 
    "lib2to3.fixes.fix_dict": {
      "file": "lib2to3/fixes/fix_dict.py", 
      "imports": [
        "lib2to3.fixer_base", 
        "lib2to3.fixer_util", 
        "lib2to3.patcomp", 
        "lib2to3.pgen2.token", 
        "lib2to3.pytree"
      ]
    }, 
    "lib2to3.fixes.fix_except": {
      "file": "lib2to3/fixes/fix_except.py", 
      "imports": [
        "lib2to3.fixer_base", 
        "lib2to3.fixer_util", 
        "lib2to3.pgen2.token", 
        "lib2to3.pytree"
      ]
    }, 
    "lib2to3.fixes.fix_exec": {
      "file": "lib2to3/fixes/fix_exec.py", 
      "imports": [
        "lib2to3.fixer_base", 
        "lib2to3.fixer_util", 
        "lib2to3.pytree"
      ]
    }, 
    "lib2to3.fixes.fix_execfile": {
      "file": "lib2to3/fixes/fix_execfile.py", 
      "imports": [
        "lib2to3.fixer_base", 
        "lib2to3.fixer_util"
      ]
    }, 
    "lib2to3.fixes.fix_exitfunc": {
      "file": "lib2to3/fixes/fix_exitfunc.py", 
      "imports": [
        "lib2to3.fixer_base", 
        "lib2to3.fixer_util", 
        "lib2to3.pytree"
      ]
    }, 
    "lib2to3.fixes.fix_filter": {
      "file": "lib2to3/fixes/fix_filter.py", 
      "imports": [
        "lib2to3.fixer_base", 
        "lib2to3.fixer_util", 
        "lib2to3.pgen2.token"
      ]
    }, 
    "lib2to3.fixes.fix_funcattrs": {
      "file": "lib2to3/fixes/fix_funcattrs.py", 
      "imports": [
        "lib2to3.fixer_base", 
        "lib2to3.fixer_util"
      ]
    }, 
    "lib2to3.fixes.fix_future": {
      "file": "lib2to3/fixes/fix_future.py", 
      "imports": [
        "lib2to3.fixer_base", 
        "lib2to3.fixer_util"
      ]
    }, 
    "lib2to3.fixes.fix_getcwdu": {
      "file": "lib2to3/fixes/fix_getcwdu.py", 
      "imports": [
        "lib2to3.fixer_base", 
        "lib2to3.fixer_util"
      ]
    }, 
    "lib2to3.fixes.fix_has_key": {
      "file": "lib2to3/fixes/fix_has_key.py", 
      "imports": [
        "lib2to3.fixer_base", 
        "lib2to3.fixer_util", 
        "lib2to3.pgen2.token", 
        "lib2to3.pytree"
      ]
    }, 
    "lib2to3.fixes.fix_idioms": {
      "file": "lib2to3/fixes/fix_idioms.py", 
      "imports": [
        "lib2to3.fixer_base", 
        "lib2to3.fixer_util"
      ]
    }, 
    "lib2to3.fixes.fix_import": {
      "file": "lib2to3/fixes/fix_import.py", 
      "imports": [
        "lib2to3.fixer_base", 
        "lib2to3.fixer_util", 
        "os"
      ]
    }, 
    "lib2to3.fixes.fix_imports": {
      "file": "lib2to3/fixes/fix_imports.py", 
      "imports": [
        "lib2to3.fixer_base", 
        "lib2to3.fixer_util"
      ]
    }, 
    "lib2to3.fixes.fix_imports2": {
      "file": "lib2to3/fixes/fix_imports2.py", 
      "imports": [
        "lib2to3.fixes.fix_imports"
      ]
    }, 
    "lib2to3.fixes.fix_input": {
      "file": "lib2to3/fixes/fix_input.py", 
      "imports": [
        "lib2to3.fixer_base", 
        "lib2to3.fixer_util", 
        "lib2to3.patcomp"
      ]
    }, 
    "lib2to3.fixes.fix_intern": {
      "file": "lib2to3/fixes/fix_intern.py", 
      "imports": [
        "lib2to3.fixer_base", 
        "lib2to3.fixer_util", 
        "lib2to3.pytree"
      ]
    }, 
    "lib2to3.fixes.fix_isinstance": {
      "file": "lib2to3/fixes/fix_isinstance.py", 
      "imports": [
        "lib2to3.fixer_base", 
        "lib2to3.fixer_util"
      ]
    }, 
    "lib2to3.fixes.fix_itertools": {
      "file": "lib2to3/fixes/fix_itertools.py", 
      "imports": [
        "lib2to3.fixer_base", 
        "lib2to3.fixer_util"
      ]
    }, 
    "lib2to3.fixes.fix_itertools_imports": {
      "file": "lib2to3/fixes/fix_itertools_imports.py", 
      "imports": [
        "lib2to3.fixer_base", 
        "lib2to3.fixer_util"
      ]
    }, 
    "lib2to3.fixes.fix_long": {
      "file": "lib2to3/fixes/fix_long.py", 
      "imports": [
        "lib2to3.fixer_base", 
        "lib2to3.fixer_util"
      ]
    }, 
    "lib2to3.fixes.fix_map": {
      "file": "lib2to3/fixes/fix_map.py", 
      "imports": [
        "lib2to3.fixer_base", 
        "lib2to3.fixer_util", 
        "lib2to3.pgen2.token", 
        "lib2to3.pygram"
      ]
    }, 
    "lib2to3.fixes.fix_metaclass": {
      "file": "lib2to3/fixes/fix_metaclass.py", 
      "imports": [
        "lib2to3.fixer_base", 
        "lib2to3.fixer_util", 
        "lib2to3.pygram"
      ]
    }, 
    "lib2to3.fixes.fix_methodattrs": {
      "file": "lib2to3/fixes/fix_methodattrs.py", 
      "imports": [
        "lib2to3.fixer_base", 
        "lib2to3.fixer_util"
      ]
    }, 
    "lib2to3.fixes.fix_ne": {
      "file": "lib2to3/fixes/fix_ne.py", 
      "imports": [
        "lib2to3.fixer_base", 
        "lib2to3.pgen2.token", 
        "lib2to3.pytree"
      ]
    }, 
    "lib2to3.fixes.fix_next": {
      "file": "lib2to3/fixes/fix_next.py", 
      "imports": [
        "lib2to3.fixer_base", 
        "lib2to3.fixer_util", 
        "lib2to3.pgen2.token", 
        "lib2to3.pygram"
      ]
    }, 
    "lib2to3.fixes.fix_nonzero": {
      "file": "lib2to3/fixes/fix_nonzero.py", 
      "imports": [
        "lib2to3.fixer_base", 
        "lib2to3.fixer_util"
      ]
    }, 
    "lib2to3.fixes.fix_numliterals": {
      "file": "lib2to3/fixes/fix_numliterals.py", 
      "imports": [
        "lib2to3.fixer_base", 
        "lib2to3.fixer_util", 
        "lib2to3.pgen2.token"
      ]
    }, 
    "lib2to3.fixes.fix_operator": {
      "file": "lib2to3/fixes/fix_operator.py", 
      "imports": [
        "lib2to3.fixer_base", 
        "lib2to3.fixer_util"
      ]
    }, 
    "lib2to3.fixes.fix_paren": {
      "file": "lib2to3/fixes/fix_paren.py", 
      "imports": [
        "lib2to3.fixer_base", 
        "lib2to3.fixer_util"
      ]
    }, 
    "lib2to3.fixes.fix_print": {
      "file": "lib2to3/fixes/fix_print.py", 
      "imports": [
        "lib2to3.fixer_base", 
        "lib2to3.fixer_util", 
        "lib2to3.patcomp", 
        "lib2to3.pgen2.token", 
        "lib2to3.pytree"
      ]
    }, 
    "lib2to3.fixes.fix_raise": {
      "file": "lib2to3/fixes/fix_raise.py", 
      "imports": [
        "lib2to3.fixer_base", 
        "lib2to3.fixer_util", 
        "lib2to3.pgen2.token", 
        "lib2to3.pytree"
      ]
    }, 
    "lib2to3.fixes.fix_raw_input": {
      "file": "lib2to3/fixes/fix_raw_input.py", 
      "imports": [
        "lib2to3.fixer_base", 
        "lib2to3.fixer_util"
      ]
    }, 
    "lib2to3.fixes.fix_reduce": {
      "file": "lib2to3/fixes/fix_reduce.py", 
      "imports": [
        "lib2to3.fixer_base", 
        "lib2to3.fixer_util"
      ]
    }, 
    "lib2to3.fixes.fix_renames": {
      "file": "lib2to3/fixes/fix_renames.py", 
      "imports": [
        "lib2to3.fixer_base", 
        "lib2to3.fixer_util"
      ]
    }, 
    "lib2to3.fixes.fix_repr": {
      "file": "lib2to3/fixes/fix_repr.py", 
      "imports": [
        "lib2to3.fixer_base", 
        "lib2to3.fixer_util"
      ]
    }, 
    "lib2to3.fixes.fix_set_literal": {
      "file": "lib2to3/fixes/fix_set_literal.py", 
      "imports": [
        "lib2to3.fixer_base", 
        "lib2to3.fixer_util", 
        "lib2to3.pytree"
      ]
    }, 
    "lib2to3.fixes.fix_standarderror": {
      "file": "lib2to3/fixes/fix_standarderror.py", 
      "imports": [
        "lib2to3.fixer_base", 
        "lib2to3.fixer_util"
      ]
    }, 
    "lib2to3.fixes.fix_sys_exc": {
      "file": "lib2to3/fixes/fix_sys_exc.py", 
      "imports": [
        "lib2to3.fixer_base", 
        "lib2to3.fixer_util"
      ]
    }, 
    "lib2to3.fixes.fix_throw": {
      "file": "lib2to3/fixes/fix_throw.py", 
      "imports": [
        "lib2to3.fixer_base", 
        "lib2to3.fixer_util", 
        "lib2to3.pgen2.token", 
        "lib2to3.pytree"
      ]
    }, 
    "lib2to3.fixes.fix_tuple_params": {
      "file": "lib2to3/fixes/fix_tuple_params.py", 
      "imports": [
        "lib2to3.fixer_base", 
        "lib2to3.fixer_util", 
        "lib2to3.pgen2.token", 
        "lib2to3.pytree"
      ]
    }, 
    "lib2to3.fixes.fix_types": {
      "file": "lib2to3/fixes/fix_types.py", 
      "imports": [
        "lib2to3.fixer_base", 
        "lib2to3.fixer_util", 
        "lib2to3.pgen2.token"
      ]
    }, 
    "lib2to3.fixes.fix_unicode": {
      "file": "lib2to3/fixes/fix_unicode.py", 
      "imports": [
        "lib2to3.fixer_base", 
        "lib2to3.pgen2.token"
      ]
    }, 
    "lib2to3.fixes.fix_urllib": {
      "file": "lib2to3/fixes/fix_urllib.py", 
      "imports": [
        "lib2to3.fixer_base", 
        "lib2to3.fixer_util", 
        "lib2to3.fixes.fix_imports"
      ]
    }, 
    "lib2to3.fixes.fix_ws_comma": {
      "file": "lib2to3/fixes/fix_ws_comma.py", 
      "imports": [
        "lib2to3.fixer_base", 
        "lib2to3.pgen2.token", 
        "lib2to3.pytree"
      ]
    }, 
    "lib2to3.fixes.fix_xrange": {
      "file": "lib2to3/fixes/fix_xrange.py", 
      "imports": [
        "lib2to3.fixer_base", 
        "lib2to3.fixer_util", 
        "lib2to3.patcomp"
      ]
    }, 
    "lib2to3.fixes.fix_xreadlines": {
      "file": "lib2to3/fixes/fix_xreadlines.py", 
      "imports": [
        "lib2to3.fixer_base", 
        "lib2to3.fixer_util"
      ]
    }, 
    "lib2to3.fixes.fix_zip": {
      "file": "lib2to3/fixes/fix_zip.py", 
      "imports": [
        "lib2to3.fixer_base", 
        "lib2to3.fixer_util"
      ]
    }, 
    "lib2to3.main": {
      "file": "lib2to3/main.py", 
      "imports": [
        "__future__", 
        "difflib", 
        "lib2to3.refactor", 
        "sys", 
        "logging", 
        "optparse", 
        "os", 
        "shutil"
      ]
    }, 
    "lib2to3.patcomp": {
      "file": "lib2to3/patcomp.py", 
      "imports": [
        "lib2to3.pgen2.driver", 
        "lib2to3.pgen2.grammar", 
        "lib2to3.pgen2.literals", 
        "lib2to3.pgen2.parse", 
        "lib2to3.pgen2.token", 
        "lib2to3.pgen2.tokenize", 
        "lib2to3.pygram", 
        "lib2to3.pytree", 
        "os", 
        "StringIO"
      ]
    }, 
    "lib2to3.pgen2": {
      "dir": "lib2to3/pgen2"
    }, 
    "lib2to3.pgen2.__init__": {
      "file": "lib2to3/pgen2/__init__.py", 
      "imports": []
    }, 
    "lib2to3.pgen2.conv": {
      "file": "lib2to3/pgen2/conv.py", 
      "imports": [
        "lib2to3.pgen2.grammar", 
        "lib2to3.pgen2.token", 
        "re"
      ]
    }, 
    "lib2to3.pgen2.driver": {
      "file": "lib2to3/pgen2/driver.py", 
      "imports": [
        "codecs", 
        "lib2to3.pgen2.grammar", 
        "lib2to3.pgen2.parse", 
        "lib2to3.pgen2.pgen", 
        "lib2to3.pgen2.token", 
        "lib2to3.pgen2.tokenize", 
        "sys", 
        "logging", 
        "os", 
        "StringIO"
      ]
    }, 
    "lib2to3.pgen2.grammar": {
      "file": "lib2to3/pgen2/grammar.py", 
      "imports": [
        "lib2to3.pgen2.token", 
        "lib2to3.pgen2.tokenize", 
        "pickle", 
        "pprint"
      ]
    }, 
    "lib2to3.pgen2.literals": {
      "file": "lib2to3/pgen2/literals.py", 
      "imports": [
        "re"
      ]
    }, 
    "lib2to3.pgen2.parse": {
      "file": "lib2to3/pgen2/parse.py", 
      "imports": [
        "lib2to3.pgen2.token"
      ]
    }, 
    "lib2to3.pgen2.pgen": {
      "file": "lib2to3/pgen2/pgen.py", 
      "imports": [
        "lib2to3.pgen2.grammar", 
        "lib2to3.pgen2.token", 
        "lib2to3.pgen2.tokenize"
      ]
    }, 
    "lib2to3.pgen2.token": {
      "file": "lib2to3/pgen2/token.py", 
      "imports": []
    }, 
    "lib2to3.pgen2.tokenize": {
      "file": "lib2to3/pgen2/tokenize.py", 
      "imports": [
        "codecs", 
        "lib2to3.pgen2.token", 
        "sys", 
        "re", 
        "string"
      ]
    }, 
    "lib2to3.pygram": {
      "file": "lib2to3/pygram.py", 
      "imports": [
        "lib2to3.pgen2.driver", 
        "lib2to3.pgen2.token", 
        "lib2to3.pytree", 
        "os"
      ]
    }, 
    "lib2to3.pytree": {
      "file": "lib2to3/pytree.py", 
      "imports": [
        "lib2to3.pygram", 
        "sys", 
        "StringIO", 
        "warnings"
      ]
    }, 
    "lib2to3.refactor": {
      "file": "lib2to3/refactor.py", 
      "imports": [
        "__future__", 
        "codecs", 
        "collections", 
        "itertools.chain", 
        "lib2to3.btm_matcher", 
        "lib2to3.btm_utils", 
        "lib2to3.fixer_util", 
        "lib2to3.pgen2.driver", 
        "lib2to3.pgen2.token", 
        "lib2to3.pgen2.tokenize", 
        "lib2to3.pygram", 
        "lib2to3.pytree", 
        "multiprocessing", 
        "operator", 
        "sys", 
        "logging", 
        "os", 
        "StringIO"
      ]
    }, 
    "lib2to3.tests": {
      "dir": "lib2to3/tests"
    }, 
    "lib2to3.tests.__init__": {
      "file": "lib2to3/tests/__init__.py", 
      "imports": [
        "lib2to3.tests.support", 
        "os", 
        "types", 
        "unittest"
      ]
    }, 
    "lib2to3.tests.pytree_idempotency": {
      "file": "lib2to3/tests/pytree_idempotency.py", 
      "imports": [
        "lib2to3.pgen2", 
        "lib2to3.pgen2.driver", 
        "lib2to3.pytree", 
        "lib2to3.tests.support", 
        "sys", 
        "logging", 
        "os"
      ]
    }, 
    "lib2to3.tests.support": {
      "file": "lib2to3/tests/support.py", 
      "imports": [
        "lib2to3.pgen2.driver", 
        "lib2to3.pytree", 
        "lib2to3.refactor", 
        "sys", 
        "os", 
        "re", 
        "textwrap", 
        "unittest"
      ]
    }, 
    "lib2to3.tests.test_all_fixers": {
      "file": "lib2to3/tests/test_all_fixers.py", 
      "imports": [
        "lib2to3.refactor", 
        "lib2to3.tests.support", 
        "unittest"
      ]
    }, 
    "lib2to3.tests.test_fixers": {
      "file": "lib2to3/tests/test_fixers.py", 
      "imports": [
        "itertools.chain", 
        "lib2to3.fixer_util", 
        "lib2to3.fixes.fix_import", 
        "lib2to3.fixes.fix_imports", 
        "lib2to3.fixes.fix_imports2", 
        "lib2to3.fixes.fix_urllib", 
        "lib2to3.pygram", 
        "lib2to3.pytree", 
        "lib2to3.refactor", 
        "lib2to3.tests.support", 
        "operator.itemgetter", 
        "os", 
        "unittest"
      ]
    }, 
    "lib2to3.tests.test_main": {
      "file": "lib2to3/tests/test_main.py", 
      "imports": [
        "codecs", 
        "lib2to3.main", 
        "sys", 
        "logging", 
        "os", 
        "re", 
        "shutil", 
        "StringIO", 
        "tempfile", 
        "unittest"
      ]
    }, 
    "lib2to3.tests.test_parser": {
      "file": "lib2to3/tests/test_parser.py", 
      "imports": [
        "__future__", 
        "lib2to3.pgen2.parse", 
        "lib2to3.pgen2.tokenize", 
        "lib2to3.pygram", 
        "lib2to3.tests.support", 
        "sys", 
        "os"
      ]
    }, 
    "lib2to3.tests.test_pytree": {
      "file": "lib2to3/tests/test_pytree.py", 
      "imports": [
        "__future__", 
        "lib2to3.pytree", 
        "lib2to3.tests.support", 
        "sys", 
        "warnings"
      ]
    }, 
    "lib2to3.tests.test_refactor": {
      "file": "lib2to3/tests/test_refactor.py", 
      "imports": [
        "__future__", 
        "codecs", 
        "lib2to3.fixer_base", 
        "lib2to3.pgen2.token", 
        "lib2to3.pygram", 
        "lib2to3.refactor", 
        "lib2to3.tests.support", 
        "myfixes.fix_explicit.FixExplicit", 
        "myfixes.fix_first.FixFirst", 
        "myfixes.fix_last.FixLast", 
        "myfixes.fix_parrot.FixParrot", 
        "myfixes.fix_preorder.FixPreorder", 
        "operator", 
        "sys", 
        "os", 
        "shutil", 
        "StringIO", 
        "tempfile", 
        "unittest", 
        "warnings"
      ]
    }, 
    "lib2to3.tests.test_util": {
      "file": "lib2to3/tests/test_util.py", 
      "imports": [
        "lib2to3.fixer_util", 
        "lib2to3.pgen2.token", 
        "lib2to3.pytree", 
        "lib2to3.tests.support", 
        "os"
      ]
    }, 
    "linecache": {
      "file": "linecache.py", 
      "imports": [
        "sys", 
        "os"
      ]
    }, 
    "locale": {
      "file": "locale.py", 
      "imports": [
        "_locale", 
        "_locale.*", 
        "encodings", 
        "encodings.aliases", 
        "functools", 
        "operator", 
        "sys", 
        "os", 
        "re"
      ]
    }, 
    "logging": {
      "dir": "logging"
    }, 
    "logging.__init__": {
      "file": "logging/__init__.py", 
      "imports": [
        "atexit", 
        "cStringIO", 
        "codecs", 
        "collections", 
        "sys", 
        "thread", 
        "threading", 
        "time", 
        "os", 
        "traceback", 
        "warnings", 
        "weakref"
      ]
    }, 
    "logging.config": {
      "file": "logging/config.py", 
      "imports": [
        "ConfigParser", 
        "cStringIO", 
        "errno", 
        "io", 
        "json", 
        "logging", 
        "logging.handlers", 
        "select", 
        "sys", 
        "thread", 
        "threading", 
        "os", 
        "re", 
        "socket", 
        "SocketServer", 
        "struct", 
        "tempfile", 
        "traceback", 
        "types"
      ]
    }, 
    "logging.handlers": {
      "file": "logging/handlers.py", 
      "imports": [
        "codecs", 
        "email.utils", 
        "errno", 
        "httplib", 
        "logging", 
        "time", 
        "win32evtlog", 
        "win32evtlogutil", 
        "os", 
        "re", 
        "smtplib", 
        "socket", 
        "stat", 
        "struct", 
        "urllib", 
        "cPickle"
      ]
    }, 
    "macurl2path": {
      "file": "macurl2path.py", 
      "imports": [
        "os", 
        "urllib"
      ]
    }, 
    "mailbox": {
      "file": "mailbox.py", 
      "imports": [
        "calendar", 
        "copy", 
        "email", 
        "email.generator", 
        "email.message", 
        "errno", 
        "fcntl", 
        "sys", 
        "time", 
        "os", 
        "re", 
        "rfc822", 
        "socket", 
        "StringIO", 
        "warnings"
      ]
    }, 
    "mailcap": {
      "file": "mailcap.py", 
      "imports": [
        "sys", 
        "os"
      ]
    }, 
    "markupbase": {
      "file": "markupbase.py", 
      "imports": [
        "re"
      ]
    }, 
    "marshal": {
      "file": "marshal.py", 
      "imports": [
        "_marshal"
      ]
    }, 
    "md5": {
      "file": "md5.py", 
      "imports": [
        "hashlib", 
        "warnings"
      ]
    }, 
    "mhlib": {
      "file": "mhlib.py", 
      "imports": [
        "bisect", 
        "cStringIO.StringIO", 
        "sys", 
        "mimetools", 
        "multifile", 
        "os", 
        "re", 
        "shutil", 
        "StringIO", 
        "warnings"
      ]
    }, 
    "mimetools": {
      "file": "mimetools.py", 
      "imports": [
        "base64", 
        "dummy_thread", 
        "sys", 
        "thread", 
        "time", 
        "os", 
        "quopri", 
        "rfc822", 
        "socket", 
        "tempfile", 
        "uu", 
        "warnings"
      ]
    }, 
    "mimetypes": {
      "file": "mimetypes.py", 
      "imports": [
        "_winreg", 
        "getopt", 
        "sys", 
        "os", 
        "posixpath", 
        "urllib"
      ]
    }, 
    "mimify": {
      "file": "mimify.py", 
      "imports": [
        "base64", 
        "getopt", 
        "sys", 
        "os", 
        "re", 
        "warnings"
      ]
    }, 
    "modulefinder": {
      "file": "modulefinder.py", 
      "imports": [
        "__future__", 
        "dis", 
        "getopt", 
        "imp", 
        "marshal", 
        "sys", 
        "os", 
        "struct", 
        "types"
      ]
    }, 
    "multifile": {
      "file": "multifile.py", 
      "imports": [
        "warnings"
      ]
    }, 
    "mutex": {
      "file": "mutex.py", 
      "imports": [
        "collections", 
        "warnings"
      ]
    }, 
    "netrc": {
      "file": "netrc.py", 
      "imports": [
        "os", 
        "shlex", 
        "stat", 
        "pwd"
      ]
    }, 
    "new": {
      "file": "new.py", 
      "imports": [
        "types", 
        "warnings"
      ]
    }, 
    "nntplib": {
      "file": "nntplib.py", 
      "imports": [
        "netrc", 
        "os", 
        "re", 
        "socket"
      ]
    }, 
    "nturl2path": {
      "file": "nturl2path.py", 
      "imports": [
        "string", 
        "urllib"
      ]
    }, 
    "numbers": {
      "file": "numbers.py", 
      "imports": [
        "__future__", 
        "abc"
      ]
    }, 
    "opcode": {
      "file": "opcode.py", 
      "imports": []
    }, 
    "optparse": {
      "file": "optparse.py", 
      "imports": [
        "__builtin__", 
        "gettext", 
        "sys", 
        "os", 
        "textwrap", 
        "types"
      ]
    }, 
    "os": {
      "file": "os.py", 
      "imports": [
        "_emx_link.link", 
        "ce", 
        "ce.*", 
        "ce._exit", 
        "copy_reg", 
        "errno", 
        "nt", 
        "nt.*", 
        "nt._exit", 
        "ntpath", 
        "os", 
        "os2", 
        "os2.*", 
        "os2._exit", 
        "os2emxpath", 
        "posix", 
        "posix.*", 
        "posix._exit", 
        "riscos", 
        "riscos.*", 
        "riscos._exit", 
        "riscosenviron._Environ", 
        "riscospath", 
        "subprocess", 
        "sys", 
        "posixpath", 
        "UserDict", 
        "warnings"
      ]
    }, 
    "pdb": {
      "file": "pdb.py", 
      "imports": [
        "__main__", 
        "bdb", 
        "cmd", 
        "linecache", 
        "os", 
        "pdb", 
        "readline", 
        "sys", 
        "pprint", 
        "re", 
        "repr", 
        "shlex", 
        "traceback"
      ]
    }, 
    "pickle": {
      "file": "pickle.py", 
      "imports": [
        "binascii", 
        "cStringIO.StringIO", 
        "copy_reg", 
        "doctest", 
        "marshal", 
        "org.python.core.PyStringMap", 
        "sys", 
        "re", 
        "StringIO", 
        "struct", 
        "types"
      ]
    }, 
    "pickletools": {
      "file": "pickletools.py", 
      "imports": [
        "cStringIO", 
        "doctest", 
        "pickle", 
        "re", 
        "struct"
      ]
    }, 
    "pipes": {
      "file": "pipes.py", 
      "imports": [
        "os", 
        "re", 
        "string", 
        "tempfile"
      ]
    }, 
    "pkgutil": {
      "file": "pkgutil.py", 
      "imports": [
        "imp", 
        "inspect", 
        "marshal", 
        "os", 
        "sys", 
        "zipimport", 
        "zipimport.zipimporter", 
        "types"
      ]
    }, 
    "platform": {
      "file": "platform.py", 
      "imports": [
        "MacOS", 
        "_winreg", 
        "gestalt", 
        "gestalt.gestalt", 
        "java.lang", 
        "java.lang.System", 
        "os", 
        "subprocess", 
        "sys", 
        "vms_lib", 
        "win32api", 
        "win32api.GetVersionEx", 
        "win32api.RegCloseKey", 
        "win32api.RegOpenKeyEx", 
        "win32api.RegQueryValueEx", 
        "win32con.HKEY_LOCAL_MACHINE", 
        "win32con.VER_NT_WORKSTATION", 
        "win32con.VER_PLATFORM_WIN32_NT", 
        "win32con.VER_PLATFORM_WIN32_WINDOWS", 
        "win32pipe", 
        "plistlib", 
        "re", 
        "socket", 
        "string", 
        "struct", 
        "tempfile"
      ]
    }, 
    "plistlib": {
      "file": "plistlib.py", 
      "imports": [
        "Carbon.File.FSGetResourceForkName", 
        "Carbon.File.FSRef", 
        "Carbon.Files.fsRdPerm", 
        "Carbon.Files.fsRdWrPerm", 
        "Carbon.Res", 
        "binascii", 
        "cStringIO.StringIO", 
        "re", 
        "warnings", 
        "xml.parsers.expat", 
        "datetime"
      ]
    }, 
    "popen2": {
      "file": "popen2.py", 
      "imports": [
        "os", 
        "sys", 
        "warnings"
      ]
    }, 
    "poplib": {
      "file": "poplib.py", 
      "imports": [
        "hashlib", 
        "ssl", 
        "sys", 
        "re", 
        "socket"
      ]
    }, 
    "posixfile": {
      "file": "posixfile.py", 
      "imports": [
        "__builtin__", 
        "fcntl", 
        "os", 
        "posix", 
        "sys", 
        "struct", 
        "types", 
        "warnings"
      ]
    }, 
    "posixpath": {
      "file": "posixpath.py", 
      "imports": [
        "genericpath", 
        "os", 
        "sys", 
        "re", 
        "stat", 
        "warnings", 
        "pwd"
      ]
    }, 
    "pprint": {
      "file": "pprint.py", 
      "imports": [
        "cStringIO.StringIO", 
        "sys", 
        "time", 
        "StringIO", 
        "warnings"
      ]
    }, 
    "profile": {
      "file": "profile.py", 
      "imports": [
        "__main__", 
        "marshal", 
        "optparse", 
        "os", 
        "sys", 
        "time", 
        "pstats", 
        "resource"
      ]
    }, 
    "pstats": {
      "file": "pstats.py", 
      "imports": [
        "cmd", 
        "functools", 
        "marshal", 
        "os", 
        "readline", 
        "sys", 
        "time", 
        "re"
      ]
    }, 
    "pty": {
      "file": "pty.py", 
      "imports": [
        "fcntl.I_PUSH", 
        "fcntl.ioctl", 
        "os", 
        "select.select", 
        "sgi", 
        "tty"
      ]
    }, 
    "pwd": {
      "file": "pwd.py", 
      "imports": [
        "__pypy__.builtinify", 
        "_pwdgrp_cffi.ffi", 
        "_pwdgrp_cffi.lib", 
        "_structseq", 
        "os"
      ]
    }, 
    "py_compile": {
      "file": "py_compile.py", 
      "imports": [
        "__builtin__", 
        "imp", 
        "marshal", 
        "os", 
        "sys", 
        "traceback"
      ]
    }, 
    "pyclbr": {
      "file": "pyclbr.py", 
      "imports": [
        "imp", 
        "operator.itemgetter", 
        "os", 
        "sys", 
        "token.DEDENT", 
        "token.NAME", 
        "token.OP", 
        "tokenize"
      ]
    }, 
    "pydoc": {
      "file": "pydoc.py", 
      "imports": [
        "BaseHTTPServer", 
        "Tkinter", 
        "__builtin__", 
        "collections", 
        "formatter", 
        "getopt", 
        "imp", 
        "inspect", 
        "locale", 
        "mimetools", 
        "nturl2path", 
        "os", 
        "pkgutil", 
        "select", 
        "sys", 
        "threading", 
        "pydoc_data.topics", 
        "re", 
        "repr", 
        "string", 
        "StringIO", 
        "tempfile", 
        "traceback", 
        "tty", 
        "types", 
        "warnings", 
        "webbrowser"
      ]
    }, 
    "pydoc_data": {
      "dir": "pydoc_data"
    }, 
    "pydoc_data.__init__": {
      "file": "pydoc_data/__init__.py", 
      "imports": []
    }, 
    "pydoc_data.topics": {
      "file": "pydoc_data/topics.py", 
      "imports": []
    }, 
    "pyrepl": {
      "dir": "pyrepl"
    }, 
    "pyrepl.__init__": {
      "file": "pyrepl/__init__.py", 
      "imports": []
    }, 
    "pyrepl.cmdrepl": {
      "file": "pyrepl/cmdrepl.py", 
      "imports": [
        "__future__", 
        "cmd", 
        "pyrepl.completer", 
        "pyrepl.completing_reader", 
        "pyrepl.reader"
      ]
    }, 
    "pyrepl.commands": {
      "file": "pyrepl/commands.py", 
      "imports": [
        "os", 
        "pyrepl.input", 
        "signal", 
        "sys"
      ]
    }, 
    "pyrepl.completer": {
      "file": "pyrepl/completer.py", 
      "imports": [
        "__builtin__", 
        "keyword", 
        "re"
      ]
    }, 
    "pyrepl.completing_reader": {
      "file": "pyrepl/completing_reader.py", 
      "imports": [
        "pyrepl.commands", 
        "pyrepl.reader", 
        "re"
      ]
    }, 
    "pyrepl.console": {
      "file": "pyrepl/console.py", 
      "imports": []
    }, 
    "pyrepl.copy_code": {
      "file": "pyrepl/copy_code.py", 
      "imports": [
        "new"
      ]
    }, 
    "pyrepl.curses": {
      "file": "pyrepl/curses.py", 
      "imports": [
        "_curses", 
        "_minimal_curses", 
        "pyrepl.curses", 
        "sys"
      ]
    }, 
    "pyrepl.fancy_termios": {
      "file": "pyrepl/fancy_termios.py", 
      "imports": [
        "termios"
      ]
    }, 
    "pyrepl.historical_reader": {
      "file": "pyrepl/historical_reader.py", 
      "imports": [
        "pyrepl.commands", 
        "pyrepl.input", 
        "pyrepl.reader", 
        "pyrepl.unix_console"
      ]
    }, 
    "pyrepl.input": {
      "file": "pyrepl/input.py", 
      "imports": [
        "pyrepl.keymap", 
        "pyrepl.unicodedata_"
      ]
    }, 
    "pyrepl.keymap": {
      "file": "pyrepl/keymap.py", 
      "imports": []
    }, 
    "pyrepl.keymaps": {
      "file": "pyrepl/keymaps.py", 
      "imports": []
    }, 
    "pyrepl.module_lister": {
      "file": "pyrepl/module_lister.py", 
      "imports": [
        "imp", 
        "os", 
        "sys"
      ]
    }, 
    "pyrepl.pygame_console": {
      "file": "pyrepl/pygame_console.py", 
      "imports": [
        "pygame", 
        "pygame.locals.*", 
        "pyrepl.console", 
        "pyrepl.pygame_keymap", 
        "pyrepl.reader", 
        "types"
      ]
    }, 
    "pyrepl.pygame_keymap": {
      "file": "pyrepl/pygame_keymap.py", 
      "imports": [
        "pygame.locals.*"
      ]
    }, 
    "pyrepl.python_reader": {
      "file": "pyrepl/python_reader.py", 
      "imports": [
        "_tkinter", 
        "atexit", 
        "cPickle", 
        "cocoasupport.CocoaInteracter", 
        "code", 
        "imp", 
        "locale", 
        "new", 
        "os", 
        "pickle", 
        "pyrepl.commands", 
        "pyrepl.completer", 
        "pyrepl.completing_reader", 
        "pyrepl.copy_code", 
        "pyrepl.historical_reader", 
        "pyrepl.module_lister", 
        "pyrepl.pygame_console", 
        "pyrepl.reader", 
        "pyrepl.unix_console", 
        "re", 
        "signal", 
        "sys", 
        "traceback", 
        "twisted.internet.abstract.FileDescriptor", 
        "twisted.internet.reactor", 
        "warnings"
      ]
    }, 
    "pyrepl.reader": {
      "file": "pyrepl/reader.py", 
      "imports": [
        "_pyrepl_utils.disp_str", 
        "_pyrepl_utils.init_unctrl_map", 
        "pyrepl.commands", 
        "pyrepl.input", 
        "pyrepl.unicodedata_", 
        "pyrepl.unix_console", 
        "re", 
        "types"
      ]
    }, 
    "pyrepl.readline": {
      "file": "pyrepl/readline.py", 
      "imports": [
        "__builtin__", 
        "os", 
        "pyrepl.commands", 
        "pyrepl.completing_reader", 
        "pyrepl.historical_reader", 
        "pyrepl.unix_console", 
        "sys", 
        "warnings"
      ]
    }, 
    "pyrepl.simple_interact": {
      "file": "pyrepl/simple_interact.py", 
      "imports": [
        "__main__", 
        "code", 
        "pyrepl.readline", 
        "sys"
      ]
    }, 
    "pyrepl.unicodedata_": {
      "file": "pyrepl/unicodedata_.py", 
      "imports": [
        "unicodedata.*"
      ]
    }, 
    "pyrepl.unix_console": {
      "file": "pyrepl/unix_console.py", 
      "imports": [
        "errno", 
        "fcntl.ioctl", 
        "os", 
        "pyrepl.console", 
        "pyrepl.curses", 
        "pyrepl.fancy_termios", 
        "pyrepl.unix_eventqueue", 
        "re", 
        "select", 
        "signal", 
        "struct", 
        "sys", 
        "termios", 
        "time"
      ]
    }, 
    "pyrepl.unix_eventqueue": {
      "file": "pyrepl/unix_eventqueue.py", 
      "imports": [
        "os", 
        "pyrepl.console", 
        "pyrepl.curses", 
        "pyrepl.keymap", 
        "termios.VERASE", 
        "termios.tcgetattr"
      ]
    }, 
    "quopri": {
      "file": "quopri.py", 
      "imports": [
        "binascii.a2b_qp", 
        "binascii.b2a_qp", 
        "cStringIO.StringIO", 
        "getopt", 
        "sys"
      ]
    }, 
    "random": {
      "file": "random.py", 
      "imports": [
        "__future__", 
        "_random", 
        "binascii.hexlify", 
        "hashlib", 
        "math.acos", 
        "math.ceil", 
        "math.cos", 
        "math.e", 
        "math.exp", 
        "math.log", 
        "math.pi", 
        "math.sin", 
        "math.sqrt", 
        "os", 
        "time", 
        "warnings"
      ]
    }, 
    "re": {
      "file": "re.py", 
      "imports": [
        "_locale", 
        "copy_reg", 
        "sys", 
        "sre_compile", 
        "sre_constants", 
        "sre_parse"
      ]
    }, 
    "repr": {
      "file": "repr.py", 
      "imports": [
        "__builtin__", 
        "itertools.islice"
      ]
    }, 
    "resource": {
      "file": "resource.py", 
      "imports": [
        "__pypy__.builtinify", 
        "_structseq", 
        "ctypes.POINTER", 
        "ctypes.Structure", 
        "ctypes.byref", 
        "ctypes.c_int", 
        "ctypes.c_long", 
        "ctypes_config_cache._resource_cache", 
        "ctypes_support.get_errno", 
        "ctypes_support.standard_c_lib", 
        "errno.EINVAL", 
        "errno.EPERM", 
        "os", 
        "sys"
      ]
    }, 
    "rexec": {
      "file": "rexec.py", 
      "imports": [
        "__builtin__", 
        "code", 
        "getopt", 
        "ihooks", 
        "imp", 
        "os", 
        "readline", 
        "sys", 
        "traceback", 
        "warnings"
      ]
    }, 
    "rfc822": {
      "file": "rfc822.py", 
      "imports": [
        "os", 
        "sys", 
        "time", 
        "warnings"
      ]
    }, 
    "rlcompleter": {
      "file": "rlcompleter.py", 
      "imports": [
        "__builtin__", 
        "__main__", 
        "keyword", 
        "re", 
        "readline"
      ]
    }, 
    "robotparser": {
      "file": "robotparser.py", 
      "imports": [
        "time", 
        "urllib", 
        "urlparse"
      ]
    }, 
    "runpy": {
      "file": "runpy.py", 
      "imports": [
        "imp", 
        "imp.get_loader", 
        "pkgutil", 
        "sys"
      ]
    }, 
    "sched": {
      "file": "sched.py", 
      "imports": [
        "collections", 
        "heapq"
      ]
    }, 
    "sets": {
      "file": "sets.py", 
      "imports": [
        "copy", 
        "itertools.ifilter", 
        "itertools.ifilterfalse", 
        "warnings"
      ]
    }, 
    "sgmllib": {
      "file": "sgmllib.py", 
      "imports": [
        "markupbase", 
        "re", 
        "sys", 
        "warnings"
      ]
    }, 
    "sha": {
      "file": "sha.py", 
      "imports": [
        "hashlib", 
        "warnings"
      ]
    }, 
    "shelve": {
      "file": "shelve.py", 
      "imports": [
        "anydbm", 
        "cStringIO.StringIO", 
        "pickle", 
        "StringIO", 
        "UserDict", 
        "cPickle"
      ]
    }, 
    "shlex": {
      "file": "shlex.py", 
      "imports": [
        "cStringIO.StringIO", 
        "collections", 
        "os", 
        "sys", 
        "StringIO"
      ]
    }, 
    "shutil": {
      "file": "shutil.py", 
      "imports": [
        "collections", 
        "distutils.errors", 
        "distutils.spawn", 
        "errno", 
        "fnmatch", 
        "os", 
        "sys", 
        "stat", 
        "tarfile", 
        "zipfile", 
        "grp", 
        "pwd"
      ]
    }, 
    "site": {
      "file": "site.py", 
      "imports": [
        "__builtin__", 
        "codecs", 
        "distutils.sysconfig", 
        "encodings", 
        "exceptions", 
        "locale", 
        "os", 
        "pydoc", 
        "sitecustomize", 
        "sys", 
        "usercustomize", 
        "zipimport", 
        "sysconfig", 
        "textwrap", 
        "traceback"
      ]
    }, 
    "six": {
      "file": "six.py", 
      "imports": [
        "StringIO", 
        "__future__", 
        "functools", 
        "io", 
        "itertools", 
        "operator", 
        "struct", 
        "sys", 
        "types"
      ]
    }, 
    "smtpd": {
      "file": "smtpd.py", 
      "imports": [
        "Mailman.MailList", 
        "Mailman.Message", 
        "Mailman.Utils", 
        "__main__", 
        "asynchat", 
        "asyncore", 
        "cStringIO.StringIO", 
        "errno", 
        "getopt", 
        "os", 
        "sys", 
        "time", 
        "smtplib", 
        "socket", 
        "pwd"
      ]
    }, 
    "smtplib": {
      "file": "smtplib.py", 
      "imports": [
        "base64", 
        "email.base64mime", 
        "email.utils", 
        "hmac", 
        "re", 
        "ssl", 
        "sys", 
        "sys.stderr", 
        "socket"
      ]
    }, 
    "sndhdr": {
      "file": "sndhdr.py", 
      "imports": [
        "aifc", 
        "glob", 
        "os", 
        "sys"
      ]
    }, 
    "socket": {
      "file": "socket.py", 
      "imports": [
        "_socket", 
        "_socket.*", 
        "_ssl", 
        "_ssl.RAND_add", 
        "_ssl.RAND_egd", 
        "_ssl.RAND_status", 
        "_ssl.SSLError", 
        "_ssl.SSL_ERROR_EOF", 
        "_ssl.SSL_ERROR_INVALID_ERROR_CODE", 
        "_ssl.SSL_ERROR_SSL", 
        "_ssl.SSL_ERROR_SYSCALL", 
        "_ssl.SSL_ERROR_WANT_CONNECT", 
        "_ssl.SSL_ERROR_WANT_READ", 
        "_ssl.SSL_ERROR_WANT_WRITE", 
        "_ssl.SSL_ERROR_WANT_X509_LOOKUP", 
        "_ssl.SSL_ERROR_ZERO_RETURN", 
        "cStringIO.StringIO", 
        "errno", 
        "os", 
        "ssl", 
        "sys", 
        "StringIO", 
        "warnings"
      ]
    }, 
    "sre": {
      "file": "sre.py", 
      "imports": [
        "re", 
        "warnings"
      ]
    }, 
    "sre_compile": {
      "file": "sre_compile.py", 
      "imports": [
        "_sre", 
        "array", 
        "sys", 
        "sre_constants", 
        "sre_parse"
      ]
    }, 
    "sre_constants": {
      "file": "sre_constants.py", 
      "imports": [
        "_sre", 
        "_sre.MAXREPEAT"
      ]
    }, 
    "sre_parse": {
      "file": "sre_parse.py", 
      "imports": [
        "__pypy__.newdict", 
        "sre_constants", 
        "sys"
      ]
    }, 
    "stackless": {
      "file": "stackless.py", 
      "imports": [
        "_continuation", 
        "collections", 
        "operator", 
        "threading.local"
      ]
    }, 
    "stat": {
      "file": "stat.py", 
      "imports": []
    }, 
    "statvfs": {
      "file": "statvfs.py", 
      "imports": [
        "warnings"
      ]
    }, 
    "string": {
      "file": "string.py", 
      "imports": [
        "re", 
        "strop.lowercase", 
        "strop.maketrans", 
        "strop.uppercase", 
        "strop.whitespace"
      ]
    }, 
    "stringold": {
      "file": "stringold.py", 
      "imports": [
        "stringold", 
        "strop.lowercase", 
        "strop.maketrans", 
        "strop.uppercase", 
        "strop.whitespace", 
        "warnings"
      ]
    }, 
    "stringprep": {
      "file": "stringprep.py", 
      "imports": [
        "unicodedata.ucd_3_2_0"
      ]
    }, 
    "struct": {
      "file": "struct.py", 
      "imports": [
        "_struct.*", 
        "_struct.__doc__", 
        "_struct._clearcache"
      ]
    }, 
    "subprocess": {
      "file": "subprocess.py", 
      "imports": []
    }, 
    "symbol": {
      "file": "symbol.py", 
      "imports": [
        "sys", 
        "token"
      ]
    }, 
    "sysconfig": {
      "file": "sysconfig.py", 
      "imports": [
        "_osx_support", 
        "imp", 
        "os", 
        "pprint", 
        "re", 
        "sys"
      ]
    }, 
    "syslog": {
      "file": "syslog.py", 
      "imports": [
        "__pypy__.builtinify", 
        "_syslog_cffi.ffi", 
        "_syslog_cffi.lib", 
        "sys"
      ]
    }, 
    "tabnanny": {
      "file": "tabnanny.py", 
      "imports": [
        "getopt", 
        "os", 
        "sys", 
        "tokenize"
      ]
    }, 
    "tarfile": {
      "file": "tarfile.py", 
      "imports": [
        "StringIO", 
        "bz2", 
        "cStringIO.StringIO", 
        "calendar", 
        "copy", 
        "errno", 
        "gzip", 
        "operator", 
        "os", 
        "re", 
        "shutil", 
        "stat", 
        "struct", 
        "sys", 
        "time", 
        "zlib", 
        "warnings", 
        "grp", 
        "pwd"
      ]
    }, 
    "telnetlib": {
      "file": "telnetlib.py", 
      "imports": [
        "errno", 
        "re", 
        "select", 
        "socket", 
        "sys", 
        "thread", 
        "time.time"
      ]
    }, 
    "tempfile": {
      "file": "tempfile.py", 
      "imports": [
        "StringIO", 
        "cStringIO.StringIO", 
        "dummy_thread", 
        "errno", 
        "fcntl", 
        "io", 
        "os", 
        "random", 
        "thread"
      ]
    }, 
    "test": {
      "dir": "test"
    }, 
    "test.__init__": {
      "file": "test/__init__.py", 
      "imports": []
    }, 
    "test._mock_backport": {
      "file": "test/_mock_backport.py", 
      "imports": [
        "__builtin__", 
        "_io", 
        "functools", 
        "inspect", 
        "java", 
        "pprint", 
        "sys", 
        "types"
      ]
    }, 
    "test.audiotests": {
      "file": "test/audiotests.py", 
      "imports": [
        "array", 
        "base64", 
        "io", 
        "pickle", 
        "sys", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.autotest": {
      "file": "test/autotest.py", 
      "imports": [
        "test.regrtest"
      ]
    }, 
    "test.bad_coding": {
      "file": "test/bad_coding.py", 
      "imports": []
    }, 
    "test.bad_coding2": {
      "file": "test/bad_coding2.py", 
      "imports": []
    }, 
    "test.bad_coding3": {
      "file": "test/bad_coding3.py", 
      "imports": []
    }, 
    "test.badsyntax_future3": {
      "file": "test/badsyntax_future3.py", 
      "imports": [
        "__future__"
      ]
    }, 
    "test.badsyntax_future4": {
      "file": "test/badsyntax_future4.py", 
      "imports": [
        "__future__"
      ]
    }, 
    "test.badsyntax_future5": {
      "file": "test/badsyntax_future5.py", 
      "imports": [
        "__future__", 
        "foo"
      ]
    }, 
    "test.badsyntax_future6": {
      "file": "test/badsyntax_future6.py", 
      "imports": [
        "__future__"
      ]
    }, 
    "test.badsyntax_future7": {
      "file": "test/badsyntax_future7.py", 
      "imports": [
        "__future__", 
        "string"
      ]
    }, 
    "test.badsyntax_future8": {
      "file": "test/badsyntax_future8.py", 
      "imports": [
        "__future__"
      ]
    }, 
    "test.badsyntax_future9": {
      "file": "test/badsyntax_future9.py", 
      "imports": [
        "__future__"
      ]
    }, 
    "test.badsyntax_nocaret": {
      "file": "test/badsyntax_nocaret.py", 
      "imports": []
    }, 
    "test.buffer_tests": {
      "file": "test/buffer_tests.py", 
      "imports": [
        "struct", 
        "sys"
      ]
    }, 
    "test.curses_tests": {
      "file": "test/curses_tests.py", 
      "imports": [
        "curses", 
        "curses.textpad"
      ]
    }, 
    "test.doctest_aliases": {
      "file": "test/doctest_aliases.py", 
      "imports": []
    }, 
    "test.double_const": {
      "file": "test/double_const.py", 
      "imports": [
        "test.test_support"
      ]
    }, 
    "test.fork_wait": {
      "file": "test/fork_wait.py", 
      "imports": [
        "os", 
        "sys", 
        "test.test_support", 
        "time", 
        "unittest"
      ]
    }, 
    "test.gdb_sample": {
      "file": "test/gdb_sample.py", 
      "imports": []
    }, 
    "test.infinite_reload": {
      "file": "test/infinite_reload.py", 
      "imports": [
        "imp", 
        "test.infinite_reload"
      ]
    }, 
    "test.inspect_fodder": {
      "file": "test/inspect_fodder.py", 
      "imports": [
        "inspect", 
        "sys"
      ]
    }, 
    "test.inspect_fodder2": {
      "file": "test/inspect_fodder2.py", 
      "imports": []
    }, 
    "test.leakers": {
      "dir": "test/leakers"
    }, 
    "test.leakers.__init__": {
      "file": "test/leakers/__init__.py", 
      "imports": []
    }, 
    "test.leakers.test_ctypes": {
      "file": "test/leakers/test_ctypes.py", 
      "imports": [
        "ctypes.POINTER", 
        "ctypes.Structure", 
        "ctypes.c_int", 
        "gc"
      ]
    }, 
    "test.leakers.test_dictself": {
      "file": "test/leakers/test_dictself.py", 
      "imports": [
        "gc"
      ]
    }, 
    "test.leakers.test_gestalt": {
      "file": "test/leakers/test_gestalt.py", 
      "imports": [
        "MacOS", 
        "gestalt.gestalt", 
        "sys"
      ]
    }, 
    "test.leakers.test_selftype": {
      "file": "test/leakers/test_selftype.py", 
      "imports": [
        "gc"
      ]
    }, 
    "test.list_tests": {
      "file": "test/list_tests.py", 
      "imports": [
        "os", 
        "sys", 
        "test.seq_tests", 
        "test.test_support"
      ]
    }, 
    "test.lock_tests": {
      "file": "test/lock_tests.py", 
      "imports": [
        "sys", 
        "test.test_support", 
        "thread.get_ident", 
        "thread.start_new_thread", 
        "threading", 
        "time", 
        "unittest"
      ]
    }, 
    "test.make_ssl_certs": {
      "file": "test/make_ssl_certs.py", 
      "imports": [
        "os", 
        "shutil", 
        "subprocess.*", 
        "sys", 
        "tempfile"
      ]
    }, 
    "test.mapping_tests": {
      "file": "test/mapping_tests.py", 
      "imports": [
        "test.test_support", 
        "unittest", 
        "UserDict"
      ]
    }, 
    "test.mp_fork_bomb": {
      "file": "test/mp_fork_bomb.py", 
      "imports": [
        "multiprocessing"
      ]
    }, 
    "test.outstanding_bugs": {
      "file": "test/outstanding_bugs.py", 
      "imports": [
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.pickletester": {
      "file": "test/pickletester.py", 
      "imports": [
        "StringIO", 
        "__main__", 
        "cStringIO", 
        "copy_reg", 
        "locale", 
        "os", 
        "pickle", 
        "pickletools", 
        "sys", 
        "test.test_support", 
        "time", 
        "unittest", 
        "cPickle"
      ]
    }, 
    "test.profilee": {
      "file": "test/profilee.py", 
      "imports": [
        "sys"
      ]
    }, 
    "test.pyclbr_input": {
      "file": "test/pyclbr_input.py", 
      "imports": []
    }, 
    "test.pydoc_mod": {
      "file": "test/pydoc_mod.py", 
      "imports": []
    }, 
    "test.pydocfodder": {
      "file": "test/pydocfodder.py", 
      "imports": [
        "types"
      ]
    }, 
    "test.pystone": {
      "file": "test/pystone.py", 
      "imports": [
        "sys", 
        "time.time"
      ]
    }, 
    "test.re_tests": {
      "file": "test/re_tests.py", 
      "imports": []
    }, 
    "test.regrtest": {
      "file": "test/regrtest.py", 
      "imports": [
        "Queue", 
        "StringIO", 
        "_abcoll", 
        "_pyio", 
        "_strptime", 
        "copy_reg", 
        "ctypes", 
        "distutils.dir_util", 
        "doctest", 
        "filecmp", 
        "gc", 
        "getopt", 
        "imp", 
        "json", 
        "linecache", 
        "mimetypes", 
        "os", 
        "platform", 
        "random", 
        "re", 
        "shutil", 
        "stat", 
        "struct", 
        "subprocess.PIPE", 
        "subprocess.Popen", 
        "sys", 
        "sysconfig", 
        "tempfile", 
        "test.test_support", 
        "test.test_timeout", 
        "threading.Thread", 
        "time", 
        "zipimport", 
        "textwrap", 
        "trace", 
        "traceback", 
        "unittest", 
        "urllib", 
        "urllib2", 
        "urlparse", 
        "warnings", 
        "resource"
      ]
    }, 
    "test.relimport": {
      "file": "test/relimport.py", 
      "imports": [
        "test.test_import"
      ]
    }, 
    "test.reperf": {
      "file": "test/reperf.py", 
      "imports": [
        "re", 
        "time"
      ]
    }, 
    "test.sample_doctest": {
      "file": "test/sample_doctest.py", 
      "imports": [
        "doctest"
      ]
    }, 
    "test.sample_doctest_no_docstrings": {
      "file": "test/sample_doctest_no_docstrings.py", 
      "imports": []
    }, 
    "test.sample_doctest_no_doctests": {
      "file": "test/sample_doctest_no_doctests.py", 
      "imports": []
    }, 
    "test.script_helper": {
      "file": "test/script_helper.py", 
      "imports": [
        "contextlib", 
        "os", 
        "py_compile", 
        "re", 
        "shutil", 
        "subprocess", 
        "sys", 
        "tempfile", 
        "test.test_support", 
        "zipfile"
      ]
    }, 
    "test.seq_tests": {
      "file": "test/seq_tests.py", 
      "imports": [
        "itertools.chain", 
        "itertools.imap", 
        "sys", 
        "unittest"
      ]
    }, 
    "test.sortperf": {
      "file": "test/sortperf.py", 
      "imports": [
        "marshal", 
        "os", 
        "random", 
        "sys", 
        "tempfile", 
        "time"
      ]
    }, 
    "test.ssl_servers": {
      "file": "test/ssl_servers.py", 
      "imports": [
        "BaseHTTPServer", 
        "SimpleHTTPServer", 
        "argparse", 
        "os", 
        "pprint", 
        "ssl", 
        "sys", 
        "test.test_support", 
        "urllib", 
        "urlparse"
      ]
    }, 
    "test.string_tests": {
      "file": "test/string_tests.py", 
      "imports": [
        "string", 
        "struct", 
        "sys", 
        "test.test_support", 
        "zlib", 
        "unittest", 
        "UserList", 
        "_testcapi"
      ]
    }, 
    "test.symlink_support": {
      "file": "test/symlink_support.py", 
      "imports": [
        "ctypes.wintypes", 
        "os", 
        "platform", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_MimeWriter": {
      "file": "test/test_MimeWriter.py", 
      "imports": [
        "MimeWriter", 
        "StringIO", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_SimpleHTTPServer": {
      "file": "test/test_SimpleHTTPServer.py", 
      "imports": [
        "SimpleHTTPServer", 
        "os", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_StringIO": {
      "file": "test/test_StringIO.py", 
      "imports": [
        "StringIO", 
        "array", 
        "cStringIO", 
        "sys", 
        "test.test_support", 
        "types", 
        "unittest"
      ]
    }, 
    "test.test___all__": {
      "file": "test/test___all__.py", 
      "imports": [
        "__future__", 
        "_socket", 
        "locale", 
        "os", 
        "rlcompleter", 
        "sys", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test___future__": {
      "file": "test/test___future__.py", 
      "imports": [
        "__future__", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test__locale": {
      "file": "test/test__locale.py", 
      "imports": [
        "_locale.Error", 
        "_locale.LC_NUMERIC", 
        "_locale.RADIXCHAR", 
        "_locale.THOUSEP", 
        "_locale.localeconv", 
        "_locale.nl_langinfo", 
        "_locale.setlocale", 
        "platform", 
        "sys", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test__osx_support": {
      "file": "test/test__osx_support.py", 
      "imports": [
        "_osx_support", 
        "os", 
        "platform", 
        "shutil", 
        "stat", 
        "sys", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_abc": {
      "file": "test/test_abc.py", 
      "imports": [
        "abc", 
        "inspect", 
        "test.test_support", 
        "unittest", 
        "weakref"
      ]
    }, 
    "test.test_abstract_numbers": {
      "file": "test/test_abstract_numbers.py", 
      "imports": [
        "math", 
        "numbers", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_aepack": {
      "file": "test/test_aepack.py", 
      "imports": [
        "Carbon.File", 
        "os", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_aifc": {
      "file": "test/test_aifc.py", 
      "imports": [
        "aifc", 
        "io", 
        "os", 
        "struct", 
        "sys", 
        "test.audiotests", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_al": {
      "file": "test/test_al.py", 
      "imports": [
        "test.test_support"
      ]
    }, 
    "test.test_anydbm": {
      "file": "test/test_anydbm.py", 
      "imports": [
        "glob", 
        "os", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_applesingle": {
      "file": "test/test_applesingle.py", 
      "imports": [
        "applesingle", 
        "os", 
        "struct", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_argparse": {
      "file": "test/test_argparse.py", 
      "imports": [
        "StringIO", 
        "argparse", 
        "codecs", 
        "gc", 
        "inspect", 
        "os", 
        "shutil", 
        "stat", 
        "sys", 
        "tempfile", 
        "test.test_support", 
        "textwrap", 
        "unittest"
      ]
    }, 
    "test.test_array": {
      "file": "test/test_array.py", 
      "imports": [
        "array", 
        "cStringIO", 
        "copy", 
        "gc", 
        "sys", 
        "sys.maxsize", 
        "test.test_support", 
        "unittest", 
        "warnings", 
        "weakref", 
        "cPickle"
      ]
    }, 
    "test.test_ascii_formatd": {
      "file": "test/test_ascii_formatd.py", 
      "imports": [
        "ctypes.byref", 
        "ctypes.c_double", 
        "ctypes.create_string_buffer", 
        "ctypes.pythonapi", 
        "ctypes.sizeof", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_ast": {
      "file": "test/test_ast.py", 
      "imports": [
        "ast", 
        "itertools", 
        "pickle", 
        "sys", 
        "test.test_support", 
        "unittest", 
        "cPickle"
      ]
    }, 
    "test.test_asynchat": {
      "file": "test/test_asynchat.py", 
      "imports": [
        "asynchat", 
        "asyncore", 
        "errno", 
        "socket", 
        "sys", 
        "test.test_support", 
        "threading", 
        "time", 
        "unittest"
      ]
    }, 
    "test.test_asyncore": {
      "file": "test/test_asyncore.py", 
      "imports": [
        "StringIO", 
        "asyncore", 
        "errno", 
        "os", 
        "select", 
        "socket", 
        "struct", 
        "sys", 
        "test.test_support", 
        "threading", 
        "time", 
        "unittest", 
        "warnings"
      ]
    }, 
    "test.test_atexit": {
      "file": "test/test_atexit.py", 
      "imports": [
        "StringIO", 
        "atexit", 
        "imp.reload", 
        "sys", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_audioop": {
      "file": "test/test_audioop.py", 
      "imports": [
        "audioop", 
        "struct", 
        "sys", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_augassign": {
      "file": "test/test_augassign.py", 
      "imports": [
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_base64": {
      "file": "test/test_base64.py", 
      "imports": [
        "base64", 
        "cStringIO.StringIO", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_bastion": {
      "file": "test/test_bastion.py", 
      "imports": []
    }, 
    "test.test_bigaddrspace": {
      "file": "test/test_bigaddrspace.py", 
      "imports": [
        "operator", 
        "sys", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_bigmem": {
      "file": "test/test_bigmem.py", 
      "imports": [
        "operator", 
        "string", 
        "sys", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_binascii": {
      "file": "test/test_binascii.py", 
      "imports": [
        "array", 
        "binascii", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_binhex": {
      "file": "test/test_binhex.py", 
      "imports": [
        "binhex", 
        "os", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_binop": {
      "file": "test/test_binop.py", 
      "imports": [
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_bisect": {
      "file": "test/test_bisect.py", 
      "imports": [
        "bisect", 
        "gc", 
        "random", 
        "sys", 
        "test.test_bisect", 
        "test.test_support", 
        "unittest", 
        "UserList"
      ]
    }, 
    "test.test_bool": {
      "file": "test/test_bool.py", 
      "imports": [
        "marshal", 
        "operator", 
        "os", 
        "pickle", 
        "test.test_support", 
        "unittest", 
        "cPickle"
      ]
    }, 
    "test.test_bsddb": {
      "file": "test/test_bsddb.py", 
      "imports": [
        "os", 
        "sys", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_bsddb185": {
      "file": "test/test_bsddb185.py", 
      "imports": [
        "anydbm", 
        "os", 
        "shutil", 
        "tempfile", 
        "test.test_support", 
        "unittest", 
        "whichdb"
      ]
    }, 
    "test.test_bsddb3": {
      "file": "test/test_bsddb3.py", 
      "imports": [
        "bsddb.db", 
        "bsddb.test.test_all", 
        "os", 
        "sys", 
        "tempfile", 
        "test.test_support", 
        "time", 
        "unittest"
      ]
    }, 
    "test.test_buffer": {
      "file": "test/test_buffer.py", 
      "imports": [
        "sys", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_bufio": {
      "file": "test/test_bufio.py", 
      "imports": [
        "_pyio", 
        "io", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_builtin": {
      "file": "test/test_builtin.py", 
      "imports": [
        "cStringIO", 
        "gc", 
        "marshal", 
        "math.sqrt", 
        "operator.neg", 
        "os", 
        "platform", 
        "random", 
        "string", 
        "sys", 
        "test.test_support", 
        "types", 
        "unittest", 
        "UserDict", 
        "UserList", 
        "warnings"
      ]
    }, 
    "test.test_bytes": {
      "file": "test/test_bytes.py", 
      "imports": [
        "copy", 
        "functools", 
        "os", 
        "pickle", 
        "re", 
        "sys", 
        "tempfile", 
        "test.buffer_tests", 
        "test.string_tests", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_bz2": {
      "file": "test/test_bz2.py", 
      "imports": [
        "bz2.BZ2Compressor", 
        "bz2.BZ2Decompressor", 
        "bz2.BZ2File", 
        "cStringIO.StringIO", 
        "os", 
        "subprocess", 
        "sys", 
        "test.test_support", 
        "threading", 
        "unittest"
      ]
    }, 
    "test.test_calendar": {
      "file": "test/test_calendar.py", 
      "imports": [
        "calendar", 
        "locale", 
        "test.test_support", 
        "unittest", 
        "datetime"
      ]
    }, 
    "test.test_call": {
      "file": "test/test_call.py", 
      "imports": [
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_capi": {
      "file": "test/test_capi.py", 
      "imports": [
        "__future__", 
        "random", 
        "sys", 
        "test.test_support", 
        "thread", 
        "threading", 
        "time", 
        "unittest"
      ]
    }, 
    "test.test_cd": {
      "file": "test/test_cd.py", 
      "imports": [
        "test.test_support"
      ]
    }, 
    "test.test_cfgparser": {
      "file": "test/test_cfgparser.py", 
      "imports": [
        "ConfigParser", 
        "StringIO", 
        "os", 
        "pickle", 
        "test.test_support", 
        "unittest", 
        "UserDict"
      ]
    }, 
    "test.test_cgi": {
      "file": "test/test_cgi.py", 
      "imports": [
        "StringIO", 
        "cStringIO.StringIO", 
        "cgi", 
        "collections", 
        "os", 
        "sys", 
        "tempfile", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_charmapcodec": {
      "file": "test/test_charmapcodec.py", 
      "imports": [
        "codecs", 
        "test.test_support", 
        "test.testcodec", 
        "unittest"
      ]
    }, 
    "test.test_cl": {
      "file": "test/test_cl.py", 
      "imports": [
        "test.test_support"
      ]
    }, 
    "test.test_class": {
      "file": "test/test_class.py", 
      "imports": [
        "gc", 
        "test.test_support", 
        "unittest", 
        "_testcapi"
      ]
    }, 
    "test.test_cmath": {
      "file": "test/test_cmath.py", 
      "imports": [
        "cmath", 
        "cmath.phase", 
        "cmath.pi", 
        "cmath.polar", 
        "cmath.rect", 
        "math", 
        "test.test_math", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_cmd": {
      "file": "test/test_cmd.py", 
      "imports": [
        "StringIO", 
        "cmd", 
        "re", 
        "sys", 
        "test.test_cmd", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_cmd_line": {
      "file": "test/test_cmd_line.py", 
      "imports": [
        "sys", 
        "test.script_helper", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_cmd_line_script": {
      "file": "test/test_cmd_line_script.py", 
      "imports": [
        "os", 
        "test.script_helper", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_code": {
      "file": "test/test_code.py", 
      "imports": [
        "test.test_code", 
        "test.test_support", 
        "unittest", 
        "weakref", 
        "_testcapi"
      ]
    }, 
    "test.test_codeccallbacks": {
      "file": "test/test_codeccallbacks.py", 
      "imports": [
        "codecs", 
        "htmlentitydefs", 
        "sys", 
        "test.test_support", 
        "unicodedata", 
        "unittest"
      ]
    }, 
    "test.test_codecencodings_cn": {
      "file": "test/test_codecencodings_cn.py", 
      "imports": [
        "test.test_multibytecodec_support", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_codecencodings_hk": {
      "file": "test/test_codecencodings_hk.py", 
      "imports": [
        "test.test_multibytecodec_support", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_codecencodings_iso2022": {
      "file": "test/test_codecencodings_iso2022.py", 
      "imports": [
        "test.test_multibytecodec_support", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_codecencodings_jp": {
      "file": "test/test_codecencodings_jp.py", 
      "imports": [
        "test.test_multibytecodec_support", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_codecencodings_kr": {
      "file": "test/test_codecencodings_kr.py", 
      "imports": [
        "test.test_multibytecodec_support", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_codecencodings_tw": {
      "file": "test/test_codecencodings_tw.py", 
      "imports": [
        "test.test_multibytecodec_support", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_codecmaps_cn": {
      "file": "test/test_codecmaps_cn.py", 
      "imports": [
        "test.test_multibytecodec_support", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_codecmaps_hk": {
      "file": "test/test_codecmaps_hk.py", 
      "imports": [
        "test.test_multibytecodec_support", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_codecmaps_jp": {
      "file": "test/test_codecmaps_jp.py", 
      "imports": [
        "test.test_multibytecodec_support", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_codecmaps_kr": {
      "file": "test/test_codecmaps_kr.py", 
      "imports": [
        "test.test_multibytecodec_support", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_codecmaps_tw": {
      "file": "test/test_codecmaps_tw.py", 
      "imports": [
        "test.test_multibytecodec_support", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_codecs": {
      "file": "test/test_codecs.py", 
      "imports": [
        "StringIO", 
        "array", 
        "bz2", 
        "codecs", 
        "encodings.cp1140", 
        "encodings.idna", 
        "locale", 
        "sys", 
        "test.test_support", 
        "zlib", 
        "unittest", 
        "_testcapi"
      ]
    }, 
    "test.test_codeop": {
      "file": "test/test_codeop.py", 
      "imports": [
        "cStringIO", 
        "codeop", 
        "sys", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_coding": {
      "file": "test/test_coding.py", 
      "imports": [
        "os", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_coercion": {
      "file": "test/test_coercion.py", 
      "imports": [
        "copy", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_collections": {
      "file": "test/test_collections.py", 
      "imports": [
        "collections", 
        "copy", 
        "doctest", 
        "inspect", 
        "keyword", 
        "operator", 
        "pickle", 
        "random", 
        "re", 
        "string", 
        "sys", 
        "test.mapping_tests", 
        "test.test_support", 
        "unittest", 
        "cPickle"
      ]
    }, 
    "test.test_colorsys": {
      "file": "test/test_colorsys.py", 
      "imports": [
        "colorsys", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_commands": {
      "file": "test/test_commands.py", 
      "imports": [
        "os", 
        "re", 
        "tempfile", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_compare": {
      "file": "test/test_compare.py", 
      "imports": [
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_compile": {
      "file": "test/test_compile.py", 
      "imports": [
        "__builtin__", 
        "__mangled_mod", 
        "__package__.module", 
        "_ast", 
        "math", 
        "sys", 
        "test.test_support", 
        "textwrap", 
        "unittest"
      ]
    }, 
    "test.test_compileall": {
      "file": "test/test_compileall.py", 
      "imports": [
        "compileall", 
        "imp", 
        "os", 
        "py_compile", 
        "shutil", 
        "struct", 
        "tempfile", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_compiler": {
      "file": "test/test_compiler.py", 
      "imports": [
        "StringIO", 
        "compiler.ast", 
        "math.*", 
        "os", 
        "random", 
        "sys", 
        "test.test_support", 
        "time", 
        "unittest"
      ]
    }, 
    "test.test_complex": {
      "file": "test/test_complex.py", 
      "imports": [
        "math.atan2", 
        "math.copysign", 
        "math.isnan", 
        "random", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_complex_args": {
      "file": "test/test_complex_args.py", 
      "imports": [
        "test.test_support", 
        "textwrap", 
        "unittest"
      ]
    }, 
    "test.test_contains": {
      "file": "test/test_contains.py", 
      "imports": [
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_contextlib": {
      "file": "test/test_contextlib.py", 
      "imports": [
        "contextlib", 
        "sys", 
        "tempfile", 
        "test.test_support", 
        "threading", 
        "unittest"
      ]
    }, 
    "test.test_cookie": {
      "file": "test/test_cookie.py", 
      "imports": [
        "Cookie", 
        "pickle", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_cookielib": {
      "file": "test/test_cookielib.py", 
      "imports": [
        "StringIO", 
        "cookielib", 
        "mimetools", 
        "os", 
        "re", 
        "test.test_support", 
        "time", 
        "traceback", 
        "unittest", 
        "urllib2"
      ]
    }, 
    "test.test_copy": {
      "file": "test/test_copy.py", 
      "imports": [
        "copy", 
        "copy_reg", 
        "test.test_support", 
        "unittest", 
        "weakref"
      ]
    }, 
    "test.test_copy_reg": {
      "file": "test/test_copy_reg.py", 
      "imports": [
        "copy", 
        "copy_reg", 
        "test.pickletester", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_cpickle": {
      "file": "test/test_cpickle.py", 
      "imports": [
        "cStringIO", 
        "io", 
        "test.pickletester", 
        "test.test_support", 
        "unittest", 
        "cPickle"
      ]
    }, 
    "test.test_cprofile": {
      "file": "test/test_cprofile.py", 
      "imports": [
        "_lsprof", 
        "cProfile", 
        "sys", 
        "test.test_profile", 
        "test.test_support"
      ]
    }, 
    "test.test_crypt": {
      "file": "test/test_crypt.py", 
      "imports": [
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_csv": {
      "file": "test/test_csv.py", 
      "imports": [
        "StringIO", 
        "array", 
        "csv", 
        "gc", 
        "io", 
        "itertools", 
        "os", 
        "string", 
        "sys", 
        "tempfile", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_ctypes": {
      "file": "test/test_ctypes.py", 
      "imports": [
        "ctypes.test", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_curses": {
      "file": "test/test_curses.py", 
      "imports": [
        "curses.ascii", 
        "os", 
        "sys", 
        "tempfile", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_datetime": {
      "file": "test/test_datetime.py", 
      "imports": [
        "__future__", 
        "_strptime", 
        "pickle", 
        "random", 
        "sys", 
        "test.test_support", 
        "time", 
        "unittest", 
        "cPickle", 
        "datetime"
      ]
    }, 
    "test.test_dbm": {
      "file": "test/test_dbm.py", 
      "imports": [
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_decimal": {
      "file": "test/test_decimal.py", 
      "imports": [
        "copy", 
        "decimal", 
        "locale", 
        "math", 
        "numbers", 
        "operator", 
        "optparse", 
        "os", 
        "pickle", 
        "random", 
        "sys", 
        "test.test_support", 
        "threading", 
        "unittest"
      ]
    }, 
    "test.test_decorators": {
      "file": "test/test_decorators.py", 
      "imports": [
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_defaultdict": {
      "file": "test/test_defaultdict.py", 
      "imports": [
        "collections", 
        "copy", 
        "os", 
        "tempfile", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_deque": {
      "file": "test/test_deque.py", 
      "imports": [
        "collections", 
        "copy", 
        "gc", 
        "random", 
        "struct", 
        "sys", 
        "test.seq_tests", 
        "test.test_deque", 
        "test.test_support", 
        "unittest", 
        "weakref", 
        "cPickle"
      ]
    }, 
    "test.test_descr": {
      "file": "test/test_descr.py", 
      "imports": [
        "__builtin__", 
        "abc", 
        "binascii", 
        "cStringIO", 
        "copy", 
        "gc", 
        "operator", 
        "pickle", 
        "popen2", 
        "sys", 
        "test.test_support", 
        "xxsubtype", 
        "types", 
        "unittest", 
        "weakref", 
        "_testcapi", 
        "cPickle"
      ]
    }, 
    "test.test_descrtut": {
      "file": "test/test_descrtut.py", 
      "imports": [
        "pprint", 
        "test.test_descrtut", 
        "test.test_support"
      ]
    }, 
    "test.test_dict": {
      "file": "test/test_dict.py", 
      "imports": [
        "gc", 
        "random", 
        "string", 
        "test.mapping_tests", 
        "test.test_support", 
        "unittest", 
        "UserDict", 
        "weakref"
      ]
    }, 
    "test.test_dictcomps": {
      "file": "test/test_dictcomps.py", 
      "imports": [
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_dictviews": {
      "file": "test/test_dictviews.py", 
      "imports": [
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_difflib": {
      "file": "test/test_difflib.py", 
      "imports": [
        "difflib", 
        "doctest", 
        "sys", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_dircache": {
      "file": "test/test_dircache.py", 
      "imports": [
        "os", 
        "sys", 
        "tempfile", 
        "test.test_support", 
        "time", 
        "unittest"
      ]
    }, 
    "test.test_dis": {
      "file": "test/test_dis.py", 
      "imports": [
        "StringIO", 
        "difflib", 
        "dis", 
        "sys", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_distutils": {
      "file": "test/test_distutils.py", 
      "imports": [
        "distutils.tests", 
        "test.test_support"
      ]
    }, 
    "test.test_dl": {
      "file": "test/test_dl.py", 
      "imports": [
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_doctest": {
      "file": "test/test_doctest.py", 
      "imports": [
        "doctest", 
        "sys", 
        "test.test_doctest", 
        "test.test_support"
      ]
    }, 
    "test.test_doctest2": {
      "file": "test/test_doctest2.py", 
      "imports": [
        "doctest", 
        "sys", 
        "test.test_doctest2", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_docxmlrpc": {
      "file": "test/test_docxmlrpc.py", 
      "imports": [
        "DocXMLRPCServer", 
        "httplib", 
        "socket", 
        "sys", 
        "test.test_support", 
        "time", 
        "unittest"
      ]
    }, 
    "test.test_dumbdbm": {
      "file": "test/test_dumbdbm.py", 
      "imports": [
        "dumbdbm", 
        "os", 
        "random", 
        "stat", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_dummy_thread": {
      "file": "test/test_dummy_thread.py", 
      "imports": [
        "Queue", 
        "dummy_thread", 
        "random", 
        "test.test_support", 
        "time", 
        "unittest"
      ]
    }, 
    "test.test_dummy_threading": {
      "file": "test/test_dummy_threading.py", 
      "imports": [
        "dummy_threading", 
        "test.test_support", 
        "time", 
        "unittest"
      ]
    }, 
    "test.test_email": {
      "file": "test/test_email.py", 
      "imports": [
        "email.test.test_email", 
        "email.test.test_email_renamed", 
        "test.test_support"
      ]
    }, 
    "test.test_email_codecs": {
      "file": "test/test_email_codecs.py", 
      "imports": [
        "email.test.test_email_codecs", 
        "email.test.test_email_codecs_renamed", 
        "test.test_support"
      ]
    }, 
    "test.test_email_renamed": {
      "file": "test/test_email_renamed.py", 
      "imports": [
        "email.test.test_email_renamed", 
        "test.test_support"
      ]
    }, 
    "test.test_ensurepip": {
      "file": "test/test_ensurepip.py", 
      "imports": [
        "contextlib", 
        "ensurepip", 
        "ensurepip._uninstall", 
        "os", 
        "ssl", 
        "sys", 
        "test._mock_backport", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_enumerate": {
      "file": "test/test_enumerate.py", 
      "imports": [
        "sys", 
        "test.test_iterlen", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_eof": {
      "file": "test/test_eof.py", 
      "imports": [
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_epoll": {
      "file": "test/test_epoll.py", 
      "imports": [
        "errno", 
        "select", 
        "socket", 
        "test.test_support", 
        "time", 
        "unittest"
      ]
    }, 
    "test.test_errno": {
      "file": "test/test_errno.py", 
      "imports": [
        "errno", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_exception_variations": {
      "file": "test/test_exception_variations.py", 
      "imports": [
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_exceptions": {
      "file": "test/test_exceptions.py", 
      "imports": [
        "exceptions", 
        "imp.reload", 
        "os", 
        "pickle", 
        "sys", 
        "test.test_pep352", 
        "test.test_support", 
        "unittest", 
        "_testcapi", 
        "cPickle"
      ]
    }, 
    "test.test_extcall": {
      "file": "test/test_extcall.py", 
      "imports": [
        "sys", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_fcntl": {
      "file": "test/test_fcntl.py", 
      "imports": [
        "os", 
        "struct", 
        "sys", 
        "test.test_support", 
        "unittest", 
        "_testcapi"
      ]
    }, 
    "test.test_file": {
      "file": "test/test_file.py", 
      "imports": [
        "__future__", 
        "_pyio", 
        "array.array", 
        "io", 
        "os", 
        "sys", 
        "test.test_support", 
        "unittest", 
        "UserList", 
        "weakref"
      ]
    }, 
    "test.test_file2k": {
      "file": "test/test_file2k.py", 
      "imports": [
        "array.array", 
        "itertools", 
        "os", 
        "select", 
        "signal", 
        "subprocess", 
        "sys", 
        "test.test_support", 
        "threading", 
        "time", 
        "unittest", 
        "UserList", 
        "weakref"
      ]
    }, 
    "test.test_file_eintr": {
      "file": "test/test_file_eintr.py", 
      "imports": [
        "_io.FileIO", 
        "os", 
        "select", 
        "signal", 
        "subprocess", 
        "sys", 
        "test.test_support", 
        "time", 
        "unittest"
      ]
    }, 
    "test.test_filecmp": {
      "file": "test/test_filecmp.py", 
      "imports": [
        "filecmp", 
        "os", 
        "shutil", 
        "tempfile", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_fileinput": {
      "file": "test/test_fileinput.py", 
      "imports": [
        "StringIO", 
        "fileinput", 
        "re", 
        "sys", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_fileio": {
      "file": "test/test_fileio.py", 
      "imports": [
        "__future__", 
        "_io.FileIO", 
        "array.array", 
        "errno", 
        "functools", 
        "msvcrt", 
        "os", 
        "sys", 
        "test.script_helper", 
        "test.test_support", 
        "unittest", 
        "UserList", 
        "weakref", 
        "_testcapi"
      ]
    }, 
    "test.test_float": {
      "file": "test/test_float.py", 
      "imports": [
        "fractions", 
        "locale", 
        "math", 
        "math.copysign", 
        "math.isinf", 
        "math.isnan", 
        "math.ldexp", 
        "operator", 
        "os", 
        "random", 
        "struct", 
        "sys", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_fnmatch": {
      "file": "test/test_fnmatch.py", 
      "imports": [
        "fnmatch", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_fork1": {
      "file": "test/test_fork1.py", 
      "imports": [
        "imp", 
        "os", 
        "signal", 
        "sys", 
        "test.fork_wait", 
        "test.test_support", 
        "time"
      ]
    }, 
    "test.test_format": {
      "file": "test/test_format.py", 
      "imports": [
        "sys", 
        "test.test_support", 
        "unittest", 
        "_testcapi"
      ]
    }, 
    "test.test_fpformat": {
      "file": "test/test_fpformat.py", 
      "imports": [
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_fractions": {
      "file": "test/test_fractions.py", 
      "imports": [
        "copy", 
        "decimal", 
        "fractions", 
        "math", 
        "numbers", 
        "operator", 
        "sys", 
        "test.test_support", 
        "unittest", 
        "cPickle"
      ]
    }, 
    "test.test_frozen": {
      "file": "test/test_frozen.py", 
      "imports": [
        "__hello__", 
        "__phello__", 
        "__phello__.foo", 
        "__phello__.spam", 
        "sys", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_ftplib": {
      "file": "test/test_ftplib.py", 
      "imports": [
        "StringIO", 
        "asynchat", 
        "asyncore", 
        "errno", 
        "ftplib", 
        "os", 
        "socket", 
        "ssl", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_funcattrs": {
      "file": "test/test_funcattrs.py", 
      "imports": [
        "test.test_support", 
        "types", 
        "unittest", 
        "UserDict"
      ]
    }, 
    "test.test_functools": {
      "file": "test/test_functools.py", 
      "imports": [
        "functools", 
        "gc", 
        "pickle", 
        "sys", 
        "test.test_support", 
        "unittest", 
        "weakref"
      ]
    }, 
    "test.test_future": {
      "file": "test/test_future.py", 
      "imports": [
        "re", 
        "test.badsyntax_future3", 
        "test.badsyntax_future4", 
        "test.badsyntax_future5", 
        "test.badsyntax_future6", 
        "test.badsyntax_future7", 
        "test.badsyntax_future8", 
        "test.badsyntax_future9", 
        "test.test_future1", 
        "test.test_future2", 
        "test.test_future3", 
        "test.test_future5", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_future1": {
      "file": "test/test_future1.py", 
      "imports": [
        "__future__"
      ]
    }, 
    "test.test_future2": {
      "file": "test/test_future2.py", 
      "imports": [
        "__future__", 
        "string"
      ]
    }, 
    "test.test_future3": {
      "file": "test/test_future3.py", 
      "imports": [
        "__future__", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_future4": {
      "file": "test/test_future4.py", 
      "imports": [
        "__future__", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_future5": {
      "file": "test/test_future5.py", 
      "imports": [
        "__future__", 
        "sys", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_future_builtins": {
      "file": "test/test_future_builtins.py", 
      "imports": [
        "itertools.ifilter", 
        "itertools.imap", 
        "itertools.izip", 
        "test.test_support", 
        "unittest", 
        "future_builtins"
      ]
    }, 
    "test.test_gc": {
      "file": "test/test_gc.py", 
      "imports": [
        "gc", 
        "sys", 
        "test.test_support", 
        "threading", 
        "time", 
        "unittest", 
        "weakref"
      ]
    }, 
    "test.test_gdb": {
      "file": "test/test_gdb.py", 
      "imports": [
        "os", 
        "re", 
        "subprocess", 
        "sys", 
        "sysconfig", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_gdbm": {
      "file": "test/test_gdbm.py", 
      "imports": [
        "os", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_generators": {
      "file": "test/test_generators.py", 
      "imports": [
        "test.test_generators", 
        "test.test_support"
      ]
    }, 
    "test.test_genericpath": {
      "file": "test/test_genericpath.py", 
      "imports": [
        "genericpath", 
        "os", 
        "sys", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_genexps": {
      "file": "test/test_genexps.py", 
      "imports": [
        "gc", 
        "sys", 
        "test.test_genexps", 
        "test.test_support"
      ]
    }, 
    "test.test_getargs": {
      "file": "test/test_getargs.py", 
      "imports": [
        "marshal", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_getargs2": {
      "file": "test/test_getargs2.py", 
      "imports": [
        "test.test_support", 
        "unittest", 
        "warnings", 
        "_testcapi"
      ]
    }, 
    "test.test_getopt": {
      "file": "test/test_getopt.py", 
      "imports": [
        "getopt", 
        "test.test_support", 
        "types", 
        "unittest"
      ]
    }, 
    "test.test_gettext": {
      "file": "test/test_gettext.py", 
      "imports": [
        "__builtin__", 
        "base64", 
        "gettext", 
        "os", 
        "shutil", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_gl": {
      "file": "test/test_gl.py", 
      "imports": [
        "os", 
        "test.test_support", 
        "time", 
        "unittest"
      ]
    }, 
    "test.test_glob": {
      "file": "test/test_glob.py", 
      "imports": [
        "glob", 
        "os", 
        "shutil", 
        "sys", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_global": {
      "file": "test/test_global.py", 
      "imports": [
        "test.test_support", 
        "unittest", 
        "warnings"
      ]
    }, 
    "test.test_grammar": {
      "file": "test/test_grammar.py", 
      "imports": [
        "StringIO", 
        "sys", 
        "sys.*", 
        "sys.argv", 
        "sys.maxint", 
        "sys.path", 
        "test.test_support", 
        "time", 
        "time.time", 
        "types", 
        "unittest"
      ]
    }, 
    "test.test_grp": {
      "file": "test/test_grp.py", 
      "imports": [
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_gzip": {
      "file": "test/test_gzip.py", 
      "imports": [
        "io", 
        "os", 
        "struct", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_hash": {
      "file": "test/test_hash.py", 
      "imports": [
        "collections", 
        "os", 
        "struct", 
        "subprocess", 
        "sys", 
        "test.test_support", 
        "unittest", 
        "datetime"
      ]
    }, 
    "test.test_hashlib": {
      "file": "test/test_hashlib.py", 
      "imports": [
        "_md5", 
        "array", 
        "binascii.unhexlify", 
        "hashlib", 
        "itertools", 
        "string", 
        "sys", 
        "test.test_support", 
        "threading", 
        "unittest", 
        "warnings"
      ]
    }, 
    "test.test_heapq": {
      "file": "test/test_heapq.py", 
      "imports": [
        "gc", 
        "itertools.chain", 
        "itertools.imap", 
        "random", 
        "sys", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_hmac": {
      "file": "test/test_hmac.py", 
      "imports": [
        "hashlib", 
        "hmac", 
        "test.test_support", 
        "unittest", 
        "warnings"
      ]
    }, 
    "test.test_hotshot": {
      "file": "test/test_hotshot.py", 
      "imports": [
        "_hotshot", 
        "gc", 
        "hotshot.log.ENTER", 
        "hotshot.log.EXIT", 
        "hotshot.log.LINE", 
        "hotshot.stats", 
        "os", 
        "pprint", 
        "sys", 
        "tempfile", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_htmllib": {
      "file": "test/test_htmllib.py", 
      "imports": [
        "formatter", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_htmlparser": {
      "file": "test/test_htmlparser.py", 
      "imports": [
        "HTMLParser", 
        "pprint", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_httplib": {
      "file": "test/test_httplib.py", 
      "imports": [
        "StringIO", 
        "array", 
        "errno", 
        "httplib", 
        "os", 
        "socket", 
        "ssl", 
        "test.ssl_servers", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_httpservers": {
      "file": "test/test_httpservers.py", 
      "imports": [
        "BaseHTTPServer", 
        "CGIHTTPServer", 
        "SimpleHTTPServer", 
        "StringIO", 
        "base64", 
        "httplib", 
        "os", 
        "re", 
        "shutil", 
        "sys", 
        "tempfile", 
        "test.test_support", 
        "unittest", 
        "urllib"
      ]
    }, 
    "test.test_idle": {
      "file": "test/test_idle.py", 
      "imports": [
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_imageop": {
      "file": "test/test_imageop.py", 
      "imports": [
        "imgfile", 
        "os", 
        "sys", 
        "test.test_support", 
        "unittest", 
        "uu"
      ]
    }, 
    "test.test_imaplib": {
      "file": "test/test_imaplib.py", 
      "imports": [
        "SocketServer", 
        "contextlib", 
        "imaplib", 
        "os", 
        "ssl", 
        "test.test_support", 
        "time", 
        "unittest"
      ]
    }, 
    "test.test_imgfile": {
      "file": "test/test_imgfile.py", 
      "imports": [
        "os", 
        "sys", 
        "test.test_support", 
        "uu"
      ]
    }, 
    "test.test_imghdr": {
      "file": "test/test_imghdr.py", 
      "imports": [
        "imghdr", 
        "io", 
        "sys", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_imp": {
      "file": "test/test_imp.py", 
      "imports": [
        "imp", 
        "marshal", 
        "os", 
        "test.test_support", 
        "thread", 
        "time", 
        "unittest"
      ]
    }, 
    "test.test_import": {
      "file": "test/test_import.py", 
      "imports": [
        "RAnDoM", 
        "errno", 
        "imp", 
        "marshal", 
        "os", 
        "py_compile", 
        "random", 
        "shutil", 
        "socket", 
        "stat", 
        "struct", 
        "sys", 
        "test", 
        "test.double_const", 
        "test.infinite_reload", 
        "test.relimport", 
        "test.script_helper", 
        "test.symlink_support", 
        "test.test_import", 
        "test.test_support", 
        "textwrap", 
        "unittest"
      ]
    }, 
    "test.test_importhooks": {
      "file": "test/test_importhooks.py", 
      "imports": [
        "hooktestmodule", 
        "hooktestpackage", 
        "hooktestpackage.futrel", 
        "hooktestpackage.newabs", 
        "hooktestpackage.newrel", 
        "hooktestpackage.oldabs", 
        "hooktestpackage.sub", 
        "hooktestpackage.sub.subber", 
        "hooktestpackage.sub.subber.subest", 
        "imp", 
        "os", 
        "reloadmodule", 
        "sub", 
        "sys", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_importlib": {
      "file": "test/test_importlib.py", 
      "imports": [
        "contextlib", 
        "imp", 
        "importlib", 
        "sys", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_index": {
      "file": "test/test_index.py", 
      "imports": [
        "operator", 
        "sys.maxint", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_inspect": {
      "file": "test/test_inspect.py", 
      "imports": [
        "__builtin__", 
        "abc", 
        "inspect", 
        "linecache", 
        "re", 
        "sys", 
        "test.inspect_fodder", 
        "test.inspect_fodder2", 
        "test.test_support", 
        "unicodedata", 
        "types", 
        "unittest", 
        "UserDict", 
        "UserList"
      ]
    }, 
    "test.test_int": {
      "file": "test/test_int.py", 
      "imports": [
        "math", 
        "sys", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_int_literal": {
      "file": "test/test_int_literal.py", 
      "imports": [
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_io": {
      "file": "test/test_io.py", 
      "imports": [
        "__future__", 
        "_pyio", 
        "abc", 
        "array", 
        "codecs", 
        "collections", 
        "contextlib", 
        "errno", 
        "fcntl", 
        "io", 
        "itertools.count", 
        "itertools.cycle", 
        "os", 
        "random", 
        "signal", 
        "sys", 
        "test.test_support", 
        "threading", 
        "time", 
        "unittest", 
        "UserList", 
        "warnings", 
        "weakref"
      ]
    }, 
    "test.test_ioctl": {
      "file": "test/test_ioctl.py", 
      "imports": [
        "array", 
        "os", 
        "pty", 
        "struct", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_isinstance": {
      "file": "test/test_isinstance.py", 
      "imports": [
        "sys", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_iter": {
      "file": "test/test_iter.py", 
      "imports": [
        "operator.add", 
        "operator.countOf", 
        "operator.indexOf", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_iterlen": {
      "file": "test/test_iterlen.py", 
      "imports": [
        "__builtin__.len", 
        "collections", 
        "itertools.repeat", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_itertools": {
      "file": "test/test_itertools.py", 
      "imports": [
        "copy", 
        "decimal", 
        "fractions", 
        "functools", 
        "gc", 
        "itertools.*", 
        "operator", 
        "pickle", 
        "random", 
        "sys", 
        "test.test_iterlen", 
        "test.test_support", 
        "unittest", 
        "weakref"
      ]
    }, 
    "test.test_json": {
      "file": "test/test_json.py", 
      "imports": [
        "json.tests", 
        "test.test_support"
      ]
    }, 
    "test.test_kqueue": {
      "file": "test/test_kqueue.py", 
      "imports": [
        "errno", 
        "select", 
        "socket", 
        "sys", 
        "test.test_support", 
        "time", 
        "unittest"
      ]
    }, 
    "test.test_largefile": {
      "file": "test/test_largefile.py", 
      "imports": [
        "__future__", 
        "_pyio", 
        "io", 
        "os", 
        "signal", 
        "stat", 
        "sys", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_lib2to3": {
      "file": "test/test_lib2to3.py", 
      "imports": [
        "lib2to3.tests.test_fixers", 
        "lib2to3.tests.test_main", 
        "lib2to3.tests.test_parser", 
        "lib2to3.tests.test_pytree", 
        "lib2to3.tests.test_refactor", 
        "lib2to3.tests.test_util", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_linecache": {
      "file": "test/test_linecache.py", 
      "imports": [
        "linecache", 
        "os", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_linuxaudiodev": {
      "file": "test/test_linuxaudiodev.py", 
      "imports": [
        "audioop", 
        "errno", 
        "sys", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_list": {
      "file": "test/test_list.py", 
      "imports": [
        "gc", 
        "sys", 
        "test.list_tests", 
        "test.test_support"
      ]
    }, 
    "test.test_locale": {
      "file": "test/test_locale.py", 
      "imports": [
        "codecs", 
        "locale", 
        "os", 
        "sys", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_logging": {
      "file": "test/test_logging.py", 
      "imports": [
        "SocketServer", 
        "cStringIO", 
        "codecs", 
        "gc", 
        "json", 
        "logging", 
        "logging.config", 
        "logging.handlers", 
        "os", 
        "random", 
        "re", 
        "select", 
        "socket", 
        "struct", 
        "sys", 
        "tempfile", 
        "test.test_support", 
        "threading", 
        "time", 
        "textwrap", 
        "unittest", 
        "warnings", 
        "weakref", 
        "cPickle"
      ]
    }, 
    "test.test_long": {
      "file": "test/test_long.py", 
      "imports": [
        "math", 
        "random", 
        "sys", 
        "test.test_int", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_long_future": {
      "file": "test/test_long_future.py", 
      "imports": [
        "__future__", 
        "math", 
        "random", 
        "sys", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_longexp": {
      "file": "test/test_longexp.py", 
      "imports": [
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_macos": {
      "file": "test/test_macos.py", 
      "imports": [
        "os", 
        "subprocess", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_macostools": {
      "file": "test/test_macostools.py", 
      "imports": [
        "Carbon.File", 
        "macostools", 
        "os", 
        "sys", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_macpath": {
      "file": "test/test_macpath.py", 
      "imports": [
        "macpath", 
        "test.test_genericpath", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_macurl2path": {
      "file": "test/test_macurl2path.py", 
      "imports": [
        "macurl2path", 
        "unittest"
      ]
    }, 
    "test.test_mailbox": {
      "file": "test/test_mailbox.py", 
      "imports": [
        "StringIO", 
        "email", 
        "email.message", 
        "email.parser", 
        "fcntl", 
        "glob", 
        "mailbox", 
        "os", 
        "re", 
        "shutil", 
        "socket", 
        "stat", 
        "sys", 
        "tempfile", 
        "test.test_support", 
        "time", 
        "unittest"
      ]
    }, 
    "test.test_marshal": {
      "file": "test/test_marshal.py", 
      "imports": [
        "marshal", 
        "os", 
        "sys", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_math": {
      "file": "test/test_math.py", 
      "imports": [
        "doctest", 
        "math", 
        "os", 
        "random", 
        "struct", 
        "sys", 
        "sys.float_info", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_md5": {
      "file": "test/test_md5.py", 
      "imports": [
        "md5", 
        "string", 
        "test.test_support", 
        "unittest", 
        "warnings"
      ]
    }, 
    "test.test_memoryio": {
      "file": "test/test_memoryio.py", 
      "imports": [
        "__future__", 
        "__main__", 
        "_pyio", 
        "array", 
        "io", 
        "pickle", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_memoryview": {
      "file": "test/test_memoryview.py", 
      "imports": [
        "array", 
        "gc", 
        "io", 
        "sys", 
        "test.test_support", 
        "unittest", 
        "weakref"
      ]
    }, 
    "test.test_mhlib": {
      "file": "test/test_mhlib.py", 
      "imports": [
        "StringIO", 
        "os", 
        "sys", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_mimetools": {
      "file": "test/test_mimetools.py", 
      "imports": [
        "StringIO", 
        "string", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_mimetypes": {
      "file": "test/test_mimetypes.py", 
      "imports": [
        "StringIO", 
        "_winreg", 
        "mimetypes", 
        "sys", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_minidom": {
      "file": "test/test_minidom.py", 
      "imports": [
        "StringIO", 
        "pickle", 
        "test.test_support", 
        "unittest", 
        "xml.parsers.expat", 
        "xml.dom.pulldom", 
        "xml.dom.minidom", 
        "xml.dom"
      ]
    }, 
    "test.test_mmap": {
      "file": "test/test_mmap.py", 
      "imports": [
        "itertools", 
        "os", 
        "re", 
        "socket", 
        "sys", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_module": {
      "file": "test/test_module.py", 
      "imports": [
        "sys", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_modulefinder": {
      "file": "test/test_modulefinder.py", 
      "imports": [
        "__future__", 
        "distutils.dir_util", 
        "modulefinder", 
        "os", 
        "sets", 
        "tempfile", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_msilib": {
      "file": "test/test_msilib.py", 
      "imports": [
        "os", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_multibytecodec": {
      "file": "test/test_multibytecodec.py", 
      "imports": [
        "StringIO", 
        "_multibytecodec", 
        "codecs", 
        "os", 
        "sys", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_multibytecodec_support": {
      "file": "test/test_multibytecodec_support.py", 
      "imports": [
        "StringIO", 
        "codecs", 
        "htmlentitydefs", 
        "httplib", 
        "os", 
        "re", 
        "sys", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_multifile": {
      "file": "test/test_multifile.py", 
      "imports": [
        "cStringIO", 
        "test.test_support"
      ]
    }, 
    "test.test_multiprocessing": {
      "file": "test/test_multiprocessing.py", 
      "imports": [
        "Queue", 
        "StringIO", 
        "array", 
        "ctypes.Structure", 
        "ctypes.c_double", 
        "ctypes.c_int", 
        "errno", 
        "gc", 
        "json", 
        "logging", 
        "msvcrt", 
        "multiprocessing.connection", 
        "multiprocessing.dummy", 
        "multiprocessing.forking", 
        "multiprocessing.heap", 
        "multiprocessing.managers", 
        "multiprocessing.managers.BaseManager", 
        "multiprocessing.managers.BaseProxy", 
        "multiprocessing.managers.RemoteError", 
        "multiprocessing.pool", 
        "multiprocessing.pool.MaybeEncodingError", 
        "multiprocessing.reduction", 
        "multiprocessing.sharedctypes.Value", 
        "multiprocessing.sharedctypes.copy", 
        "multiprocessing.util", 
        "os", 
        "random", 
        "signal", 
        "socket", 
        "subprocess", 
        "sys", 
        "test.script_helper", 
        "test.test_support", 
        "threading", 
        "time", 
        "unittest"
      ]
    }, 
    "test.test_mutants": {
      "file": "test/test_mutants.py", 
      "imports": [
        "os", 
        "random", 
        "test.test_support"
      ]
    }, 
    "test.test_mutex": {
      "file": "test/test_mutex.py", 
      "imports": [
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_netrc": {
      "file": "test/test_netrc.py", 
      "imports": [
        "netrc", 
        "os", 
        "sys", 
        "test.test_support", 
        "textwrap", 
        "unittest"
      ]
    }, 
    "test.test_new": {
      "file": "test/test_new.py", 
      "imports": [
        "Spam", 
        "__builtin__", 
        "sys", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_nis": {
      "file": "test/test_nis.py", 
      "imports": [
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_nntplib": {
      "file": "test/test_nntplib.py", 
      "imports": [
        "nntplib", 
        "socket", 
        "test.test_support", 
        "threading", 
        "time", 
        "unittest"
      ]
    }, 
    "test.test_normalization": {
      "file": "test/test_normalization.py", 
      "imports": [
        "httplib", 
        "os", 
        "sys", 
        "test.test_support", 
        "unicodedata.normalize", 
        "unicodedata.unidata_version", 
        "unittest"
      ]
    }, 
    "test.test_ntpath": {
      "file": "test/test_ntpath.py", 
      "imports": [
        "nt", 
        "ntpath", 
        "os", 
        "sys", 
        "test.test_genericpath", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_old_mailbox": {
      "file": "test/test_old_mailbox.py", 
      "imports": [
        "email.parser", 
        "mailbox", 
        "os", 
        "test.test_support", 
        "time", 
        "unittest"
      ]
    }, 
    "test.test_opcodes": {
      "file": "test/test_opcodes.py", 
      "imports": [
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_openpty": {
      "file": "test/test_openpty.py", 
      "imports": [
        "os", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_operator": {
      "file": "test/test_operator.py", 
      "imports": [
        "gc", 
        "operator", 
        "sys", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_optparse": {
      "file": "test/test_optparse.py", 
      "imports": [
        "StringIO", 
        "copy", 
        "optparse", 
        "os", 
        "re", 
        "sys", 
        "test.test_support", 
        "types", 
        "unittest"
      ]
    }, 
    "test.test_os": {
      "file": "test/test_os.py", 
      "imports": [
        "ctypes", 
        "ctypes.wintypes", 
        "errno", 
        "mmap", 
        "msvcrt", 
        "os", 
        "signal", 
        "stat", 
        "subprocess", 
        "sys", 
        "test.mapping_tests", 
        "test.script_helper", 
        "test.test_support", 
        "time", 
        "unittest", 
        "uuid", 
        "warnings", 
        "resource"
      ]
    }, 
    "test.test_ossaudiodev": {
      "file": "test/test_ossaudiodev.py", 
      "imports": [
        "audioop", 
        "errno", 
        "ossaudiodev.AFMT_S16_NE", 
        "sunau", 
        "sys", 
        "test.test_support", 
        "time", 
        "unittest"
      ]
    }, 
    "test.test_parser": {
      "file": "test/test_parser.py", 
      "imports": [
        "parser", 
        "struct", 
        "sys", 
        "test.script_helper", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_pdb": {
      "file": "test/test_pdb.py", 
      "imports": [
        "imp", 
        "os", 
        "subprocess", 
        "sys", 
        "test.test_doctest", 
        "test.test_pdb", 
        "test.test_support", 
        "textwrap", 
        "unittest"
      ]
    }, 
    "test.test_peepholer": {
      "file": "test/test_peepholer.py", 
      "imports": [
        "cStringIO.StringIO", 
        "dis", 
        "gc", 
        "sys", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_pep247": {
      "file": "test/test_pep247.py", 
      "imports": [
        "hmac", 
        "md5", 
        "sha", 
        "test.test_support", 
        "unittest", 
        "warnings"
      ]
    }, 
    "test.test_pep263": {
      "file": "test/test_pep263.py", 
      "imports": [
        "test.bad_coding3", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_pep277": {
      "file": "test/test_pep277.py", 
      "imports": [
        "os", 
        "sys", 
        "test.test_support", 
        "unicodedata.normalize", 
        "unittest"
      ]
    }, 
    "test.test_pep292": {
      "file": "test/test_pep292.py", 
      "imports": [
        "string", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_pep352": {
      "file": "test/test_pep352.py", 
      "imports": [
        "__builtin__", 
        "exceptions", 
        "os", 
        "platform", 
        "sys", 
        "test.test_support", 
        "unittest", 
        "warnings"
      ]
    }, 
    "test.test_pickle": {
      "file": "test/test_pickle.py", 
      "imports": [
        "cStringIO.StringIO", 
        "pickle", 
        "test.pickletester", 
        "test.test_support"
      ]
    }, 
    "test.test_pickletools": {
      "file": "test/test_pickletools.py", 
      "imports": [
        "pickle", 
        "pickletools", 
        "test.pickletester", 
        "test.test_support"
      ]
    }, 
    "test.test_pipes": {
      "file": "test/test_pipes.py", 
      "imports": [
        "os", 
        "pipes", 
        "string", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_pkg": {
      "file": "test/test_pkg.py", 
      "imports": [
        "os", 
        "sys", 
        "t1", 
        "t2.sub", 
        "t2.sub.subsub", 
        "t2.sub.subsub.spam", 
        "t3.sub.subsub", 
        "t5", 
        "t6", 
        "t7", 
        "t7.sub", 
        "t7.sub.subsub", 
        "t7.sub.subsub.spam", 
        "t8", 
        "tempfile", 
        "test.test_support", 
        "textwrap", 
        "unittest"
      ]
    }, 
    "test.test_pkgimport": {
      "file": "test/test_pkgimport.py", 
      "imports": [
        "os", 
        "random", 
        "string", 
        "sys", 
        "tempfile", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_pkgutil": {
      "file": "test/test_pkgutil.py", 
      "imports": [
        "foo", 
        "imp", 
        "os", 
        "pkgutil", 
        "shutil", 
        "sys", 
        "tempfile", 
        "test.test_support", 
        "zipimport", 
        "unittest", 
        "zipfile"
      ]
    }, 
    "test.test_platform": {
      "file": "test/test_platform.py", 
      "imports": [
        "gestalt", 
        "os", 
        "platform", 
        "subprocess", 
        "sys", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_plistlib": {
      "file": "test/test_plistlib.py", 
      "imports": [
        "StringIO", 
        "cStringIO.StringIO", 
        "os", 
        "plistlib", 
        "test.test_support", 
        "unittest", 
        "datetime"
      ]
    }, 
    "test.test_poll": {
      "file": "test/test_poll.py", 
      "imports": [
        "os", 
        "random", 
        "select", 
        "test.test_support", 
        "threading", 
        "time", 
        "unittest", 
        "_testcapi"
      ]
    }, 
    "test.test_popen": {
      "file": "test/test_popen.py", 
      "imports": [
        "os", 
        "sys", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_popen2": {
      "file": "test/test_popen2.py", 
      "imports": [
        "os", 
        "popen2", 
        "subprocess", 
        "sys", 
        "test.test_support", 
        "unittest", 
        "warnings"
      ]
    }, 
    "test.test_poplib": {
      "file": "test/test_poplib.py", 
      "imports": [
        "asynchat", 
        "asyncore", 
        "errno", 
        "os", 
        "poplib", 
        "socket", 
        "ssl", 
        "test.test_support", 
        "time", 
        "unittest"
      ]
    }, 
    "test.test_posix": {
      "file": "test/test_posix.py", 
      "imports": [
        "errno", 
        "os", 
        "platform", 
        "shutil", 
        "stat", 
        "sys", 
        "sysconfig", 
        "tempfile", 
        "test.test_support", 
        "time", 
        "unittest", 
        "warnings", 
        "pwd"
      ]
    }, 
    "test.test_posixpath": {
      "file": "test/test_posixpath.py", 
      "imports": [
        "os", 
        "posixpath", 
        "test.test_genericpath", 
        "test.test_support", 
        "unittest", 
        "pwd"
      ]
    }, 
    "test.test_pow": {
      "file": "test/test_pow.py", 
      "imports": [
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_pprint": {
      "file": "test/test_pprint.py", 
      "imports": [
        "pprint", 
        "test.test_set", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_print": {
      "file": "test/test_print.py", 
      "imports": [
        "StringIO", 
        "__future__", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_profile": {
      "file": "test/test_profile.py", 
      "imports": [
        "StringIO", 
        "profile", 
        "pstats", 
        "sys", 
        "test.profilee", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_property": {
      "file": "test/test_property.py", 
      "imports": [
        "sys", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_pstats": {
      "file": "test/test_pstats.py", 
      "imports": [
        "pstats", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_pty": {
      "file": "test/test_pty.py", 
      "imports": [
        "errno", 
        "os", 
        "pty", 
        "select", 
        "signal", 
        "socket", 
        "sys", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_pwd": {
      "file": "test/test_pwd.py", 
      "imports": [
        "sys", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_py3kwarn": {
      "file": "test/test_py3kwarn.py", 
      "imports": [
        "operator.add", 
        "operator.isCallable", 
        "operator.sequenceIncludes", 
        "sys", 
        "test.test_support", 
        "unittest", 
        "UserString", 
        "warnings"
      ]
    }, 
    "test.test_py_compile": {
      "file": "test/test_py_compile.py", 
      "imports": [
        "imp", 
        "os", 
        "py_compile", 
        "shutil", 
        "tempfile", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_pyclbr": {
      "file": "test/test_pyclbr.py", 
      "imports": [
        "commands", 
        "pyclbr", 
        "sys", 
        "test.test_support", 
        "types", 
        "unittest"
      ]
    }, 
    "test.test_pydoc": {
      "file": "test/test_pydoc.py", 
      "imports": [
        "__builtin__", 
        "collections", 
        "contextlib", 
        "difflib", 
        "inspect", 
        "keyword", 
        "nturl2path", 
        "os", 
        "pkgutil", 
        "pydoc", 
        "re", 
        "sys", 
        "test.pydoc_mod", 
        "test.pydocfodder", 
        "test.script_helper", 
        "test.test_support", 
        "types", 
        "unittest", 
        "xml.etree"
      ]
    }, 
    "test.test_pyexpat": {
      "file": "test/test_pyexpat.py", 
      "imports": [
        "StringIO", 
        "sys", 
        "test.test_support", 
        "unittest", 
        "xml.parsers.expat"
      ]
    }, 
    "test.test_queue": {
      "file": "test/test_queue.py", 
      "imports": [
        "Queue", 
        "test.test_support", 
        "time", 
        "unittest"
      ]
    }, 
    "test.test_quopri": {
      "file": "test/test_quopri.py", 
      "imports": [
        "cStringIO", 
        "quopri", 
        "subprocess", 
        "sys", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_random": {
      "file": "test/test_random.py", 
      "imports": [
        "functools", 
        "math.exp", 
        "math.fsum", 
        "math.ldexp", 
        "math.log", 
        "math.pi", 
        "math.sin", 
        "pickle", 
        "random", 
        "sys", 
        "test.test_support", 
        "time", 
        "unittest", 
        "warnings"
      ]
    }, 
    "test.test_re": {
      "file": "test/test_re.py", 
      "imports": [
        "_sre", 
        "_sre.MAXREPEAT", 
        "array", 
        "locale", 
        "pickle", 
        "re", 
        "sre", 
        "sre_constants", 
        "string", 
        "sys", 
        "test.re_tests", 
        "test.test_support", 
        "traceback", 
        "unittest", 
        "weakref", 
        "cPickle"
      ]
    }, 
    "test.test_readline": {
      "file": "test/test_readline.py", 
      "imports": [
        "os", 
        "test.script_helper", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_repr": {
      "file": "test/test_repr.py", 
      "imports": [
        "areallylongpackageandmodulenametotestreprtruncation.areallylongpackageandmodulenametotestreprtruncation.areallylongpackageandmodulenametotestreprtruncation", 
        "areallylongpackageandmodulenametotestreprtruncation.areallylongpackageandmodulenametotestreprtruncation.bar", 
        "areallylongpackageandmodulenametotestreprtruncation.areallylongpackageandmodulenametotestreprtruncation.baz", 
        "areallylongpackageandmodulenametotestreprtruncation.areallylongpackageandmodulenametotestreprtruncation.foo", 
        "areallylongpackageandmodulenametotestreprtruncation.areallylongpackageandmodulenametotestreprtruncation.qux", 
        "array.array", 
        "collections", 
        "os", 
        "repr", 
        "shutil", 
        "sys", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_resource": {
      "file": "test/test_resource.py", 
      "imports": [
        "test.test_support", 
        "time", 
        "unittest"
      ]
    }, 
    "test.test_rfc822": {
      "file": "test/test_rfc822.py", 
      "imports": [
        "StringIO", 
        "cStringIO.StringIO", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_richcmp": {
      "file": "test/test_richcmp.py", 
      "imports": [
        "operator", 
        "random", 
        "test.test_support", 
        "unittest", 
        "UserList"
      ]
    }, 
    "test.test_rlcompleter": {
      "file": "test/test_rlcompleter.py", 
      "imports": [
        "__builtin__", 
        "rlcompleter", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_robotparser": {
      "file": "test/test_robotparser.py", 
      "imports": [
        "StringIO", 
        "robotparser", 
        "test.test_support", 
        "unittest", 
        "urllib2"
      ]
    }, 
    "test.test_runpy": {
      "file": "test/test_runpy.py", 
      "imports": [
        "os", 
        "re", 
        "runpy", 
        "sys", 
        "tempfile", 
        "test.script_helper", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_sax": {
      "file": "test/test_sax.py", 
      "imports": [
        "cStringIO.StringIO", 
        "io", 
        "os", 
        "shutil", 
        "sys", 
        "test.test_support", 
        "unittest", 
        "xml.sax.xmlreader", 
        "xml.sax.saxutils", 
        "xml.sax.handler", 
        "xml.sax.expatreader", 
        "xml.sax"
      ]
    }, 
    "test.test_scope": {
      "file": "test/test_scope.py", 
      "imports": [
        "sys", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_scriptpackages": {
      "file": "test/test_scriptpackages.py", 
      "imports": [
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_select": {
      "file": "test/test_select.py", 
      "imports": [
        "os", 
        "select", 
        "sys", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_set": {
      "file": "test/test_set.py", 
      "imports": [
        "collections", 
        "copy", 
        "gc", 
        "itertools.chain", 
        "itertools.imap", 
        "operator", 
        "pickle", 
        "random", 
        "sys", 
        "test.test_support", 
        "unittest", 
        "weakref"
      ]
    }, 
    "test.test_setcomps": {
      "file": "test/test_setcomps.py", 
      "imports": [
        "gc", 
        "sys", 
        "test.test_setcomps", 
        "test.test_support"
      ]
    }, 
    "test.test_sets": {
      "file": "test/test_sets.py", 
      "imports": [
        "copy", 
        "doctest", 
        "operator", 
        "pickle", 
        "random", 
        "sets", 
        "test.test_sets", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_sgmllib": {
      "file": "test/test_sgmllib.py", 
      "imports": [
        "pprint", 
        "re", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_sha": {
      "file": "test/test_sha.py", 
      "imports": [
        "sha", 
        "test.test_support", 
        "unittest", 
        "warnings"
      ]
    }, 
    "test.test_shelve": {
      "file": "test/test_shelve.py", 
      "imports": [
        "glob", 
        "os", 
        "shelve", 
        "test.mapping_tests", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_shlex": {
      "file": "test/test_shlex.py", 
      "imports": [
        "StringIO", 
        "cStringIO.StringIO", 
        "shlex", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_shutil": {
      "file": "test/test_shutil.py", 
      "imports": [
        "distutils.spawn", 
        "errno", 
        "os", 
        "shutil", 
        "stat", 
        "sys", 
        "tarfile", 
        "tempfile", 
        "test.test_support", 
        "zlib", 
        "unittest", 
        "warnings", 
        "zipfile", 
        "grp", 
        "pwd"
      ]
    }, 
    "test.test_signal": {
      "file": "test/test_signal.py", 
      "imports": [
        "contextlib", 
        "errno", 
        "fcntl", 
        "gc", 
        "os", 
        "pickle", 
        "select", 
        "signal", 
        "subprocess", 
        "sys", 
        "test.test_support", 
        "time", 
        "traceback", 
        "unittest"
      ]
    }, 
    "test.test_site": {
      "file": "test/test_site.py", 
      "imports": [
        "__builtin__", 
        "copy", 
        "encodings", 
        "locale", 
        "os", 
        "re", 
        "site", 
        "sitecustomize", 
        "subprocess", 
        "sys", 
        "sysconfig", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_slice": {
      "file": "test/test_slice.py", 
      "imports": [
        "sys", 
        "test.test_support", 
        "unittest", 
        "cPickle"
      ]
    }, 
    "test.test_smtplib": {
      "file": "test/test_smtplib.py", 
      "imports": [
        "StringIO", 
        "asyncore", 
        "email.utils", 
        "select", 
        "smtpd", 
        "smtplib", 
        "socket", 
        "sys", 
        "test.test_support", 
        "threading", 
        "time", 
        "unittest"
      ]
    }, 
    "test.test_smtpnet": {
      "file": "test/test_smtpnet.py", 
      "imports": [
        "smtplib", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_socket": {
      "file": "test/test_socket.py", 
      "imports": [
        "Queue", 
        "_socket", 
        "array", 
        "contextlib", 
        "errno", 
        "itertools", 
        "math", 
        "os", 
        "select", 
        "signal", 
        "socket", 
        "sys", 
        "test.test_support", 
        "thread", 
        "threading", 
        "time", 
        "traceback", 
        "unittest", 
        "weakref", 
        "_testcapi"
      ]
    }, 
    "test.test_socketserver": {
      "file": "test/test_socketserver.py", 
      "imports": [
        "SocketServer", 
        "contextlib", 
        "errno", 
        "imp", 
        "os", 
        "select", 
        "signal", 
        "socket", 
        "tempfile", 
        "test.test_support", 
        "threading", 
        "unittest"
      ]
    }, 
    "test.test_softspace": {
      "file": "test/test_softspace.py", 
      "imports": [
        "StringIO", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_sort": {
      "file": "test/test_sort.py", 
      "imports": [
        "gc", 
        "random", 
        "sys", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_spwd": {
      "file": "test/test_spwd.py", 
      "imports": [
        "os", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_sqlite": {
      "file": "test/test_sqlite.py", 
      "imports": [
        "sqlite3.test.dbapi", 
        "sqlite3.test.dump", 
        "sqlite3.test.factory", 
        "sqlite3.test.hooks", 
        "sqlite3.test.py25tests", 
        "sqlite3.test.regression", 
        "sqlite3.test.transactions", 
        "sqlite3.test.types", 
        "sqlite3.test.userfunctions", 
        "test.test_support"
      ]
    }, 
    "test.test_ssl": {
      "file": "test/test_ssl.py", 
      "imports": [
        "asyncore", 
        "contextlib", 
        "errno", 
        "functools", 
        "gc", 
        "os", 
        "platform", 
        "pprint", 
        "select", 
        "socket", 
        "sys", 
        "tempfile", 
        "test.ssl_servers", 
        "test.test_support", 
        "threading", 
        "time", 
        "traceback", 
        "unittest", 
        "urllib2", 
        "weakref", 
        "datetime"
      ]
    }, 
    "test.test_startfile": {
      "file": "test/test_startfile.py", 
      "imports": [
        "os", 
        "test.test_support", 
        "time.sleep", 
        "unittest"
      ]
    }, 
    "test.test_stat": {
      "file": "test/test_stat.py", 
      "imports": [
        "os", 
        "stat", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_str": {
      "file": "test/test_str.py", 
      "imports": [
        "struct", 
        "sys", 
        "test.string_tests", 
        "test.test_support", 
        "unittest", 
        "_testcapi", 
        "datetime"
      ]
    }, 
    "test.test_strftime": {
      "file": "test/test_strftime.py", 
      "imports": [
        "calendar", 
        "java", 
        "locale", 
        "re", 
        "sys", 
        "test.test_support", 
        "time", 
        "unittest"
      ]
    }, 
    "test.test_string": {
      "file": "test/test_string.py", 
      "imports": [
        "string", 
        "test.string_tests", 
        "test.test_support", 
        "unittest", 
        "UserList"
      ]
    }, 
    "test.test_stringprep": {
      "file": "test/test_stringprep.py", 
      "imports": [
        "stringprep", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_strop": {
      "file": "test/test_strop.py", 
      "imports": [
        "strop", 
        "sys", 
        "test.test_support", 
        "unittest", 
        "warnings"
      ]
    }, 
    "test.test_strptime": {
      "file": "test/test_strptime.py", 
      "imports": [
        "_strptime", 
        "locale", 
        "re", 
        "sys", 
        "test.test_support", 
        "time", 
        "unittest", 
        "datetime"
      ]
    }, 
    "test.test_strtod": {
      "file": "test/test_strtod.py", 
      "imports": [
        "random", 
        "re", 
        "struct", 
        "sys", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_struct": {
      "file": "test/test_struct.py", 
      "imports": [
        "array", 
        "binascii", 
        "inspect", 
        "math", 
        "os", 
        "random", 
        "struct", 
        "sys", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_structmembers": {
      "file": "test/test_structmembers.py", 
      "imports": [
        "test.test_support", 
        "unittest", 
        "_testcapi"
      ]
    }, 
    "test.test_structseq": {
      "file": "test/test_structseq.py", 
      "imports": [
        "test.test_support", 
        "time", 
        "unittest"
      ]
    }, 
    "test.test_subprocess": {
      "file": "test/test_subprocess.py", 
      "imports": [
        "errno", 
        "os", 
        "re", 
        "signal", 
        "subprocess", 
        "sys", 
        "sysconfig", 
        "tempfile", 
        "test.test_support", 
        "threading", 
        "time", 
        "unittest", 
        "resource"
      ]
    }, 
    "test.test_sunau": {
      "file": "test/test_sunau.py", 
      "imports": [
        "sunau", 
        "sys", 
        "test.audiotests", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_sunaudiodev": {
      "file": "test/test_sunaudiodev.py", 
      "imports": [
        "os", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_sundry": {
      "file": "test/test_sundry.py", 
      "imports": [
        "CGIHTTPServer", 
        "audiodev", 
        "bdb", 
        "cgitb", 
        "code", 
        "compileall", 
        "distutils.bcppcompiler", 
        "distutils.ccompiler", 
        "distutils.command.bdist", 
        "distutils.command.bdist_dumb", 
        "distutils.command.bdist_msi", 
        "distutils.command.bdist_rpm", 
        "distutils.command.bdist_wininst", 
        "distutils.command.build", 
        "distutils.command.build_clib", 
        "distutils.command.build_ext", 
        "distutils.command.clean", 
        "distutils.command.config", 
        "distutils.command.install_data", 
        "distutils.command.install_egg_info", 
        "distutils.command.install_headers", 
        "distutils.command.install_lib", 
        "distutils.command.register", 
        "distutils.command.sdist", 
        "distutils.command.upload", 
        "distutils.cygwinccompiler", 
        "distutils.emxccompiler", 
        "distutils.filelist", 
        "distutils.msvccompiler", 
        "distutils.text_file", 
        "distutils.unixccompiler", 
        "encodings", 
        "formatter", 
        "getpass", 
        "htmlentitydefs", 
        "ihooks", 
        "imputil", 
        "keyword", 
        "linecache", 
        "mailcap", 
        "mimify", 
        "nntplib", 
        "nturl2path", 
        "opcode", 
        "os2emxpath", 
        "pdb", 
        "posixfile", 
        "pstats", 
        "py_compile", 
        "rexec", 
        "sched", 
        "sndhdr", 
        "statvfs", 
        "stringold", 
        "sunau", 
        "sunaudio", 
        "symbol", 
        "sys", 
        "tabnanny", 
        "test.test_support", 
        "token", 
        "timeit", 
        "toaiff", 
        "tty", 
        "unittest", 
        "webbrowser", 
        "xml"
      ]
    }, 
    "test.test_support": {
      "file": "test/test_support.py", 
      "imports": [
        "StringIO", 
        "Tkinter.Tk", 
        "contextlib", 
        "ctypes", 
        "ctypes.Structure", 
        "ctypes.c_int", 
        "ctypes.cdll", 
        "ctypes.pointer", 
        "ctypes.util.find_library", 
        "ctypes.wintypes", 
        "doctest", 
        "errno", 
        "functools", 
        "gc", 
        "importlib", 
        "locale", 
        "os", 
        "pdb", 
        "platform", 
        "re", 
        "shutil", 
        "socket", 
        "struct", 
        "subprocess", 
        "sys", 
        "sysconfig", 
        "test", 
        "thread", 
        "time", 
        "traceback", 
        "unittest", 
        "urllib2", 
        "urlparse", 
        "UserDict", 
        "warnings", 
        "_testcapi"
      ]
    }, 
    "test.test_symtable": {
      "file": "test/test_symtable.py", 
      "imports": [
        "symtable", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_syntax": {
      "file": "test/test_syntax.py", 
      "imports": [
        "re", 
        "test.test_support", 
        "test.test_syntax", 
        "unittest", 
        "warnings"
      ]
    }, 
    "test.test_sys": {
      "file": "test/test_sys.py", 
      "imports": [
        "__builtin__", 
        "_ast", 
        "cStringIO", 
        "codecs", 
        "encodings.iso8859_3", 
        "imp", 
        "inspect", 
        "operator", 
        "os", 
        "re", 
        "struct", 
        "subprocess", 
        "sys", 
        "test.script_helper", 
        "test.test_support", 
        "thread", 
        "threading", 
        "traceback", 
        "types", 
        "unittest", 
        "weakref", 
        "_testcapi", 
        "datetime"
      ]
    }, 
    "test.test_sys_setprofile": {
      "file": "test/test_sys_setprofile.py", 
      "imports": [
        "gc", 
        "pprint", 
        "sys", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_sys_settrace": {
      "file": "test/test_sys_settrace.py", 
      "imports": [
        "difflib", 
        "gc", 
        "sys", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_sysconfig": {
      "file": "test/test_sysconfig.py", 
      "imports": [
        "_osx_support", 
        "copy", 
        "os", 
        "shutil", 
        "subprocess", 
        "sys", 
        "sysconfig", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_tarfile": {
      "file": "test/test_tarfile.py", 
      "imports": [
        "StringIO", 
        "bz2", 
        "errno", 
        "gzip", 
        "hashlib", 
        "os", 
        "shutil", 
        "sys", 
        "tarfile", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_tcl": {
      "file": "test/test_tcl.py", 
      "imports": [
        "Tkinter.Tcl", 
        "_tkinter.TclError", 
        "os", 
        "subprocess.PIPE", 
        "subprocess.Popen", 
        "sys", 
        "test.test_support", 
        "unittest", 
        "_testcapi"
      ]
    }, 
    "test.test_telnetlib": {
      "file": "test/test_telnetlib.py", 
      "imports": [
        "Queue", 
        "socket", 
        "telnetlib", 
        "test.test_support", 
        "time", 
        "unittest"
      ]
    }, 
    "test.test_tempfile": {
      "file": "test/test_tempfile.py", 
      "imports": [
        "contextlib", 
        "errno", 
        "io", 
        "os", 
        "re", 
        "shutil", 
        "signal", 
        "stat", 
        "sys", 
        "tempfile", 
        "test.test_support", 
        "unittest", 
        "warnings"
      ]
    }, 
    "test.test_textwrap": {
      "file": "test/test_textwrap.py", 
      "imports": [
        "test.test_support", 
        "textwrap", 
        "unittest"
      ]
    }, 
    "test.test_thread": {
      "file": "test/test_thread.py", 
      "imports": [
        "os", 
        "random", 
        "sys", 
        "test.lock_tests", 
        "test.test_support", 
        "time", 
        "unittest", 
        "weakref"
      ]
    }, 
    "test.test_threaded_import": {
      "file": "test/test_threaded_import.py", 
      "imports": [
        "imp", 
        "random", 
        "sys", 
        "test.test_support", 
        "test.threaded_import_hangers", 
        "unittest"
      ]
    }, 
    "test.test_threadedtempfile": {
      "file": "test/test_threadedtempfile.py", 
      "imports": [
        "StringIO", 
        "tempfile", 
        "test.test_support", 
        "traceback", 
        "unittest"
      ]
    }, 
    "test.test_threading": {
      "file": "test/test_threading.py", 
      "imports": [
        "ctypes", 
        "os", 
        "random", 
        "re", 
        "subprocess", 
        "sys", 
        "test.lock_tests", 
        "test.script_helper", 
        "test.test_support", 
        "time", 
        "unittest", 
        "weakref", 
        "_testcapi"
      ]
    }, 
    "test.test_threading_local": {
      "file": "test/test_threading_local.py", 
      "imports": [
        "_threading_local", 
        "doctest", 
        "gc", 
        "test.test_support", 
        "thread._local", 
        "time", 
        "unittest", 
        "weakref"
      ]
    }, 
    "test.test_threadsignals": {
      "file": "test/test_threadsignals.py", 
      "imports": [
        "os", 
        "signal", 
        "sys", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_time": {
      "file": "test/test_time.py", 
      "imports": [
        "os", 
        "sys", 
        "test.test_support", 
        "time", 
        "unittest"
      ]
    }, 
    "test.test_timeout": {
      "file": "test/test_timeout.py", 
      "imports": [
        "socket", 
        "test.test_support", 
        "time", 
        "unittest"
      ]
    }, 
    "test.test_tk": {
      "file": "test/test_tk.py", 
      "imports": [
        "os", 
        "runtktests", 
        "test.test_support"
      ]
    }, 
    "test.test_tokenize": {
      "file": "test/test_tokenize.py", 
      "imports": [
        "StringIO", 
        "os", 
        "test.test_support", 
        "test.test_tokenize", 
        "tokenize", 
        "unittest"
      ]
    }, 
    "test.test_tools": {
      "file": "test/test_tools.py", 
      "imports": [
        "os", 
        "shutil", 
        "subprocess", 
        "sys", 
        "sysconfig", 
        "tempfile", 
        "test.script_helper", 
        "test.test_support", 
        "textwrap", 
        "unittest"
      ]
    }, 
    "test.test_trace": {
      "file": "test/test_trace.py", 
      "imports": [
        "os", 
        "sys", 
        "test.test_support", 
        "test.tracedmodules.testmod", 
        "trace", 
        "unittest"
      ]
    }, 
    "test.test_traceback": {
      "file": "test/test_traceback.py", 
      "imports": [
        "StringIO", 
        "imp.reload", 
        "os", 
        "sys", 
        "tempfile", 
        "test.badsyntax_nocaret", 
        "test.test_support", 
        "test_bug737473", 
        "time", 
        "traceback", 
        "unittest", 
        "_testcapi"
      ]
    }, 
    "test.test_transformer": {
      "file": "test/test_transformer.py", 
      "imports": [
        "compiler", 
        "compiler.ast", 
        "compiler.transformer", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_ttk_guionly": {
      "file": "test/test_ttk_guionly.py", 
      "imports": [
        "Tkinter", 
        "_tkinter.TclError", 
        "os", 
        "runtktests", 
        "test.test_support", 
        "ttk", 
        "unittest"
      ]
    }, 
    "test.test_ttk_textonly": {
      "file": "test/test_ttk_textonly.py", 
      "imports": [
        "os", 
        "runtktests", 
        "test.test_support"
      ]
    }, 
    "test.test_tuple": {
      "file": "test/test_tuple.py", 
      "imports": [
        "gc", 
        "test.seq_tests", 
        "test.test_support"
      ]
    }, 
    "test.test_typechecks": {
      "file": "test/test_typechecks.py", 
      "imports": [
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_types": {
      "file": "test/test_types.py", 
      "imports": [
        "array", 
        "locale", 
        "sys", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_ucn": {
      "file": "test/test_ucn.py", 
      "imports": [
        "sys", 
        "test.test_support", 
        "unicodedata", 
        "unittest", 
        "_testcapi"
      ]
    }, 
    "test.test_unary": {
      "file": "test/test_unary.py", 
      "imports": [
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_undocumented_details": {
      "file": "test/test_undocumented_details.py", 
      "imports": [
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_unicode": {
      "file": "test/test_unicode.py", 
      "imports": [
        "codecs", 
        "ctypes.c_int", 
        "ctypes.c_long", 
        "ctypes.c_longlong", 
        "ctypes.c_size_t", 
        "ctypes.c_ssize_t", 
        "ctypes.c_uint", 
        "ctypes.c_ulong", 
        "ctypes.c_ulonglong", 
        "ctypes.c_void_p", 
        "ctypes.py_object", 
        "ctypes.pythonapi", 
        "ctypes.sizeof", 
        "imp", 
        "struct", 
        "sys", 
        "test.string_tests", 
        "test.test_support", 
        "unittest", 
        "_testcapi", 
        "datetime"
      ]
    }, 
    "test.test_unicode_file": {
      "file": "test/test_unicode_file.py", 
      "imports": [
        "glob", 
        "os", 
        "shutil", 
        "sys", 
        "test.test_support", 
        "time", 
        "unicodedata", 
        "unittest"
      ]
    }, 
    "test.test_unicodedata": {
      "file": "test/test_unicodedata.py", 
      "imports": [
        "hashlib", 
        "subprocess", 
        "sys", 
        "test.test_support", 
        "unicodedata", 
        "unittest"
      ]
    }, 
    "test.test_unittest": {
      "file": "test/test_unittest.py", 
      "imports": [
        "test.test_support", 
        "unittest.test"
      ]
    }, 
    "test.test_univnewlines": {
      "file": "test/test_univnewlines.py", 
      "imports": [
        "__future__", 
        "_pyio", 
        "io", 
        "os", 
        "sys", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_univnewlines2k": {
      "file": "test/test_univnewlines2k.py", 
      "imports": [
        "os", 
        "sys", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_unpack": {
      "file": "test/test_unpack.py", 
      "imports": [
        "test.test_support", 
        "test.test_unpack"
      ]
    }, 
    "test.test_urllib": {
      "file": "test/test_urllib.py", 
      "imports": [
        "StringIO", 
        "base64", 
        "httplib", 
        "mimetools", 
        "os", 
        "sys", 
        "tempfile", 
        "test.test_support", 
        "unittest", 
        "urllib", 
        "warnings"
      ]
    }, 
    "test.test_urllib2": {
      "file": "test/test_urllib2.py", 
      "imports": [
        "StringIO", 
        "base64", 
        "cookielib", 
        "copy", 
        "ftplib", 
        "httplib", 
        "mimetools", 
        "os", 
        "rfc822", 
        "socket", 
        "ssl", 
        "string", 
        "test.test_cookielib", 
        "test.test_support", 
        "test.test_urllib2", 
        "unittest", 
        "urllib", 
        "urllib2"
      ]
    }, 
    "test.test_urllib2_localnet": {
      "file": "test/test_urllib2_localnet.py", 
      "imports": [
        "BaseHTTPServer", 
        "base64", 
        "hashlib", 
        "os", 
        "ssl", 
        "test.ssl_servers", 
        "test.test_support", 
        "unittest", 
        "urllib2", 
        "urlparse"
      ]
    }, 
    "test.test_urllib2net": {
      "file": "test/test_urllib2net.py", 
      "imports": [
        "httplib", 
        "logging", 
        "os", 
        "socket", 
        "sys", 
        "test.test_support", 
        "test.test_urllib2", 
        "time", 
        "unittest", 
        "urllib2"
      ]
    }, 
    "test.test_urllibnet": {
      "file": "test/test_urllibnet.py", 
      "imports": [
        "os", 
        "socket", 
        "ssl", 
        "sys", 
        "test.test_support", 
        "time", 
        "unittest", 
        "urllib"
      ]
    }, 
    "test.test_urlparse": {
      "file": "test/test_urlparse.py", 
      "imports": [
        "test.test_support", 
        "unittest", 
        "urlparse"
      ]
    }, 
    "test.test_userdict": {
      "file": "test/test_userdict.py", 
      "imports": [
        "test.mapping_tests", 
        "test.test_support", 
        "UserDict"
      ]
    }, 
    "test.test_userlist": {
      "file": "test/test_userlist.py", 
      "imports": [
        "test.list_tests", 
        "test.test_support", 
        "UserList"
      ]
    }, 
    "test.test_userstring": {
      "file": "test/test_userstring.py", 
      "imports": [
        "string", 
        "test.string_tests", 
        "test.test_support", 
        "UserString", 
        "warnings"
      ]
    }, 
    "test.test_uu": {
      "file": "test/test_uu.py", 
      "imports": [
        "cStringIO", 
        "codecs", 
        "os", 
        "sys", 
        "test.test_support", 
        "unittest", 
        "uu"
      ]
    }, 
    "test.test_uuid": {
      "file": "test/test_uuid.py", 
      "imports": [
        "io", 
        "os", 
        "test.test_support", 
        "unittest", 
        "uuid"
      ]
    }, 
    "test.test_wait3": {
      "file": "test/test_wait3.py", 
      "imports": [
        "os", 
        "test.fork_wait", 
        "test.test_support", 
        "time", 
        "unittest"
      ]
    }, 
    "test.test_wait4": {
      "file": "test/test_wait4.py", 
      "imports": [
        "os", 
        "sys", 
        "test.fork_wait", 
        "test.test_support", 
        "time"
      ]
    }, 
    "test.test_warnings": {
      "file": "test/test_warnings.py", 
      "imports": [
        "StringIO", 
        "contextlib", 
        "linecache", 
        "os", 
        "subprocess", 
        "sys", 
        "test.script_helper", 
        "test.test_support", 
        "test.warning_tests", 
        "unittest", 
        "warnings"
      ]
    }, 
    "test.test_wave": {
      "file": "test/test_wave.py", 
      "imports": [
        "sys", 
        "test.audiotests", 
        "test.test_support", 
        "wave", 
        "unittest"
      ]
    }, 
    "test.test_weakref": {
      "file": "test/test_weakref.py", 
      "imports": [
        "contextlib", 
        "copy", 
        "gc", 
        "operator", 
        "sys", 
        "test.mapping_tests", 
        "test.test_support", 
        "unittest", 
        "UserList", 
        "weakref"
      ]
    }, 
    "test.test_weakset": {
      "file": "test/test_weakset.py", 
      "imports": [
        "collections", 
        "contextlib", 
        "copy", 
        "gc", 
        "operator", 
        "os", 
        "random", 
        "string", 
        "sys", 
        "test.test_support", 
        "unittest", 
        "warnings", 
        "weakref"
      ]
    }, 
    "test.test_whichdb": {
      "file": "test/test_whichdb.py", 
      "imports": [
        "glob", 
        "os", 
        "test.test_support", 
        "unittest", 
        "whichdb"
      ]
    }, 
    "test.test_winreg": {
      "file": "test/test_winreg.py", 
      "imports": [
        "_winreg.*", 
        "errno", 
        "os", 
        "platform", 
        "sys", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_winsound": {
      "file": "test/test_winsound.py", 
      "imports": [
        "_winreg", 
        "os", 
        "subprocess", 
        "test.test_support", 
        "time", 
        "unittest"
      ]
    }, 
    "test.test_with": {
      "file": "test/test_with.py", 
      "imports": [
        "collections", 
        "contextlib", 
        "sys", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_wsgiref": {
      "file": "test/test_wsgiref.py", 
      "imports": [
        "SocketServer", 
        "StringIO", 
        "__future__", 
        "os", 
        "re", 
        "sys", 
        "test.test_support", 
        "unittest", 
        "wsgiref.validate", 
        "wsgiref.util", 
        "wsgiref.simple_server", 
        "wsgiref.headers", 
        "wsgiref.handlers"
      ]
    }, 
    "test.test_xdrlib": {
      "file": "test/test_xdrlib.py", 
      "imports": [
        "test.test_support", 
        "unittest", 
        "xdrlib"
      ]
    }, 
    "test.test_xml_etree": {
      "file": "test/test_xml_etree.py", 
      "imports": [
        "StringIO", 
        "cgi", 
        "sys", 
        "test.test_support", 
        "test.test_xml_etree", 
        "xml.etree.ElementTree", 
        "xml.etree.ElementPath"
      ]
    }, 
    "test.test_xml_etree_c": {
      "file": "test/test_xml_etree_c.py", 
      "imports": [
        "test.test_support", 
        "test.test_xml_etree", 
        "test.test_xml_etree_c", 
        "unittest"
      ]
    }, 
    "test.test_xmllib": {
      "file": "test/test_xmllib.py", 
      "imports": [
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_xmlrpc": {
      "file": "test/test_xmlrpc.py", 
      "imports": [
        "SimpleXMLRPCServer", 
        "StringIO", 
        "base64", 
        "gzip", 
        "httplib", 
        "mimetools", 
        "os", 
        "re", 
        "socket", 
        "sys", 
        "test.test_support", 
        "threading", 
        "time", 
        "unittest", 
        "xmlrpclib", 
        "datetime"
      ]
    }, 
    "test.test_xpickle": {
      "file": "test/test_xpickle.py", 
      "imports": [
        "os", 
        "pickle", 
        "subprocess", 
        "sys", 
        "test.test_support", 
        "types", 
        "unittest", 
        "cPickle"
      ]
    }, 
    "test.test_xrange": {
      "file": "test/test_xrange.py", 
      "imports": [
        "itertools", 
        "pickle", 
        "sys", 
        "test.test_support", 
        "unittest", 
        "warnings"
      ]
    }, 
    "test.test_zipfile": {
      "file": "test/test_zipfile.py", 
      "imports": [
        "StringIO", 
        "email", 
        "io", 
        "os", 
        "random", 
        "struct", 
        "sys", 
        "tempfile", 
        "test.test_support", 
        "time", 
        "zlib", 
        "unittest", 
        "zipfile"
      ]
    }, 
    "test.test_zipfile64": {
      "file": "test/test_zipfile64.py", 
      "imports": [
        "os", 
        "sys", 
        "tempfile", 
        "test.test_support", 
        "time", 
        "zlib", 
        "unittest", 
        "zipfile"
      ]
    }, 
    "test.test_zipimport": {
      "file": "test/test_zipimport.py", 
      "imports": [
        "StringIO", 
        "doctest", 
        "imp", 
        "inspect", 
        "linecache", 
        "marshal", 
        "os", 
        "struct", 
        "sys", 
        "test.test_importhooks", 
        "test.test_support", 
        "time", 
        "zipimport", 
        "zlib", 
        "traceback", 
        "unittest", 
        "zipfile"
      ]
    }, 
    "test.test_zipimport_support": {
      "file": "test/test_zipimport_support.py", 
      "imports": [
        "doctest", 
        "inspect", 
        "linecache", 
        "os", 
        "pdb", 
        "sys", 
        "test.sample_doctest", 
        "test.sample_doctest_no_docstrings", 
        "test.sample_doctest_no_doctests", 
        "test.script_helper", 
        "test.test_doctest", 
        "test.test_importhooks", 
        "test.test_support", 
        "test_zipped_doctest", 
        "zip_pkg", 
        "zipimport", 
        "textwrap", 
        "warnings", 
        "zipfile"
      ]
    }, 
    "test.test_zlib": {
      "file": "test/test_zlib.py", 
      "imports": [
        "binascii", 
        "mmap", 
        "os", 
        "random", 
        "sys", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.testall": {
      "file": "test/testall.py", 
      "imports": [
        "sys", 
        "test.regrtest", 
        "warnings"
      ]
    }, 
    "test.testcodec": {
      "file": "test/testcodec.py", 
      "imports": [
        "codecs"
      ]
    }, 
    "test.tf_inherit_check": {
      "file": "test/tf_inherit_check.py", 
      "imports": [
        "os", 
        "sys"
      ]
    }, 
    "test.threaded_import_hangers": {
      "file": "test/threaded_import_hangers.py", 
      "imports": [
        "os", 
        "tempfile", 
        "threading"
      ]
    }, 
    "test.time_hashlib": {
      "file": "test/time_hashlib.py", 
      "imports": [
        "_hashlib", 
        "hashlib", 
        "sys", 
        "time"
      ]
    }, 
    "test.tracedmodules": {
      "dir": "test/tracedmodules"
    }, 
    "test.tracedmodules.__init__": {
      "file": "test/tracedmodules/__init__.py", 
      "imports": []
    }, 
    "test.tracedmodules.testmod": {
      "file": "test/tracedmodules/testmod.py", 
      "imports": []
    }, 
    "test.warning_tests": {
      "file": "test/warning_tests.py", 
      "imports": [
        "warnings"
      ]
    }, 
    "test.win_console_handler": {
      "file": "test/win_console_handler.py", 
      "imports": [
        "ctypes", 
        "ctypes.WINFUNCTYPE", 
        "ctypes.wintypes", 
        "mmap", 
        "signal", 
        "sys"
      ]
    }, 
    "test.xmltests": {
      "file": "test/xmltests.py", 
      "imports": [
        "sys", 
        "test.test_support"
      ]
    }, 
    "textwrap": {
      "file": "textwrap.py", 
      "imports": [
        "re", 
        "string"
      ]
    }, 
    "this": {
      "file": "this.py", 
      "imports": []
    }, 
    "threading": {
      "file": "threading.py", 
      "imports": []
    }, 
    "timeit": {
      "file": "timeit.py", 
      "imports": [
        "gc", 
        "getopt", 
        "linecache", 
        "os", 
        "sys", 
        "time", 
        "traceback"
      ]
    }, 
    "toaiff": {
      "file": "toaiff.py", 
      "imports": [
        "os", 
        "pipes", 
        "sndhdr", 
        "tempfile", 
        "warnings"
      ]
    }, 
    "token": {
      "file": "token.py", 
      "imports": [
        "re", 
        "sys"
      ]
    }, 
    "tokenize": {
      "file": "tokenize.py", 
      "imports": [
        "itertools.chain", 
        "re", 
        "string", 
        "sys", 
        "token"
      ]
    }, 
    "tputil": {
      "file": "tputil.py", 
      "imports": [
        "__pypy__.tproxy", 
        "types"
      ]
    }, 
    "trace": {
      "file": "trace.py", 
      "imports": [
        "__main__", 
        "dis", 
        "gc", 
        "getopt", 
        "inspect", 
        "linecache", 
        "os", 
        "pickle", 
        "re", 
        "sys", 
        "threading", 
        "time", 
        "token", 
        "tokenize", 
        "cPickle"
      ]
    }, 
    "traceback": {
      "file": "traceback.py", 
      "imports": [
        "linecache", 
        "sys", 
        "types"
      ]
    }, 
    "tty": {
      "file": "tty.py", 
      "imports": [
        "termios.*"
      ]
    }, 
    "types": {
      "file": "types.py", 
      "imports": [
        "sys"
      ]
    }, 
    "typing": {
      "file": "typing.py", 
      "imports": [
        "__future__", 
        "abc", 
        "collections", 
        "functools", 
        "re", 
        "sys", 
        "types"
      ]
    }, 
    "urllib": {
      "file": "urllib.py", 
      "imports": [
        "StringIO", 
        "_winreg", 
        "base64", 
        "cStringIO.StringIO", 
        "email.utils", 
        "fnmatch", 
        "ftplib", 
        "getpass", 
        "httplib", 
        "mimetools", 
        "mimetypes", 
        "nturl2path", 
        "os", 
        "re", 
        "rourl2path.pathname2url", 
        "rourl2path.url2pathname", 
        "socket", 
        "ssl", 
        "string", 
        "sys", 
        "tempfile", 
        "time", 
        "urlparse", 
        "warnings", 
        "_scproxy"
      ]
    }, 
    "urllib2": {
      "file": "urllib2.py", 
      "imports": [
        "StringIO", 
        "base64", 
        "bisect", 
        "cStringIO.StringIO", 
        "cookielib", 
        "email.utils", 
        "ftplib", 
        "hashlib", 
        "httplib", 
        "mimetools", 
        "mimetypes", 
        "os", 
        "posixpath", 
        "random", 
        "re", 
        "socket", 
        "ssl", 
        "sys", 
        "time", 
        "types", 
        "urllib", 
        "urlparse", 
        "warnings"
      ]
    }, 
    "urlparse": {
      "file": "urlparse.py", 
      "imports": [
        "collections", 
        "re"
      ]
    }, 
    "user": {
      "file": "user.py", 
      "imports": [
        "os", 
        "warnings"
      ]
    }, 
    "uu": {
      "file": "uu.py", 
      "imports": [
        "binascii", 
        "optparse", 
        "os", 
        "sys"
      ]
    }, 
    "uuid": {
      "file": "uuid.py", 
      "imports": [
        "ctypes", 
        "ctypes.util", 
        "hashlib", 
        "netbios", 
        "os", 
        "random", 
        "re", 
        "socket", 
        "struct", 
        "sys", 
        "time", 
        "win32wnet"
      ]
    }, 
    "warnings": {
      "file": "warnings.py", 
      "imports": [
        "_warnings.default_action", 
        "_warnings.filters", 
        "_warnings.once_registry", 
        "_warnings.warn", 
        "_warnings.warn_explicit", 
        "linecache", 
        "re", 
        "sys", 
        "types"
      ]
    }, 
    "weakref": {
      "file": "weakref.py", 
      "imports": [
        "UserDict", 
        "_weakref.CallableProxyType", 
        "_weakref.ProxyType", 
        "_weakref.ReferenceType", 
        "_weakref.getweakrefcount", 
        "_weakref.getweakrefs", 
        "_weakref.proxy", 
        "_weakref.ref", 
        "_weakrefset", 
        "copy", 
        "exceptions.ReferenceError"
      ]
    }, 
    "webbrowser": {
      "file": "webbrowser.py", 
      "imports": [
        "copy", 
        "getopt", 
        "glob", 
        "os", 
        "shlex", 
        "socket", 
        "stat", 
        "subprocess", 
        "sys", 
        "tempfile", 
        "time", 
        "pwd"
      ]
    }, 
    "whichdb": {
      "file": "whichdb.py", 
      "imports": [
        "os", 
        "struct", 
        "sys", 
        "dbm"
      ]
    }, 
    "wsgiref": {
      "dir": "wsgiref"
    }, 
    "wsgiref.__init__": {
      "file": "wsgiref/__init__.py", 
      "imports": []
    }, 
    "wsgiref.handlers": {
      "file": "wsgiref/handlers.py", 
      "imports": [
        "os", 
        "sys", 
        "time", 
        "traceback", 
        "types", 
        "wsgiref.headers", 
        "wsgiref.util"
      ]
    }, 
    "wsgiref.headers": {
      "file": "wsgiref/headers.py", 
      "imports": [
        "re", 
        "types"
      ]
    }, 
    "wsgiref.simple_server": {
      "file": "wsgiref/simple_server.py", 
      "imports": [
        "BaseHTTPServer", 
        "StringIO", 
        "sys", 
        "urllib", 
        "webbrowser", 
        "wsgiref.handlers"
      ]
    }, 
    "wsgiref.util": {
      "file": "wsgiref/util.py", 
      "imports": [
        "StringIO", 
        "posixpath", 
        "urllib"
      ]
    }, 
    "wsgiref.validate": {
      "file": "wsgiref/validate.py", 
      "imports": [
        "re", 
        "sys", 
        "types", 
        "warnings"
      ]
    }, 
    "xdrlib": {
      "file": "xdrlib.py", 
      "imports": [
        "StringIO", 
        "cStringIO.StringIO", 
        "functools", 
        "struct"
      ]
    }, 
    "xml": {
      "dir": "xml"
    }, 
    "xml.__init__": {
      "file": "xml/__init__.py", 
      "imports": [
        "_xmlplus", 
        "sys"
      ]
    }, 
    "xml.dom": {
      "dir": "xml/dom"
    }, 
    "xml.dom.NodeFilter": {
      "file": "xml/dom/NodeFilter.py", 
      "imports": []
    }, 
    "xml.dom.__init__": {
      "file": "xml/dom/__init__.py", 
      "imports": [
        "xml.dom.domreg"
      ]
    }, 
    "xml.dom.domreg": {
      "file": "xml/dom/domreg.py", 
      "imports": [
        "os", 
        "xml.dom.minicompat"
      ]
    }, 
    "xml.dom.expatbuilder": {
      "file": "xml/dom/expatbuilder.py", 
      "imports": [
        "xml.dom", 
        "xml.dom.NodeFilter", 
        "xml.dom.minicompat", 
        "xml.dom.minidom", 
        "xml.dom.xmlbuilder", 
        "xml.parsers.expat"
      ]
    }, 
    "xml.dom.minicompat": {
      "file": "xml/dom/minicompat.py", 
      "imports": [
        "xml.dom"
      ]
    }, 
    "xml.dom.minidom": {
      "file": "xml/dom/minidom.py", 
      "imports": [
        "StringIO", 
        "codecs", 
        "xml.dom", 
        "xml.dom.domreg", 
        "xml.dom.expatbuilder", 
        "xml.dom.minicompat", 
        "xml.dom.pulldom", 
        "xml.dom.xmlbuilder"
      ]
    }, 
    "xml.dom.pulldom": {
      "file": "xml/dom/pulldom.py", 
      "imports": [
        "StringIO", 
        "cStringIO.StringIO", 
        "types", 
        "xml.dom", 
        "xml.dom.minidom", 
        "xml.sax", 
        "xml.sax.handler"
      ]
    }, 
    "xml.dom.xmlbuilder": {
      "file": "xml/dom/xmlbuilder.py", 
      "imports": [
        "copy", 
        "posixpath", 
        "urllib2", 
        "urlparse", 
        "xml.dom", 
        "xml.dom.NodeFilter", 
        "xml.dom.expatbuilder"
      ]
    }, 
    "xml.etree": {
      "dir": "xml/etree"
    }, 
    "xml.etree.ElementInclude": {
      "file": "xml/etree/ElementInclude.py", 
      "imports": [
        "copy", 
        "xml.etree.ElementTree"
      ]
    }, 
    "xml.etree.ElementPath": {
      "file": "xml/etree/ElementPath.py", 
      "imports": [
        "re"
      ]
    }, 
    "xml.etree.ElementTree": {
      "file": "xml/etree/ElementTree.py", 
      "imports": [
        "ElementC14N._serialize_c14n", 
        "pyexpat", 
        "re", 
        "sys", 
        "warnings", 
        "xml.etree.ElementPath", 
        "xml.parsers.expat"
      ]
    }, 
    "xml.etree.__init__": {
      "file": "xml/etree/__init__.py", 
      "imports": []
    }, 
    "xml.etree.cElementTree": {
      "file": "xml/etree/cElementTree.py", 
      "imports": [
        "_elementtree"
      ]
    }, 
    "xml.parsers": {
      "dir": "xml/parsers"
    }, 
    "xml.parsers.__init__": {
      "file": "xml/parsers/__init__.py", 
      "imports": []
    }, 
    "xml.parsers.expat": {
      "file": "xml/parsers/expat.py", 
      "imports": [
        "pyexpat.*"
      ]
    }, 
    "xml.sax": {
      "dir": "xml/sax"
    }, 
    "xml.sax.__init__": {
      "file": "xml/sax/__init__.py", 
      "imports": [
        "StringIO", 
        "cStringIO.StringIO", 
        "org.python.core.imp", 
        "os", 
        "sys", 
        "xml.sax._exceptions", 
        "xml.sax.expatreader", 
        "xml.sax.handler", 
        "xml.sax.xmlreader"
      ]
    }, 
    "xml.sax._exceptions": {
      "file": "xml/sax/_exceptions.py", 
      "imports": [
        "java.lang.Exception", 
        "sys"
      ]
    }, 
    "xml.sax.expatreader": {
      "file": "xml/sax/expatreader.py", 
      "imports": [
        "_weakref", 
        "sys", 
        "weakref", 
        "xml.parsers.expat", 
        "xml.sax._exceptions", 
        "xml.sax.handler", 
        "xml.sax.saxutils", 
        "xml.sax.xmlreader"
      ]
    }, 
    "xml.sax.handler": {
      "file": "xml/sax/handler.py", 
      "imports": []
    }, 
    "xml.sax.saxutils": {
      "file": "xml/sax/saxutils.py", 
      "imports": [
        "io", 
        "os", 
        "sys", 
        "types", 
        "urllib", 
        "urlparse", 
        "xml.sax.handler", 
        "xml.sax.xmlreader"
      ]
    }, 
    "xml.sax.xmlreader": {
      "file": "xml/sax/xmlreader.py", 
      "imports": [
        "xml.sax._exceptions", 
        "xml.sax.handler", 
        "xml.sax.saxutils"
      ]
    }, 
    "xmllib": {
      "file": "xmllib.py", 
      "imports": [
        "getopt", 
        "re", 
        "string", 
        "sys", 
        "time.time", 
        "warnings"
      ]
    }, 
    "xmlrpclib": {
      "file": "xmlrpclib.py", 
      "imports": [
        "StringIO", 
        "_xmlrpclib", 
        "base64", 
        "cStringIO", 
        "errno", 
        "gzip", 
        "httplib", 
        "operator", 
        "re", 
        "socket", 
        "string", 
        "sys.modules", 
        "time", 
        "types", 
        "urllib", 
        "xml.parsers.expat", 
        "xmllib", 
        "datetime"
      ]
    }, 
    "zipfile": {
      "file": "zipfile.py", 
      "imports": [
        "binascii", 
        "cStringIO", 
        "io", 
        "os", 
        "py_compile", 
        "re", 
        "shutil", 
        "stat", 
        "string", 
        "struct", 
        "sys", 
        "textwrap", 
        "time", 
        "warnings", 
        "zlib"
      ]
    }
  }, 
  "preload": {
    "StringIO": "r\"\"\"File-like objects that read from or write to a string buffer.\n\nThis implements (nearly) all stdio methods.\n\nf = StringIO()      # ready for writing\nf = StringIO(buf)   # ready for reading\nf.close()           # explicitly release resources held\nflag = f.isatty()   # always false\npos = f.tell()      # get current position\nf.seek(pos)         # set current position\nf.seek(pos, mode)   # mode 0: absolute; 1: relative; 2: relative to EOF\nbuf = f.read()      # read until EOF\nbuf = f.read(n)     # read up to n bytes\nbuf = f.readline()  # read until end of line ('\\n') or EOF\nlist = f.readlines()# list of f.readline() results until EOF\nf.truncate([size])  # truncate file at to at most size (default: current pos)\nf.write(buf)        # write at current position\nf.writelines(list)  # for line in list: f.write(line)\nf.getvalue()        # return whole file's contents as a string\n\nNotes:\n- Using a real file is often faster (but less convenient).\n- There's also a much faster implementation in C, called cStringIO, but\n  it's not subclassable.\n- fileno() is left unimplemented so that code which uses it triggers\n  an exception early.\n- Seeking far beyond EOF and then writing will insert real null\n  bytes that occupy space in the buffer.\n- There's a simple test set (see end of this file).\n\"\"\"\ntry:\n    from errno import EINVAL\nexcept ImportError:\n    EINVAL = 22\n\n__all__ = [\"StringIO\"]\n\ndef _complain_ifclosed(closed):\n    if closed:\n        raise ValueError, \"I/O operation on closed file\"\n\nclass StringIO:\n    \"\"\"class StringIO([buffer])\n\n    When a StringIO object is created, it can be initialized to an existing\n    string by passing the string to the constructor. If no string is given,\n    the StringIO will start empty.\n\n    The StringIO object can accept either Unicode or 8-bit strings, but\n    mixing the two may take some care. If both are used, 8-bit strings that\n    cannot be interpreted as 7-bit ASCII (that use the 8th bit) will cause\n    a UnicodeError to be raised when getvalue() is called.\n    \"\"\"\n    def __init__(self, buf = ''):\n        # Force self.buf to be a string or unicode\n        if not isinstance(buf, basestring):\n            buf = str(buf)\n        self.buf = buf\n        self.len = len(buf)\n        self.buflist = []\n        self.pos = 0\n        self.closed = False\n        self.softspace = 0\n\n    def __iter__(self):\n        return self\n\n    def next(self):\n        \"\"\"A file object is its own iterator, for example iter(f) returns f\n        (unless f is closed). When a file is used as an iterator, typically\n        in a for loop (for example, for line in f: print line), the next()\n        method is called repeatedly. This method returns the next input line,\n        or raises StopIteration when EOF is hit.\n        \"\"\"\n        _complain_ifclosed(self.closed)\n        r = self.readline()\n        if not r:\n            raise StopIteration\n        return r\n\n    def close(self):\n        \"\"\"Free the memory buffer.\n        \"\"\"\n        if not self.closed:\n            self.closed = True\n            del self.buf, self.pos\n\n    def isatty(self):\n        \"\"\"Returns False because StringIO objects are not connected to a\n        tty-like device.\n        \"\"\"\n        _complain_ifclosed(self.closed)\n        return False\n\n    def seek(self, pos, mode = 0):\n        \"\"\"Set the file's current position.\n\n        The mode argument is optional and defaults to 0 (absolute file\n        positioning); other values are 1 (seek relative to the current\n        position) and 2 (seek relative to the file's end).\n\n        There is no return value.\n        \"\"\"\n        _complain_ifclosed(self.closed)\n        if self.buflist:\n            self.buf += ''.join(self.buflist)\n            self.buflist = []\n        if mode == 1:\n            pos += self.pos\n        elif mode == 2:\n            pos += self.len\n        self.pos = max(0, pos)\n\n    def tell(self):\n        \"\"\"Return the file's current position.\"\"\"\n        _complain_ifclosed(self.closed)\n        return self.pos\n\n    def read(self, n = -1):\n        \"\"\"Read at most size bytes from the file\n        (less if the read hits EOF before obtaining size bytes).\n\n        If the size argument is negative or omitted, read all data until EOF\n        is reached. The bytes are returned as a string object. An empty\n        string is returned when EOF is encountered immediately.\n        \"\"\"\n        _complain_ifclosed(self.closed)\n        if self.buflist:\n            self.buf += ''.join(self.buflist)\n            self.buflist = []\n        if n is None or n < 0:\n            newpos = self.len\n        else:\n            newpos = min(self.pos+n, self.len)\n        r = self.buf[self.pos:newpos]\n        self.pos = newpos\n        return r\n\n    def readline(self, length=None):\n        r\"\"\"Read one entire line from the file.\n\n        A trailing newline character is kept in the string (but may be absent\n        when a file ends with an incomplete line). If the size argument is\n        present and non-negative, it is a maximum byte count (including the\n        trailing newline) and an incomplete line may be returned.\n\n        An empty string is returned only when EOF is encountered immediately.\n\n        Note: Unlike stdio's fgets(), the returned string contains null\n        characters ('\\0') if they occurred in the input.\n        \"\"\"\n        _complain_ifclosed(self.closed)\n        if self.buflist:\n            self.buf += ''.join(self.buflist)\n            self.buflist = []\n        i = self.buf.find('\\n', self.pos)\n        if i < 0:\n            newpos = self.len\n        else:\n            newpos = i+1\n        if length is not None and length >= 0:\n            if self.pos + length < newpos:\n                newpos = self.pos + length\n        r = self.buf[self.pos:newpos]\n        self.pos = newpos\n        return r\n\n    def readlines(self, sizehint = 0):\n        \"\"\"Read until EOF using readline() and return a list containing the\n        lines thus read.\n\n        If the optional sizehint argument is present, instead of reading up\n        to EOF, whole lines totalling approximately sizehint bytes (or more\n        to accommodate a final whole line).\n        \"\"\"\n        total = 0\n        lines = []\n        line = self.readline()\n        while line:\n            lines.append(line)\n            total += len(line)\n            if 0 < sizehint <= total:\n                break\n            line = self.readline()\n        return lines\n\n    def truncate(self, size=None):\n        \"\"\"Truncate the file's size.\n\n        If the optional size argument is present, the file is truncated to\n        (at most) that size. The size defaults to the current position.\n        The current file position is not changed unless the position\n        is beyond the new file size.\n\n        If the specified size exceeds the file's current size, the\n        file remains unchanged.\n        \"\"\"\n        _complain_ifclosed(self.closed)\n        if size is None:\n            size = self.pos\n        elif size < 0:\n            raise IOError(EINVAL, \"Negative size not allowed\")\n        elif size < self.pos:\n            self.pos = size\n        self.buf = self.getvalue()[:size]\n        self.len = size\n\n    def write(self, s):\n        \"\"\"Write a string to the file.\n\n        There is no return value.\n        \"\"\"\n        _complain_ifclosed(self.closed)\n        if not s: return\n        # Force s to be a string or unicode\n        if not isinstance(s, basestring):\n            s = str(s)\n        spos = self.pos\n        slen = self.len\n        if spos == slen:\n            self.buflist.append(s)\n            self.len = self.pos = spos + len(s)\n            return\n        if spos > slen:\n            self.buflist.append('\\0'*(spos - slen))\n            slen = spos\n        newpos = spos + len(s)\n        if spos < slen:\n            if self.buflist:\n                self.buf += ''.join(self.buflist)\n            self.buflist = [self.buf[:spos], s, self.buf[newpos:]]\n            self.buf = ''\n            if newpos > slen:\n                slen = newpos\n        else:\n            self.buflist.append(s)\n            slen = newpos\n        self.len = slen\n        self.pos = newpos\n\n    def writelines(self, iterable):\n        \"\"\"Write a sequence of strings to the file. The sequence can be any\n        iterable object producing strings, typically a list of strings. There\n        is no return value.\n\n        (The name is intended to match readlines(); writelines() does not add\n        line separators.)\n        \"\"\"\n        write = self.write\n        for line in iterable:\n            write(line)\n\n    def flush(self):\n        \"\"\"Flush the internal buffer\n        \"\"\"\n        _complain_ifclosed(self.closed)\n\n    def getvalue(self):\n        \"\"\"\n        Retrieve the entire contents of the \"file\" at any time before\n        the StringIO object's close() method is called.\n\n        The StringIO object can accept either Unicode or 8-bit strings,\n        but mixing the two may take some care. If both are used, 8-bit\n        strings that cannot be interpreted as 7-bit ASCII (that use the\n        8th bit) will cause a UnicodeError to be raised when getvalue()\n        is called.\n        \"\"\"\n        _complain_ifclosed(self.closed)\n        if self.buflist:\n            self.buf += ''.join(self.buflist)\n            self.buflist = []\n        return self.buf\n\n\n# A little test suite\n\ndef test():\n    import sys\n    if sys.argv[1:]:\n        file = sys.argv[1]\n    else:\n        file = '/etc/passwd'\n    lines = open(file, 'r').readlines()\n    text = open(file, 'r').read()\n    f = StringIO()\n    for line in lines[:-2]:\n        f.write(line)\n    f.writelines(lines[-2:])\n    if f.getvalue() != text:\n        raise RuntimeError, 'write failed'\n    length = f.tell()\n    print 'File length =', length\n    f.seek(len(lines[0]))\n    f.write(lines[1])\n    f.seek(0)\n    print 'First line =', repr(f.readline())\n    print 'Position =', f.tell()\n    line = f.readline()\n    print 'Second line =', repr(line)\n    f.seek(-len(line), 1)\n    line2 = f.read(len(line))\n    if line != line2:\n        raise RuntimeError, 'bad result after seek back'\n    f.seek(len(line2), 1)\n    list = f.readlines()\n    line = list[-1]\n    f.seek(f.tell() - len(line))\n    line2 = f.read()\n    if line != line2:\n        raise RuntimeError, 'bad result after seek back from EOF'\n    print 'Read', len(list), 'more lines'\n    print 'File length =', f.tell()\n    if f.tell() != length:\n        raise RuntimeError, 'bad length'\n    f.truncate(length/2)\n    f.seek(0, 2)\n    print 'Truncated length =', f.tell()\n    if f.tell() != length/2:\n        raise RuntimeError, 'truncate did not adjust length'\n    f.close()\n\nif __name__ == '__main__':\n    test()\n", 
    "UserDict": "\"\"\"A more or less complete user-defined wrapper around dictionary objects.\"\"\"\n\nclass UserDict:\n    def __init__(self, dict=None, **kwargs):\n        self.data = {}\n        if dict is not None:\n            self.update(dict)\n        if len(kwargs):\n            self.update(kwargs)\n    def __repr__(self): return repr(self.data)\n    def __cmp__(self, dict):\n        if isinstance(dict, UserDict):\n            return cmp(self.data, dict.data)\n        else:\n            return cmp(self.data, dict)\n    __hash__ = None # Avoid Py3k warning\n    def __len__(self): return len(self.data)\n    def __getitem__(self, key):\n        if key in self.data:\n            return self.data[key]\n        if hasattr(self.__class__, \"__missing__\"):\n            return self.__class__.__missing__(self, key)\n        raise KeyError(key)\n    def __setitem__(self, key, item): self.data[key] = item\n    def __delitem__(self, key): del self.data[key]\n    def clear(self): self.data.clear()\n    def copy(self):\n        if self.__class__ is UserDict:\n            return UserDict(self.data.copy())\n        import copy\n        data = self.data\n        try:\n            self.data = {}\n            c = copy.copy(self)\n        finally:\n            self.data = data\n        c.update(self)\n        return c\n    def keys(self): return self.data.keys()\n    def items(self): return self.data.items()\n    def iteritems(self): return self.data.iteritems()\n    def iterkeys(self): return self.data.iterkeys()\n    def itervalues(self): return self.data.itervalues()\n    def values(self): return self.data.values()\n    def has_key(self, key): return key in self.data\n    def update(self, dict=None, **kwargs):\n        if dict is None:\n            pass\n        elif isinstance(dict, UserDict):\n            self.data.update(dict.data)\n        elif isinstance(dict, type({})) or not hasattr(dict, 'items'):\n            self.data.update(dict)\n        else:\n            for k, v in dict.items():\n                self[k] = v\n        if len(kwargs):\n            self.data.update(kwargs)\n    def get(self, key, failobj=None):\n        if key not in self:\n            return failobj\n        return self[key]\n    def setdefault(self, key, failobj=None):\n        if key not in self:\n            self[key] = failobj\n        return self[key]\n    def pop(self, key, *args):\n        return self.data.pop(key, *args)\n    def popitem(self):\n        return self.data.popitem()\n    def __contains__(self, key):\n        return key in self.data\n    @classmethod\n    def fromkeys(cls, iterable, value=None):\n        d = cls()\n        for key in iterable:\n            d[key] = value\n        return d\n\nclass IterableUserDict(UserDict):\n    def __iter__(self):\n        return iter(self.data)\n\ntry:\n    import _abcoll\nexcept ImportError:\n    pass    # e.g. no '_weakref' module on this pypy\nelse:\n    _abcoll.MutableMapping.register(IterableUserDict)\n\n\nclass DictMixin:\n    # Mixin defining all dictionary methods for classes that already have\n    # a minimum dictionary interface including getitem, setitem, delitem,\n    # and keys. Without knowledge of the subclass constructor, the mixin\n    # does not define __init__() or copy().  In addition to the four base\n    # methods, progressively more efficiency comes with defining\n    # __contains__(), __iter__(), and iteritems().\n\n    # second level definitions support higher levels\n    def __iter__(self):\n        for k in self.keys():\n            yield k\n    def has_key(self, key):\n        try:\n            self[key]\n        except KeyError:\n            return False\n        return True\n    def __contains__(self, key):\n        return self.has_key(key)\n\n    # third level takes advantage of second level definitions\n    def iteritems(self):\n        for k in self:\n            yield (k, self[k])\n    def iterkeys(self):\n        return self.__iter__()\n\n    # fourth level uses definitions from lower levels\n    def itervalues(self):\n        for _, v in self.iteritems():\n            yield v\n    def values(self):\n        return [v for _, v in self.iteritems()]\n    def items(self):\n        return list(self.iteritems())\n    def clear(self):\n        for key in self.keys():\n            del self[key]\n    def setdefault(self, key, default=None):\n        try:\n            return self[key]\n        except KeyError:\n            self[key] = default\n        return default\n    def pop(self, key, *args):\n        if len(args) > 1:\n            raise TypeError, \"pop expected at most 2 arguments, got \"\\\n                              + repr(1 + len(args))\n        try:\n            value = self[key]\n        except KeyError:\n            if args:\n                return args[0]\n            raise\n        del self[key]\n        return value\n    def popitem(self):\n        try:\n            k, v = self.iteritems().next()\n        except StopIteration:\n            raise KeyError, 'container is empty'\n        del self[k]\n        return (k, v)\n    def update(self, other=None, **kwargs):\n        # Make progressively weaker assumptions about \"other\"\n        if other is None:\n            pass\n        elif hasattr(other, 'iteritems'):  # iteritems saves memory and lookups\n            for k, v in other.iteritems():\n                self[k] = v\n        elif hasattr(other, 'keys'):\n            for k in other.keys():\n                self[k] = other[k]\n        else:\n            for k, v in other:\n                self[k] = v\n        if kwargs:\n            self.update(kwargs)\n    def get(self, key, default=None):\n        try:\n            return self[key]\n        except KeyError:\n            return default\n    def __repr__(self):\n        return repr(dict(self.iteritems()))\n    def __cmp__(self, other):\n        if other is None:\n            return 1\n        if isinstance(other, DictMixin):\n            other = dict(other.iteritems())\n        return cmp(dict(self.iteritems()), other)\n    def __len__(self):\n        return len(self.keys())\n", 
    "__future__": "\"\"\"Record of phased-in incompatible language changes.\n\nEach line is of the form:\n\n    FeatureName = \"_Feature(\" OptionalRelease \",\" MandatoryRelease \",\"\n                              CompilerFlag \")\"\n\nwhere, normally, OptionalRelease < MandatoryRelease, and both are 5-tuples\nof the same form as sys.version_info:\n\n    (PY_MAJOR_VERSION, # the 2 in 2.1.0a3; an int\n     PY_MINOR_VERSION, # the 1; an int\n     PY_MICRO_VERSION, # the 0; an int\n     PY_RELEASE_LEVEL, # \"alpha\", \"beta\", \"candidate\" or \"final\"; string\n     PY_RELEASE_SERIAL # the 3; an int\n    )\n\nOptionalRelease records the first release in which\n\n    from __future__ import FeatureName\n\nwas accepted.\n\nIn the case of MandatoryReleases that have not yet occurred,\nMandatoryRelease predicts the release in which the feature will become part\nof the language.\n\nElse MandatoryRelease records when the feature became part of the language;\nin releases at or after that, modules no longer need\n\n    from __future__ import FeatureName\n\nto use the feature in question, but may continue to use such imports.\n\nMandatoryRelease may also be None, meaning that a planned feature got\ndropped.\n\nInstances of class _Feature have two corresponding methods,\n.getOptionalRelease() and .getMandatoryRelease().\n\nCompilerFlag is the (bitfield) flag that should be passed in the fourth\nargument to the builtin function compile() to enable the feature in\ndynamically compiled code.  This flag is stored in the .compiler_flag\nattribute on _Future instances.  These values must match the appropriate\n#defines of CO_xxx flags in Include/compile.h.\n\nNo feature line is ever to be deleted from this file.\n\"\"\"\n\nall_feature_names = [\n    \"nested_scopes\",\n    \"generators\",\n    \"division\",\n    \"absolute_import\",\n    \"with_statement\",\n    \"print_function\",\n    \"unicode_literals\",\n]\n\n__all__ = [\"all_feature_names\"] + all_feature_names\n\n# The CO_xxx symbols are defined here under the same names used by\n# compile.h, so that an editor search will find them here.  However,\n# they're not exported in __all__, because they don't really belong to\n# this module.\nCO_NESTED            = 0x0010   # nested_scopes\nCO_GENERATOR_ALLOWED = 0        # generators (obsolete, was 0x1000)\nCO_FUTURE_DIVISION   = 0x2000   # division\nCO_FUTURE_ABSOLUTE_IMPORT = 0x4000 # perform absolute imports by default\nCO_FUTURE_WITH_STATEMENT  = 0x8000   # with statement\nCO_FUTURE_PRINT_FUNCTION  = 0x10000   # print function\nCO_FUTURE_UNICODE_LITERALS = 0x20000 # unicode string literals\n\nclass _Feature:\n    def __init__(self, optionalRelease, mandatoryRelease, compiler_flag):\n        self.optional = optionalRelease\n        self.mandatory = mandatoryRelease\n        self.compiler_flag = compiler_flag\n\n    def getOptionalRelease(self):\n        \"\"\"Return first release in which this feature was recognized.\n\n        This is a 5-tuple, of the same form as sys.version_info.\n        \"\"\"\n\n        return self.optional\n\n    def getMandatoryRelease(self):\n        \"\"\"Return release in which this feature will become mandatory.\n\n        This is a 5-tuple, of the same form as sys.version_info, or, if\n        the feature was dropped, is None.\n        \"\"\"\n\n        return self.mandatory\n\n    def __repr__(self):\n        return \"_Feature\" + repr((self.optional,\n                                  self.mandatory,\n                                  self.compiler_flag))\n\nnested_scopes = _Feature((2, 1, 0, \"beta\",  1),\n                         (2, 2, 0, \"alpha\", 0),\n                         CO_NESTED)\n\ngenerators = _Feature((2, 2, 0, \"alpha\", 1),\n                      (2, 3, 0, \"final\", 0),\n                      CO_GENERATOR_ALLOWED)\n\ndivision = _Feature((2, 2, 0, \"alpha\", 2),\n                    (3, 0, 0, \"alpha\", 0),\n                    CO_FUTURE_DIVISION)\n\nabsolute_import = _Feature((2, 5, 0, \"alpha\", 1),\n                           (3, 0, 0, \"alpha\", 0),\n                           CO_FUTURE_ABSOLUTE_IMPORT)\n\nwith_statement = _Feature((2, 5, 0, \"alpha\", 1),\n                          (2, 6, 0, \"alpha\", 0),\n                          CO_FUTURE_WITH_STATEMENT)\n\nprint_function = _Feature((2, 6, 0, \"alpha\", 2),\n                          (3, 0, 0, \"alpha\", 0),\n                          CO_FUTURE_PRINT_FUNCTION)\n\nunicode_literals = _Feature((2, 6, 0, \"alpha\", 2),\n                            (3, 0, 0, \"alpha\", 0),\n                            CO_FUTURE_UNICODE_LITERALS)\n", 
    "_abcoll": "# Copyright 2007 Google, Inc. All Rights Reserved.\n# Licensed to PSF under a Contributor Agreement.\n\n\"\"\"Abstract Base Classes (ABCs) for collections, according to PEP 3119.\n\nDON'T USE THIS MODULE DIRECTLY!  The classes here should be imported\nvia collections; they are defined here only to alleviate certain\nbootstrapping issues.  Unit tests are in test_collections.\n\"\"\"\n\nfrom abc import ABCMeta, abstractmethod\nimport sys\n\n__all__ = [\"Hashable\", \"Iterable\", \"Iterator\",\n           \"Sized\", \"Container\", \"Callable\",\n           \"Set\", \"MutableSet\",\n           \"Mapping\", \"MutableMapping\",\n           \"MappingView\", \"KeysView\", \"ItemsView\", \"ValuesView\",\n           \"Sequence\", \"MutableSequence\",\n           ]\n\n### ONE-TRICK PONIES ###\n\ndef _hasattr(C, attr):\n    try:\n        return any(attr in B.__dict__ for B in C.__mro__)\n    except AttributeError:\n        # Old-style class\n        return hasattr(C, attr)\n\n\nclass Hashable:\n    __metaclass__ = ABCMeta\n\n    @abstractmethod\n    def __hash__(self):\n        return 0\n\n    @classmethod\n    def __subclasshook__(cls, C):\n        if cls is Hashable:\n            try:\n                for B in C.__mro__:\n                    if \"__hash__\" in B.__dict__:\n                        if B.__dict__[\"__hash__\"]:\n                            return True\n                        break\n            except AttributeError:\n                # Old-style class\n                if getattr(C, \"__hash__\", None):\n                    return True\n        return NotImplemented\n\n\nclass Iterable:\n    __metaclass__ = ABCMeta\n\n    @abstractmethod\n    def __iter__(self):\n        while False:\n            yield None\n\n    @classmethod\n    def __subclasshook__(cls, C):\n        if cls is Iterable:\n            if _hasattr(C, \"__iter__\"):\n                return True\n        return NotImplemented\n\nIterable.register(str)\n\n\nclass Iterator(Iterable):\n\n    @abstractmethod\n    def next(self):\n        'Return the next item from the iterator. When exhausted, raise StopIteration'\n        raise StopIteration\n\n    def __iter__(self):\n        return self\n\n    @classmethod\n    def __subclasshook__(cls, C):\n        if cls is Iterator:\n            if _hasattr(C, \"next\") and _hasattr(C, \"__iter__\"):\n                return True\n        return NotImplemented\n\n\nclass Sized:\n    __metaclass__ = ABCMeta\n\n    @abstractmethod\n    def __len__(self):\n        return 0\n\n    @classmethod\n    def __subclasshook__(cls, C):\n        if cls is Sized:\n            if _hasattr(C, \"__len__\"):\n                return True\n        return NotImplemented\n\n\nclass Container:\n    __metaclass__ = ABCMeta\n\n    @abstractmethod\n    def __contains__(self, x):\n        return False\n\n    @classmethod\n    def __subclasshook__(cls, C):\n        if cls is Container:\n            if _hasattr(C, \"__contains__\"):\n                return True\n        return NotImplemented\n\n\nclass Callable:\n    __metaclass__ = ABCMeta\n\n    @abstractmethod\n    def __call__(self, *args, **kwds):\n        return False\n\n    @classmethod\n    def __subclasshook__(cls, C):\n        if cls is Callable:\n            if _hasattr(C, \"__call__\"):\n                return True\n        return NotImplemented\n\n\n### SETS ###\n\n\nclass Set(Sized, Iterable, Container):\n    \"\"\"A set is a finite, iterable container.\n\n    This class provides concrete generic implementations of all\n    methods except for __contains__, __iter__ and __len__.\n\n    To override the comparisons (presumably for speed, as the\n    semantics are fixed), redefine __le__ and __ge__,\n    then the other operations will automatically follow suit.\n    \"\"\"\n\n    def __le__(self, other):\n        if not isinstance(other, Set):\n            return NotImplemented\n        if len(self) > len(other):\n            return False\n        for elem in self:\n            if elem not in other:\n                return False\n        return True\n\n    def __lt__(self, other):\n        if not isinstance(other, Set):\n            return NotImplemented\n        return len(self) < len(other) and self.__le__(other)\n\n    def __gt__(self, other):\n        if not isinstance(other, Set):\n            return NotImplemented\n        return len(self) > len(other) and self.__ge__(other)\n\n    def __ge__(self, other):\n        if not isinstance(other, Set):\n            return NotImplemented\n        if len(self) < len(other):\n            return False\n        for elem in other:\n            if elem not in self:\n                return False\n        return True\n\n    def __eq__(self, other):\n        if not isinstance(other, Set):\n            return NotImplemented\n        return len(self) == len(other) and self.__le__(other)\n\n    def __ne__(self, other):\n        return not (self == other)\n\n    @classmethod\n    def _from_iterable(cls, it):\n        '''Construct an instance of the class from any iterable input.\n\n        Must override this method if the class constructor signature\n        does not accept an iterable for an input.\n        '''\n        return cls(it)\n\n    def __and__(self, other):\n        if not isinstance(other, Iterable):\n            return NotImplemented\n        return self._from_iterable(value for value in other if value in self)\n\n    __rand__ = __and__\n\n    def isdisjoint(self, other):\n        'Return True if two sets have a null intersection.'\n        for value in other:\n            if value in self:\n                return False\n        return True\n\n    def __or__(self, other):\n        if not isinstance(other, Iterable):\n            return NotImplemented\n        chain = (e for s in (self, other) for e in s)\n        return self._from_iterable(chain)\n\n    __ror__ = __or__\n\n    def __sub__(self, other):\n        if not isinstance(other, Set):\n            if not isinstance(other, Iterable):\n                return NotImplemented\n            other = self._from_iterable(other)\n        return self._from_iterable(value for value in self\n                                   if value not in other)\n\n    def __rsub__(self, other):\n        if not isinstance(other, Set):\n            if not isinstance(other, Iterable):\n                return NotImplemented\n            other = self._from_iterable(other)\n        return self._from_iterable(value for value in other\n                                   if value not in self)\n\n    def __xor__(self, other):\n        if not isinstance(other, Set):\n            if not isinstance(other, Iterable):\n                return NotImplemented\n            other = self._from_iterable(other)\n        return (self - other) | (other - self)\n\n    __rxor__ = __xor__\n\n    # Sets are not hashable by default, but subclasses can change this\n    __hash__ = None\n\n    def _hash(self):\n        \"\"\"Compute the hash value of a set.\n\n        Note that we don't define __hash__: not all sets are hashable.\n        But if you define a hashable set type, its __hash__ should\n        call this function.\n\n        This must be compatible __eq__.\n\n        All sets ought to compare equal if they contain the same\n        elements, regardless of how they are implemented, and\n        regardless of the order of the elements; so there's not much\n        freedom for __eq__ or __hash__.  We match the algorithm used\n        by the built-in frozenset type.\n        \"\"\"\n        MAX = sys.maxint\n        MASK = 2 * MAX + 1\n        n = len(self)\n        h = 1927868237 * (n + 1)\n        h &= MASK\n        for x in self:\n            hx = hash(x)\n            h ^= (hx ^ (hx << 16) ^ 89869747)  * 3644798167\n            h &= MASK\n        h = h * 69069 + 907133923\n        h &= MASK\n        if h > MAX:\n            h -= MASK + 1\n        if h == -1:\n            h = 590923713\n        return h\n\nSet.register(frozenset)\n\n\nclass MutableSet(Set):\n    \"\"\"A mutable set is a finite, iterable container.\n\n    This class provides concrete generic implementations of all\n    methods except for __contains__, __iter__, __len__,\n    add(), and discard().\n\n    To override the comparisons (presumably for speed, as the\n    semantics are fixed), all you have to do is redefine __le__ and\n    then the other operations will automatically follow suit.\n    \"\"\"\n\n    @abstractmethod\n    def add(self, value):\n        \"\"\"Add an element.\"\"\"\n        raise NotImplementedError\n\n    @abstractmethod\n    def discard(self, value):\n        \"\"\"Remove an element.  Do not raise an exception if absent.\"\"\"\n        raise NotImplementedError\n\n    def remove(self, value):\n        \"\"\"Remove an element. If not a member, raise a KeyError.\"\"\"\n        if value not in self:\n            raise KeyError(value)\n        self.discard(value)\n\n    def pop(self):\n        \"\"\"Return the popped value.  Raise KeyError if empty.\"\"\"\n        it = iter(self)\n        try:\n            value = next(it)\n        except StopIteration:\n            raise KeyError\n        self.discard(value)\n        return value\n\n    def clear(self):\n        \"\"\"This is slow (creates N new iterators!) but effective.\"\"\"\n        try:\n            while True:\n                self.pop()\n        except KeyError:\n            pass\n\n    def __ior__(self, it):\n        for value in it:\n            self.add(value)\n        return self\n\n    def __iand__(self, it):\n        for value in (self - it):\n            self.discard(value)\n        return self\n\n    def __ixor__(self, it):\n        if it is self:\n            self.clear()\n        else:\n            if not isinstance(it, Set):\n                it = self._from_iterable(it)\n            for value in it:\n                if value in self:\n                    self.discard(value)\n                else:\n                    self.add(value)\n        return self\n\n    def __isub__(self, it):\n        if it is self:\n            self.clear()\n        else:\n            for value in it:\n                self.discard(value)\n        return self\n\nMutableSet.register(set)\n\n\n### MAPPINGS ###\n\n\nclass Mapping(Sized, Iterable, Container):\n\n    \"\"\"A Mapping is a generic container for associating key/value\n    pairs.\n\n    This class provides concrete generic implementations of all\n    methods except for __getitem__, __iter__, and __len__.\n\n    \"\"\"\n\n    @abstractmethod\n    def __getitem__(self, key):\n        raise KeyError\n\n    def get(self, key, default=None):\n        'D.get(k[,d]) -> D[k] if k in D, else d.  d defaults to None.'\n        try:\n            return self[key]\n        except KeyError:\n            return default\n\n    def __contains__(self, key):\n        try:\n            self[key]\n        except KeyError:\n            return False\n        else:\n            return True\n\n    def iterkeys(self):\n        'D.iterkeys() -> an iterator over the keys of D'\n        return iter(self)\n\n    def itervalues(self):\n        'D.itervalues() -> an iterator over the values of D'\n        for key in self:\n            yield self[key]\n\n    def iteritems(self):\n        'D.iteritems() -> an iterator over the (key, value) items of D'\n        for key in self:\n            yield (key, self[key])\n\n    def keys(self):\n        \"D.keys() -> list of D's keys\"\n        return list(self)\n\n    def items(self):\n        \"D.items() -> list of D's (key, value) pairs, as 2-tuples\"\n        return [(key, self[key]) for key in self]\n\n    def values(self):\n        \"D.values() -> list of D's values\"\n        return [self[key] for key in self]\n\n    # Mappings are not hashable by default, but subclasses can change this\n    __hash__ = None\n\n    def __eq__(self, other):\n        if not isinstance(other, Mapping):\n            return NotImplemented\n        return dict(self.items()) == dict(other.items())\n\n    def __ne__(self, other):\n        return not (self == other)\n\nclass MappingView(Sized):\n\n    def __init__(self, mapping):\n        self._mapping = mapping\n\n    def __len__(self):\n        return len(self._mapping)\n\n    def __repr__(self):\n        return '{0.__class__.__name__}({0._mapping!r})'.format(self)\n\n\nclass KeysView(MappingView, Set):\n\n    @classmethod\n    def _from_iterable(self, it):\n        return set(it)\n\n    def __contains__(self, key):\n        return key in self._mapping\n\n    def __iter__(self):\n        for key in self._mapping:\n            yield key\n\n\nclass ItemsView(MappingView, Set):\n\n    @classmethod\n    def _from_iterable(self, it):\n        return set(it)\n\n    def __contains__(self, item):\n        key, value = item\n        try:\n            v = self._mapping[key]\n        except KeyError:\n            return False\n        else:\n            return v == value\n\n    def __iter__(self):\n        for key in self._mapping:\n            yield (key, self._mapping[key])\n\n\nclass ValuesView(MappingView):\n\n    def __contains__(self, value):\n        for key in self._mapping:\n            if value == self._mapping[key]:\n                return True\n        return False\n\n    def __iter__(self):\n        for key in self._mapping:\n            yield self._mapping[key]\n\n\nclass MutableMapping(Mapping):\n\n    \"\"\"A MutableMapping is a generic container for associating\n    key/value pairs.\n\n    This class provides concrete generic implementations of all\n    methods except for __getitem__, __setitem__, __delitem__,\n    __iter__, and __len__.\n\n    \"\"\"\n\n    @abstractmethod\n    def __setitem__(self, key, value):\n        raise KeyError\n\n    @abstractmethod\n    def __delitem__(self, key):\n        raise KeyError\n\n    __marker = object()\n\n    def pop(self, key, default=__marker):\n        '''D.pop(k[,d]) -> v, remove specified key and return the corresponding value.\n          If key is not found, d is returned if given, otherwise KeyError is raised.\n        '''\n        try:\n            value = self[key]\n        except KeyError:\n            if default is self.__marker:\n                raise\n            return default\n        else:\n            del self[key]\n            return value\n\n    def popitem(self):\n        '''D.popitem() -> (k, v), remove and return some (key, value) pair\n           as a 2-tuple; but raise KeyError if D is empty.\n        '''\n        try:\n            key = next(iter(self))\n        except StopIteration:\n            raise KeyError\n        value = self[key]\n        del self[key]\n        return key, value\n\n    def clear(self):\n        'D.clear() -> None.  Remove all items from D.'\n        try:\n            while True:\n                self.popitem()\n        except KeyError:\n            pass\n\n    def update(*args, **kwds):\n        ''' D.update([E, ]**F) -> None.  Update D from mapping/iterable E and F.\n            If E present and has a .keys() method, does:     for k in E: D[k] = E[k]\n            If E present and lacks .keys() method, does:     for (k, v) in E: D[k] = v\n            In either case, this is followed by: for k, v in F.items(): D[k] = v\n        '''\n        if len(args) > 2:\n            raise TypeError(\"update() takes at most 2 positional \"\n                            \"arguments ({} given)\".format(len(args)))\n        elif not args:\n            raise TypeError(\"update() takes at least 1 argument (0 given)\")\n        self = args[0]\n        other = args[1] if len(args) >= 2 else ()\n\n        if isinstance(other, Mapping):\n            for key in other:\n                self[key] = other[key]\n        elif hasattr(other, \"keys\"):\n            for key in other.keys():\n                self[key] = other[key]\n        else:\n            for key, value in other:\n                self[key] = value\n        for key, value in kwds.items():\n            self[key] = value\n\n    def setdefault(self, key, default=None):\n        'D.setdefault(k[,d]) -> D.get(k,d), also set D[k]=d if k not in D'\n        try:\n            return self[key]\n        except KeyError:\n            self[key] = default\n        return default\n\nMutableMapping.register(dict)\n\n\n### SEQUENCES ###\n\n\nclass Sequence(Sized, Iterable, Container):\n    \"\"\"All the operations on a read-only sequence.\n\n    Concrete subclasses must override __new__ or __init__,\n    __getitem__, and __len__.\n    \"\"\"\n\n    @abstractmethod\n    def __getitem__(self, index):\n        raise IndexError\n\n    def __iter__(self):\n        i = 0\n        try:\n            while True:\n                v = self[i]\n                yield v\n                i += 1\n        except IndexError:\n            return\n\n    def __contains__(self, value):\n        for v in self:\n            if v == value:\n                return True\n        return False\n\n    def __reversed__(self):\n        for i in reversed(range(len(self))):\n            yield self[i]\n\n    def index(self, value):\n        '''S.index(value) -> integer -- return first index of value.\n           Raises ValueError if the value is not present.\n        '''\n        for i, v in enumerate(self):\n            if v == value:\n                return i\n        raise ValueError\n\n    def count(self, value):\n        'S.count(value) -> integer -- return number of occurrences of value'\n        return sum(1 for v in self if v == value)\n\nSequence.register(tuple)\nSequence.register(basestring)\nSequence.register(buffer)\nSequence.register(xrange)\n\n\nclass MutableSequence(Sequence):\n\n    \"\"\"All the operations on a read-only sequence.\n\n    Concrete subclasses must provide __new__ or __init__,\n    __getitem__, __setitem__, __delitem__, __len__, and insert().\n\n    \"\"\"\n\n    @abstractmethod\n    def __setitem__(self, index, value):\n        raise IndexError\n\n    @abstractmethod\n    def __delitem__(self, index):\n        raise IndexError\n\n    @abstractmethod\n    def insert(self, index, value):\n        'S.insert(index, object) -- insert object before index'\n        raise IndexError\n\n    def append(self, value):\n        'S.append(object) -- append object to the end of the sequence'\n        self.insert(len(self), value)\n\n    def reverse(self):\n        'S.reverse() -- reverse *IN PLACE*'\n        n = len(self)\n        for i in range(n//2):\n            self[i], self[n-i-1] = self[n-i-1], self[i]\n\n    def extend(self, values):\n        'S.extend(iterable) -- extend sequence by appending elements from the iterable'\n        for v in values:\n            self.append(v)\n\n    def pop(self, index=-1):\n        '''S.pop([index]) -> item -- remove and return item at index (default last).\n           Raise IndexError if list is empty or index is out of range.\n        '''\n        v = self[index]\n        del self[index]\n        return v\n\n    def remove(self, value):\n        '''S.remove(value) -- remove first occurrence of value.\n           Raise ValueError if the value is not present.\n        '''\n        del self[self.index(value)]\n\n    def __iadd__(self, values):\n        self.extend(values)\n        return self\n\nMutableSequence.register(list)\n", 
    "_functools": "\"\"\" Supplies the internal functions for functools.py in the standard library \"\"\"\n\n# reduce() has moved to _functools in Python 2.6+.\nreduce = reduce\n\nclass partial(object):\n    \"\"\"\n    partial(func, *args, **keywords) - new function with partial application\n    of the given arguments and keywords.\n    \"\"\"\n    def __init__(*args, **keywords):\n        if len(args) < 2:\n            raise TypeError('__init__() takes at least 2 arguments (%d given)'\n                            % len(args))\n        self, func, args = args[0], args[1], args[2:]\n        if not callable(func):\n            raise TypeError(\"the first argument must be callable\")\n        self._func = func\n        self._args = args\n        self._keywords = keywords\n\n    def __delattr__(self, key):\n        if key == '__dict__':\n            raise TypeError(\"a partial object's dictionary may not be deleted\")\n        object.__delattr__(self, key)\n\n    @property\n    def func(self):\n        return self._func\n\n    @property\n    def args(self):\n        return self._args\n\n    @property\n    def keywords(self):\n        return self._keywords\n\n    def __call__(self, *fargs, **fkeywords):\n        if self._keywords:\n            fkeywords = dict(self._keywords, **fkeywords)\n        return self._func(*(self._args + fargs), **fkeywords)\n\n    def __reduce__(self):\n        d = dict((k, v) for k, v in self.__dict__.iteritems() if k not in\n                ('_func', '_args', '_keywords'))\n        if len(d) == 0:\n            d = None\n        return (type(self), (self._func,),\n                (self._func, self._args, self._keywords, d))\n\n    def __setstate__(self, state):\n        func, args, keywords, d = state\n        if d is not None:\n            self.__dict__.update(d)\n        self._func = func\n        self._args = args\n        self._keywords = keywords\n", 
    "_marshal": "\"\"\"Internal Python object serialization\n\nThis module contains functions that can read and write Python values in a binary format. The format is specific to Python, but independent of machine architecture issues (e.g., you can write a Python value to a file on a PC, transport the file to a Sun, and read it back there). Details of the format may change between Python versions.\n\"\"\"\n\n# NOTE: This module is used in the Python3 interpreter, but also by\n# the \"sandboxed\" process.  It must work for Python2 as well.\n\nimport types\n\ntry:\n    intern\nexcept NameError:\n    from sys import intern\n\ntry: from __pypy__ import builtinify\nexcept ImportError: builtinify = lambda f: f\n\n\nTYPE_NULL     = '0'\nTYPE_NONE     = 'N'\nTYPE_FALSE    = 'F'\nTYPE_TRUE     = 'T'\nTYPE_STOPITER = 'S'\nTYPE_ELLIPSIS = '.'\nTYPE_INT      = 'i'\nTYPE_INT64    = 'I'\nTYPE_FLOAT    = 'f'\nTYPE_COMPLEX  = 'x'\nTYPE_LONG     = 'l'\nTYPE_STRING   = 's'\nTYPE_INTERNED = 't'\nTYPE_STRINGREF= 'R'\nTYPE_TUPLE    = '('\nTYPE_LIST     = '['\nTYPE_DICT     = '{'\nTYPE_CODE     = 'c'\nTYPE_UNICODE  = 'u'\nTYPE_UNKNOWN  = '?'\nTYPE_SET      = '<'\nTYPE_FROZENSET= '>'\n\nclass _Marshaller:\n\n    dispatch = {}\n\n    def __init__(self, writefunc):\n        self._write = writefunc\n\n    def dump(self, x):\n        try:\n            self.dispatch[type(x)](self, x)\n        except KeyError:\n            for tp in type(x).mro():\n                func = self.dispatch.get(tp)\n                if func:\n                    break\n            else:\n                raise ValueError(\"unmarshallable object\")\n            func(self, x)\n\n    def w_long64(self, x):\n        self.w_long(x)\n        self.w_long(x>>32)\n\n    def w_long(self, x):\n        a = chr(x & 0xff)\n        x >>= 8\n        b = chr(x & 0xff)\n        x >>= 8\n        c = chr(x & 0xff)\n        x >>= 8\n        d = chr(x & 0xff)\n        self._write(a + b + c + d)\n\n    def w_short(self, x):\n        self._write(chr((x)     & 0xff))\n        self._write(chr((x>> 8) & 0xff))\n\n    def dump_none(self, x):\n        self._write(TYPE_NONE)\n    dispatch[type(None)] = dump_none\n\n    def dump_bool(self, x):\n        if x:\n            self._write(TYPE_TRUE)\n        else:\n            self._write(TYPE_FALSE)\n    dispatch[bool] = dump_bool\n\n    def dump_stopiter(self, x):\n        if x is not StopIteration:\n            raise ValueError(\"unmarshallable object\")\n        self._write(TYPE_STOPITER)\n    dispatch[type(StopIteration)] = dump_stopiter\n\n    def dump_ellipsis(self, x):\n        self._write(TYPE_ELLIPSIS)\n    \n    try:\n        dispatch[type(Ellipsis)] = dump_ellipsis\n    except NameError:\n        pass\n\n    # In Python3, this function is not used; see dump_long() below.\n    def dump_int(self, x):\n        y = x>>31\n        if y and y != -1:\n            self._write(TYPE_INT64)\n            self.w_long64(x)\n        else:\n            self._write(TYPE_INT)\n            self.w_long(x)\n    dispatch[int] = dump_int\n\n    def dump_long(self, x):\n        self._write(TYPE_LONG)\n        sign = 1\n        if x < 0:\n            sign = -1\n            x = -x\n        digits = []\n        while x:\n            digits.append(x & 0x7FFF)\n            x = x>>15\n        self.w_long(len(digits) * sign)\n        for d in digits:\n            self.w_short(d)\n    try:\n        long\n    except NameError:\n        dispatch[int] = dump_long\n    else:\n        dispatch[long] = dump_long\n\n    def dump_float(self, x):\n        write = self._write\n        write(TYPE_FLOAT)\n        s = repr(x)\n        write(chr(len(s)))\n        write(s)\n    dispatch[float] = dump_float\n\n    def dump_complex(self, x):\n        write = self._write\n        write(TYPE_COMPLEX)\n        s = repr(x.real)\n        write(chr(len(s)))\n        write(s)\n        s = repr(x.imag)\n        write(chr(len(s)))\n        write(s)\n    try:\n        dispatch[complex] = dump_complex\n    except NameError:\n        pass\n\n    def dump_string(self, x):\n        # XXX we can't check for interned strings, yet,\n        # so we (for now) never create TYPE_INTERNED or TYPE_STRINGREF\n        self._write(TYPE_STRING)\n        self.w_long(len(x))\n        self._write(x)\n    dispatch[bytes] = dump_string\n\n    def dump_unicode(self, x):\n        self._write(TYPE_UNICODE)\n        s = x.encode('utf8')\n        self.w_long(len(s))\n        self._write(s)\n    try:\n        unicode\n    except NameError:\n        dispatch[str] = dump_unicode\n    else:\n        dispatch[unicode] = dump_unicode\n\n    def dump_tuple(self, x):\n        self._write(TYPE_TUPLE)\n        self.w_long(len(x))\n        for item in x:\n            self.dump(item)\n    dispatch[tuple] = dump_tuple\n\n    def dump_list(self, x):\n        self._write(TYPE_LIST)\n        self.w_long(len(x))\n        for item in x:\n            self.dump(item)\n    dispatch[list] = dump_list\n\n    def dump_dict(self, x):\n        self._write(TYPE_DICT)\n        for key, value in x.items():\n            self.dump(key)\n            self.dump(value)\n        self._write(TYPE_NULL)\n    dispatch[dict] = dump_dict\n\n    def dump_code(self, x):\n        self._write(TYPE_CODE)\n        self.w_long(x.co_argcount)\n        self.w_long(x.co_nlocals)\n        self.w_long(x.co_stacksize)\n        self.w_long(x.co_flags)\n        self.dump(x.co_code)\n        self.dump(x.co_consts)\n        self.dump(x.co_names)\n        self.dump(x.co_varnames)\n        self.dump(x.co_freevars)\n        self.dump(x.co_cellvars)\n        self.dump(x.co_filename)\n        self.dump(x.co_name)\n        self.w_long(x.co_firstlineno)\n        self.dump(x.co_lnotab)\n    try:\n        dispatch[types.CodeType] = dump_code\n    except NameError:\n        pass\n\n    def dump_set(self, x):\n        self._write(TYPE_SET)\n        self.w_long(len(x))\n        for each in x:\n            self.dump(each)\n    try:\n        dispatch[set] = dump_set\n    except NameError:\n        pass\n\n    def dump_frozenset(self, x):\n        self._write(TYPE_FROZENSET)\n        self.w_long(len(x))\n        for each in x:\n            self.dump(each)\n    try:\n        dispatch[frozenset] = dump_frozenset\n    except NameError:\n        pass\n\nclass _NULL:\n    pass\n\nclass _StringBuffer:\n    def __init__(self, value):\n        self.bufstr = value\n        self.bufpos = 0\n\n    def read(self, n):\n        pos = self.bufpos\n        newpos = pos + n\n        ret = self.bufstr[pos : newpos]\n        self.bufpos = newpos\n        return ret\n\n\nclass _Unmarshaller:\n\n    dispatch = {}\n\n    def __init__(self, readfunc):\n        self._read = readfunc\n        self._stringtable = []\n\n    def load(self):\n        c = self._read(1)\n        if not c:\n            raise EOFError\n        try:\n            return self.dispatch[c](self)\n        except KeyError:\n            raise ValueError(\"bad marshal code: %c (%d)\" % (c, ord(c)))\n\n    def r_short(self):\n        lo = ord(self._read(1))\n        hi = ord(self._read(1))\n        x = lo | (hi<<8)\n        if x & 0x8000:\n            x = x - 0x10000\n        return x\n\n    def r_long(self):\n        s = self._read(4)\n        a = ord(s[0])\n        b = ord(s[1])\n        c = ord(s[2])\n        d = ord(s[3])\n        x = a | (b<<8) | (c<<16) | (d<<24)\n        if d & 0x80 and x > 0:\n            x = -((1<<32) - x)\n            return int(x)\n        else:\n            return x\n\n    def r_long64(self):\n        a = ord(self._read(1))\n        b = ord(self._read(1))\n        c = ord(self._read(1))\n        d = ord(self._read(1))\n        e = ord(self._read(1))\n        f = ord(self._read(1))\n        g = ord(self._read(1))\n        h = ord(self._read(1))\n        x = a | (b<<8) | (c<<16) | (d<<24)\n        x = x | (e<<32) | (f<<40) | (g<<48) | (h<<56)\n        if h & 0x80 and x > 0:\n            x = -((1<<64) - x)\n        return x\n\n    def load_null(self):\n        return _NULL\n    dispatch[TYPE_NULL] = load_null\n\n    def load_none(self):\n        return None\n    dispatch[TYPE_NONE] = load_none\n\n    def load_true(self):\n        return True\n    dispatch[TYPE_TRUE] = load_true\n\n    def load_false(self):\n        return False\n    dispatch[TYPE_FALSE] = load_false\n\n    def load_stopiter(self):\n        return StopIteration\n    dispatch[TYPE_STOPITER] = load_stopiter\n\n    def load_ellipsis(self):\n        return Ellipsis\n    dispatch[TYPE_ELLIPSIS] = load_ellipsis\n\n    dispatch[TYPE_INT] = r_long\n\n    dispatch[TYPE_INT64] = r_long64\n\n    def load_long(self):\n        size = self.r_long()\n        sign = 1\n        if size < 0:\n            sign = -1\n            size = -size\n        x = 0\n        for i in range(size):\n            d = self.r_short()\n            x = x | (d<<(i*15))\n        return x * sign\n    dispatch[TYPE_LONG] = load_long\n\n    def load_float(self):\n        n = ord(self._read(1))\n        s = self._read(n)\n        return float(s)\n    dispatch[TYPE_FLOAT] = load_float\n\n    def load_complex(self):\n        n = ord(self._read(1))\n        s = self._read(n)\n        real = float(s)\n        n = ord(self._read(1))\n        s = self._read(n)\n        imag = float(s)\n        return complex(real, imag)\n    dispatch[TYPE_COMPLEX] = load_complex\n\n    def load_string(self):\n        n = self.r_long()\n        return self._read(n)\n    dispatch[TYPE_STRING] = load_string\n\n    def load_interned(self):\n        n = self.r_long()\n        ret = intern(self._read(n))\n        self._stringtable.append(ret)\n        return ret\n    dispatch[TYPE_INTERNED] = load_interned\n\n    def load_stringref(self):\n        n = self.r_long()\n        return self._stringtable[n]\n    dispatch[TYPE_STRINGREF] = load_stringref\n\n    def load_unicode(self):\n        n = self.r_long()\n        s = self._read(n)\n        ret = s.decode('utf8')\n        return ret\n    dispatch[TYPE_UNICODE] = load_unicode\n\n    def load_tuple(self):\n        return tuple(self.load_list())\n    dispatch[TYPE_TUPLE] = load_tuple\n\n    def load_list(self):\n        n = self.r_long()\n        list = [self.load() for i in range(n)]\n        return list\n    dispatch[TYPE_LIST] = load_list\n\n    def load_dict(self):\n        d = {}\n        while 1:\n            key = self.load()\n            if key is _NULL:\n                break\n            value = self.load()\n            d[key] = value\n        return d\n    dispatch[TYPE_DICT] = load_dict\n\n    def load_code(self):\n        argcount = self.r_long()\n        nlocals = self.r_long()\n        stacksize = self.r_long()\n        flags = self.r_long()\n        code = self.load()\n        consts = self.load()\n        names = self.load()\n        varnames = self.load()\n        freevars = self.load()\n        cellvars = self.load()\n        filename = self.load()\n        name = self.load()\n        firstlineno = self.r_long()\n        lnotab = self.load()\n        return types.CodeType(argcount, nlocals, stacksize, flags, code, consts,\n                              names, varnames, filename, name, firstlineno,\n                              lnotab, freevars, cellvars)\n    dispatch[TYPE_CODE] = load_code\n\n    def load_set(self):\n        n = self.r_long()\n        args = [self.load() for i in range(n)]\n        return set(args)\n    dispatch[TYPE_SET] = load_set\n\n    def load_frozenset(self):\n        n = self.r_long()\n        args = [self.load() for i in range(n)]\n        return frozenset(args)\n    dispatch[TYPE_FROZENSET] = load_frozenset\n\n# ________________________________________________________________\n\ndef _read(self, n):\n    pos = self.bufpos\n    newpos = pos + n\n    if newpos > len(self.bufstr): raise EOFError\n    ret = self.bufstr[pos : newpos]\n    self.bufpos = newpos\n    return ret\n\ndef _read1(self):\n    ret = self.bufstr[self.bufpos]\n    self.bufpos += 1\n    return ret\n\ndef _r_short(self):\n    lo = ord(_read1(self))\n    hi = ord(_read1(self))\n    x = lo | (hi<<8)\n    if x & 0x8000:\n        x = x - 0x10000\n    return x\n\ndef _r_long(self):\n    # inlined this most common case\n    p = self.bufpos\n    s = self.bufstr\n    a = ord(s[p])\n    b = ord(s[p+1])\n    c = ord(s[p+2])\n    d = ord(s[p+3])\n    self.bufpos += 4\n    x = a | (b<<8) | (c<<16) | (d<<24)\n    if d & 0x80 and x > 0:\n        x = -((1<<32) - x)\n        return int(x)\n    else:\n        return x\n\ndef _r_long64(self):\n    a = ord(_read1(self))\n    b = ord(_read1(self))\n    c = ord(_read1(self))\n    d = ord(_read1(self))\n    e = ord(_read1(self))\n    f = ord(_read1(self))\n    g = ord(_read1(self))\n    h = ord(_read1(self))\n    x = a | (b<<8) | (c<<16) | (d<<24)\n    x = x | (e<<32) | (f<<40) | (g<<48) | (h<<56)\n    if h & 0x80 and x > 0:\n        x = -((1<<64) - x)\n    return x\n\n_load_dispatch = {}\n\nclass _FastUnmarshaller:\n\n    dispatch = {}\n\n    def __init__(self, buffer):\n        self.bufstr = buffer\n        self.bufpos = 0\n        self._stringtable = []\n\n    def load(self):\n        # make flow space happy\n        c = '?'\n        try:\n            c = self.bufstr[self.bufpos]\n            self.bufpos += 1\n            return _load_dispatch[c](self)\n        except KeyError:\n            raise ValueError(\"bad marshal code: %c (%d)\" % (c, ord(c)))\n        except IndexError:\n            raise EOFError\n\n    def load_null(self):\n        return _NULL\n    dispatch[TYPE_NULL] = load_null\n\n    def load_none(self):\n        return None\n    dispatch[TYPE_NONE] = load_none\n\n    def load_true(self):\n        return True\n    dispatch[TYPE_TRUE] = load_true\n\n    def load_false(self):\n        return False\n    dispatch[TYPE_FALSE] = load_false\n\n    def load_stopiter(self):\n        return StopIteration\n    dispatch[TYPE_STOPITER] = load_stopiter\n\n    def load_ellipsis(self):\n        return Ellipsis\n    dispatch[TYPE_ELLIPSIS] = load_ellipsis\n\n    def load_int(self):\n        return _r_long(self)\n    dispatch[TYPE_INT] = load_int\n\n    def load_int64(self):\n        return _r_long64(self)\n    dispatch[TYPE_INT64] = load_int64\n\n    def load_long(self):\n        size = _r_long(self)\n        sign = 1\n        if size < 0:\n            sign = -1\n            size = -size\n        x = 0\n        for i in range(size):\n            d = _r_short(self)\n            x = x | (d<<(i*15))\n        return x * sign\n    dispatch[TYPE_LONG] = load_long\n\n    def load_float(self):\n        n = ord(_read1(self))\n        s = _read(self, n)\n        return float(s)\n    dispatch[TYPE_FLOAT] = load_float\n\n    def load_complex(self):\n        n = ord(_read1(self))\n        s = _read(self, n)\n        real = float(s)\n        n = ord(_read1(self))\n        s = _read(self, n)\n        imag = float(s)\n        return complex(real, imag)\n    dispatch[TYPE_COMPLEX] = load_complex\n\n    def load_string(self):\n        n = _r_long(self)\n        return _read(self, n)\n    dispatch[TYPE_STRING] = load_string\n\n    def load_interned(self):\n        n = _r_long(self)\n        ret = intern(_read(self, n))\n        self._stringtable.append(ret)\n        return ret\n    dispatch[TYPE_INTERNED] = load_interned\n\n    def load_stringref(self):\n        n = _r_long(self)\n        return self._stringtable[n]\n    dispatch[TYPE_STRINGREF] = load_stringref\n\n    def load_unicode(self):\n        n = _r_long(self)\n        s = _read(self, n)\n        ret = s.decode('utf8')\n        return ret\n    dispatch[TYPE_UNICODE] = load_unicode\n\n    def load_tuple(self):\n        return tuple(self.load_list())\n    dispatch[TYPE_TUPLE] = load_tuple\n\n    def load_list(self):\n        n = _r_long(self)\n        list = []\n        for i in range(n):\n            list.append(self.load())\n        return list\n    dispatch[TYPE_LIST] = load_list\n\n    def load_dict(self):\n        d = {}\n        while 1:\n            key = self.load()\n            if key is _NULL:\n                break\n            value = self.load()\n            d[key] = value\n        return d\n    dispatch[TYPE_DICT] = load_dict\n\n    def load_code(self):\n        argcount = _r_long(self)\n        nlocals = _r_long(self)\n        stacksize = _r_long(self)\n        flags = _r_long(self)\n        code = self.load()\n        consts = self.load()\n        names = self.load()\n        varnames = self.load()\n        freevars = self.load()\n        cellvars = self.load()\n        filename = self.load()\n        name = self.load()\n        firstlineno = _r_long(self)\n        lnotab = self.load()\n        return types.CodeType(argcount, nlocals, stacksize, flags, code, consts,\n                              names, varnames, filename, name, firstlineno,\n                              lnotab, freevars, cellvars)\n    dispatch[TYPE_CODE] = load_code\n\n    def load_set(self):\n        n = _r_long(self)\n        args = [self.load() for i in range(n)]\n        return set(args)\n    dispatch[TYPE_SET] = load_set\n\n    def load_frozenset(self):\n        n = _r_long(self)\n        args = [self.load() for i in range(n)]\n        return frozenset(args)\n    dispatch[TYPE_FROZENSET] = load_frozenset\n\n_load_dispatch = _FastUnmarshaller.dispatch\n\n# _________________________________________________________________\n#\n# user interface\n\nversion = 1\n\n@builtinify\ndef dump(x, f, version=version):\n    # XXX 'version' is ignored, we always dump in a version-0-compatible format\n    m = _Marshaller(f.write)\n    m.dump(x)\n\n@builtinify\ndef load(f):\n    um = _Unmarshaller(f.read)\n    return um.load()\n\n@builtinify\ndef dumps(x, version=version):\n    # XXX 'version' is ignored, we always dump in a version-0-compatible format\n    buffer = []\n    m = _Marshaller(buffer.append)\n    m.dump(x)\n    return ''.join(buffer)\n\n@builtinify\ndef loads(s):\n    um = _FastUnmarshaller(s)\n    return um.load()\n", 
    "_md5": "#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n\n# Note that PyPy contains also a built-in module 'md5' which will hide\n# this one if compiled in.\n\n\"\"\"A sample implementation of MD5 in pure Python.\n\nThis is an implementation of the MD5 hash function, as specified by\nRFC 1321, in pure Python. It was implemented using Bruce Schneier's\nexcellent book \"Applied Cryptography\", 2nd ed., 1996.\n\nSurely this is not meant to compete with the existing implementation\nof the Python standard library (written in C). Rather, it should be\nseen as a Python complement that is more readable than C and can be\nused more conveniently for learning and experimenting purposes in\nthe field of cryptography.\n\nThis module tries very hard to follow the API of the existing Python\nstandard library's \"md5\" module, but although it seems to work fine,\nit has not been extensively tested! (But note that there is a test\nmodule, test_md5py.py, that compares this Python implementation with\nthe C one of the Python standard library.\n\nBEWARE: this comes with no guarantee whatsoever about fitness and/or\nother properties! Specifically, do not use this in any production\ncode! License is Python License!\n\nSpecial thanks to Aurelian Coman who fixed some nasty bugs!\n\nDinu C. Gherman\n\"\"\"\n\n\n__date__    = '2004-11-17'\n__version__ = 0.91 # Modernised by J. Hall\u00e9n and L. Creighton for Pypy\n\n__metaclass__ = type # or genrpy won't work\n\nimport struct, copy\n\n\n# ======================================================================\n# Bit-Manipulation helpers\n# ======================================================================\n\ndef _bytelist2long(list):\n    \"Transform a list of characters into a list of longs.\"\n\n    imax = len(list) // 4\n    hl = [0] * imax\n\n    j = 0\n    i = 0\n    while i < imax:\n        b0 = ord(list[j])\n        b1 = ord(list[j+1]) << 8\n        b2 = ord(list[j+2]) << 16\n        b3 = ord(list[j+3]) << 24\n        hl[i] = b0 | b1 |b2 | b3\n        i = i+1\n        j = j+4\n\n    return hl\n\n\ndef _rotateLeft(x, n):\n    \"Rotate x (32 bit) left n bits circularly.\"\n\n    return (x << n) | (x >> (32-n))\n\n\n# ======================================================================\n# The real MD5 meat...\n#\n#   Implemented after \"Applied Cryptography\", 2nd ed., 1996,\n#   pp. 436-441 by Bruce Schneier.\n# ======================================================================\n\n# F, G, H and I are basic MD5 functions.\n\ndef F(x, y, z):\n    return (x & y) | ((~x) & z)\n\ndef G(x, y, z):\n    return (x & z) | (y & (~z))\n\ndef H(x, y, z):\n    return x ^ y ^ z\n\ndef I(x, y, z):\n    return y ^ (x | (~z))\n\n\ndef XX(func, a, b, c, d, x, s, ac):\n    \"\"\"Wrapper for call distribution to functions F, G, H and I.\n\n    This replaces functions FF, GG, HH and II from \"Appl. Crypto.\"\n    Rotation is separate from addition to prevent recomputation\n    (now summed-up in one function).\n    \"\"\"\n\n    res = 0\n    res = res + a + func(b, c, d)\n    res = res + x \n    res = res + ac\n    res = res & 0xffffffff\n    res = _rotateLeft(res, s)\n    res = res & 0xffffffff\n    res = res + b\n\n    return res & 0xffffffff\n\n\nclass MD5Type:\n    \"An implementation of the MD5 hash function in pure Python.\"\n\n    digest_size = digestsize = 16\n    block_size = 64\n\n    def __init__(self):\n        \"Initialisation.\"\n        \n        # Initial message length in bits(!).\n        self.length = 0\n        self.count = [0, 0]\n\n        # Initial empty message as a sequence of bytes (8 bit characters).\n        self.input = []\n\n        # Call a separate init function, that can be used repeatedly\n        # to start from scratch on the same object.\n        self.init()\n\n\n    def init(self):\n        \"Initialize the message-digest and set all fields to zero.\"\n\n        self.length = 0\n        self.count = [0, 0]\n        self.input = []\n\n        # Load magic initialization constants.\n        self.A = 0x67452301\n        self.B = 0xefcdab89\n        self.C = 0x98badcfe\n        self.D = 0x10325476\n\n\n    def _transform(self, inp):\n        \"\"\"Basic MD5 step transforming the digest based on the input.\n\n        Note that if the Mysterious Constants are arranged backwards\n        in little-endian order and decrypted with the DES they produce\n        OCCULT MESSAGES!\n        \"\"\"\n\n        a, b, c, d = A, B, C, D = self.A, self.B, self.C, self.D\n\n        # Round 1.\n\n        S11, S12, S13, S14 = 7, 12, 17, 22\n\n        a = XX(F, a, b, c, d, inp[ 0], S11, 0xD76AA478) # 1 \n        d = XX(F, d, a, b, c, inp[ 1], S12, 0xE8C7B756) # 2 \n        c = XX(F, c, d, a, b, inp[ 2], S13, 0x242070DB) # 3 \n        b = XX(F, b, c, d, a, inp[ 3], S14, 0xC1BDCEEE) # 4 \n        a = XX(F, a, b, c, d, inp[ 4], S11, 0xF57C0FAF) # 5 \n        d = XX(F, d, a, b, c, inp[ 5], S12, 0x4787C62A) # 6 \n        c = XX(F, c, d, a, b, inp[ 6], S13, 0xA8304613) # 7 \n        b = XX(F, b, c, d, a, inp[ 7], S14, 0xFD469501) # 8 \n        a = XX(F, a, b, c, d, inp[ 8], S11, 0x698098D8) # 9 \n        d = XX(F, d, a, b, c, inp[ 9], S12, 0x8B44F7AF) # 10 \n        c = XX(F, c, d, a, b, inp[10], S13, 0xFFFF5BB1) # 11 \n        b = XX(F, b, c, d, a, inp[11], S14, 0x895CD7BE) # 12 \n        a = XX(F, a, b, c, d, inp[12], S11, 0x6B901122) # 13 \n        d = XX(F, d, a, b, c, inp[13], S12, 0xFD987193) # 14 \n        c = XX(F, c, d, a, b, inp[14], S13, 0xA679438E) # 15 \n        b = XX(F, b, c, d, a, inp[15], S14, 0x49B40821) # 16 \n\n        # Round 2.\n\n        S21, S22, S23, S24 = 5, 9, 14, 20\n\n        a = XX(G, a, b, c, d, inp[ 1], S21, 0xF61E2562) # 17 \n        d = XX(G, d, a, b, c, inp[ 6], S22, 0xC040B340) # 18 \n        c = XX(G, c, d, a, b, inp[11], S23, 0x265E5A51) # 19 \n        b = XX(G, b, c, d, a, inp[ 0], S24, 0xE9B6C7AA) # 20 \n        a = XX(G, a, b, c, d, inp[ 5], S21, 0xD62F105D) # 21 \n        d = XX(G, d, a, b, c, inp[10], S22, 0x02441453) # 22 \n        c = XX(G, c, d, a, b, inp[15], S23, 0xD8A1E681) # 23 \n        b = XX(G, b, c, d, a, inp[ 4], S24, 0xE7D3FBC8) # 24 \n        a = XX(G, a, b, c, d, inp[ 9], S21, 0x21E1CDE6) # 25 \n        d = XX(G, d, a, b, c, inp[14], S22, 0xC33707D6) # 26 \n        c = XX(G, c, d, a, b, inp[ 3], S23, 0xF4D50D87) # 27 \n        b = XX(G, b, c, d, a, inp[ 8], S24, 0x455A14ED) # 28 \n        a = XX(G, a, b, c, d, inp[13], S21, 0xA9E3E905) # 29 \n        d = XX(G, d, a, b, c, inp[ 2], S22, 0xFCEFA3F8) # 30 \n        c = XX(G, c, d, a, b, inp[ 7], S23, 0x676F02D9) # 31 \n        b = XX(G, b, c, d, a, inp[12], S24, 0x8D2A4C8A) # 32 \n\n        # Round 3.\n\n        S31, S32, S33, S34 = 4, 11, 16, 23\n\n        a = XX(H, a, b, c, d, inp[ 5], S31, 0xFFFA3942) # 33 \n        d = XX(H, d, a, b, c, inp[ 8], S32, 0x8771F681) # 34 \n        c = XX(H, c, d, a, b, inp[11], S33, 0x6D9D6122) # 35 \n        b = XX(H, b, c, d, a, inp[14], S34, 0xFDE5380C) # 36 \n        a = XX(H, a, b, c, d, inp[ 1], S31, 0xA4BEEA44) # 37 \n        d = XX(H, d, a, b, c, inp[ 4], S32, 0x4BDECFA9) # 38 \n        c = XX(H, c, d, a, b, inp[ 7], S33, 0xF6BB4B60) # 39 \n        b = XX(H, b, c, d, a, inp[10], S34, 0xBEBFBC70) # 40 \n        a = XX(H, a, b, c, d, inp[13], S31, 0x289B7EC6) # 41 \n        d = XX(H, d, a, b, c, inp[ 0], S32, 0xEAA127FA) # 42 \n        c = XX(H, c, d, a, b, inp[ 3], S33, 0xD4EF3085) # 43 \n        b = XX(H, b, c, d, a, inp[ 6], S34, 0x04881D05) # 44 \n        a = XX(H, a, b, c, d, inp[ 9], S31, 0xD9D4D039) # 45 \n        d = XX(H, d, a, b, c, inp[12], S32, 0xE6DB99E5) # 46 \n        c = XX(H, c, d, a, b, inp[15], S33, 0x1FA27CF8) # 47 \n        b = XX(H, b, c, d, a, inp[ 2], S34, 0xC4AC5665) # 48 \n\n        # Round 4.\n\n        S41, S42, S43, S44 = 6, 10, 15, 21\n\n        a = XX(I, a, b, c, d, inp[ 0], S41, 0xF4292244) # 49 \n        d = XX(I, d, a, b, c, inp[ 7], S42, 0x432AFF97) # 50 \n        c = XX(I, c, d, a, b, inp[14], S43, 0xAB9423A7) # 51 \n        b = XX(I, b, c, d, a, inp[ 5], S44, 0xFC93A039) # 52 \n        a = XX(I, a, b, c, d, inp[12], S41, 0x655B59C3) # 53 \n        d = XX(I, d, a, b, c, inp[ 3], S42, 0x8F0CCC92) # 54 \n        c = XX(I, c, d, a, b, inp[10], S43, 0xFFEFF47D) # 55 \n        b = XX(I, b, c, d, a, inp[ 1], S44, 0x85845DD1) # 56 \n        a = XX(I, a, b, c, d, inp[ 8], S41, 0x6FA87E4F) # 57 \n        d = XX(I, d, a, b, c, inp[15], S42, 0xFE2CE6E0) # 58 \n        c = XX(I, c, d, a, b, inp[ 6], S43, 0xA3014314) # 59 \n        b = XX(I, b, c, d, a, inp[13], S44, 0x4E0811A1) # 60 \n        a = XX(I, a, b, c, d, inp[ 4], S41, 0xF7537E82) # 61 \n        d = XX(I, d, a, b, c, inp[11], S42, 0xBD3AF235) # 62 \n        c = XX(I, c, d, a, b, inp[ 2], S43, 0x2AD7D2BB) # 63 \n        b = XX(I, b, c, d, a, inp[ 9], S44, 0xEB86D391) # 64 \n\n        A = (A + a) & 0xffffffff\n        B = (B + b) & 0xffffffff\n        C = (C + c) & 0xffffffff\n        D = (D + d) & 0xffffffff\n\n        self.A, self.B, self.C, self.D = A, B, C, D\n\n\n    # Down from here all methods follow the Python Standard Library\n    # API of the md5 module.\n\n    def update(self, inBuf):\n        \"\"\"Add to the current message.\n\n        Update the md5 object with the string arg. Repeated calls\n        are equivalent to a single call with the concatenation of all\n        the arguments, i.e. m.update(a); m.update(b) is equivalent\n        to m.update(a+b).\n\n        The hash is immediately calculated for all full blocks. The final\n        calculation is made in digest(). This allows us to keep an\n        intermediate value for the hash, so that we only need to make\n        minimal recalculation if we call update() to add moredata to\n        the hashed string.\n        \"\"\"\n\n        leninBuf = len(inBuf)\n\n        # Compute number of bytes mod 64.\n        index = (self.count[0] >> 3) & 0x3F\n\n        # Update number of bits.\n        self.count[0] = self.count[0] + (leninBuf << 3)\n        if self.count[0] < (leninBuf << 3):\n            self.count[1] = self.count[1] + 1\n        self.count[1] = self.count[1] + (leninBuf >> 29)\n\n        partLen = 64 - index\n\n        if leninBuf >= partLen:\n            self.input[index:] = list(inBuf[:partLen])\n            self._transform(_bytelist2long(self.input))\n            i = partLen\n            while i + 63 < leninBuf:\n                self._transform(_bytelist2long(list(inBuf[i:i+64])))\n                i = i + 64\n            else:\n                self.input = list(inBuf[i:leninBuf])\n        else:\n            i = 0\n            self.input = self.input + list(inBuf)\n\n\n    def digest(self):\n        \"\"\"Terminate the message-digest computation and return digest.\n\n        Return the digest of the strings passed to the update()\n        method so far. This is a 16-byte string which may contain\n        non-ASCII characters, including null bytes.\n        \"\"\"\n\n        A = self.A\n        B = self.B\n        C = self.C\n        D = self.D\n        input = [] + self.input\n        count = [] + self.count\n\n        index = (self.count[0] >> 3) & 0x3f\n\n        if index < 56:\n            padLen = 56 - index\n        else:\n            padLen = 120 - index\n\n        padding = [b'\\200'] + [b'\\000'] * 63\n        self.update(padding[:padLen])\n\n        # Append length (before padding).\n        bits = _bytelist2long(self.input[:56]) + count\n\n        self._transform(bits)\n\n        # Store state in digest.\n        digest = struct.pack(\"<IIII\", self.A, self.B, self.C, self.D)\n\n        self.A = A \n        self.B = B\n        self.C = C\n        self.D = D\n        self.input = input \n        self.count = count \n\n        return digest\n\n\n    def hexdigest(self):\n        \"\"\"Terminate and return digest in HEX form.\n\n        Like digest() except the digest is returned as a string of\n        length 32, containing only hexadecimal digits. This may be\n        used to exchange the value safely in email or other non-\n        binary environments.\n        \"\"\"\n\n        return ''.join(['%02x' % ord(c) for c in self.digest()])\n\n    def copy(self):\n        \"\"\"Return a clone object.\n\n        Return a copy ('clone') of the md5 object. This can be used\n        to efficiently compute the digests of strings that share\n        a common initial substring.\n        \"\"\"\n        if 0: # set this to 1 to make the flow space crash\n            return copy.deepcopy(self)\n        clone = self.__class__()\n        clone.length = self.length\n        clone.count  = [] + self.count[:]\n        clone.input  = [] + self.input\n        clone.A = self.A\n        clone.B = self.B\n        clone.C = self.C\n        clone.D = self.D\n        return clone\n\n\n# ======================================================================\n# Mimic Python top-level functions from standard library API\n# for consistency with the _md5 module of the standard library.\n# ======================================================================\n\ndigest_size = 16\n\ndef new(arg=None):\n    \"\"\"Return a new md5 crypto object.\n    If arg is present, the method call update(arg) is made.\n    \"\"\"\n\n    crypto = MD5Type()\n    if arg:\n        crypto.update(arg)\n\n    return crypto\n\n", 
    "_sha": "#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n\n# Note that PyPy contains also a built-in module 'sha' which will hide\n# this one if compiled in.\n\n\"\"\"A sample implementation of SHA-1 in pure Python.\n\n   Framework adapted from Dinu Gherman's MD5 implementation by\n   J. Hall\u00e9n and L. Creighton. SHA-1 implementation based directly on\n   the text of the NIST standard FIPS PUB 180-1.\n\"\"\"\n\n\n__date__    = '2004-11-17'\n__version__ = 0.91 # Modernised by J. Hall\u00e9n and L. Creighton for Pypy\n\n\nimport struct, copy\n\n\n# ======================================================================\n# Bit-Manipulation helpers\n#\n#   _long2bytes() was contributed by Barry Warsaw\n#   and is reused here with tiny modifications.\n# ======================================================================\n\ndef _long2bytesBigEndian(n, blocksize=0):\n    \"\"\"Convert a long integer to a byte string.\n\n    If optional blocksize is given and greater than zero, pad the front\n    of the byte string with binary zeros so that the length is a multiple\n    of blocksize.\n    \"\"\"\n\n    # After much testing, this algorithm was deemed to be the fastest.\n    s = b''\n    pack = struct.pack\n    while n > 0:\n        s = pack('>I', n & 0xffffffff) + s\n        n = n >> 32\n\n    # Strip off leading zeros.\n    for i in range(len(s)):\n        if s[i] != '\\000':\n            break\n    else:\n        # Only happens when n == 0.\n        s = '\\000'\n        i = 0\n\n    s = s[i:]\n\n    # Add back some pad bytes. This could be done more efficiently\n    # w.r.t. the de-padding being done above, but sigh...\n    if blocksize > 0 and len(s) % blocksize:\n        s = (blocksize - len(s) % blocksize) * '\\000' + s\n\n    return s\n\n\ndef _bytelist2longBigEndian(list):\n    \"Transform a list of characters into a list of longs.\"\n\n    imax = len(list) // 4\n    hl = [0] * imax\n\n    j = 0\n    i = 0\n    while i < imax:\n        b0 = ord(list[j]) << 24\n        b1 = ord(list[j+1]) << 16\n        b2 = ord(list[j+2]) << 8\n        b3 = ord(list[j+3])\n        hl[i] = b0 | b1 | b2 | b3\n        i = i+1\n        j = j+4\n\n    return hl\n\n\ndef _rotateLeft(x, n):\n    \"Rotate x (32 bit) left n bits circularly.\"\n\n    return (x << n) | (x >> (32-n))\n\n\n# ======================================================================\n# The SHA transformation functions\n#\n# ======================================================================\n\ndef f0_19(B, C, D):\n    return (B & C) | ((~ B) & D)\n\ndef f20_39(B, C, D):\n    return B ^ C ^ D\n\ndef f40_59(B, C, D):\n    return (B & C) | (B & D) | (C & D)\n\ndef f60_79(B, C, D):\n    return B ^ C ^ D\n\n\nf = [f0_19, f20_39, f40_59, f60_79]\n\n# Constants to be used\nK = [\n    0x5A827999, # ( 0 <= t <= 19)\n    0x6ED9EBA1, # (20 <= t <= 39)\n    0x8F1BBCDC, # (40 <= t <= 59)\n    0xCA62C1D6  # (60 <= t <= 79)\n    ]\n\nclass sha:\n    \"An implementation of the SHA hash function in pure Python.\"\n\n    digest_size = digestsize = 20\n    block_size = 512 // 8\n\n    def __init__(self):\n        \"Initialisation.\"\n\n        # Initial message length in bits(!).\n        self.length = 0\n        self.count = [0, 0]\n\n        # Initial empty message as a sequence of bytes (8 bit characters).\n        self.input = []\n\n        # Call a separate init function, that can be used repeatedly\n        # to start from scratch on the same object.\n        self.init()\n\n\n    def init(self):\n        \"Initialize the message-digest and set all fields to zero.\"\n\n        self.length = 0\n        self.input = []\n\n        # Initial 160 bit message digest (5 times 32 bit).\n        self.H0 = 0x67452301\n        self.H1 = 0xEFCDAB89\n        self.H2 = 0x98BADCFE\n        self.H3 = 0x10325476\n        self.H4 = 0xC3D2E1F0\n\n    def _transform(self, W):\n\n        for t in range(16, 80):\n            W.append(_rotateLeft(\n                W[t-3] ^ W[t-8] ^ W[t-14] ^ W[t-16], 1) & 0xffffffff)\n\n        A = self.H0\n        B = self.H1\n        C = self.H2\n        D = self.H3\n        E = self.H4\n\n        \"\"\"\n        This loop was unrolled to gain about 10% in speed\n        for t in range(0, 80):\n            TEMP = _rotateLeft(A, 5) + f[t/20] + E + W[t] + K[t/20]\n            E = D\n            D = C\n            C = _rotateLeft(B, 30) & 0xffffffff\n            B = A\n            A = TEMP & 0xffffffff\n        \"\"\"\n\n        for t in range(0, 20):\n            TEMP = _rotateLeft(A, 5) + ((B & C) | ((~ B) & D)) + E + W[t] + K[0]\n            E = D\n            D = C\n            C = _rotateLeft(B, 30) & 0xffffffff\n            B = A\n            A = TEMP & 0xffffffff\n\n        for t in range(20, 40):\n            TEMP = _rotateLeft(A, 5) + (B ^ C ^ D) + E + W[t] + K[1]\n            E = D\n            D = C\n            C = _rotateLeft(B, 30) & 0xffffffff\n            B = A\n            A = TEMP & 0xffffffff\n\n        for t in range(40, 60):\n            TEMP = _rotateLeft(A, 5) + ((B & C) | (B & D) | (C & D)) + E + W[t] + K[2]\n            E = D\n            D = C\n            C = _rotateLeft(B, 30) & 0xffffffff\n            B = A\n            A = TEMP & 0xffffffff\n\n        for t in range(60, 80):\n            TEMP = _rotateLeft(A, 5) + (B ^ C ^ D)  + E + W[t] + K[3]\n            E = D\n            D = C\n            C = _rotateLeft(B, 30) & 0xffffffff\n            B = A\n            A = TEMP & 0xffffffff\n\n\n        self.H0 = (self.H0 + A) & 0xffffffff\n        self.H1 = (self.H1 + B) & 0xffffffff\n        self.H2 = (self.H2 + C) & 0xffffffff\n        self.H3 = (self.H3 + D) & 0xffffffff\n        self.H4 = (self.H4 + E) & 0xffffffff\n\n\n    # Down from here all methods follow the Python Standard Library\n    # API of the sha module.\n\n    def update(self, inBuf):\n        \"\"\"Add to the current message.\n\n        Update the md5 object with the string arg. Repeated calls\n        are equivalent to a single call with the concatenation of all\n        the arguments, i.e. m.update(a); m.update(b) is equivalent\n        to m.update(a+b).\n\n        The hash is immediately calculated for all full blocks. The final\n        calculation is made in digest(). It will calculate 1-2 blocks,\n        depending on how much padding we have to add. This allows us to\n        keep an intermediate value for the hash, so that we only need to\n        make minimal recalculation if we call update() to add more data\n        to the hashed string.\n        \"\"\"\n\n        leninBuf = len(inBuf)\n\n        # Compute number of bytes mod 64.\n        index = (self.count[1] >> 3) & 0x3F\n\n        # Update number of bits.\n        self.count[1] = self.count[1] + (leninBuf << 3)\n        if self.count[1] < (leninBuf << 3):\n            self.count[0] = self.count[0] + 1\n        self.count[0] = self.count[0] + (leninBuf >> 29)\n\n        partLen = 64 - index\n\n        if leninBuf >= partLen:\n            self.input[index:] = list(inBuf[:partLen])\n            self._transform(_bytelist2longBigEndian(self.input))\n            i = partLen\n            while i + 63 < leninBuf:\n                self._transform(_bytelist2longBigEndian(list(inBuf[i:i+64])))\n                i = i + 64\n            else:\n                self.input = list(inBuf[i:leninBuf])\n        else:\n            i = 0\n            self.input = self.input + list(inBuf)\n\n\n    def digest(self):\n        \"\"\"Terminate the message-digest computation and return digest.\n\n        Return the digest of the strings passed to the update()\n        method so far. This is a 16-byte string which may contain\n        non-ASCII characters, including null bytes.\n        \"\"\"\n\n        H0 = self.H0\n        H1 = self.H1\n        H2 = self.H2\n        H3 = self.H3\n        H4 = self.H4\n        input = [] + self.input\n        count = [] + self.count\n\n        index = (self.count[1] >> 3) & 0x3f\n\n        if index < 56:\n            padLen = 56 - index\n        else:\n            padLen = 120 - index\n\n        padding = ['\\200'] + ['\\000'] * 63\n        self.update(padding[:padLen])\n\n        # Append length (before padding).\n        bits = _bytelist2longBigEndian(self.input[:56]) + count\n\n        self._transform(bits)\n\n        # Store state in digest.\n        digest = _long2bytesBigEndian(self.H0, 4) + \\\n                 _long2bytesBigEndian(self.H1, 4) + \\\n                 _long2bytesBigEndian(self.H2, 4) + \\\n                 _long2bytesBigEndian(self.H3, 4) + \\\n                 _long2bytesBigEndian(self.H4, 4)\n\n        self.H0 = H0\n        self.H1 = H1\n        self.H2 = H2\n        self.H3 = H3\n        self.H4 = H4\n        self.input = input\n        self.count = count\n\n        return digest\n\n\n    def hexdigest(self):\n        \"\"\"Terminate and return digest in HEX form.\n\n        Like digest() except the digest is returned as a string of\n        length 32, containing only hexadecimal digits. This may be\n        used to exchange the value safely in email or other non-\n        binary environments.\n        \"\"\"\n        return ''.join(['%02x' % ord(c) for c in self.digest()])\n\n    def copy(self):\n        \"\"\"Return a clone object.\n\n        Return a copy ('clone') of the md5 object. This can be used\n        to efficiently compute the digests of strings that share\n        a common initial substring.\n        \"\"\"\n\n        return copy.deepcopy(self)\n\n\n# ======================================================================\n# Mimic Python top-level functions from standard library API\n# for consistency with the _sha module of the standard library.\n# ======================================================================\n\n# These are mandatory variables in the module. They have constant values\n# in the SHA standard.\n\ndigest_size = 20\ndigestsize = 20\nblocksize = 1\n\ndef new(arg=None):\n    \"\"\"Return a new sha crypto object.\n\n    If arg is present, the method call update(arg) is made.\n    \"\"\"\n\n    crypto = sha()\n    if arg:\n        crypto.update(arg)\n\n    return crypto\n", 
    "_sha256": "import struct\n\nSHA_BLOCKSIZE = 64\nSHA_DIGESTSIZE = 32\n\n\ndef new_shaobject():\n    return {\n        'digest': [0]*8,\n        'count_lo': 0,\n        'count_hi': 0,\n        'data': [0]* SHA_BLOCKSIZE,\n        'local': 0,\n        'digestsize': 0\n    }\n\nROR = lambda x, y: (((x & 0xffffffff) >> (y & 31)) | (x << (32 - (y & 31)))) & 0xffffffff\nCh = lambda x, y, z: (z ^ (x & (y ^ z)))\nMaj = lambda x, y, z: (((x | y) & z) | (x & y))\nS = lambda x, n: ROR(x, n)\nR = lambda x, n: (x & 0xffffffff) >> n\nSigma0 = lambda x: (S(x, 2) ^ S(x, 13) ^ S(x, 22))\nSigma1 = lambda x: (S(x, 6) ^ S(x, 11) ^ S(x, 25))\nGamma0 = lambda x: (S(x, 7) ^ S(x, 18) ^ R(x, 3))\nGamma1 = lambda x: (S(x, 17) ^ S(x, 19) ^ R(x, 10))\n\ndef sha_transform(sha_info):\n    W = []\n    \n    d = sha_info['data']\n    for i in xrange(0,16):\n        W.append( (d[4*i]<<24) + (d[4*i+1]<<16) + (d[4*i+2]<<8) + d[4*i+3])\n    \n    for i in xrange(16,64):\n        W.append( (Gamma1(W[i - 2]) + W[i - 7] + Gamma0(W[i - 15]) + W[i - 16]) & 0xffffffff )\n    \n    ss = sha_info['digest'][:]\n    \n    def RND(a,b,c,d,e,f,g,h,i,ki):\n        t0 = h + Sigma1(e) + Ch(e, f, g) + ki + W[i];\n        t1 = Sigma0(a) + Maj(a, b, c);\n        d += t0;\n        h  = t0 + t1;\n        return d & 0xffffffff, h & 0xffffffff\n    \n    ss[3], ss[7] = RND(ss[0],ss[1],ss[2],ss[3],ss[4],ss[5],ss[6],ss[7],0,0x428a2f98);\n    ss[2], ss[6] = RND(ss[7],ss[0],ss[1],ss[2],ss[3],ss[4],ss[5],ss[6],1,0x71374491);\n    ss[1], ss[5] = RND(ss[6],ss[7],ss[0],ss[1],ss[2],ss[3],ss[4],ss[5],2,0xb5c0fbcf);\n    ss[0], ss[4] = RND(ss[5],ss[6],ss[7],ss[0],ss[1],ss[2],ss[3],ss[4],3,0xe9b5dba5);\n    ss[7], ss[3] = RND(ss[4],ss[5],ss[6],ss[7],ss[0],ss[1],ss[2],ss[3],4,0x3956c25b);\n    ss[6], ss[2] = RND(ss[3],ss[4],ss[5],ss[6],ss[7],ss[0],ss[1],ss[2],5,0x59f111f1);\n    ss[5], ss[1] = RND(ss[2],ss[3],ss[4],ss[5],ss[6],ss[7],ss[0],ss[1],6,0x923f82a4);\n    ss[4], ss[0] = RND(ss[1],ss[2],ss[3],ss[4],ss[5],ss[6],ss[7],ss[0],7,0xab1c5ed5);\n    ss[3], ss[7] = RND(ss[0],ss[1],ss[2],ss[3],ss[4],ss[5],ss[6],ss[7],8,0xd807aa98);\n    ss[2], ss[6] = RND(ss[7],ss[0],ss[1],ss[2],ss[3],ss[4],ss[5],ss[6],9,0x12835b01);\n    ss[1], ss[5] = RND(ss[6],ss[7],ss[0],ss[1],ss[2],ss[3],ss[4],ss[5],10,0x243185be);\n    ss[0], ss[4] = RND(ss[5],ss[6],ss[7],ss[0],ss[1],ss[2],ss[3],ss[4],11,0x550c7dc3);\n    ss[7], ss[3] = RND(ss[4],ss[5],ss[6],ss[7],ss[0],ss[1],ss[2],ss[3],12,0x72be5d74);\n    ss[6], ss[2] = RND(ss[3],ss[4],ss[5],ss[6],ss[7],ss[0],ss[1],ss[2],13,0x80deb1fe);\n    ss[5], ss[1] = RND(ss[2],ss[3],ss[4],ss[5],ss[6],ss[7],ss[0],ss[1],14,0x9bdc06a7);\n    ss[4], ss[0] = RND(ss[1],ss[2],ss[3],ss[4],ss[5],ss[6],ss[7],ss[0],15,0xc19bf174);\n    ss[3], ss[7] = RND(ss[0],ss[1],ss[2],ss[3],ss[4],ss[5],ss[6],ss[7],16,0xe49b69c1);\n    ss[2], ss[6] = RND(ss[7],ss[0],ss[1],ss[2],ss[3],ss[4],ss[5],ss[6],17,0xefbe4786);\n    ss[1], ss[5] = RND(ss[6],ss[7],ss[0],ss[1],ss[2],ss[3],ss[4],ss[5],18,0x0fc19dc6);\n    ss[0], ss[4] = RND(ss[5],ss[6],ss[7],ss[0],ss[1],ss[2],ss[3],ss[4],19,0x240ca1cc);\n    ss[7], ss[3] = RND(ss[4],ss[5],ss[6],ss[7],ss[0],ss[1],ss[2],ss[3],20,0x2de92c6f);\n    ss[6], ss[2] = RND(ss[3],ss[4],ss[5],ss[6],ss[7],ss[0],ss[1],ss[2],21,0x4a7484aa);\n    ss[5], ss[1] = RND(ss[2],ss[3],ss[4],ss[5],ss[6],ss[7],ss[0],ss[1],22,0x5cb0a9dc);\n    ss[4], ss[0] = RND(ss[1],ss[2],ss[3],ss[4],ss[5],ss[6],ss[7],ss[0],23,0x76f988da);\n    ss[3], ss[7] = RND(ss[0],ss[1],ss[2],ss[3],ss[4],ss[5],ss[6],ss[7],24,0x983e5152);\n    ss[2], ss[6] = RND(ss[7],ss[0],ss[1],ss[2],ss[3],ss[4],ss[5],ss[6],25,0xa831c66d);\n    ss[1], ss[5] = RND(ss[6],ss[7],ss[0],ss[1],ss[2],ss[3],ss[4],ss[5],26,0xb00327c8);\n    ss[0], ss[4] = RND(ss[5],ss[6],ss[7],ss[0],ss[1],ss[2],ss[3],ss[4],27,0xbf597fc7);\n    ss[7], ss[3] = RND(ss[4],ss[5],ss[6],ss[7],ss[0],ss[1],ss[2],ss[3],28,0xc6e00bf3);\n    ss[6], ss[2] = RND(ss[3],ss[4],ss[5],ss[6],ss[7],ss[0],ss[1],ss[2],29,0xd5a79147);\n    ss[5], ss[1] = RND(ss[2],ss[3],ss[4],ss[5],ss[6],ss[7],ss[0],ss[1],30,0x06ca6351);\n    ss[4], ss[0] = RND(ss[1],ss[2],ss[3],ss[4],ss[5],ss[6],ss[7],ss[0],31,0x14292967);\n    ss[3], ss[7] = RND(ss[0],ss[1],ss[2],ss[3],ss[4],ss[5],ss[6],ss[7],32,0x27b70a85);\n    ss[2], ss[6] = RND(ss[7],ss[0],ss[1],ss[2],ss[3],ss[4],ss[5],ss[6],33,0x2e1b2138);\n    ss[1], ss[5] = RND(ss[6],ss[7],ss[0],ss[1],ss[2],ss[3],ss[4],ss[5],34,0x4d2c6dfc);\n    ss[0], ss[4] = RND(ss[5],ss[6],ss[7],ss[0],ss[1],ss[2],ss[3],ss[4],35,0x53380d13);\n    ss[7], ss[3] = RND(ss[4],ss[5],ss[6],ss[7],ss[0],ss[1],ss[2],ss[3],36,0x650a7354);\n    ss[6], ss[2] = RND(ss[3],ss[4],ss[5],ss[6],ss[7],ss[0],ss[1],ss[2],37,0x766a0abb);\n    ss[5], ss[1] = RND(ss[2],ss[3],ss[4],ss[5],ss[6],ss[7],ss[0],ss[1],38,0x81c2c92e);\n    ss[4], ss[0] = RND(ss[1],ss[2],ss[3],ss[4],ss[5],ss[6],ss[7],ss[0],39,0x92722c85);\n    ss[3], ss[7] = RND(ss[0],ss[1],ss[2],ss[3],ss[4],ss[5],ss[6],ss[7],40,0xa2bfe8a1);\n    ss[2], ss[6] = RND(ss[7],ss[0],ss[1],ss[2],ss[3],ss[4],ss[5],ss[6],41,0xa81a664b);\n    ss[1], ss[5] = RND(ss[6],ss[7],ss[0],ss[1],ss[2],ss[3],ss[4],ss[5],42,0xc24b8b70);\n    ss[0], ss[4] = RND(ss[5],ss[6],ss[7],ss[0],ss[1],ss[2],ss[3],ss[4],43,0xc76c51a3);\n    ss[7], ss[3] = RND(ss[4],ss[5],ss[6],ss[7],ss[0],ss[1],ss[2],ss[3],44,0xd192e819);\n    ss[6], ss[2] = RND(ss[3],ss[4],ss[5],ss[6],ss[7],ss[0],ss[1],ss[2],45,0xd6990624);\n    ss[5], ss[1] = RND(ss[2],ss[3],ss[4],ss[5],ss[6],ss[7],ss[0],ss[1],46,0xf40e3585);\n    ss[4], ss[0] = RND(ss[1],ss[2],ss[3],ss[4],ss[5],ss[6],ss[7],ss[0],47,0x106aa070);\n    ss[3], ss[7] = RND(ss[0],ss[1],ss[2],ss[3],ss[4],ss[5],ss[6],ss[7],48,0x19a4c116);\n    ss[2], ss[6] = RND(ss[7],ss[0],ss[1],ss[2],ss[3],ss[4],ss[5],ss[6],49,0x1e376c08);\n    ss[1], ss[5] = RND(ss[6],ss[7],ss[0],ss[1],ss[2],ss[3],ss[4],ss[5],50,0x2748774c);\n    ss[0], ss[4] = RND(ss[5],ss[6],ss[7],ss[0],ss[1],ss[2],ss[3],ss[4],51,0x34b0bcb5);\n    ss[7], ss[3] = RND(ss[4],ss[5],ss[6],ss[7],ss[0],ss[1],ss[2],ss[3],52,0x391c0cb3);\n    ss[6], ss[2] = RND(ss[3],ss[4],ss[5],ss[6],ss[7],ss[0],ss[1],ss[2],53,0x4ed8aa4a);\n    ss[5], ss[1] = RND(ss[2],ss[3],ss[4],ss[5],ss[6],ss[7],ss[0],ss[1],54,0x5b9cca4f);\n    ss[4], ss[0] = RND(ss[1],ss[2],ss[3],ss[4],ss[5],ss[6],ss[7],ss[0],55,0x682e6ff3);\n    ss[3], ss[7] = RND(ss[0],ss[1],ss[2],ss[3],ss[4],ss[5],ss[6],ss[7],56,0x748f82ee);\n    ss[2], ss[6] = RND(ss[7],ss[0],ss[1],ss[2],ss[3],ss[4],ss[5],ss[6],57,0x78a5636f);\n    ss[1], ss[5] = RND(ss[6],ss[7],ss[0],ss[1],ss[2],ss[3],ss[4],ss[5],58,0x84c87814);\n    ss[0], ss[4] = RND(ss[5],ss[6],ss[7],ss[0],ss[1],ss[2],ss[3],ss[4],59,0x8cc70208);\n    ss[7], ss[3] = RND(ss[4],ss[5],ss[6],ss[7],ss[0],ss[1],ss[2],ss[3],60,0x90befffa);\n    ss[6], ss[2] = RND(ss[3],ss[4],ss[5],ss[6],ss[7],ss[0],ss[1],ss[2],61,0xa4506ceb);\n    ss[5], ss[1] = RND(ss[2],ss[3],ss[4],ss[5],ss[6],ss[7],ss[0],ss[1],62,0xbef9a3f7);\n    ss[4], ss[0] = RND(ss[1],ss[2],ss[3],ss[4],ss[5],ss[6],ss[7],ss[0],63,0xc67178f2);\n    \n    dig = []\n    for i, x in enumerate(sha_info['digest']):\n        dig.append( (x + ss[i]) & 0xffffffff )\n    sha_info['digest'] = dig\n\ndef sha_init():\n    sha_info = new_shaobject()\n    sha_info['digest'] = [0x6A09E667, 0xBB67AE85, 0x3C6EF372, 0xA54FF53A, 0x510E527F, 0x9B05688C, 0x1F83D9AB, 0x5BE0CD19]\n    sha_info['count_lo'] = 0\n    sha_info['count_hi'] = 0\n    sha_info['local'] = 0\n    sha_info['digestsize'] = 32\n    return sha_info\n\ndef sha224_init():\n    sha_info = new_shaobject()\n    sha_info['digest'] = [0xc1059ed8, 0x367cd507, 0x3070dd17, 0xf70e5939, 0xffc00b31, 0x68581511, 0x64f98fa7, 0xbefa4fa4]\n    sha_info['count_lo'] = 0\n    sha_info['count_hi'] = 0\n    sha_info['local'] = 0\n    sha_info['digestsize'] = 28\n    return sha_info\n\ndef getbuf(s):\n    if isinstance(s, str):\n        return s\n    elif isinstance(s, unicode):\n        return str(s)\n    else:\n        return buffer(s)\n\ndef sha_update(sha_info, buffer):\n    count = len(buffer)\n    buffer_idx = 0\n    clo = (sha_info['count_lo'] + (count << 3)) & 0xffffffff\n    if clo < sha_info['count_lo']:\n        sha_info['count_hi'] += 1\n    sha_info['count_lo'] = clo\n    \n    sha_info['count_hi'] += (count >> 29)\n    \n    if sha_info['local']:\n        i = SHA_BLOCKSIZE - sha_info['local']\n        if i > count:\n            i = count\n        \n        # copy buffer\n        for x in enumerate(buffer[buffer_idx:buffer_idx+i]):\n            sha_info['data'][sha_info['local']+x[0]] = struct.unpack('B', x[1])[0]\n        \n        count -= i\n        buffer_idx += i\n        \n        sha_info['local'] += i\n        if sha_info['local'] == SHA_BLOCKSIZE:\n            sha_transform(sha_info)\n            sha_info['local'] = 0\n        else:\n            return\n    \n    while count >= SHA_BLOCKSIZE:\n        # copy buffer\n        sha_info['data'] = [struct.unpack('B',c)[0] for c in buffer[buffer_idx:buffer_idx + SHA_BLOCKSIZE]]\n        count -= SHA_BLOCKSIZE\n        buffer_idx += SHA_BLOCKSIZE\n        sha_transform(sha_info)\n        \n    \n    # copy buffer\n    pos = sha_info['local']\n    sha_info['data'][pos:pos+count] = [struct.unpack('B',c)[0] for c in buffer[buffer_idx:buffer_idx + count]]\n    sha_info['local'] = count\n\ndef sha_final(sha_info):\n    lo_bit_count = sha_info['count_lo']\n    hi_bit_count = sha_info['count_hi']\n    count = (lo_bit_count >> 3) & 0x3f\n    sha_info['data'][count] = 0x80;\n    count += 1\n    if count > SHA_BLOCKSIZE - 8:\n        # zero the bytes in data after the count\n        sha_info['data'] = sha_info['data'][:count] + ([0] * (SHA_BLOCKSIZE - count))\n        sha_transform(sha_info)\n        # zero bytes in data\n        sha_info['data'] = [0] * SHA_BLOCKSIZE\n    else:\n        sha_info['data'] = sha_info['data'][:count] + ([0] * (SHA_BLOCKSIZE - count))\n    \n    sha_info['data'][56] = (hi_bit_count >> 24) & 0xff\n    sha_info['data'][57] = (hi_bit_count >> 16) & 0xff\n    sha_info['data'][58] = (hi_bit_count >>  8) & 0xff\n    sha_info['data'][59] = (hi_bit_count >>  0) & 0xff\n    sha_info['data'][60] = (lo_bit_count >> 24) & 0xff\n    sha_info['data'][61] = (lo_bit_count >> 16) & 0xff\n    sha_info['data'][62] = (lo_bit_count >>  8) & 0xff\n    sha_info['data'][63] = (lo_bit_count >>  0) & 0xff\n    \n    sha_transform(sha_info)\n    \n    dig = []\n    for i in sha_info['digest']:\n        dig.extend([ ((i>>24) & 0xff), ((i>>16) & 0xff), ((i>>8) & 0xff), (i & 0xff) ])\n    return ''.join([chr(i) for i in dig])\n\nclass sha256(object):\n    digest_size = digestsize = SHA_DIGESTSIZE\n    block_size = SHA_BLOCKSIZE\n\n    def __init__(self, s=None):\n        self._sha = sha_init()\n        if s:\n            sha_update(self._sha, getbuf(s))\n    \n    def update(self, s):\n        sha_update(self._sha, getbuf(s))\n    \n    def digest(self):\n        return sha_final(self._sha.copy())[:self._sha['digestsize']]\n    \n    def hexdigest(self):\n        return ''.join(['%.2x' % ord(i) for i in self.digest()])\n\n    def copy(self):\n        new = sha256.__new__(sha256)\n        new._sha = self._sha.copy()\n        return new\n\nclass sha224(sha256):\n    digest_size = digestsize = 28\n\n    def __init__(self, s=None):\n        self._sha = sha224_init()\n        if s:\n            sha_update(self._sha, getbuf(s))\n\n    def copy(self):\n        new = sha224.__new__(sha224)\n        new._sha = self._sha.copy()\n        return new\n\ndef test():\n    a_str = \"just a test string\"\n    \n    assert 'e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855' == sha256().hexdigest()\n    assert 'd7b553c6f09ac85d142415f857c5310f3bbbe7cdd787cce4b985acedd585266f' == sha256(a_str).hexdigest()\n    assert '8113ebf33c97daa9998762aacafe750c7cefc2b2f173c90c59663a57fe626f21' == sha256(a_str*7).hexdigest()\n    \n    s = sha256(a_str)\n    s.update(a_str)\n    assert '03d9963e05a094593190b6fc794cb1a3e1ac7d7883f0b5855268afeccc70d461' == s.hexdigest()\n\nif __name__ == \"__main__\":\n    test()\n\n\n", 
    "_sha512": "\"\"\"\nThis code was Ported from CPython's sha512module.c\n\"\"\"\n\nimport struct\n\nSHA_BLOCKSIZE = 128\nSHA_DIGESTSIZE = 64\n\n\ndef new_shaobject():\n    return {\n        'digest': [0]*8,\n        'count_lo': 0,\n        'count_hi': 0,\n        'data': [0]* SHA_BLOCKSIZE,\n        'local': 0,\n        'digestsize': 0\n    }\n\nROR64 = lambda x, y: (((x & 0xffffffffffffffff) >> (y & 63)) | (x << (64 - (y & 63)))) & 0xffffffffffffffff\nCh = lambda x, y, z: (z ^ (x & (y ^ z)))\nMaj = lambda x, y, z: (((x | y) & z) | (x & y))\nS = lambda x, n: ROR64(x, n)\nR = lambda x, n: (x & 0xffffffffffffffff) >> n\nSigma0 = lambda x: (S(x, 28) ^ S(x, 34) ^ S(x, 39))\nSigma1 = lambda x: (S(x, 14) ^ S(x, 18) ^ S(x, 41))\nGamma0 = lambda x: (S(x, 1) ^ S(x, 8) ^ R(x, 7))\nGamma1 = lambda x: (S(x, 19) ^ S(x, 61) ^ R(x, 6))\n\ndef sha_transform(sha_info):\n    W = []\n\n    d = sha_info['data']\n    for i in xrange(0,16):\n        W.append( (d[8*i]<<56) + (d[8*i+1]<<48) + (d[8*i+2]<<40) + (d[8*i+3]<<32) + (d[8*i+4]<<24) + (d[8*i+5]<<16) + (d[8*i+6]<<8) + d[8*i+7])\n\n    for i in xrange(16,80):\n        W.append( (Gamma1(W[i - 2]) + W[i - 7] + Gamma0(W[i - 15]) + W[i - 16]) & 0xffffffffffffffff )\n\n    ss = sha_info['digest'][:]\n\n    def RND(a,b,c,d,e,f,g,h,i,ki):\n        t0 = (h + Sigma1(e) + Ch(e, f, g) + ki + W[i]) & 0xffffffffffffffff\n        t1 = (Sigma0(a) + Maj(a, b, c)) & 0xffffffffffffffff\n        d = (d + t0) & 0xffffffffffffffff\n        h = (t0 + t1) & 0xffffffffffffffff\n        return d & 0xffffffffffffffff, h & 0xffffffffffffffff\n\n    ss[3], ss[7] = RND(ss[0],ss[1],ss[2],ss[3],ss[4],ss[5],ss[6],ss[7],0,0x428a2f98d728ae22)\n    ss[2], ss[6] = RND(ss[7],ss[0],ss[1],ss[2],ss[3],ss[4],ss[5],ss[6],1,0x7137449123ef65cd)\n    ss[1], ss[5] = RND(ss[6],ss[7],ss[0],ss[1],ss[2],ss[3],ss[4],ss[5],2,0xb5c0fbcfec4d3b2f)\n    ss[0], ss[4] = RND(ss[5],ss[6],ss[7],ss[0],ss[1],ss[2],ss[3],ss[4],3,0xe9b5dba58189dbbc)\n    ss[7], ss[3] = RND(ss[4],ss[5],ss[6],ss[7],ss[0],ss[1],ss[2],ss[3],4,0x3956c25bf348b538)\n    ss[6], ss[2] = RND(ss[3],ss[4],ss[5],ss[6],ss[7],ss[0],ss[1],ss[2],5,0x59f111f1b605d019)\n    ss[5], ss[1] = RND(ss[2],ss[3],ss[4],ss[5],ss[6],ss[7],ss[0],ss[1],6,0x923f82a4af194f9b)\n    ss[4], ss[0] = RND(ss[1],ss[2],ss[3],ss[4],ss[5],ss[6],ss[7],ss[0],7,0xab1c5ed5da6d8118)\n    ss[3], ss[7] = RND(ss[0],ss[1],ss[2],ss[3],ss[4],ss[5],ss[6],ss[7],8,0xd807aa98a3030242)\n    ss[2], ss[6] = RND(ss[7],ss[0],ss[1],ss[2],ss[3],ss[4],ss[5],ss[6],9,0x12835b0145706fbe)\n    ss[1], ss[5] = RND(ss[6],ss[7],ss[0],ss[1],ss[2],ss[3],ss[4],ss[5],10,0x243185be4ee4b28c)\n    ss[0], ss[4] = RND(ss[5],ss[6],ss[7],ss[0],ss[1],ss[2],ss[3],ss[4],11,0x550c7dc3d5ffb4e2)\n    ss[7], ss[3] = RND(ss[4],ss[5],ss[6],ss[7],ss[0],ss[1],ss[2],ss[3],12,0x72be5d74f27b896f)\n    ss[6], ss[2] = RND(ss[3],ss[4],ss[5],ss[6],ss[7],ss[0],ss[1],ss[2],13,0x80deb1fe3b1696b1)\n    ss[5], ss[1] = RND(ss[2],ss[3],ss[4],ss[5],ss[6],ss[7],ss[0],ss[1],14,0x9bdc06a725c71235)\n    ss[4], ss[0] = RND(ss[1],ss[2],ss[3],ss[4],ss[5],ss[6],ss[7],ss[0],15,0xc19bf174cf692694)\n    ss[3], ss[7] = RND(ss[0],ss[1],ss[2],ss[3],ss[4],ss[5],ss[6],ss[7],16,0xe49b69c19ef14ad2)\n    ss[2], ss[6] = RND(ss[7],ss[0],ss[1],ss[2],ss[3],ss[4],ss[5],ss[6],17,0xefbe4786384f25e3)\n    ss[1], ss[5] = RND(ss[6],ss[7],ss[0],ss[1],ss[2],ss[3],ss[4],ss[5],18,0x0fc19dc68b8cd5b5)\n    ss[0], ss[4] = RND(ss[5],ss[6],ss[7],ss[0],ss[1],ss[2],ss[3],ss[4],19,0x240ca1cc77ac9c65)\n    ss[7], ss[3] = RND(ss[4],ss[5],ss[6],ss[7],ss[0],ss[1],ss[2],ss[3],20,0x2de92c6f592b0275)\n    ss[6], ss[2] = RND(ss[3],ss[4],ss[5],ss[6],ss[7],ss[0],ss[1],ss[2],21,0x4a7484aa6ea6e483)\n    ss[5], ss[1] = RND(ss[2],ss[3],ss[4],ss[5],ss[6],ss[7],ss[0],ss[1],22,0x5cb0a9dcbd41fbd4)\n    ss[4], ss[0] = RND(ss[1],ss[2],ss[3],ss[4],ss[5],ss[6],ss[7],ss[0],23,0x76f988da831153b5)\n    ss[3], ss[7] = RND(ss[0],ss[1],ss[2],ss[3],ss[4],ss[5],ss[6],ss[7],24,0x983e5152ee66dfab)\n    ss[2], ss[6] = RND(ss[7],ss[0],ss[1],ss[2],ss[3],ss[4],ss[5],ss[6],25,0xa831c66d2db43210)\n    ss[1], ss[5] = RND(ss[6],ss[7],ss[0],ss[1],ss[2],ss[3],ss[4],ss[5],26,0xb00327c898fb213f)\n    ss[0], ss[4] = RND(ss[5],ss[6],ss[7],ss[0],ss[1],ss[2],ss[3],ss[4],27,0xbf597fc7beef0ee4)\n    ss[7], ss[3] = RND(ss[4],ss[5],ss[6],ss[7],ss[0],ss[1],ss[2],ss[3],28,0xc6e00bf33da88fc2)\n    ss[6], ss[2] = RND(ss[3],ss[4],ss[5],ss[6],ss[7],ss[0],ss[1],ss[2],29,0xd5a79147930aa725)\n    ss[5], ss[1] = RND(ss[2],ss[3],ss[4],ss[5],ss[6],ss[7],ss[0],ss[1],30,0x06ca6351e003826f)\n    ss[4], ss[0] = RND(ss[1],ss[2],ss[3],ss[4],ss[5],ss[6],ss[7],ss[0],31,0x142929670a0e6e70)\n    ss[3], ss[7] = RND(ss[0],ss[1],ss[2],ss[3],ss[4],ss[5],ss[6],ss[7],32,0x27b70a8546d22ffc)\n    ss[2], ss[6] = RND(ss[7],ss[0],ss[1],ss[2],ss[3],ss[4],ss[5],ss[6],33,0x2e1b21385c26c926)\n    ss[1], ss[5] = RND(ss[6],ss[7],ss[0],ss[1],ss[2],ss[3],ss[4],ss[5],34,0x4d2c6dfc5ac42aed)\n    ss[0], ss[4] = RND(ss[5],ss[6],ss[7],ss[0],ss[1],ss[2],ss[3],ss[4],35,0x53380d139d95b3df)\n    ss[7], ss[3] = RND(ss[4],ss[5],ss[6],ss[7],ss[0],ss[1],ss[2],ss[3],36,0x650a73548baf63de)\n    ss[6], ss[2] = RND(ss[3],ss[4],ss[5],ss[6],ss[7],ss[0],ss[1],ss[2],37,0x766a0abb3c77b2a8)\n    ss[5], ss[1] = RND(ss[2],ss[3],ss[4],ss[5],ss[6],ss[7],ss[0],ss[1],38,0x81c2c92e47edaee6)\n    ss[4], ss[0] = RND(ss[1],ss[2],ss[3],ss[4],ss[5],ss[6],ss[7],ss[0],39,0x92722c851482353b)\n    ss[3], ss[7] = RND(ss[0],ss[1],ss[2],ss[3],ss[4],ss[5],ss[6],ss[7],40,0xa2bfe8a14cf10364)\n    ss[2], ss[6] = RND(ss[7],ss[0],ss[1],ss[2],ss[3],ss[4],ss[5],ss[6],41,0xa81a664bbc423001)\n    ss[1], ss[5] = RND(ss[6],ss[7],ss[0],ss[1],ss[2],ss[3],ss[4],ss[5],42,0xc24b8b70d0f89791)\n    ss[0], ss[4] = RND(ss[5],ss[6],ss[7],ss[0],ss[1],ss[2],ss[3],ss[4],43,0xc76c51a30654be30)\n    ss[7], ss[3] = RND(ss[4],ss[5],ss[6],ss[7],ss[0],ss[1],ss[2],ss[3],44,0xd192e819d6ef5218)\n    ss[6], ss[2] = RND(ss[3],ss[4],ss[5],ss[6],ss[7],ss[0],ss[1],ss[2],45,0xd69906245565a910)\n    ss[5], ss[1] = RND(ss[2],ss[3],ss[4],ss[5],ss[6],ss[7],ss[0],ss[1],46,0xf40e35855771202a)\n    ss[4], ss[0] = RND(ss[1],ss[2],ss[3],ss[4],ss[5],ss[6],ss[7],ss[0],47,0x106aa07032bbd1b8)\n    ss[3], ss[7] = RND(ss[0],ss[1],ss[2],ss[3],ss[4],ss[5],ss[6],ss[7],48,0x19a4c116b8d2d0c8)\n    ss[2], ss[6] = RND(ss[7],ss[0],ss[1],ss[2],ss[3],ss[4],ss[5],ss[6],49,0x1e376c085141ab53)\n    ss[1], ss[5] = RND(ss[6],ss[7],ss[0],ss[1],ss[2],ss[3],ss[4],ss[5],50,0x2748774cdf8eeb99)\n    ss[0], ss[4] = RND(ss[5],ss[6],ss[7],ss[0],ss[1],ss[2],ss[3],ss[4],51,0x34b0bcb5e19b48a8)\n    ss[7], ss[3] = RND(ss[4],ss[5],ss[6],ss[7],ss[0],ss[1],ss[2],ss[3],52,0x391c0cb3c5c95a63)\n    ss[6], ss[2] = RND(ss[3],ss[4],ss[5],ss[6],ss[7],ss[0],ss[1],ss[2],53,0x4ed8aa4ae3418acb)\n    ss[5], ss[1] = RND(ss[2],ss[3],ss[4],ss[5],ss[6],ss[7],ss[0],ss[1],54,0x5b9cca4f7763e373)\n    ss[4], ss[0] = RND(ss[1],ss[2],ss[3],ss[4],ss[5],ss[6],ss[7],ss[0],55,0x682e6ff3d6b2b8a3)\n    ss[3], ss[7] = RND(ss[0],ss[1],ss[2],ss[3],ss[4],ss[5],ss[6],ss[7],56,0x748f82ee5defb2fc)\n    ss[2], ss[6] = RND(ss[7],ss[0],ss[1],ss[2],ss[3],ss[4],ss[5],ss[6],57,0x78a5636f43172f60)\n    ss[1], ss[5] = RND(ss[6],ss[7],ss[0],ss[1],ss[2],ss[3],ss[4],ss[5],58,0x84c87814a1f0ab72)\n    ss[0], ss[4] = RND(ss[5],ss[6],ss[7],ss[0],ss[1],ss[2],ss[3],ss[4],59,0x8cc702081a6439ec)\n    ss[7], ss[3] = RND(ss[4],ss[5],ss[6],ss[7],ss[0],ss[1],ss[2],ss[3],60,0x90befffa23631e28)\n    ss[6], ss[2] = RND(ss[3],ss[4],ss[5],ss[6],ss[7],ss[0],ss[1],ss[2],61,0xa4506cebde82bde9)\n    ss[5], ss[1] = RND(ss[2],ss[3],ss[4],ss[5],ss[6],ss[7],ss[0],ss[1],62,0xbef9a3f7b2c67915)\n    ss[4], ss[0] = RND(ss[1],ss[2],ss[3],ss[4],ss[5],ss[6],ss[7],ss[0],63,0xc67178f2e372532b)\n    ss[3], ss[7] = RND(ss[0],ss[1],ss[2],ss[3],ss[4],ss[5],ss[6],ss[7],64,0xca273eceea26619c)\n    ss[2], ss[6] = RND(ss[7],ss[0],ss[1],ss[2],ss[3],ss[4],ss[5],ss[6],65,0xd186b8c721c0c207)\n    ss[1], ss[5] = RND(ss[6],ss[7],ss[0],ss[1],ss[2],ss[3],ss[4],ss[5],66,0xeada7dd6cde0eb1e)\n    ss[0], ss[4] = RND(ss[5],ss[6],ss[7],ss[0],ss[1],ss[2],ss[3],ss[4],67,0xf57d4f7fee6ed178)\n    ss[7], ss[3] = RND(ss[4],ss[5],ss[6],ss[7],ss[0],ss[1],ss[2],ss[3],68,0x06f067aa72176fba)\n    ss[6], ss[2] = RND(ss[3],ss[4],ss[5],ss[6],ss[7],ss[0],ss[1],ss[2],69,0x0a637dc5a2c898a6)\n    ss[5], ss[1] = RND(ss[2],ss[3],ss[4],ss[5],ss[6],ss[7],ss[0],ss[1],70,0x113f9804bef90dae)\n    ss[4], ss[0] = RND(ss[1],ss[2],ss[3],ss[4],ss[5],ss[6],ss[7],ss[0],71,0x1b710b35131c471b)\n    ss[3], ss[7] = RND(ss[0],ss[1],ss[2],ss[3],ss[4],ss[5],ss[6],ss[7],72,0x28db77f523047d84)\n    ss[2], ss[6] = RND(ss[7],ss[0],ss[1],ss[2],ss[3],ss[4],ss[5],ss[6],73,0x32caab7b40c72493)\n    ss[1], ss[5] = RND(ss[6],ss[7],ss[0],ss[1],ss[2],ss[3],ss[4],ss[5],74,0x3c9ebe0a15c9bebc)\n    ss[0], ss[4] = RND(ss[5],ss[6],ss[7],ss[0],ss[1],ss[2],ss[3],ss[4],75,0x431d67c49c100d4c)\n    ss[7], ss[3] = RND(ss[4],ss[5],ss[6],ss[7],ss[0],ss[1],ss[2],ss[3],76,0x4cc5d4becb3e42b6)\n    ss[6], ss[2] = RND(ss[3],ss[4],ss[5],ss[6],ss[7],ss[0],ss[1],ss[2],77,0x597f299cfc657e2a)\n    ss[5], ss[1] = RND(ss[2],ss[3],ss[4],ss[5],ss[6],ss[7],ss[0],ss[1],78,0x5fcb6fab3ad6faec)\n    ss[4], ss[0] = RND(ss[1],ss[2],ss[3],ss[4],ss[5],ss[6],ss[7],ss[0],79,0x6c44198c4a475817)\n\n    dig = []\n    for i, x in enumerate(sha_info['digest']):\n        dig.append( (x + ss[i]) & 0xffffffffffffffff )\n    sha_info['digest'] = dig\n\ndef sha_init():\n    sha_info = new_shaobject()\n    sha_info['digest'] = [ 0x6a09e667f3bcc908, 0xbb67ae8584caa73b, 0x3c6ef372fe94f82b, 0xa54ff53a5f1d36f1, 0x510e527fade682d1, 0x9b05688c2b3e6c1f, 0x1f83d9abfb41bd6b, 0x5be0cd19137e2179]\n    sha_info['count_lo'] = 0\n    sha_info['count_hi'] = 0\n    sha_info['local'] = 0\n    sha_info['digestsize'] = 64\n    return sha_info\n\ndef sha384_init():\n    sha_info = new_shaobject()\n    sha_info['digest'] = [ 0xcbbb9d5dc1059ed8, 0x629a292a367cd507, 0x9159015a3070dd17, 0x152fecd8f70e5939, 0x67332667ffc00b31, 0x8eb44a8768581511, 0xdb0c2e0d64f98fa7, 0x47b5481dbefa4fa4]\n    sha_info['count_lo'] = 0\n    sha_info['count_hi'] = 0\n    sha_info['local'] = 0\n    sha_info['digestsize'] = 48\n    return sha_info\n\ndef getbuf(s):\n    if isinstance(s, str):\n        return s\n    elif isinstance(s, unicode):\n        return str(s)\n    else:\n        return buffer(s)\n\ndef sha_update(sha_info, buffer):\n    count = len(buffer)\n    buffer_idx = 0\n    clo = (sha_info['count_lo'] + (count << 3)) & 0xffffffff\n    if clo < sha_info['count_lo']:\n        sha_info['count_hi'] += 1\n    sha_info['count_lo'] = clo\n\n    sha_info['count_hi'] += (count >> 29)\n\n    if sha_info['local']:\n        i = SHA_BLOCKSIZE - sha_info['local']\n        if i > count:\n            i = count\n\n        # copy buffer\n        for x in enumerate(buffer[buffer_idx:buffer_idx+i]):\n            sha_info['data'][sha_info['local']+x[0]] = struct.unpack('B', x[1])[0]\n\n        count -= i\n        buffer_idx += i\n\n        sha_info['local'] += i\n        if sha_info['local'] == SHA_BLOCKSIZE:\n            sha_transform(sha_info)\n            sha_info['local'] = 0\n        else:\n            return\n\n    while count >= SHA_BLOCKSIZE:\n        # copy buffer\n        sha_info['data'] = [struct.unpack('B',c)[0] for c in buffer[buffer_idx:buffer_idx + SHA_BLOCKSIZE]]\n        count -= SHA_BLOCKSIZE\n        buffer_idx += SHA_BLOCKSIZE\n        sha_transform(sha_info)\n\n    # copy buffer\n    pos = sha_info['local']\n    sha_info['data'][pos:pos+count] = [struct.unpack('B',c)[0] for c in buffer[buffer_idx:buffer_idx + count]]\n    sha_info['local'] = count\n\ndef sha_final(sha_info):\n    lo_bit_count = sha_info['count_lo']\n    hi_bit_count = sha_info['count_hi']\n    count = (lo_bit_count >> 3) & 0x7f\n    sha_info['data'][count] = 0x80;\n    count += 1\n    if count > SHA_BLOCKSIZE - 16:\n        # zero the bytes in data after the count\n        sha_info['data'] = sha_info['data'][:count] + ([0] * (SHA_BLOCKSIZE - count))\n        sha_transform(sha_info)\n        # zero bytes in data\n        sha_info['data'] = [0] * SHA_BLOCKSIZE\n    else:\n        sha_info['data'] = sha_info['data'][:count] + ([0] * (SHA_BLOCKSIZE - count))\n\n    sha_info['data'][112] = 0;\n    sha_info['data'][113] = 0;\n    sha_info['data'][114] = 0;\n    sha_info['data'][115] = 0;\n    sha_info['data'][116] = 0;\n    sha_info['data'][117] = 0;\n    sha_info['data'][118] = 0;\n    sha_info['data'][119] = 0;\n\n    sha_info['data'][120] = (hi_bit_count >> 24) & 0xff\n    sha_info['data'][121] = (hi_bit_count >> 16) & 0xff\n    sha_info['data'][122] = (hi_bit_count >>  8) & 0xff\n    sha_info['data'][123] = (hi_bit_count >>  0) & 0xff\n    sha_info['data'][124] = (lo_bit_count >> 24) & 0xff\n    sha_info['data'][125] = (lo_bit_count >> 16) & 0xff\n    sha_info['data'][126] = (lo_bit_count >>  8) & 0xff\n    sha_info['data'][127] = (lo_bit_count >>  0) & 0xff\n\n    sha_transform(sha_info)\n\n    dig = []\n    for i in sha_info['digest']:\n        dig.extend([ ((i>>56) & 0xff), ((i>>48) & 0xff), ((i>>40) & 0xff), ((i>>32) & 0xff), ((i>>24) & 0xff), ((i>>16) & 0xff), ((i>>8) & 0xff), (i & 0xff) ])\n    return ''.join([chr(i) for i in dig])\n\nclass sha512(object):\n    digest_size = digestsize = SHA_DIGESTSIZE\n    block_size = SHA_BLOCKSIZE\n\n    def __init__(self, s=None):\n        self._sha = sha_init()\n        if s:\n            sha_update(self._sha, getbuf(s))\n\n    def update(self, s):\n        sha_update(self._sha, getbuf(s))\n\n    def digest(self):\n        return sha_final(self._sha.copy())[:self._sha['digestsize']]\n\n    def hexdigest(self):\n        return ''.join(['%.2x' % ord(i) for i in self.digest()])\n\n    def copy(self):\n        new = sha512.__new__(sha512)\n        new._sha = self._sha.copy()\n        return new\n\nclass sha384(sha512):\n    digest_size = digestsize = 48\n\n    def __init__(self, s=None):\n        self._sha = sha384_init()\n        if s:\n            sha_update(self._sha, getbuf(s))\n\n    def copy(self):\n        new = sha384.__new__(sha384)\n        new._sha = self._sha.copy()\n        return new\n\ndef test():\n    import _sha512\n\n    a_str = \"just a test string\"\n\n    assert _sha512.sha512().hexdigest() == sha512().hexdigest()\n    assert _sha512.sha512(a_str).hexdigest() == sha512(a_str).hexdigest()\n    assert _sha512.sha512(a_str*7).hexdigest() == sha512(a_str*7).hexdigest()\n\n    s = sha512(a_str)\n    s.update(a_str)\n    assert _sha512.sha512(a_str+a_str).hexdigest() == s.hexdigest()\n\nif __name__ == \"__main__\":\n    test()\n", 
    "_structseq": "\"\"\"\nImplementation helper: a struct that looks like a tuple.  See timemodule\nand posixmodule for example uses.\n\"\"\"\n\nclass structseqfield(object):\n    \"\"\"Definition of field of a structseq.  The 'index' is for positional\n    tuple-like indexing.  Fields whose index is after a gap in the numbers\n    cannot be accessed like this, but only by name.\n    \"\"\"\n    def __init__(self, index, doc=None, default=lambda self: None):\n        self.__name__ = '?'\n        self.index    = index    # patched to None if not positional\n        self._index   = index\n        self.__doc__  = doc\n        self._default = default\n\n    def __repr__(self):\n        return '<field %s (%s)>' % (self.__name__,\n                                    self.__doc__ or 'undocumented')\n\n    def __get__(self, obj, typ=None):\n        if obj is None:\n            return self\n        if self.index is None:\n            return obj.__dict__[self.__name__]\n        else:\n            return obj[self.index]\n\n    def __set__(self, obj, value):\n        raise TypeError(\"readonly attribute\")\n\n\nclass structseqtype(type):\n\n    def __new__(metacls, classname, bases, dict):\n        assert not bases\n        fields_by_index = {}\n        for name, field in dict.items():\n            if isinstance(field, structseqfield):\n                assert field._index not in fields_by_index\n                fields_by_index[field._index] = field\n                field.__name__ = name\n        dict['n_fields'] = len(fields_by_index)\n\n        extra_fields = sorted(fields_by_index.iteritems())\n        n_sequence_fields = 0\n        while extra_fields and extra_fields[0][0] == n_sequence_fields:\n            extra_fields.pop(0)\n            n_sequence_fields += 1\n        dict['n_sequence_fields'] = n_sequence_fields\n        dict['n_unnamed_fields'] = 0     # no fully anonymous fields in PyPy\n\n        extra_fields = [field for index, field in extra_fields]\n        for field in extra_fields:\n            field.index = None     # no longer relevant\n\n        assert '__new__' not in dict\n        dict['_extra_fields'] = tuple(extra_fields)\n        dict['__new__'] = structseq_new\n        dict['__reduce__'] = structseq_reduce\n        dict['__setattr__'] = structseq_setattr\n        dict['__repr__'] = structseq_repr\n        dict['_name'] = dict.get('name', '')\n        return type.__new__(metacls, classname, (tuple,), dict)\n\n\nbuiltin_dict = dict\n\ndef structseq_new(cls, sequence, dict={}):\n    sequence = tuple(sequence)\n    dict = builtin_dict(dict)\n    N = cls.n_sequence_fields\n    if len(sequence) < N:\n        if N < cls.n_fields:\n            msg = \"at least\"\n        else:\n            msg = \"exactly\"\n        raise TypeError(\"expected a sequence with %s %d items\" % (\n            msg, N))\n    if len(sequence) > N:\n        if len(sequence) > cls.n_fields:\n            if N < cls.n_fields:\n                msg = \"at most\"\n            else:\n                msg = \"exactly\"\n            raise TypeError(\"expected a sequence with %s %d items\" % (\n                msg, cls.n_fields))\n        for field, value in zip(cls._extra_fields, sequence[N:]):\n            name = field.__name__\n            if name in dict:\n                raise TypeError(\"duplicate value for %r\" % (name,))\n            dict[name] = value\n        sequence = sequence[:N]\n    result = tuple.__new__(cls, sequence)\n    object.__setattr__(result, '__dict__', dict)\n    for field in cls._extra_fields:\n        name = field.__name__\n        if name not in dict:\n            dict[name] = field._default(result)\n    return result\n\ndef structseq_reduce(self):\n    return type(self), (tuple(self), self.__dict__)\n\ndef structseq_setattr(self, attr, value):\n    raise AttributeError(\"%r object has no attribute %r\" % (\n        self.__class__.__name__, attr))\n\ndef structseq_repr(self):\n    fields = {}\n    for field in type(self).__dict__.values():\n        if isinstance(field, structseqfield):\n            fields[field._index] = field\n    parts = [\"%s=%r\" % (fields[index].__name__, value)\n             for index, value in enumerate(self)]\n    return \"%s(%s)\" % (self._name, \", \".join(parts))\n", 
    "_weakrefset": "# Access WeakSet through the weakref module.\n# This code is separated-out because it is needed\n# by abc.py to load everything else at startup.\n\nfrom _weakref import ref\n\n__all__ = ['WeakSet']\n\n\nclass _IterationGuard(object):\n    # This context manager registers itself in the current iterators of the\n    # weak container, such as to delay all removals until the context manager\n    # exits.\n    # This technique should be relatively thread-safe (since sets are).\n\n    def __init__(self, weakcontainer):\n        # Don't create cycles\n        self.weakcontainer = ref(weakcontainer)\n\n    def __enter__(self):\n        w = self.weakcontainer()\n        if w is not None:\n            w._iterating.add(self)\n        return self\n\n    def __exit__(self, e, t, b):\n        w = self.weakcontainer()\n        if w is not None:\n            s = w._iterating\n            s.remove(self)\n            if not s:\n                w._commit_removals()\n\n\nclass WeakSet(object):\n    def __init__(self, data=None):\n        self.data = set()\n        def _remove(item, selfref=ref(self)):\n            self = selfref()\n            if self is not None:\n                if self._iterating:\n                    self._pending_removals.append(item)\n                else:\n                    self.data.discard(item)\n        self._remove = _remove\n        # A list of keys to be removed\n        self._pending_removals = []\n        self._iterating = set()\n        if data is not None:\n            self.update(data)\n\n    def _commit_removals(self):\n        l = self._pending_removals\n        discard = self.data.discard\n        while l:\n            discard(l.pop())\n\n    def __iter__(self):\n        with _IterationGuard(self):\n            for itemref in self.data:\n                item = itemref()\n                if item is not None:\n                    # Caveat: the iterator will keep a strong reference to\n                    # `item` until it is resumed or closed.\n                    yield item\n\n    def __len__(self):\n        return len(self.data) - len(self._pending_removals)\n\n    def __contains__(self, item):\n        try:\n            wr = ref(item)\n        except TypeError:\n            return False\n        return wr in self.data\n\n    def __reduce__(self):\n        return (self.__class__, (list(self),),\n                getattr(self, '__dict__', None))\n\n    __hash__ = None\n\n    def add(self, item):\n        if self._pending_removals:\n            self._commit_removals()\n        self.data.add(ref(item, self._remove))\n\n    def clear(self):\n        if self._pending_removals:\n            self._commit_removals()\n        self.data.clear()\n\n    def copy(self):\n        return self.__class__(self)\n\n    def pop(self):\n        if self._pending_removals:\n            self._commit_removals()\n        while True:\n            try:\n                itemref = self.data.pop()\n            except KeyError:\n                raise KeyError('pop from empty WeakSet')\n            item = itemref()\n            if item is not None:\n                return item\n\n    def remove(self, item):\n        if self._pending_removals:\n            self._commit_removals()\n        self.data.remove(ref(item))\n\n    def discard(self, item):\n        if self._pending_removals:\n            self._commit_removals()\n        self.data.discard(ref(item))\n\n    def update(self, other):\n        if self._pending_removals:\n            self._commit_removals()\n        for element in other:\n            self.add(element)\n\n    def __ior__(self, other):\n        self.update(other)\n        return self\n\n    def difference(self, other):\n        newset = self.copy()\n        newset.difference_update(other)\n        return newset\n    __sub__ = difference\n\n    def difference_update(self, other):\n        self.__isub__(other)\n    def __isub__(self, other):\n        if self._pending_removals:\n            self._commit_removals()\n        if self is other:\n            self.data.clear()\n        else:\n            self.data.difference_update(ref(item) for item in other)\n        return self\n\n    def intersection(self, other):\n        return self.__class__(item for item in other if item in self)\n    __and__ = intersection\n\n    def intersection_update(self, other):\n        self.__iand__(other)\n    def __iand__(self, other):\n        if self._pending_removals:\n            self._commit_removals()\n        self.data.intersection_update(ref(item) for item in other)\n        return self\n\n    def issubset(self, other):\n        return self.data.issubset(ref(item) for item in other)\n    __le__ = issubset\n\n    def __lt__(self, other):\n        return self.data < set(ref(item) for item in other)\n\n    def issuperset(self, other):\n        return self.data.issuperset(ref(item) for item in other)\n    __ge__ = issuperset\n\n    def __gt__(self, other):\n        return self.data > set(ref(item) for item in other)\n\n    def __eq__(self, other):\n        if not isinstance(other, self.__class__):\n            return NotImplemented\n        return self.data == set(ref(item) for item in other)\n\n    def __ne__(self, other):\n        opposite = self.__eq__(other)\n        if opposite is NotImplemented:\n            return NotImplemented\n        return not opposite\n\n    def symmetric_difference(self, other):\n        newset = self.copy()\n        newset.symmetric_difference_update(other)\n        return newset\n    __xor__ = symmetric_difference\n\n    def symmetric_difference_update(self, other):\n        self.__ixor__(other)\n    def __ixor__(self, other):\n        if self._pending_removals:\n            self._commit_removals()\n        if self is other:\n            self.data.clear()\n        else:\n            self.data.symmetric_difference_update(ref(item, self._remove) for item in other)\n        return self\n\n    def union(self, other):\n        return self.__class__(e for s in (self, other) for e in s)\n    __or__ = union\n\n    def isdisjoint(self, other):\n        return len(self.intersection(other)) == 0\n", 
    "abc": "# Copyright 2007 Google, Inc. All Rights Reserved.\n# Licensed to PSF under a Contributor Agreement.\n\n\"\"\"Abstract Base Classes (ABCs) according to PEP 3119.\"\"\"\n\nimport types\n\nfrom _weakrefset import WeakSet\n\n# Instance of old-style class\nclass _C: pass\n_InstanceType = type(_C())\n\n\ndef abstractmethod(funcobj):\n    \"\"\"A decorator indicating abstract methods.\n\n    Requires that the metaclass is ABCMeta or derived from it.  A\n    class that has a metaclass derived from ABCMeta cannot be\n    instantiated unless all of its abstract methods are overridden.\n    The abstract methods can be called using any of the normal\n    'super' call mechanisms.\n\n    Usage:\n\n        class C:\n            __metaclass__ = ABCMeta\n            @abstractmethod\n            def my_abstract_method(self, ...):\n                ...\n    \"\"\"\n    funcobj.__isabstractmethod__ = True\n    return funcobj\n\n\nclass abstractproperty(property):\n    \"\"\"A decorator indicating abstract properties.\n\n    Requires that the metaclass is ABCMeta or derived from it.  A\n    class that has a metaclass derived from ABCMeta cannot be\n    instantiated unless all of its abstract properties are overridden.\n    The abstract properties can be called using any of the normal\n    'super' call mechanisms.\n\n    Usage:\n\n        class C:\n            __metaclass__ = ABCMeta\n            @abstractproperty\n            def my_abstract_property(self):\n                ...\n\n    This defines a read-only property; you can also define a read-write\n    abstract property using the 'long' form of property declaration:\n\n        class C:\n            __metaclass__ = ABCMeta\n            def getx(self): ...\n            def setx(self, value): ...\n            x = abstractproperty(getx, setx)\n    \"\"\"\n    __isabstractmethod__ = True\n\n\nclass ABCMeta(type):\n\n    \"\"\"Metaclass for defining Abstract Base Classes (ABCs).\n\n    Use this metaclass to create an ABC.  An ABC can be subclassed\n    directly, and then acts as a mix-in class.  You can also register\n    unrelated concrete classes (even built-in classes) and unrelated\n    ABCs as 'virtual subclasses' -- these and their descendants will\n    be considered subclasses of the registering ABC by the built-in\n    issubclass() function, but the registering ABC won't show up in\n    their MRO (Method Resolution Order) nor will method\n    implementations defined by the registering ABC be callable (not\n    even via super()).\n\n    \"\"\"\n\n    # A global counter that is incremented each time a class is\n    # registered as a virtual subclass of anything.  It forces the\n    # negative cache to be cleared before its next use.\n    _abc_invalidation_counter = 0\n\n    def __new__(mcls, name, bases, namespace):\n        cls = super(ABCMeta, mcls).__new__(mcls, name, bases, namespace)\n        # Compute set of abstract method names\n        abstracts = set(name\n                     for name, value in namespace.items()\n                     if getattr(value, \"__isabstractmethod__\", False))\n        for base in bases:\n            for name in getattr(base, \"__abstractmethods__\", set()):\n                value = getattr(cls, name, None)\n                if getattr(value, \"__isabstractmethod__\", False):\n                    abstracts.add(name)\n        cls.__abstractmethods__ = frozenset(abstracts)\n        # Set up inheritance registry\n        cls._abc_registry = WeakSet()\n        cls._abc_cache = WeakSet()\n        cls._abc_negative_cache = WeakSet()\n        cls._abc_negative_cache_version = ABCMeta._abc_invalidation_counter\n        return cls\n\n    def register(cls, subclass):\n        \"\"\"Register a virtual subclass of an ABC.\"\"\"\n        if not isinstance(subclass, (type, types.ClassType)):\n            raise TypeError(\"Can only register classes\")\n        if issubclass(subclass, cls):\n            return  # Already a subclass\n        # Subtle: test for cycles *after* testing for \"already a subclass\";\n        # this means we allow X.register(X) and interpret it as a no-op.\n        if issubclass(cls, subclass):\n            # This would create a cycle, which is bad for the algorithm below\n            raise RuntimeError(\"Refusing to create an inheritance cycle\")\n        cls._abc_registry.add(subclass)\n        ABCMeta._abc_invalidation_counter += 1  # Invalidate negative cache\n\n    def _dump_registry(cls, file=None):\n        \"\"\"Debug helper to print the ABC registry.\"\"\"\n        print >> file, \"Class: %s.%s\" % (cls.__module__, cls.__name__)\n        print >> file, \"Inv.counter: %s\" % ABCMeta._abc_invalidation_counter\n        for name in sorted(cls.__dict__.keys()):\n            if name.startswith(\"_abc_\"):\n                value = getattr(cls, name)\n                print >> file, \"%s: %r\" % (name, value)\n\n    def __instancecheck__(cls, instance):\n        \"\"\"Override for isinstance(instance, cls).\"\"\"\n        # Inline the cache checking when it's simple.\n        subclass = getattr(instance, '__class__', None)\n        if subclass is not None and subclass in cls._abc_cache:\n            return True\n        subtype = type(instance)\n        # Old-style instances\n        if subtype is _InstanceType:\n            subtype = subclass\n        if subtype is subclass or subclass is None:\n            if (cls._abc_negative_cache_version ==\n                ABCMeta._abc_invalidation_counter and\n                subtype in cls._abc_negative_cache):\n                return False\n            # Fall back to the subclass check.\n            return cls.__subclasscheck__(subtype)\n        return (cls.__subclasscheck__(subclass) or\n                cls.__subclasscheck__(subtype))\n\n    def __subclasscheck__(cls, subclass):\n        \"\"\"Override for issubclass(subclass, cls).\"\"\"\n        # Check cache\n        if subclass in cls._abc_cache:\n            return True\n        # Check negative cache; may have to invalidate\n        if cls._abc_negative_cache_version < ABCMeta._abc_invalidation_counter:\n            # Invalidate the negative cache\n            cls._abc_negative_cache = WeakSet()\n            cls._abc_negative_cache_version = ABCMeta._abc_invalidation_counter\n        elif subclass in cls._abc_negative_cache:\n            return False\n        # Check the subclass hook\n        ok = cls.__subclasshook__(subclass)\n        if ok is not NotImplemented:\n            assert isinstance(ok, bool)\n            if ok:\n                cls._abc_cache.add(subclass)\n            else:\n                cls._abc_negative_cache.add(subclass)\n            return ok\n        # Check if it's a direct subclass\n        if cls in getattr(subclass, '__mro__', ()):\n            cls._abc_cache.add(subclass)\n            return True\n        # Check if it's a subclass of a registered class (recursive)\n        for rcls in cls._abc_registry:\n            if issubclass(subclass, rcls):\n                cls._abc_cache.add(subclass)\n                return True\n        # Check if it's a subclass of a subclass (recursive)\n        for scls in cls.__subclasses__():\n            if issubclass(subclass, scls):\n                cls._abc_cache.add(subclass)\n                return True\n        # No dice; update negative cache\n        cls._abc_negative_cache.add(subclass)\n        return False\n", 
    "atexit": "\"\"\"\natexit.py - allow programmer to define multiple exit functions to be executed\nupon normal program termination.\n\nOne public function, register, is defined.\n\"\"\"\n\n__all__ = [\"register\"]\n\nimport sys\n\n_exithandlers = []\ndef _run_exitfuncs():\n    \"\"\"run any registered exit functions\n\n    _exithandlers is traversed in reverse order so functions are executed\n    last in, first out.\n    \"\"\"\n\n    exc_info = None\n    while _exithandlers:\n        func, targs, kargs = _exithandlers.pop()\n        try:\n            func(*targs, **kargs)\n        except SystemExit:\n            exc_info = sys.exc_info()\n        except:\n            import traceback\n            print >> sys.stderr, \"Error in atexit._run_exitfuncs:\"\n            traceback.print_exc()\n            exc_info = sys.exc_info()\n\n    if exc_info is not None:\n        raise exc_info[0], exc_info[1], exc_info[2]\n\n\ndef register(func, *targs, **kargs):\n    \"\"\"register a function to be executed upon normal program termination\n\n    func - function to be called at exit\n    targs - optional arguments to pass to func\n    kargs - optional keyword arguments to pass to func\n\n    func is returned to facilitate usage as a decorator.\n    \"\"\"\n    _exithandlers.append((func, targs, kargs))\n    return func\n\nif hasattr(sys, \"exitfunc\"):\n    # Assume it's another registered exit function - append it to our list\n    register(sys.exitfunc)\nsys.exitfunc = _run_exitfuncs\n\nif __name__ == \"__main__\":\n    def x1():\n        print \"running x1\"\n    def x2(n):\n        print \"running x2(%r)\" % (n,)\n    def x3(n, kwd=None):\n        print \"running x3(%r, kwd=%r)\" % (n, kwd)\n\n    register(x1)\n    register(x2, 12)\n    register(x3, 5, \"bar\")\n    register(x3, \"no kwd args\")\n", 
    "base64": "#! /usr/bin/env python\n\n\"\"\"RFC 3548: Base16, Base32, Base64 Data Encodings\"\"\"\n\n# Modified 04-Oct-1995 by Jack Jansen to use binascii module\n# Modified 30-Dec-2003 by Barry Warsaw to add full RFC 3548 support\n\nimport re\nimport struct\nimport binascii\n\n\n__all__ = [\n    # Legacy interface exports traditional RFC 1521 Base64 encodings\n    'encode', 'decode', 'encodestring', 'decodestring',\n    # Generalized interface for other encodings\n    'b64encode', 'b64decode', 'b32encode', 'b32decode',\n    'b16encode', 'b16decode',\n    # Standard Base64 encoding\n    'standard_b64encode', 'standard_b64decode',\n    # Some common Base64 alternatives.  As referenced by RFC 3458, see thread\n    # starting at:\n    #\n    # http://zgp.org/pipermail/p2p-hackers/2001-September/000316.html\n    'urlsafe_b64encode', 'urlsafe_b64decode',\n    ]\n\n_translation = [chr(_x) for _x in range(256)]\nEMPTYSTRING = ''\n\n\ndef _translate(s, altchars):\n    translation = _translation[:]\n    for k, v in altchars.items():\n        translation[ord(k)] = v\n    return s.translate(''.join(translation))\n\n\n\f\n# Base64 encoding/decoding uses binascii\n\ndef b64encode(s, altchars=None):\n    \"\"\"Encode a string using Base64.\n\n    s is the string to encode.  Optional altchars must be a string of at least\n    length 2 (additional characters are ignored) which specifies an\n    alternative alphabet for the '+' and '/' characters.  This allows an\n    application to e.g. generate url or filesystem safe Base64 strings.\n\n    The encoded string is returned.\n    \"\"\"\n    # Strip off the trailing newline\n    encoded = binascii.b2a_base64(s)[:-1]\n    if altchars is not None:\n        return _translate(encoded, {'+': altchars[0], '/': altchars[1]})\n    return encoded\n\n\ndef b64decode(s, altchars=None):\n    \"\"\"Decode a Base64 encoded string.\n\n    s is the string to decode.  Optional altchars must be a string of at least\n    length 2 (additional characters are ignored) which specifies the\n    alternative alphabet used instead of the '+' and '/' characters.\n\n    The decoded string is returned.  A TypeError is raised if s were\n    incorrectly padded or if there are non-alphabet characters present in the\n    string.\n    \"\"\"\n    if altchars is not None:\n        s = _translate(s, {altchars[0]: '+', altchars[1]: '/'})\n    try:\n        return binascii.a2b_base64(s)\n    except binascii.Error, msg:\n        # Transform this exception for consistency\n        raise TypeError(msg)\n\n\ndef standard_b64encode(s):\n    \"\"\"Encode a string using the standard Base64 alphabet.\n\n    s is the string to encode.  The encoded string is returned.\n    \"\"\"\n    return b64encode(s)\n\ndef standard_b64decode(s):\n    \"\"\"Decode a string encoded with the standard Base64 alphabet.\n\n    s is the string to decode.  The decoded string is returned.  A TypeError\n    is raised if the string is incorrectly padded or if there are non-alphabet\n    characters present in the string.\n    \"\"\"\n    return b64decode(s)\n\ndef urlsafe_b64encode(s):\n    \"\"\"Encode a string using a url-safe Base64 alphabet.\n\n    s is the string to encode.  The encoded string is returned.  The alphabet\n    uses '-' instead of '+' and '_' instead of '/'.\n    \"\"\"\n    return b64encode(s, '-_')\n\ndef urlsafe_b64decode(s):\n    \"\"\"Decode a string encoded with the standard Base64 alphabet.\n\n    s is the string to decode.  The decoded string is returned.  A TypeError\n    is raised if the string is incorrectly padded or if there are non-alphabet\n    characters present in the string.\n\n    The alphabet uses '-' instead of '+' and '_' instead of '/'.\n    \"\"\"\n    return b64decode(s, '-_')\n\n\n\f\n# Base32 encoding/decoding must be done in Python\n_b32alphabet = {\n    0: 'A',  9: 'J', 18: 'S', 27: '3',\n    1: 'B', 10: 'K', 19: 'T', 28: '4',\n    2: 'C', 11: 'L', 20: 'U', 29: '5',\n    3: 'D', 12: 'M', 21: 'V', 30: '6',\n    4: 'E', 13: 'N', 22: 'W', 31: '7',\n    5: 'F', 14: 'O', 23: 'X',\n    6: 'G', 15: 'P', 24: 'Y',\n    7: 'H', 16: 'Q', 25: 'Z',\n    8: 'I', 17: 'R', 26: '2',\n    }\n\n_b32tab = _b32alphabet.items()\n_b32tab.sort()\n_b32tab = [v for k, v in _b32tab]\n_b32rev = dict([(v, long(k)) for k, v in _b32alphabet.items()])\n\n\ndef b32encode(s):\n    \"\"\"Encode a string using Base32.\n\n    s is the string to encode.  The encoded string is returned.\n    \"\"\"\n    parts = []\n    quanta, leftover = divmod(len(s), 5)\n    # Pad the last quantum with zero bits if necessary\n    if leftover:\n        s += ('\\0' * (5 - leftover))\n        quanta += 1\n    for i in range(quanta):\n        # c1 and c2 are 16 bits wide, c3 is 8 bits wide.  The intent of this\n        # code is to process the 40 bits in units of 5 bits.  So we take the 1\n        # leftover bit of c1 and tack it onto c2.  Then we take the 2 leftover\n        # bits of c2 and tack them onto c3.  The shifts and masks are intended\n        # to give us values of exactly 5 bits in width.\n        c1, c2, c3 = struct.unpack('!HHB', s[i*5:(i+1)*5])\n        c2 += (c1 & 1) << 16 # 17 bits wide\n        c3 += (c2 & 3) << 8  # 10 bits wide\n        parts.extend([_b32tab[c1 >> 11],         # bits 1 - 5\n                      _b32tab[(c1 >> 6) & 0x1f], # bits 6 - 10\n                      _b32tab[(c1 >> 1) & 0x1f], # bits 11 - 15\n                      _b32tab[c2 >> 12],         # bits 16 - 20 (1 - 5)\n                      _b32tab[(c2 >> 7) & 0x1f], # bits 21 - 25 (6 - 10)\n                      _b32tab[(c2 >> 2) & 0x1f], # bits 26 - 30 (11 - 15)\n                      _b32tab[c3 >> 5],          # bits 31 - 35 (1 - 5)\n                      _b32tab[c3 & 0x1f],        # bits 36 - 40 (1 - 5)\n                      ])\n    encoded = EMPTYSTRING.join(parts)\n    # Adjust for any leftover partial quanta\n    if leftover == 1:\n        return encoded[:-6] + '======'\n    elif leftover == 2:\n        return encoded[:-4] + '===='\n    elif leftover == 3:\n        return encoded[:-3] + '==='\n    elif leftover == 4:\n        return encoded[:-1] + '='\n    return encoded\n\n\ndef b32decode(s, casefold=False, map01=None):\n    \"\"\"Decode a Base32 encoded string.\n\n    s is the string to decode.  Optional casefold is a flag specifying whether\n    a lowercase alphabet is acceptable as input.  For security purposes, the\n    default is False.\n\n    RFC 3548 allows for optional mapping of the digit 0 (zero) to the letter O\n    (oh), and for optional mapping of the digit 1 (one) to either the letter I\n    (eye) or letter L (el).  The optional argument map01 when not None,\n    specifies which letter the digit 1 should be mapped to (when map01 is not\n    None, the digit 0 is always mapped to the letter O).  For security\n    purposes the default is None, so that 0 and 1 are not allowed in the\n    input.\n\n    The decoded string is returned.  A TypeError is raised if s were\n    incorrectly padded or if there are non-alphabet characters present in the\n    string.\n    \"\"\"\n    quanta, leftover = divmod(len(s), 8)\n    if leftover:\n        raise TypeError('Incorrect padding')\n    # Handle section 2.4 zero and one mapping.  The flag map01 will be either\n    # False, or the character to map the digit 1 (one) to.  It should be\n    # either L (el) or I (eye).\n    if map01:\n        s = _translate(s, {'0': 'O', '1': map01})\n    if casefold:\n        s = s.upper()\n    # Strip off pad characters from the right.  We need to count the pad\n    # characters because this will tell us how many null bytes to remove from\n    # the end of the decoded string.\n    padchars = 0\n    mo = re.search('(?P<pad>[=]*)$', s)\n    if mo:\n        padchars = len(mo.group('pad'))\n        if padchars > 0:\n            s = s[:-padchars]\n    # Now decode the full quanta\n    parts = []\n    acc = 0\n    shift = 35\n    for c in s:\n        val = _b32rev.get(c)\n        if val is None:\n            raise TypeError('Non-base32 digit found')\n        acc += _b32rev[c] << shift\n        shift -= 5\n        if shift < 0:\n            parts.append(binascii.unhexlify('%010x' % acc))\n            acc = 0\n            shift = 35\n    # Process the last, partial quanta\n    last = binascii.unhexlify('%010x' % acc)\n    if padchars == 0:\n        last = ''                       # No characters\n    elif padchars == 1:\n        last = last[:-1]\n    elif padchars == 3:\n        last = last[:-2]\n    elif padchars == 4:\n        last = last[:-3]\n    elif padchars == 6:\n        last = last[:-4]\n    else:\n        raise TypeError('Incorrect padding')\n    parts.append(last)\n    return EMPTYSTRING.join(parts)\n\n\n\f\n# RFC 3548, Base 16 Alphabet specifies uppercase, but hexlify() returns\n# lowercase.  The RFC also recommends against accepting input case\n# insensitively.\ndef b16encode(s):\n    \"\"\"Encode a string using Base16.\n\n    s is the string to encode.  The encoded string is returned.\n    \"\"\"\n    return binascii.hexlify(s).upper()\n\n\ndef b16decode(s, casefold=False):\n    \"\"\"Decode a Base16 encoded string.\n\n    s is the string to decode.  Optional casefold is a flag specifying whether\n    a lowercase alphabet is acceptable as input.  For security purposes, the\n    default is False.\n\n    The decoded string is returned.  A TypeError is raised if s were\n    incorrectly padded or if there are non-alphabet characters present in the\n    string.\n    \"\"\"\n    if casefold:\n        s = s.upper()\n    if re.search('[^0-9A-F]', s):\n        raise TypeError('Non-base16 digit found')\n    return binascii.unhexlify(s)\n\n\n\f\n# Legacy interface.  This code could be cleaned up since I don't believe\n# binascii has any line length limitations.  It just doesn't seem worth it\n# though.\n\nMAXLINESIZE = 76 # Excluding the CRLF\nMAXBINSIZE = (MAXLINESIZE//4)*3\n\ndef encode(input, output):\n    \"\"\"Encode a file.\"\"\"\n    while True:\n        s = input.read(MAXBINSIZE)\n        if not s:\n            break\n        while len(s) < MAXBINSIZE:\n            ns = input.read(MAXBINSIZE-len(s))\n            if not ns:\n                break\n            s += ns\n        line = binascii.b2a_base64(s)\n        output.write(line)\n\n\ndef decode(input, output):\n    \"\"\"Decode a file.\"\"\"\n    while True:\n        line = input.readline()\n        if not line:\n            break\n        s = binascii.a2b_base64(line)\n        output.write(s)\n\n\ndef encodestring(s):\n    \"\"\"Encode a string into multiple lines of base-64 data.\"\"\"\n    pieces = []\n    for i in range(0, len(s), MAXBINSIZE):\n        chunk = s[i : i + MAXBINSIZE]\n        pieces.append(binascii.b2a_base64(chunk))\n    return \"\".join(pieces)\n\n\ndef decodestring(s):\n    \"\"\"Decode a string.\"\"\"\n    return binascii.a2b_base64(s)\n\n\n\f\n# Useable as a script...\ndef test():\n    \"\"\"Small test program\"\"\"\n    import sys, getopt\n    try:\n        opts, args = getopt.getopt(sys.argv[1:], 'deut')\n    except getopt.error, msg:\n        sys.stdout = sys.stderr\n        print msg\n        print \"\"\"usage: %s [-d|-e|-u|-t] [file|-]\n        -d, -u: decode\n        -e: encode (default)\n        -t: encode and decode string 'Aladdin:open sesame'\"\"\"%sys.argv[0]\n        sys.exit(2)\n    func = encode\n    for o, a in opts:\n        if o == '-e': func = encode\n        if o == '-d': func = decode\n        if o == '-u': func = decode\n        if o == '-t': test1(); return\n    if args and args[0] != '-':\n        with open(args[0], 'rb') as f:\n            func(f, sys.stdout)\n    else:\n        func(sys.stdin, sys.stdout)\n\n\ndef test1():\n    s0 = \"Aladdin:open sesame\"\n    s1 = encodestring(s0)\n    s2 = decodestring(s1)\n    print s0, repr(s1), s2\n\n\nif __name__ == '__main__':\n    test()\n", 
    "bdb": "\"\"\"Debugger basics\"\"\"\n\nimport fnmatch\nimport sys\nimport os\nimport types\n\n__all__ = [\"BdbQuit\",\"Bdb\",\"Breakpoint\"]\n\nclass BdbQuit(Exception):\n    \"\"\"Exception to give up completely\"\"\"\n\n\nclass Bdb:\n\n    \"\"\"Generic Python debugger base class.\n\n    This class takes care of details of the trace facility;\n    a derived class should implement user interaction.\n    The standard debugger class (pdb.Pdb) is an example.\n    \"\"\"\n\n    def __init__(self, skip=None):\n        self.skip = set(skip) if skip else None\n        self.breaks = {}\n        self.fncache = {}\n        self.frame_returning = None\n\n    def canonic(self, filename):\n        if filename == \"<\" + filename[1:-1] + \">\":\n            return filename\n        canonic = self.fncache.get(filename)\n        if not canonic:\n            canonic = os.path.abspath(filename)\n            canonic = os.path.normcase(canonic)\n            self.fncache[filename] = canonic\n        return canonic\n\n    def reset(self):\n        import linecache\n        linecache.checkcache()\n        self.botframe = None\n        self._set_stopinfo(None, None)\n\n    def trace_dispatch(self, frame, event, arg):\n        if self.quitting:\n            return # None\n        if event == 'line':\n            return self.dispatch_line(frame)\n        if event == 'call':\n            return self.dispatch_call(frame, arg)\n        if event == 'return':\n            return self.dispatch_return(frame, arg)\n        if event == 'exception':\n            return self.dispatch_exception(frame, arg)\n        if event == 'c_call':\n            return self.trace_dispatch\n        if event == 'c_exception':\n            return self.trace_dispatch\n        if event == 'c_return':\n            return self.trace_dispatch\n        print 'bdb.Bdb.dispatch: unknown debugging event:', repr(event)\n        return self.trace_dispatch\n\n    def dispatch_line(self, frame):\n        if self.stop_here(frame) or self.break_here(frame):\n            self.user_line(frame)\n            if self.quitting: raise BdbQuit\n        return self.trace_dispatch\n\n    def dispatch_call(self, frame, arg):\n        # XXX 'arg' is no longer used\n        if self.botframe is None:\n            # First call of dispatch since reset()\n            self.botframe = frame.f_back # (CT) Note that this may also be None!\n            return self.trace_dispatch\n        if not (self.stop_here(frame) or self.break_anywhere(frame)):\n            # No need to trace this function\n            return # None\n        self.user_call(frame, arg)\n        if self.quitting: raise BdbQuit\n        return self.trace_dispatch\n\n    def dispatch_return(self, frame, arg):\n        if self.stop_here(frame) or frame == self.returnframe:\n            try:\n                self.frame_returning = frame\n                self.user_return(frame, arg)\n            finally:\n                self.frame_returning = None\n            if self.quitting: raise BdbQuit\n        return self.trace_dispatch\n\n    def dispatch_exception(self, frame, arg):\n        if self.stop_here(frame):\n            self.user_exception(frame, arg)\n            if self.quitting: raise BdbQuit\n        return self.trace_dispatch\n\n    # Normally derived classes don't override the following\n    # methods, but they may if they want to redefine the\n    # definition of stopping and breakpoints.\n\n    def is_skipped_module(self, module_name):\n        for pattern in self.skip:\n            if fnmatch.fnmatch(module_name, pattern):\n                return True\n        return False\n\n    def stop_here(self, frame):\n        # (CT) stopframe may now also be None, see dispatch_call.\n        # (CT) the former test for None is therefore removed from here.\n        if self.skip and \\\n               self.is_skipped_module(frame.f_globals.get('__name__')):\n            return False\n        if frame is self.stopframe:\n            if self.stoplineno == -1:\n                return False\n            return frame.f_lineno >= self.stoplineno\n        while frame is not None and frame is not self.stopframe:\n            if frame is self.botframe:\n                return True\n            frame = frame.f_back\n        return False\n\n    def break_here(self, frame):\n        filename = self.canonic(frame.f_code.co_filename)\n        if not filename in self.breaks:\n            return False\n        lineno = frame.f_lineno\n        if not lineno in self.breaks[filename]:\n            # The line itself has no breakpoint, but maybe the line is the\n            # first line of a function with breakpoint set by function name.\n            lineno = frame.f_code.co_firstlineno\n            if not lineno in self.breaks[filename]:\n                return False\n\n        # flag says ok to delete temp. bp\n        (bp, flag) = effective(filename, lineno, frame)\n        if bp:\n            self.currentbp = bp.number\n            if (flag and bp.temporary):\n                self.do_clear(str(bp.number))\n            return True\n        else:\n            return False\n\n    def do_clear(self, arg):\n        raise NotImplementedError, \"subclass of bdb must implement do_clear()\"\n\n    def break_anywhere(self, frame):\n        return self.canonic(frame.f_code.co_filename) in self.breaks\n\n    # Derived classes should override the user_* methods\n    # to gain control.\n\n    def user_call(self, frame, argument_list):\n        \"\"\"This method is called when there is the remote possibility\n        that we ever need to stop in this function.\"\"\"\n        pass\n\n    def user_line(self, frame):\n        \"\"\"This method is called when we stop or break at this line.\"\"\"\n        pass\n\n    def user_return(self, frame, return_value):\n        \"\"\"This method is called when a return trap is set here.\"\"\"\n        pass\n\n    def user_exception(self, frame, exc_info):\n        exc_type, exc_value, exc_traceback = exc_info\n        \"\"\"This method is called if an exception occurs,\n        but only if we are to stop at or just below this level.\"\"\"\n        pass\n\n    def _set_stopinfo(self, stopframe, returnframe, stoplineno=0):\n        self.stopframe = stopframe\n        self.returnframe = returnframe\n        self.quitting = 0\n        # stoplineno >= 0 means: stop at line >= the stoplineno\n        # stoplineno -1 means: don't stop at all\n        self.stoplineno = stoplineno\n\n    # Derived classes and clients can call the following methods\n    # to affect the stepping state.\n\n    def set_until(self, frame): #the name \"until\" is borrowed from gdb\n        \"\"\"Stop when the line with the line no greater than the current one is\n        reached or when returning from current frame\"\"\"\n        self._set_stopinfo(frame, frame, frame.f_lineno+1)\n\n    def set_step(self):\n        \"\"\"Stop after one line of code.\"\"\"\n        # Issue #13183: pdb skips frames after hitting a breakpoint and running\n        # step commands.\n        # Restore the trace function in the caller (that may not have been set\n        # for performance reasons) when returning from the current frame.\n        if self.frame_returning:\n            caller_frame = self.frame_returning.f_back\n            if caller_frame and not caller_frame.f_trace:\n                caller_frame.f_trace = self.trace_dispatch\n        self._set_stopinfo(None, None)\n\n    def set_next(self, frame):\n        \"\"\"Stop on the next line in or below the given frame.\"\"\"\n        self._set_stopinfo(frame, None)\n\n    def set_return(self, frame):\n        \"\"\"Stop when returning from the given frame.\"\"\"\n        self._set_stopinfo(frame.f_back, frame)\n\n    def set_trace(self, frame=None):\n        \"\"\"Start debugging from `frame`.\n\n        If frame is not specified, debugging starts from caller's frame.\n        \"\"\"\n        if frame is None:\n            frame = sys._getframe().f_back\n        self.reset()\n        while frame:\n            frame.f_trace = self.trace_dispatch\n            self.botframe = frame\n            frame = frame.f_back\n        self.set_step()\n        sys.settrace(self.trace_dispatch)\n\n    def set_continue(self):\n        # Don't stop except at breakpoints or when finished\n        self._set_stopinfo(self.botframe, None, -1)\n        if not self.breaks:\n            # no breakpoints; run without debugger overhead\n            sys.settrace(None)\n            frame = sys._getframe().f_back\n            while frame and frame is not self.botframe:\n                del frame.f_trace\n                frame = frame.f_back\n\n    def set_quit(self):\n        self.stopframe = self.botframe\n        self.returnframe = None\n        self.quitting = 1\n        sys.settrace(None)\n\n    # Derived classes and clients can call the following methods\n    # to manipulate breakpoints.  These methods return an\n    # error message is something went wrong, None if all is well.\n    # Set_break prints out the breakpoint line and file:lineno.\n    # Call self.get_*break*() to see the breakpoints or better\n    # for bp in Breakpoint.bpbynumber: if bp: bp.bpprint().\n\n    def set_break(self, filename, lineno, temporary=0, cond = None,\n                  funcname=None):\n        filename = self.canonic(filename)\n        import linecache # Import as late as possible\n        line = linecache.getline(filename, lineno)\n        if not line:\n            return 'Line %s:%d does not exist' % (filename,\n                                   lineno)\n        if not filename in self.breaks:\n            self.breaks[filename] = []\n        list = self.breaks[filename]\n        if not lineno in list:\n            list.append(lineno)\n        bp = Breakpoint(filename, lineno, temporary, cond, funcname)\n\n    def _prune_breaks(self, filename, lineno):\n        if (filename, lineno) not in Breakpoint.bplist:\n            self.breaks[filename].remove(lineno)\n        if not self.breaks[filename]:\n            del self.breaks[filename]\n\n    def clear_break(self, filename, lineno):\n        filename = self.canonic(filename)\n        if not filename in self.breaks:\n            return 'There are no breakpoints in %s' % filename\n        if lineno not in self.breaks[filename]:\n            return 'There is no breakpoint at %s:%d' % (filename,\n                                    lineno)\n        # If there's only one bp in the list for that file,line\n        # pair, then remove the breaks entry\n        for bp in Breakpoint.bplist[filename, lineno][:]:\n            bp.deleteMe()\n        self._prune_breaks(filename, lineno)\n\n    def clear_bpbynumber(self, arg):\n        try:\n            number = int(arg)\n        except:\n            return 'Non-numeric breakpoint number (%s)' % arg\n        try:\n            bp = Breakpoint.bpbynumber[number]\n        except IndexError:\n            return 'Breakpoint number (%d) out of range' % number\n        if not bp:\n            return 'Breakpoint (%d) already deleted' % number\n        bp.deleteMe()\n        self._prune_breaks(bp.file, bp.line)\n\n    def clear_all_file_breaks(self, filename):\n        filename = self.canonic(filename)\n        if not filename in self.breaks:\n            return 'There are no breakpoints in %s' % filename\n        for line in self.breaks[filename]:\n            blist = Breakpoint.bplist[filename, line]\n            for bp in blist:\n                bp.deleteMe()\n        del self.breaks[filename]\n\n    def clear_all_breaks(self):\n        if not self.breaks:\n            return 'There are no breakpoints'\n        for bp in Breakpoint.bpbynumber:\n            if bp:\n                bp.deleteMe()\n        self.breaks = {}\n\n    def get_break(self, filename, lineno):\n        filename = self.canonic(filename)\n        return filename in self.breaks and \\\n            lineno in self.breaks[filename]\n\n    def get_breaks(self, filename, lineno):\n        filename = self.canonic(filename)\n        return filename in self.breaks and \\\n            lineno in self.breaks[filename] and \\\n            Breakpoint.bplist[filename, lineno] or []\n\n    def get_file_breaks(self, filename):\n        filename = self.canonic(filename)\n        if filename in self.breaks:\n            return self.breaks[filename]\n        else:\n            return []\n\n    def get_all_breaks(self):\n        return self.breaks\n\n    # Derived classes and clients can call the following method\n    # to get a data structure representing a stack trace.\n\n    def get_stack(self, f, t):\n        stack = []\n        if t and t.tb_frame is f:\n            t = t.tb_next\n        while f is not None:\n            stack.append((f, f.f_lineno))\n            if f is self.botframe:\n                break\n            f = f.f_back\n        stack.reverse()\n        i = max(0, len(stack) - 1)\n        while t is not None:\n            stack.append((t.tb_frame, t.tb_lineno))\n            t = t.tb_next\n        if f is None:\n            i = max(0, len(stack) - 1)\n        return stack, i\n\n    #\n\n    def format_stack_entry(self, frame_lineno, lprefix=': '):\n        import linecache, repr\n        frame, lineno = frame_lineno\n        filename = self.canonic(frame.f_code.co_filename)\n        s = '%s(%r)' % (filename, lineno)\n        if frame.f_code.co_name:\n            s = s + frame.f_code.co_name\n        else:\n            s = s + \"<lambda>\"\n        if '__args__' in frame.f_locals:\n            args = frame.f_locals['__args__']\n        else:\n            args = None\n        if args:\n            s = s + repr.repr(args)\n        else:\n            s = s + '()'\n        if '__return__' in frame.f_locals:\n            rv = frame.f_locals['__return__']\n            s = s + '->'\n            s = s + repr.repr(rv)\n        line = linecache.getline(filename, lineno, frame.f_globals)\n        if line: s = s + lprefix + line.strip()\n        return s\n\n    # The following two methods can be called by clients to use\n    # a debugger to debug a statement, given as a string.\n\n    def run(self, cmd, globals=None, locals=None):\n        if globals is None:\n            import __main__\n            globals = __main__.__dict__\n        if locals is None:\n            locals = globals\n        self.reset()\n        sys.settrace(self.trace_dispatch)\n        if not isinstance(cmd, types.CodeType):\n            cmd = cmd+'\\n'\n        try:\n            exec cmd in globals, locals\n        except BdbQuit:\n            pass\n        finally:\n            self.quitting = 1\n            sys.settrace(None)\n\n    def runeval(self, expr, globals=None, locals=None):\n        if globals is None:\n            import __main__\n            globals = __main__.__dict__\n        if locals is None:\n            locals = globals\n        self.reset()\n        sys.settrace(self.trace_dispatch)\n        if not isinstance(expr, types.CodeType):\n            expr = expr+'\\n'\n        try:\n            return eval(expr, globals, locals)\n        except BdbQuit:\n            pass\n        finally:\n            self.quitting = 1\n            sys.settrace(None)\n\n    def runctx(self, cmd, globals, locals):\n        # B/W compatibility\n        self.run(cmd, globals, locals)\n\n    # This method is more useful to debug a single function call.\n\n    def runcall(self, func, *args, **kwds):\n        self.reset()\n        sys.settrace(self.trace_dispatch)\n        res = None\n        try:\n            res = func(*args, **kwds)\n        except BdbQuit:\n            pass\n        finally:\n            self.quitting = 1\n            sys.settrace(None)\n        return res\n\n\ndef set_trace():\n    Bdb().set_trace()\n\n\nclass Breakpoint:\n\n    \"\"\"Breakpoint class\n\n    Implements temporary breakpoints, ignore counts, disabling and\n    (re)-enabling, and conditionals.\n\n    Breakpoints are indexed by number through bpbynumber and by\n    the file,line tuple using bplist.  The former points to a\n    single instance of class Breakpoint.  The latter points to a\n    list of such instances since there may be more than one\n    breakpoint per line.\n\n    \"\"\"\n\n    # XXX Keeping state in the class is a mistake -- this means\n    # you cannot have more than one active Bdb instance.\n\n    next = 1        # Next bp to be assigned\n    bplist = {}     # indexed by (file, lineno) tuple\n    bpbynumber = [None] # Each entry is None or an instance of Bpt\n                # index 0 is unused, except for marking an\n                # effective break .... see effective()\n\n    def __init__(self, file, line, temporary=0, cond=None, funcname=None):\n        self.funcname = funcname\n        # Needed if funcname is not None.\n        self.func_first_executable_line = None\n        self.file = file    # This better be in canonical form!\n        self.line = line\n        self.temporary = temporary\n        self.cond = cond\n        self.enabled = 1\n        self.ignore = 0\n        self.hits = 0\n        self.number = Breakpoint.next\n        Breakpoint.next = Breakpoint.next + 1\n        # Build the two lists\n        self.bpbynumber.append(self)\n        if (file, line) in self.bplist:\n            self.bplist[file, line].append(self)\n        else:\n            self.bplist[file, line] = [self]\n\n\n    def deleteMe(self):\n        index = (self.file, self.line)\n        self.bpbynumber[self.number] = None   # No longer in list\n        self.bplist[index].remove(self)\n        if not self.bplist[index]:\n            # No more bp for this f:l combo\n            del self.bplist[index]\n\n    def enable(self):\n        self.enabled = 1\n\n    def disable(self):\n        self.enabled = 0\n\n    def bpprint(self, out=None):\n        if out is None:\n            out = sys.stdout\n        if self.temporary:\n            disp = 'del  '\n        else:\n            disp = 'keep '\n        if self.enabled:\n            disp = disp + 'yes  '\n        else:\n            disp = disp + 'no   '\n        print >>out, '%-4dbreakpoint   %s at %s:%d' % (self.number, disp,\n                                                       self.file, self.line)\n        if self.cond:\n            print >>out, '\\tstop only if %s' % (self.cond,)\n        if self.ignore:\n            print >>out, '\\tignore next %d hits' % (self.ignore)\n        if (self.hits):\n            if (self.hits > 1): ss = 's'\n            else: ss = ''\n            print >>out, ('\\tbreakpoint already hit %d time%s' %\n                          (self.hits, ss))\n\n# -----------end of Breakpoint class----------\n\ndef checkfuncname(b, frame):\n    \"\"\"Check whether we should break here because of `b.funcname`.\"\"\"\n    if not b.funcname:\n        # Breakpoint was set via line number.\n        if b.line != frame.f_lineno:\n            # Breakpoint was set at a line with a def statement and the function\n            # defined is called: don't break.\n            return False\n        return True\n\n    # Breakpoint set via function name.\n\n    if frame.f_code.co_name != b.funcname:\n        # It's not a function call, but rather execution of def statement.\n        return False\n\n    # We are in the right frame.\n    if not b.func_first_executable_line:\n        # The function is entered for the 1st time.\n        b.func_first_executable_line = frame.f_lineno\n\n    if  b.func_first_executable_line != frame.f_lineno:\n        # But we are not at the first line number: don't break.\n        return False\n    return True\n\n# Determines if there is an effective (active) breakpoint at this\n# line of code.  Returns breakpoint number or 0 if none\ndef effective(file, line, frame):\n    \"\"\"Determine which breakpoint for this file:line is to be acted upon.\n\n    Called only if we know there is a bpt at this\n    location.  Returns breakpoint that was triggered and a flag\n    that indicates if it is ok to delete a temporary bp.\n\n    \"\"\"\n    possibles = Breakpoint.bplist[file,line]\n    for i in range(0, len(possibles)):\n        b = possibles[i]\n        if b.enabled == 0:\n            continue\n        if not checkfuncname(b, frame):\n            continue\n        # Count every hit when bp is enabled\n        b.hits = b.hits + 1\n        if not b.cond:\n            # If unconditional, and ignoring,\n            # go on to next, else break\n            if b.ignore > 0:\n                b.ignore = b.ignore -1\n                continue\n            else:\n                # breakpoint and marker that's ok\n                # to delete if temporary\n                return (b,1)\n        else:\n            # Conditional bp.\n            # Ignore count applies only to those bpt hits where the\n            # condition evaluates to true.\n            try:\n                val = eval(b.cond, frame.f_globals,\n                       frame.f_locals)\n                if val:\n                    if b.ignore > 0:\n                        b.ignore = b.ignore -1\n                        # continue\n                    else:\n                        return (b,1)\n                # else:\n                #   continue\n            except:\n                # if eval fails, most conservative\n                # thing is to stop on breakpoint\n                # regardless of ignore count.\n                # Don't delete temporary,\n                # as another hint to user.\n                return (b,0)\n    return (None, None)\n\n# -------------------- testing --------------------\n\nclass Tdb(Bdb):\n    def user_call(self, frame, args):\n        name = frame.f_code.co_name\n        if not name: name = '???'\n        print '+++ call', name, args\n    def user_line(self, frame):\n        import linecache\n        name = frame.f_code.co_name\n        if not name: name = '???'\n        fn = self.canonic(frame.f_code.co_filename)\n        line = linecache.getline(fn, frame.f_lineno, frame.f_globals)\n        print '+++', fn, frame.f_lineno, name, ':', line.strip()\n    def user_return(self, frame, retval):\n        print '+++ return', retval\n    def user_exception(self, frame, exc_stuff):\n        print '+++ exception', exc_stuff\n        self.set_continue()\n\ndef foo(n):\n    print 'foo(', n, ')'\n    x = bar(n*10)\n    print 'bar returned', x\n\ndef bar(a):\n    print 'bar(', a, ')'\n    return a/2\n\ndef test():\n    t = Tdb()\n    t.run('import bdb; bdb.foo(10)')\n\n# end\n", 
    "cPickle": "#\n# Reimplementation of cPickle, mostly as a copy of pickle.py\n#\n\nfrom pickle import Pickler, dump, dumps, PickleError, PicklingError, UnpicklingError, _EmptyClass\nfrom pickle import __doc__, __version__, format_version, compatible_formats\nfrom types import *\nfrom copy_reg import dispatch_table\nfrom copy_reg import _extension_registry, _inverted_registry, _extension_cache\nimport marshal, struct, sys\n\ntry: from __pypy__ import builtinify\nexcept ImportError: builtinify = lambda f: f\n\n# These are purely informational; no code uses these.\nformat_version = \"2.0\"                  # File format version we write\ncompatible_formats = [\"1.0\",            # Original protocol 0\n                      \"1.1\",            # Protocol 0 with INST added\n                      \"1.2\",            # Original protocol 1\n                      \"1.3\",            # Protocol 1 with BINFLOAT added\n                      \"2.0\",            # Protocol 2\n                      ]                 # Old format versions we can read\n\n# Keep in synch with cPickle.  This is the highest protocol number we\n# know how to read.\nHIGHEST_PROTOCOL = 2\n\nBadPickleGet = KeyError\nUnpickleableError = PicklingError\n\nMARK            = ord('(')   # push special markobject on stack\nSTOP            = ord('.')   # every pickle ends with STOP\nPOP             = ord('0')   # discard topmost stack item\nPOP_MARK        = ord('1')   # discard stack top through topmost markobject\nDUP             = ord('2')   # duplicate top stack item\nFLOAT           = ord('F')   # push float object; decimal string argument\nINT             = ord('I')   # push integer or bool; decimal string argument\nBININT          = ord('J')   # push four-byte signed int\nBININT1         = ord('K')   # push 1-byte unsigned int\nLONG            = ord('L')   # push long; decimal string argument\nBININT2         = ord('M')   # push 2-byte unsigned int\nNONE            = ord('N')   # push None\nPERSID          = ord('P')   # push persistent object; id is taken from string arg\nBINPERSID       = ord('Q')   #  \"       \"         \"  ;  \"  \"   \"     \"  stack\nREDUCE          = ord('R')   # apply callable to argtuple, both on stack\nSTRING          = ord('S')   # push string; NL-terminated string argument\nBINSTRING       = ord('T')   # push string; counted binary string argument\nSHORT_BINSTRING = ord('U')   #  \"     \"   ;    \"      \"       \"      \" < 256 bytes\nUNICODE         = ord('V')   # push Unicode string; raw-unicode-escaped'd argument\nBINUNICODE      = ord('X')   #   \"     \"       \"  ; counted UTF-8 string argument\nAPPEND          = ord('a')   # append stack top to list below it\nBUILD           = ord('b')   # call __setstate__ or __dict__.update()\nGLOBAL          = ord('c')   # push self.find_class(modname, name); 2 string args\nDICT            = ord('d')   # build a dict from stack items\nEMPTY_DICT      = ord('}')   # push empty dict\nAPPENDS         = ord('e')   # extend list on stack by topmost stack slice\nGET             = ord('g')   # push item from memo on stack; index is string arg\nBINGET          = ord('h')   #   \"    \"    \"    \"   \"   \"  ;   \"    \" 1-byte arg\nINST            = ord('i')   # build & push class instance\nLONG_BINGET     = ord('j')   # push item from memo on stack; index is 4-byte arg\nLIST            = ord('l')   # build list from topmost stack items\nEMPTY_LIST      = ord(']')   # push empty list\nOBJ             = ord('o')   # build & push class instance\nPUT             = ord('p')   # store stack top in memo; index is string arg\nBINPUT          = ord('q')   #   \"     \"    \"   \"   \" ;   \"    \" 1-byte arg\nLONG_BINPUT     = ord('r')   #   \"     \"    \"   \"   \" ;   \"    \" 4-byte arg\nSETITEM         = ord('s')   # add key+value pair to dict\nTUPLE           = ord('t')   # build tuple from topmost stack items\nEMPTY_TUPLE     = ord(')')   # push empty tuple\nSETITEMS        = ord('u')   # modify dict by adding topmost key+value pairs\nBINFLOAT        = ord('G')   # push float; arg is 8-byte float encoding\n\nTRUE            = 'I01\\n'  # not an opcode; see INT docs in pickletools.py\nFALSE           = 'I00\\n'  # not an opcode; see INT docs in pickletools.py\n\n# Protocol 2\n\nPROTO           = ord('\\x80')  # identify pickle protocol\nNEWOBJ          = ord('\\x81')  # build object by applying cls.__new__ to argtuple\nEXT1            = ord('\\x82')  # push object from extension registry; 1-byte index\nEXT2            = ord('\\x83')  # ditto, but 2-byte index\nEXT4            = ord('\\x84')  # ditto, but 4-byte index\nTUPLE1          = ord('\\x85')  # build 1-tuple from stack top\nTUPLE2          = ord('\\x86')  # build 2-tuple from two topmost stack items\nTUPLE3          = ord('\\x87')  # build 3-tuple from three topmost stack items\nNEWTRUE         = ord('\\x88')  # push True\nNEWFALSE        = ord('\\x89')  # push False\nLONG1           = ord('\\x8a')  # push long from < 256 bytes\nLONG4           = ord('\\x8b')  # push really big long\n\n_tuplesize2code = [EMPTY_TUPLE, TUPLE1, TUPLE2, TUPLE3]\n\n\n# ____________________________________________________________\n# XXX some temporary dark magic to produce pickled dumps that are\n#     closer to the ones produced by cPickle in CPython\n\nfrom pickle import StringIO\n\nPythonPickler = Pickler\nclass Pickler(PythonPickler):\n    def __init__(self, *args, **kw):\n        self.__f = None\n        if len(args) == 1 and isinstance(args[0], int):\n            self.__f = StringIO()\n            PythonPickler.__init__(self, self.__f, args[0], **kw)\n        else:\n            PythonPickler.__init__(self, *args, **kw)\n\n    def memoize(self, obj):\n        self.memo[id(None)] = None   # cPickle starts counting at one\n        return PythonPickler.memoize(self, obj)\n\n    def getvalue(self):\n        return self.__f and self.__f.getvalue()\n\n@builtinify\ndef dump(obj, file, protocol=None):\n    Pickler(file, protocol).dump(obj)\n\n@builtinify\ndef dumps(obj, protocol=None):\n    file = StringIO()\n    Pickler(file, protocol).dump(obj)\n    return file.getvalue()\n\n# Why use struct.pack() for pickling but marshal.loads() for\n# unpickling?  struct.pack() is 40% faster than marshal.dumps(), but\n# marshal.loads() is twice as fast as struct.unpack()!\nmloads = marshal.loads\n\n# Unpickling machinery\n\nclass _Stack(list):\n    def pop(self, index=-1):\n        try:\n            return list.pop(self, index)\n        except IndexError:\n            raise UnpicklingError(\"unpickling stack underflow\")\n\nclass Unpickler(object):\n\n    def __init__(self, file):\n        \"\"\"This takes a file-like object for reading a pickle data stream.\n\n        The protocol version of the pickle is detected automatically, so no\n        proto argument is needed.\n\n        The file-like object must have two methods, a read() method that\n        takes an integer argument, and a readline() method that requires no\n        arguments.  Both methods should return a string.  Thus file-like\n        object can be a file object opened for reading, a StringIO object,\n        or any other custom object that meets this interface.\n        \"\"\"\n        self.readline = file.readline\n        self.read = file.read\n        self.memo = {}\n\n    def load(self):\n        \"\"\"Read a pickled object representation from the open file.\n\n        Return the reconstituted object hierarchy specified in the file.\n        \"\"\"\n        self.mark = object() # any new unique object\n        self.stack = _Stack()\n        self.append = self.stack.append\n        try:\n            key = ord(self.read(1))\n            while key != STOP:\n                self.dispatch[key](self)\n                key = ord(self.read(1))\n        except TypeError:\n            if self.read(1) == '':\n                raise EOFError\n            raise\n        return self.stack.pop()\n\n    # Return largest index k such that self.stack[k] is self.mark.\n    # If the stack doesn't contain a mark, eventually raises IndexError.\n    # This could be sped by maintaining another stack, of indices at which\n    # the mark appears.  For that matter, the latter stack would suffice,\n    # and we wouldn't need to push mark objects on self.stack at all.\n    # Doing so is probably a good thing, though, since if the pickle is\n    # corrupt (or hostile) we may get a clue from finding self.mark embedded\n    # in unpickled objects.\n    def marker(self):\n        k = len(self.stack)-1\n        while self.stack[k] is not self.mark: k -= 1\n        return k\n\n    dispatch = {}\n\n    def load_proto(self):\n        proto = ord(self.read(1))\n        if not 0 <= proto <= 2:\n            raise ValueError, \"unsupported pickle protocol: %d\" % proto\n    dispatch[PROTO] = load_proto\n\n    def load_persid(self):\n        pid = self.readline()[:-1]\n        self.append(self.persistent_load(pid))\n    dispatch[PERSID] = load_persid\n\n    def load_binpersid(self):\n        pid = self.stack.pop()\n        self.append(self.persistent_load(pid))\n    dispatch[BINPERSID] = load_binpersid\n\n    def load_none(self):\n        self.append(None)\n    dispatch[NONE] = load_none\n\n    def load_false(self):\n        self.append(False)\n    dispatch[NEWFALSE] = load_false\n\n    def load_true(self):\n        self.append(True)\n    dispatch[NEWTRUE] = load_true\n\n    def load_int(self):\n        data = self.readline()\n        if data == FALSE[1:]:\n            val = False\n        elif data == TRUE[1:]:\n            val = True\n        else:\n            val = int(data)\n        self.append(val)\n    dispatch[INT] = load_int\n\n    def load_binint(self):\n        self.append(mloads('i' + self.read(4)))\n    dispatch[BININT] = load_binint\n\n    def load_binint1(self):\n        self.append(ord(self.read(1)))\n    dispatch[BININT1] = load_binint1\n\n    def load_binint2(self):\n        self.append(mloads('i' + self.read(2) + '\\000\\000'))\n    dispatch[BININT2] = load_binint2\n\n    def load_long(self):\n        self.append(long(self.readline()[:-1], 0))\n    dispatch[LONG] = load_long\n\n    def load_long1(self):\n        n = ord(self.read(1))\n        bytes = self.read(n)\n        self.append(decode_long(bytes))\n    dispatch[LONG1] = load_long1\n\n    def load_long4(self):\n        n = mloads('i' + self.read(4))\n        bytes = self.read(n)\n        self.append(decode_long(bytes))\n    dispatch[LONG4] = load_long4\n\n    def load_float(self):\n        self.append(float(self.readline()[:-1]))\n    dispatch[FLOAT] = load_float\n\n    def load_binfloat(self, unpack=struct.unpack):\n        self.append(unpack('>d', self.read(8))[0])\n    dispatch[BINFLOAT] = load_binfloat\n\n    def load_string(self):\n        rep = self.readline()\n        if len(rep) < 3:\n            raise ValueError, \"insecure string pickle\"\n        if rep[0] == \"'\" == rep[-2]:\n            rep = rep[1:-2]\n        elif rep[0] == '\"' == rep[-2]:\n            rep = rep[1:-2]\n        else:\n            raise ValueError, \"insecure string pickle\"\n        self.append(rep.decode(\"string-escape\"))\n    dispatch[STRING] = load_string\n\n    def load_binstring(self):\n        L = mloads('i' + self.read(4))\n        self.append(self.read(L))\n    dispatch[BINSTRING] = load_binstring\n\n    def load_unicode(self):\n        self.append(unicode(self.readline()[:-1],'raw-unicode-escape'))\n    dispatch[UNICODE] = load_unicode\n\n    def load_binunicode(self):\n        L = mloads('i' + self.read(4))\n        self.append(unicode(self.read(L),'utf-8'))\n    dispatch[BINUNICODE] = load_binunicode\n\n    def load_short_binstring(self):\n        L = ord(self.read(1))\n        self.append(self.read(L))\n    dispatch[SHORT_BINSTRING] = load_short_binstring\n\n    def load_tuple(self):\n        k = self.marker()\n        self.stack[k:] = [tuple(self.stack[k+1:])]\n    dispatch[TUPLE] = load_tuple\n\n    def load_empty_tuple(self):\n        self.stack.append(())\n    dispatch[EMPTY_TUPLE] = load_empty_tuple\n\n    def load_tuple1(self):\n        self.stack[-1] = (self.stack[-1],)\n    dispatch[TUPLE1] = load_tuple1\n\n    def load_tuple2(self):\n        self.stack[-2:] = [(self.stack[-2], self.stack[-1])]\n    dispatch[TUPLE2] = load_tuple2\n\n    def load_tuple3(self):\n        self.stack[-3:] = [(self.stack[-3], self.stack[-2], self.stack[-1])]\n    dispatch[TUPLE3] = load_tuple3\n\n    def load_empty_list(self):\n        self.stack.append([])\n    dispatch[EMPTY_LIST] = load_empty_list\n\n    def load_empty_dictionary(self):\n        self.stack.append({})\n    dispatch[EMPTY_DICT] = load_empty_dictionary\n\n    def load_list(self):\n        k = self.marker()\n        self.stack[k:] = [self.stack[k+1:]]\n    dispatch[LIST] = load_list\n\n    def load_dict(self):\n        k = self.marker()\n        d = {}\n        items = self.stack[k+1:]\n        for i in range(0, len(items), 2):\n            key = items[i]\n            value = items[i+1]\n            d[key] = value\n        self.stack[k:] = [d]\n    dispatch[DICT] = load_dict\n\n    # INST and OBJ differ only in how they get a class object.  It's not\n    # only sensible to do the rest in a common routine, the two routines\n    # previously diverged and grew different bugs.\n    # klass is the class to instantiate, and k points to the topmost mark\n    # object, following which are the arguments for klass.__init__.\n    def _instantiate(self, klass, k):\n        args = tuple(self.stack[k+1:])\n        del self.stack[k:]\n        instantiated = 0\n        if (not args and\n                type(klass) is ClassType and\n                not hasattr(klass, \"__getinitargs__\")):\n            try:\n                value = _EmptyClass()\n                value.__class__ = klass\n                instantiated = 1\n            except RuntimeError:\n                # In restricted execution, assignment to inst.__class__ is\n                # prohibited\n                pass\n        if not instantiated:\n            try:\n                value = klass(*args)\n            except TypeError, err:\n                raise TypeError, \"in constructor for %s: %s\" % (\n                    klass.__name__, str(err)), sys.exc_info()[2]\n        self.append(value)\n\n    def load_inst(self):\n        module = self.readline()[:-1]\n        name = self.readline()[:-1]\n        klass = self.find_class(module, name)\n        self._instantiate(klass, self.marker())\n    dispatch[INST] = load_inst\n\n    def load_obj(self):\n        # Stack is ... markobject classobject arg1 arg2 ...\n        k = self.marker()\n        klass = self.stack.pop(k+1)\n        self._instantiate(klass, k)\n    dispatch[OBJ] = load_obj\n\n    def load_newobj(self):\n        args = self.stack.pop()\n        cls = self.stack[-1]\n        obj = cls.__new__(cls, *args)\n        self.stack[-1] = obj\n    dispatch[NEWOBJ] = load_newobj\n\n    def load_global(self):\n        module = self.readline()[:-1]\n        name = self.readline()[:-1]\n        klass = self.find_class(module, name)\n        self.append(klass)\n    dispatch[GLOBAL] = load_global\n\n    def load_ext1(self):\n        code = ord(self.read(1))\n        self.get_extension(code)\n    dispatch[EXT1] = load_ext1\n\n    def load_ext2(self):\n        code = mloads('i' + self.read(2) + '\\000\\000')\n        self.get_extension(code)\n    dispatch[EXT2] = load_ext2\n\n    def load_ext4(self):\n        code = mloads('i' + self.read(4))\n        self.get_extension(code)\n    dispatch[EXT4] = load_ext4\n\n    def get_extension(self, code):\n        nil = []\n        obj = _extension_cache.get(code, nil)\n        if obj is not nil:\n            self.append(obj)\n            return\n        key = _inverted_registry.get(code)\n        if not key:\n            raise ValueError(\"unregistered extension code %d\" % code)\n        obj = self.find_class(*key)\n        _extension_cache[code] = obj\n        self.append(obj)\n\n    def find_class(self, module, name):\n        # Subclasses may override this\n        __import__(module)\n        mod = sys.modules[module]\n        klass = getattr(mod, name)\n        return klass\n\n    def load_reduce(self):\n        args = self.stack.pop()\n        func = self.stack[-1]\n        value = self.stack[-1](*args)\n        self.stack[-1] = value\n    dispatch[REDUCE] = load_reduce\n\n    def load_pop(self):\n        del self.stack[-1]\n    dispatch[POP] = load_pop\n\n    def load_pop_mark(self):\n        k = self.marker()\n        del self.stack[k:]\n    dispatch[POP_MARK] = load_pop_mark\n\n    def load_dup(self):\n        self.append(self.stack[-1])\n    dispatch[DUP] = load_dup\n\n    def load_get(self):\n        self.append(self.memo[self.readline()[:-1]])\n    dispatch[GET] = load_get\n\n    def load_binget(self):\n        i = ord(self.read(1))\n        self.append(self.memo[repr(i)])\n    dispatch[BINGET] = load_binget\n\n    def load_long_binget(self):\n        i = mloads('i' + self.read(4))\n        self.append(self.memo[repr(i)])\n    dispatch[LONG_BINGET] = load_long_binget\n\n    def load_put(self):\n        self.memo[self.readline()[:-1]] = self.stack[-1]\n    dispatch[PUT] = load_put\n\n    def load_binput(self):\n        i = ord(self.read(1))\n        self.memo[repr(i)] = self.stack[-1]\n    dispatch[BINPUT] = load_binput\n\n    def load_long_binput(self):\n        i = mloads('i' + self.read(4))\n        self.memo[repr(i)] = self.stack[-1]\n    dispatch[LONG_BINPUT] = load_long_binput\n\n    def load_append(self):\n        value = self.stack.pop()\n        self.stack[-1].append(value)\n    dispatch[APPEND] = load_append\n\n    def load_appends(self):\n        stack = self.stack\n        mark = self.marker()\n        lst = stack[mark - 1]\n        lst.extend(stack[mark + 1:])\n        del stack[mark:]\n    dispatch[APPENDS] = load_appends\n\n    def load_setitem(self):\n        stack = self.stack\n        value = stack.pop()\n        key = stack.pop()\n        dict = stack[-1]\n        dict[key] = value\n    dispatch[SETITEM] = load_setitem\n\n    def load_setitems(self):\n        stack = self.stack\n        mark = self.marker()\n        dict = stack[mark - 1]\n        for i in range(mark + 1, len(stack), 2):\n            dict[stack[i]] = stack[i + 1]\n\n        del stack[mark:]\n    dispatch[SETITEMS] = load_setitems\n\n    def load_build(self):\n        stack = self.stack\n        state = stack.pop()\n        inst = stack[-1]\n        setstate = getattr(inst, \"__setstate__\", None)\n        if setstate:\n            setstate(state)\n            return\n        slotstate = None\n        if isinstance(state, tuple) and len(state) == 2:\n            state, slotstate = state\n        if state:\n            try:\n                d = inst.__dict__\n                try:\n                    for k, v in state.iteritems():\n                        d[intern(k)] = v\n                # keys in state don't have to be strings\n                # don't blow up, but don't go out of our way\n                except TypeError:\n                    d.update(state)\n\n            except RuntimeError:\n                # XXX In restricted execution, the instance's __dict__\n                # is not accessible.  Use the old way of unpickling\n                # the instance variables.  This is a semantic\n                # difference when unpickling in restricted\n                # vs. unrestricted modes.\n                # Note, however, that cPickle has never tried to do the\n                # .update() business, and always uses\n                #     PyObject_SetItem(inst.__dict__, key, value) in a\n                # loop over state.items().\n                for k, v in state.items():\n                    setattr(inst, k, v)\n        if slotstate:\n            for k, v in slotstate.items():\n                setattr(inst, k, v)\n    dispatch[BUILD] = load_build\n\n    def load_mark(self):\n        self.append(self.mark)\n    dispatch[MARK] = load_mark\n\n#from pickle import decode_long\n\ndef decode_long(data):\n    r\"\"\"Decode a long from a two's complement little-endian binary string.\n\n    >>> decode_long('')\n    0L\n    >>> decode_long(\"\\xff\\x00\")\n    255L\n    >>> decode_long(\"\\xff\\x7f\")\n    32767L\n    >>> decode_long(\"\\x00\\xff\")\n    -256L\n    >>> decode_long(\"\\x00\\x80\")\n    -32768L\n    >>> decode_long(\"\\x80\")\n    -128L\n    >>> decode_long(\"\\x7f\")\n    127L\n    \"\"\"\n\n    nbytes = len(data)\n    if nbytes == 0:\n        return 0L\n    ind = nbytes - 1\n    while ind and ord(data[ind]) == 0:\n        ind -= 1\n    n = ord(data[ind])\n    while ind:\n        n <<= 8\n        ind -= 1\n        if ord(data[ind]):\n            n += ord(data[ind])\n    if ord(data[nbytes - 1]) >= 128:\n        n -= 1L << (nbytes << 3)\n    return n\n\ndef load(f):\n    return Unpickler(f).load()\n\ndef loads(str):\n    f = StringIO(str)\n    return Unpickler(f).load()\n", 
    "cStringIO": "#\n# StringIO-based cStringIO implementation.\n#\n\n# Note that PyPy also contains a built-in module 'cStringIO' which will hide\n# this one if compiled in.\n\nfrom StringIO import *\nfrom StringIO import __doc__\n\nclass StringIO(StringIO):\n    def reset(self):\n        \"\"\"\n        reset() -- Reset the file position to the beginning\n        \"\"\"\n        self.seek(0, 0)\n", 
    "cmd": "\"\"\"A generic class to build line-oriented command interpreters.\n\nInterpreters constructed with this class obey the following conventions:\n\n1. End of file on input is processed as the command 'EOF'.\n2. A command is parsed out of each line by collecting the prefix composed\n   of characters in the identchars member.\n3. A command `foo' is dispatched to a method 'do_foo()'; the do_ method\n   is passed a single argument consisting of the remainder of the line.\n4. Typing an empty line repeats the last command.  (Actually, it calls the\n   method `emptyline', which may be overridden in a subclass.)\n5. There is a predefined `help' method.  Given an argument `topic', it\n   calls the command `help_topic'.  With no arguments, it lists all topics\n   with defined help_ functions, broken into up to three topics; documented\n   commands, miscellaneous help topics, and undocumented commands.\n6. The command '?' is a synonym for `help'.  The command '!' is a synonym\n   for `shell', if a do_shell method exists.\n7. If completion is enabled, completing commands will be done automatically,\n   and completing of commands args is done by calling complete_foo() with\n   arguments text, line, begidx, endidx.  text is string we are matching\n   against, all returned matches must begin with it.  line is the current\n   input line (lstripped), begidx and endidx are the beginning and end\n   indexes of the text being matched, which could be used to provide\n   different completion depending upon which position the argument is in.\n\nThe `default' method may be overridden to intercept commands for which there\nis no do_ method.\n\nThe `completedefault' method may be overridden to intercept completions for\ncommands that have no complete_ method.\n\nThe data member `self.ruler' sets the character used to draw separator lines\nin the help messages.  If empty, no ruler line is drawn.  It defaults to \"=\".\n\nIf the value of `self.intro' is nonempty when the cmdloop method is called,\nit is printed out on interpreter startup.  This value may be overridden\nvia an optional argument to the cmdloop() method.\n\nThe data members `self.doc_header', `self.misc_header', and\n`self.undoc_header' set the headers used for the help function's\nlistings of documented functions, miscellaneous topics, and undocumented\nfunctions respectively.\n\nThese interpreters use raw_input; thus, if the readline module is loaded,\nthey automatically support Emacs-like command history and editing features.\n\"\"\"\n\nimport string\n\n__all__ = [\"Cmd\"]\n\nPROMPT = '(Cmd) '\nIDENTCHARS = string.ascii_letters + string.digits + '_'\n\nclass Cmd:\n    \"\"\"A simple framework for writing line-oriented command interpreters.\n\n    These are often useful for test harnesses, administrative tools, and\n    prototypes that will later be wrapped in a more sophisticated interface.\n\n    A Cmd instance or subclass instance is a line-oriented interpreter\n    framework.  There is no good reason to instantiate Cmd itself; rather,\n    it's useful as a superclass of an interpreter class you define yourself\n    in order to inherit Cmd's methods and encapsulate action methods.\n\n    \"\"\"\n    prompt = PROMPT\n    identchars = IDENTCHARS\n    ruler = '='\n    lastcmd = ''\n    intro = None\n    doc_leader = \"\"\n    doc_header = \"Documented commands (type help <topic>):\"\n    misc_header = \"Miscellaneous help topics:\"\n    undoc_header = \"Undocumented commands:\"\n    nohelp = \"*** No help on %s\"\n    use_rawinput = 1\n\n    def __init__(self, completekey='tab', stdin=None, stdout=None):\n        \"\"\"Instantiate a line-oriented interpreter framework.\n\n        The optional argument 'completekey' is the readline name of a\n        completion key; it defaults to the Tab key. If completekey is\n        not None and the readline module is available, command completion\n        is done automatically. The optional arguments stdin and stdout\n        specify alternate input and output file objects; if not specified,\n        sys.stdin and sys.stdout are used.\n\n        \"\"\"\n        import sys\n        if stdin is not None:\n            self.stdin = stdin\n        else:\n            self.stdin = sys.stdin\n        if stdout is not None:\n            self.stdout = stdout\n        else:\n            self.stdout = sys.stdout\n        self.cmdqueue = []\n        self.completekey = completekey\n\n    def cmdloop(self, intro=None):\n        \"\"\"Repeatedly issue a prompt, accept input, parse an initial prefix\n        off the received input, and dispatch to action methods, passing them\n        the remainder of the line as argument.\n\n        \"\"\"\n\n        self.preloop()\n        if self.use_rawinput and self.completekey:\n            try:\n                import readline\n                self.old_completer = readline.get_completer()\n                readline.set_completer(self.complete)\n                readline.parse_and_bind(self.completekey+\": complete\")\n            except ImportError:\n                pass\n        try:\n            if intro is not None:\n                self.intro = intro\n            if self.intro:\n                self.stdout.write(str(self.intro)+\"\\n\")\n            stop = None\n            while not stop:\n                if self.cmdqueue:\n                    line = self.cmdqueue.pop(0)\n                else:\n                    if self.use_rawinput:\n                        try:\n                            line = raw_input(self.prompt)\n                        except EOFError:\n                            line = 'EOF'\n                    else:\n                        self.stdout.write(self.prompt)\n                        self.stdout.flush()\n                        line = self.stdin.readline()\n                        if not len(line):\n                            line = 'EOF'\n                        else:\n                            line = line.rstrip('\\r\\n')\n                line = self.precmd(line)\n                stop = self.onecmd(line)\n                stop = self.postcmd(stop, line)\n            self.postloop()\n        finally:\n            if self.use_rawinput and self.completekey:\n                try:\n                    import readline\n                    readline.set_completer(self.old_completer)\n                except ImportError:\n                    pass\n\n\n    def precmd(self, line):\n        \"\"\"Hook method executed just before the command line is\n        interpreted, but after the input prompt is generated and issued.\n\n        \"\"\"\n        return line\n\n    def postcmd(self, stop, line):\n        \"\"\"Hook method executed just after a command dispatch is finished.\"\"\"\n        return stop\n\n    def preloop(self):\n        \"\"\"Hook method executed once when the cmdloop() method is called.\"\"\"\n        pass\n\n    def postloop(self):\n        \"\"\"Hook method executed once when the cmdloop() method is about to\n        return.\n\n        \"\"\"\n        pass\n\n    def parseline(self, line):\n        \"\"\"Parse the line into a command name and a string containing\n        the arguments.  Returns a tuple containing (command, args, line).\n        'command' and 'args' may be None if the line couldn't be parsed.\n        \"\"\"\n        line = line.strip()\n        if not line:\n            return None, None, line\n        elif line[0] == '?':\n            line = 'help ' + line[1:]\n        elif line[0] == '!':\n            if hasattr(self, 'do_shell'):\n                line = 'shell ' + line[1:]\n            else:\n                return None, None, line\n        i, n = 0, len(line)\n        while i < n and line[i] in self.identchars: i = i+1\n        cmd, arg = line[:i], line[i:].strip()\n        return cmd, arg, line\n\n    def onecmd(self, line):\n        \"\"\"Interpret the argument as though it had been typed in response\n        to the prompt.\n\n        This may be overridden, but should not normally need to be;\n        see the precmd() and postcmd() methods for useful execution hooks.\n        The return value is a flag indicating whether interpretation of\n        commands by the interpreter should stop.\n\n        \"\"\"\n        cmd, arg, line = self.parseline(line)\n        if not line:\n            return self.emptyline()\n        if cmd is None:\n            return self.default(line)\n        self.lastcmd = line\n        if line == 'EOF' :\n            self.lastcmd = ''\n        if cmd == '':\n            return self.default(line)\n        else:\n            try:\n                func = getattr(self, 'do_' + cmd)\n            except AttributeError:\n                return self.default(line)\n            return func(arg)\n\n    def emptyline(self):\n        \"\"\"Called when an empty line is entered in response to the prompt.\n\n        If this method is not overridden, it repeats the last nonempty\n        command entered.\n\n        \"\"\"\n        if self.lastcmd:\n            return self.onecmd(self.lastcmd)\n\n    def default(self, line):\n        \"\"\"Called on an input line when the command prefix is not recognized.\n\n        If this method is not overridden, it prints an error message and\n        returns.\n\n        \"\"\"\n        self.stdout.write('*** Unknown syntax: %s\\n'%line)\n\n    def completedefault(self, *ignored):\n        \"\"\"Method called to complete an input line when no command-specific\n        complete_*() method is available.\n\n        By default, it returns an empty list.\n\n        \"\"\"\n        return []\n\n    def completenames(self, text, *ignored):\n        dotext = 'do_'+text\n        return [a[3:] for a in self.get_names() if a.startswith(dotext)]\n\n    def complete(self, text, state):\n        \"\"\"Return the next possible completion for 'text'.\n\n        If a command has not been entered, then complete against command list.\n        Otherwise try to call complete_<command> to get list of completions.\n        \"\"\"\n        if state == 0:\n            import readline\n            origline = readline.get_line_buffer()\n            line = origline.lstrip()\n            stripped = len(origline) - len(line)\n            begidx = readline.get_begidx() - stripped\n            endidx = readline.get_endidx() - stripped\n            if begidx>0:\n                cmd, args, foo = self.parseline(line)\n                if cmd == '':\n                    compfunc = self.completedefault\n                else:\n                    try:\n                        compfunc = getattr(self, 'complete_' + cmd)\n                    except AttributeError:\n                        compfunc = self.completedefault\n            else:\n                compfunc = self.completenames\n            self.completion_matches = compfunc(text, line, begidx, endidx)\n        try:\n            return self.completion_matches[state]\n        except IndexError:\n            return None\n\n    def get_names(self):\n        # This method used to pull in base class attributes\n        # at a time dir() didn't do it yet.\n        return dir(self.__class__)\n\n    def complete_help(self, *args):\n        commands = set(self.completenames(*args))\n        topics = set(a[5:] for a in self.get_names()\n                     if a.startswith('help_' + args[0]))\n        return list(commands | topics)\n\n    def do_help(self, arg):\n        'List available commands with \"help\" or detailed help with \"help cmd\".'\n        if arg:\n            # XXX check arg syntax\n            try:\n                func = getattr(self, 'help_' + arg)\n            except AttributeError:\n                try:\n                    doc=getattr(self, 'do_' + arg).__doc__\n                    if doc:\n                        self.stdout.write(\"%s\\n\"%str(doc))\n                        return\n                except AttributeError:\n                    pass\n                self.stdout.write(\"%s\\n\"%str(self.nohelp % (arg,)))\n                return\n            func()\n        else:\n            names = self.get_names()\n            cmds_doc = []\n            cmds_undoc = []\n            help = {}\n            for name in names:\n                if name[:5] == 'help_':\n                    help[name[5:]]=1\n            names.sort()\n            # There can be duplicates if routines overridden\n            prevname = ''\n            for name in names:\n                if name[:3] == 'do_':\n                    if name == prevname:\n                        continue\n                    prevname = name\n                    cmd=name[3:]\n                    if cmd in help:\n                        cmds_doc.append(cmd)\n                        del help[cmd]\n                    elif getattr(self, name).__doc__:\n                        cmds_doc.append(cmd)\n                    else:\n                        cmds_undoc.append(cmd)\n            self.stdout.write(\"%s\\n\"%str(self.doc_leader))\n            self.print_topics(self.doc_header,   cmds_doc,   15,80)\n            self.print_topics(self.misc_header,  help.keys(),15,80)\n            self.print_topics(self.undoc_header, cmds_undoc, 15,80)\n\n    def print_topics(self, header, cmds, cmdlen, maxcol):\n        if cmds:\n            self.stdout.write(\"%s\\n\"%str(header))\n            if self.ruler:\n                self.stdout.write(\"%s\\n\"%str(self.ruler * len(header)))\n            self.columnize(cmds, maxcol-1)\n            self.stdout.write(\"\\n\")\n\n    def columnize(self, list, displaywidth=80):\n        \"\"\"Display a list of strings as a compact set of columns.\n\n        Each column is only as wide as necessary.\n        Columns are separated by two spaces (one was not legible enough).\n        \"\"\"\n        if not list:\n            self.stdout.write(\"<empty>\\n\")\n            return\n        nonstrings = [i for i in range(len(list))\n                        if not isinstance(list[i], str)]\n        if nonstrings:\n            raise TypeError, (\"list[i] not a string for i in %s\" %\n                              \", \".join(map(str, nonstrings)))\n        size = len(list)\n        if size == 1:\n            self.stdout.write('%s\\n'%str(list[0]))\n            return\n        # Try every row count from 1 upwards\n        for nrows in range(1, len(list)):\n            ncols = (size+nrows-1) // nrows\n            colwidths = []\n            totwidth = -2\n            for col in range(ncols):\n                colwidth = 0\n                for row in range(nrows):\n                    i = row + nrows*col\n                    if i >= size:\n                        break\n                    x = list[i]\n                    colwidth = max(colwidth, len(x))\n                colwidths.append(colwidth)\n                totwidth += colwidth + 2\n                if totwidth > displaywidth:\n                    break\n            if totwidth <= displaywidth:\n                break\n        else:\n            nrows = len(list)\n            ncols = 1\n            colwidths = [0]\n        for row in range(nrows):\n            texts = []\n            for col in range(ncols):\n                i = row + nrows*col\n                if i >= size:\n                    x = \"\"\n                else:\n                    x = list[i]\n                texts.append(x)\n            while texts and not texts[-1]:\n                del texts[-1]\n            for col in range(len(texts)):\n                texts[col] = texts[col].ljust(colwidths[col])\n            self.stdout.write(\"%s\\n\"%str(\"  \".join(texts)))\n", 
    "code": "\"\"\"Utilities needed to emulate Python's interactive interpreter.\n\n\"\"\"\n\n# Inspired by similar code by Jeff Epler and Fredrik Lundh.\n\n\nimport sys\nimport traceback\nfrom codeop import CommandCompiler, compile_command\n\n__all__ = [\"InteractiveInterpreter\", \"InteractiveConsole\", \"interact\",\n           \"compile_command\"]\n\ndef softspace(file, newvalue):\n    oldvalue = 0\n    try:\n        oldvalue = file.softspace\n    except AttributeError:\n        pass\n    try:\n        file.softspace = newvalue\n    except (AttributeError, TypeError):\n        # \"attribute-less object\" or \"read-only attributes\"\n        pass\n    return oldvalue\n\nclass InteractiveInterpreter:\n    \"\"\"Base class for InteractiveConsole.\n\n    This class deals with parsing and interpreter state (the user's\n    namespace); it doesn't deal with input buffering or prompting or\n    input file naming (the filename is always passed in explicitly).\n\n    \"\"\"\n\n    def __init__(self, locals=None):\n        \"\"\"Constructor.\n\n        The optional 'locals' argument specifies the dictionary in\n        which code will be executed; it defaults to a newly created\n        dictionary with key \"__name__\" set to \"__console__\" and key\n        \"__doc__\" set to None.\n\n        \"\"\"\n        if locals is None:\n            locals = {\"__name__\": \"__console__\", \"__doc__\": None}\n        self.locals = locals\n        self.compile = CommandCompiler()\n\n    def runsource(self, source, filename=\"<input>\", symbol=\"single\"):\n        \"\"\"Compile and run some source in the interpreter.\n\n        Arguments are as for compile_command().\n\n        One several things can happen:\n\n        1) The input is incorrect; compile_command() raised an\n        exception (SyntaxError or OverflowError).  A syntax traceback\n        will be printed by calling the showsyntaxerror() method.\n\n        2) The input is incomplete, and more input is required;\n        compile_command() returned None.  Nothing happens.\n\n        3) The input is complete; compile_command() returned a code\n        object.  The code is executed by calling self.runcode() (which\n        also handles run-time exceptions, except for SystemExit).\n\n        The return value is True in case 2, False in the other cases (unless\n        an exception is raised).  The return value can be used to\n        decide whether to use sys.ps1 or sys.ps2 to prompt the next\n        line.\n\n        \"\"\"\n        try:\n            code = self.compile(source, filename, symbol)\n        except (OverflowError, SyntaxError, ValueError):\n            # Case 1\n            self.showsyntaxerror(filename)\n            return False\n\n        if code is None:\n            # Case 2\n            return True\n\n        # Case 3\n        self.runcode(code)\n        return False\n\n    def runcode(self, code):\n        \"\"\"Execute a code object.\n\n        When an exception occurs, self.showtraceback() is called to\n        display a traceback.  All exceptions are caught except\n        SystemExit, which is reraised.\n\n        A note about KeyboardInterrupt: this exception may occur\n        elsewhere in this code, and may not always be caught.  The\n        caller should be prepared to deal with it.\n\n        \"\"\"\n        try:\n            exec code in self.locals\n        except SystemExit:\n            raise\n        except:\n            self.showtraceback()\n        else:\n            if softspace(sys.stdout, 0):\n                print\n\n    def showsyntaxerror(self, filename=None):\n        \"\"\"Display the syntax error that just occurred.\n\n        This doesn't display a stack trace because there isn't one.\n\n        If a filename is given, it is stuffed in the exception instead\n        of what was there before (because Python's parser always uses\n        \"<string>\" when reading from a string).\n\n        The output is written by self.write(), below.\n\n        \"\"\"\n        type, value, sys.last_traceback = sys.exc_info()\n        sys.last_type = type\n        sys.last_value = value\n        if filename and type is SyntaxError:\n            # Work hard to stuff the correct filename in the exception\n            try:\n                msg, (dummy_filename, lineno, offset, line) = value\n            except:\n                # Not the format we expect; leave it alone\n                pass\n            else:\n                # Stuff in the right filename\n                value = SyntaxError(msg, (filename, lineno, offset, line))\n                sys.last_value = value\n        list = traceback.format_exception_only(type, value)\n        map(self.write, list)\n\n    def showtraceback(self):\n        \"\"\"Display the exception that just occurred.\n\n        We remove the first stack item because it is our own code.\n\n        The output is written by self.write(), below.\n\n        \"\"\"\n        try:\n            type, value, tb = sys.exc_info()\n            sys.last_type = type\n            sys.last_value = value\n            sys.last_traceback = tb\n            tblist = traceback.extract_tb(tb)\n            del tblist[:1]\n            list = traceback.format_list(tblist)\n            if list:\n                list.insert(0, \"Traceback (most recent call last):\\n\")\n            list[len(list):] = traceback.format_exception_only(type, value)\n        finally:\n            tblist = tb = None\n        map(self.write, list)\n\n    def write(self, data):\n        \"\"\"Write a string.\n\n        The base implementation writes to sys.stderr; a subclass may\n        replace this with a different implementation.\n\n        \"\"\"\n        sys.stderr.write(data)\n\n\nclass InteractiveConsole(InteractiveInterpreter):\n    \"\"\"Closely emulate the behavior of the interactive Python interpreter.\n\n    This class builds on InteractiveInterpreter and adds prompting\n    using the familiar sys.ps1 and sys.ps2, and input buffering.\n\n    \"\"\"\n\n    def __init__(self, locals=None, filename=\"<console>\"):\n        \"\"\"Constructor.\n\n        The optional locals argument will be passed to the\n        InteractiveInterpreter base class.\n\n        The optional filename argument should specify the (file)name\n        of the input stream; it will show up in tracebacks.\n\n        \"\"\"\n        InteractiveInterpreter.__init__(self, locals)\n        self.filename = filename\n        self.resetbuffer()\n\n    def resetbuffer(self):\n        \"\"\"Reset the input buffer.\"\"\"\n        self.buffer = []\n\n    def interact(self, banner=None):\n        \"\"\"Closely emulate the interactive Python console.\n\n        The optional banner argument specify the banner to print\n        before the first interaction; by default it prints a banner\n        similar to the one printed by the real Python interpreter,\n        followed by the current class name in parentheses (so as not\n        to confuse this with the real interpreter -- since it's so\n        close!).\n\n        \"\"\"\n        try:\n            sys.ps1\n        except AttributeError:\n            sys.ps1 = \">>> \"\n        try:\n            sys.ps2\n        except AttributeError:\n            sys.ps2 = \"... \"\n        cprt = 'Type \"help\", \"copyright\", \"credits\" or \"license\" for more information.'\n        if banner is None:\n            self.write(\"Python %s on %s\\n%s\\n(%s)\\n\" %\n                       (sys.version, sys.platform, cprt,\n                        self.__class__.__name__))\n        else:\n            self.write(\"%s\\n\" % str(banner))\n        more = 0\n        while 1:\n            try:\n                if more:\n                    prompt = sys.ps2\n                else:\n                    prompt = sys.ps1\n                try:\n                    line = self.raw_input(prompt)\n                    # Can be None if sys.stdin was redefined\n                    encoding = getattr(sys.stdin, \"encoding\", None)\n                    if encoding and not isinstance(line, unicode):\n                        line = line.decode(encoding)\n                except EOFError:\n                    self.write(\"\\n\")\n                    break\n                else:\n                    more = self.push(line)\n            except KeyboardInterrupt:\n                self.write(\"\\nKeyboardInterrupt\\n\")\n                self.resetbuffer()\n                more = 0\n\n    def push(self, line):\n        \"\"\"Push a line to the interpreter.\n\n        The line should not have a trailing newline; it may have\n        internal newlines.  The line is appended to a buffer and the\n        interpreter's runsource() method is called with the\n        concatenated contents of the buffer as source.  If this\n        indicates that the command was executed or invalid, the buffer\n        is reset; otherwise, the command is incomplete, and the buffer\n        is left as it was after the line was appended.  The return\n        value is 1 if more input is required, 0 if the line was dealt\n        with in some way (this is the same as runsource()).\n\n        \"\"\"\n        self.buffer.append(line)\n        source = \"\\n\".join(self.buffer)\n        more = self.runsource(source, self.filename)\n        if not more:\n            self.resetbuffer()\n        return more\n\n    def raw_input(self, prompt=\"\"):\n        \"\"\"Write a prompt and read a line.\n\n        The returned line does not include the trailing newline.\n        When the user enters the EOF key sequence, EOFError is raised.\n\n        The base implementation uses the built-in function\n        raw_input(); a subclass may replace this with a different\n        implementation.\n\n        \"\"\"\n        return raw_input(prompt)\n\n\ndef interact(banner=None, readfunc=None, local=None):\n    \"\"\"Closely emulate the interactive Python interpreter.\n\n    This is a backwards compatible interface to the InteractiveConsole\n    class.  When readfunc is not specified, it attempts to import the\n    readline module to enable GNU readline if it is available.\n\n    Arguments (all optional, all default to None):\n\n    banner -- passed to InteractiveConsole.interact()\n    readfunc -- if not None, replaces InteractiveConsole.raw_input()\n    local -- passed to InteractiveInterpreter.__init__()\n\n    \"\"\"\n    console = InteractiveConsole(local)\n    if readfunc is not None:\n        console.raw_input = readfunc\n    else:\n        try:\n            import readline\n        except ImportError:\n            pass\n    console.interact(banner)\n\n\nif __name__ == \"__main__\":\n    interact()\n", 
    "codecs": "\"\"\" codecs -- Python Codec Registry, API and helpers.\n\n\nWritten by Marc-Andre Lemburg (mal@lemburg.com).\n\n(c) Copyright CNRI, All Rights Reserved. NO WARRANTY.\n\n\"\"\"#\"\n\nimport __builtin__, sys\n\n### Registry and builtin stateless codec functions\n\ntry:\n    from _codecs import *\nexcept ImportError, why:\n    raise SystemError('Failed to load the builtin codecs: %s' % why)\n\n__all__ = [\"register\", \"lookup\", \"open\", \"EncodedFile\", \"BOM\", \"BOM_BE\",\n           \"BOM_LE\", \"BOM32_BE\", \"BOM32_LE\", \"BOM64_BE\", \"BOM64_LE\",\n           \"BOM_UTF8\", \"BOM_UTF16\", \"BOM_UTF16_LE\", \"BOM_UTF16_BE\",\n           \"BOM_UTF32\", \"BOM_UTF32_LE\", \"BOM_UTF32_BE\",\n           \"strict_errors\", \"ignore_errors\", \"replace_errors\",\n           \"xmlcharrefreplace_errors\",\n           \"register_error\", \"lookup_error\"]\n\n### Constants\n\n#\n# Byte Order Mark (BOM = ZERO WIDTH NO-BREAK SPACE = U+FEFF)\n# and its possible byte string values\n# for UTF8/UTF16/UTF32 output and little/big endian machines\n#\n\n# UTF-8\nBOM_UTF8 = '\\xef\\xbb\\xbf'\n\n# UTF-16, little endian\nBOM_LE = BOM_UTF16_LE = '\\xff\\xfe'\n\n# UTF-16, big endian\nBOM_BE = BOM_UTF16_BE = '\\xfe\\xff'\n\n# UTF-32, little endian\nBOM_UTF32_LE = '\\xff\\xfe\\x00\\x00'\n\n# UTF-32, big endian\nBOM_UTF32_BE = '\\x00\\x00\\xfe\\xff'\n\nif sys.byteorder == 'little':\n\n    # UTF-16, native endianness\n    BOM = BOM_UTF16 = BOM_UTF16_LE\n\n    # UTF-32, native endianness\n    BOM_UTF32 = BOM_UTF32_LE\n\nelse:\n\n    # UTF-16, native endianness\n    BOM = BOM_UTF16 = BOM_UTF16_BE\n\n    # UTF-32, native endianness\n    BOM_UTF32 = BOM_UTF32_BE\n\n# Old broken names (don't use in new code)\nBOM32_LE = BOM_UTF16_LE\nBOM32_BE = BOM_UTF16_BE\nBOM64_LE = BOM_UTF32_LE\nBOM64_BE = BOM_UTF32_BE\n\n\n### Codec base classes (defining the API)\n\nclass CodecInfo(tuple):\n\n    def __new__(cls, encode, decode, streamreader=None, streamwriter=None,\n        incrementalencoder=None, incrementaldecoder=None, name=None):\n        self = tuple.__new__(cls, (encode, decode, streamreader, streamwriter))\n        self.name = name\n        self.encode = encode\n        self.decode = decode\n        self.incrementalencoder = incrementalencoder\n        self.incrementaldecoder = incrementaldecoder\n        self.streamwriter = streamwriter\n        self.streamreader = streamreader\n        return self\n\n    def __repr__(self):\n        return \"<%s.%s object for encoding %s at 0x%x>\" % (self.__class__.__module__, self.__class__.__name__, self.name, id(self))\n\nclass Codec:\n\n    \"\"\" Defines the interface for stateless encoders/decoders.\n\n        The .encode()/.decode() methods may use different error\n        handling schemes by providing the errors argument. These\n        string values are predefined:\n\n         'strict' - raise a ValueError error (or a subclass)\n         'ignore' - ignore the character and continue with the next\n         'replace' - replace with a suitable replacement character;\n                    Python will use the official U+FFFD REPLACEMENT\n                    CHARACTER for the builtin Unicode codecs on\n                    decoding and '?' on encoding.\n         'xmlcharrefreplace' - Replace with the appropriate XML\n                               character reference (only for encoding).\n         'backslashreplace'  - Replace with backslashed escape sequences\n                               (only for encoding).\n\n        The set of allowed values can be extended via register_error.\n\n    \"\"\"\n    def encode(self, input, errors='strict'):\n\n        \"\"\" Encodes the object input and returns a tuple (output\n            object, length consumed).\n\n            errors defines the error handling to apply. It defaults to\n            'strict' handling.\n\n            The method may not store state in the Codec instance. Use\n            StreamCodec for codecs which have to keep state in order to\n            make encoding/decoding efficient.\n\n            The encoder must be able to handle zero length input and\n            return an empty object of the output object type in this\n            situation.\n\n        \"\"\"\n        raise NotImplementedError\n\n    def decode(self, input, errors='strict'):\n\n        \"\"\" Decodes the object input and returns a tuple (output\n            object, length consumed).\n\n            input must be an object which provides the bf_getreadbuf\n            buffer slot. Python strings, buffer objects and memory\n            mapped files are examples of objects providing this slot.\n\n            errors defines the error handling to apply. It defaults to\n            'strict' handling.\n\n            The method may not store state in the Codec instance. Use\n            StreamCodec for codecs which have to keep state in order to\n            make encoding/decoding efficient.\n\n            The decoder must be able to handle zero length input and\n            return an empty object of the output object type in this\n            situation.\n\n        \"\"\"\n        raise NotImplementedError\n\nclass IncrementalEncoder(object):\n    \"\"\"\n    An IncrementalEncoder encodes an input in multiple steps. The input can be\n    passed piece by piece to the encode() method. The IncrementalEncoder remembers\n    the state of the Encoding process between calls to encode().\n    \"\"\"\n    def __init__(self, errors='strict'):\n        \"\"\"\n        Creates an IncrementalEncoder instance.\n\n        The IncrementalEncoder may use different error handling schemes by\n        providing the errors keyword argument. See the module docstring\n        for a list of possible values.\n        \"\"\"\n        self.errors = errors\n        self.buffer = \"\"\n\n    def encode(self, input, final=False):\n        \"\"\"\n        Encodes input and returns the resulting object.\n        \"\"\"\n        raise NotImplementedError\n\n    def reset(self):\n        \"\"\"\n        Resets the encoder to the initial state.\n        \"\"\"\n\n    def getstate(self):\n        \"\"\"\n        Return the current state of the encoder.\n        \"\"\"\n        return 0\n\n    def setstate(self, state):\n        \"\"\"\n        Set the current state of the encoder. state must have been\n        returned by getstate().\n        \"\"\"\n\nclass BufferedIncrementalEncoder(IncrementalEncoder):\n    \"\"\"\n    This subclass of IncrementalEncoder can be used as the baseclass for an\n    incremental encoder if the encoder must keep some of the output in a\n    buffer between calls to encode().\n    \"\"\"\n    def __init__(self, errors='strict'):\n        IncrementalEncoder.__init__(self, errors)\n        self.buffer = \"\" # unencoded input that is kept between calls to encode()\n\n    def _buffer_encode(self, input, errors, final):\n        # Overwrite this method in subclasses: It must encode input\n        # and return an (output, length consumed) tuple\n        raise NotImplementedError\n\n    def encode(self, input, final=False):\n        # encode input (taking the buffer into account)\n        data = self.buffer + input\n        (result, consumed) = self._buffer_encode(data, self.errors, final)\n        # keep unencoded input until the next call\n        self.buffer = data[consumed:]\n        return result\n\n    def reset(self):\n        IncrementalEncoder.reset(self)\n        self.buffer = \"\"\n\n    def getstate(self):\n        return self.buffer or 0\n\n    def setstate(self, state):\n        self.buffer = state or \"\"\n\nclass IncrementalDecoder(object):\n    \"\"\"\n    An IncrementalDecoder decodes an input in multiple steps. The input can be\n    passed piece by piece to the decode() method. The IncrementalDecoder\n    remembers the state of the decoding process between calls to decode().\n    \"\"\"\n    def __init__(self, errors='strict'):\n        \"\"\"\n        Creates a IncrementalDecoder instance.\n\n        The IncrementalDecoder may use different error handling schemes by\n        providing the errors keyword argument. See the module docstring\n        for a list of possible values.\n        \"\"\"\n        self.errors = errors\n\n    def decode(self, input, final=False):\n        \"\"\"\n        Decodes input and returns the resulting object.\n        \"\"\"\n        raise NotImplementedError\n\n    def reset(self):\n        \"\"\"\n        Resets the decoder to the initial state.\n        \"\"\"\n\n    def getstate(self):\n        \"\"\"\n        Return the current state of the decoder.\n\n        This must be a (buffered_input, additional_state_info) tuple.\n        buffered_input must be a bytes object containing bytes that\n        were passed to decode() that have not yet been converted.\n        additional_state_info must be a non-negative integer\n        representing the state of the decoder WITHOUT yet having\n        processed the contents of buffered_input.  In the initial state\n        and after reset(), getstate() must return (b\"\", 0).\n        \"\"\"\n        return (b\"\", 0)\n\n    def setstate(self, state):\n        \"\"\"\n        Set the current state of the decoder.\n\n        state must have been returned by getstate().  The effect of\n        setstate((b\"\", 0)) must be equivalent to reset().\n        \"\"\"\n\nclass BufferedIncrementalDecoder(IncrementalDecoder):\n    \"\"\"\n    This subclass of IncrementalDecoder can be used as the baseclass for an\n    incremental decoder if the decoder must be able to handle incomplete byte\n    sequences.\n    \"\"\"\n    def __init__(self, errors='strict'):\n        IncrementalDecoder.__init__(self, errors)\n        self.buffer = \"\" # undecoded input that is kept between calls to decode()\n\n    def _buffer_decode(self, input, errors, final):\n        # Overwrite this method in subclasses: It must decode input\n        # and return an (output, length consumed) tuple\n        raise NotImplementedError\n\n    def decode(self, input, final=False):\n        # decode input (taking the buffer into account)\n        data = self.buffer + input\n        (result, consumed) = self._buffer_decode(data, self.errors, final)\n        # keep undecoded input until the next call\n        self.buffer = data[consumed:]\n        return result\n\n    def reset(self):\n        IncrementalDecoder.reset(self)\n        self.buffer = \"\"\n\n    def getstate(self):\n        # additional state info is always 0\n        return (self.buffer, 0)\n\n    def setstate(self, state):\n        # ignore additional state info\n        self.buffer = state[0]\n\n#\n# The StreamWriter and StreamReader class provide generic working\n# interfaces which can be used to implement new encoding submodules\n# very easily. See encodings/utf_8.py for an example on how this is\n# done.\n#\n\nclass StreamWriter(Codec):\n\n    def __init__(self, stream, errors='strict'):\n\n        \"\"\" Creates a StreamWriter instance.\n\n            stream must be a file-like object open for writing\n            (binary) data.\n\n            The StreamWriter may use different error handling\n            schemes by providing the errors keyword argument. These\n            parameters are predefined:\n\n             'strict' - raise a ValueError (or a subclass)\n             'ignore' - ignore the character and continue with the next\n             'replace'- replace with a suitable replacement character\n             'xmlcharrefreplace' - Replace with the appropriate XML\n                                   character reference.\n             'backslashreplace'  - Replace with backslashed escape\n                                   sequences (only for encoding).\n\n            The set of allowed parameter values can be extended via\n            register_error.\n        \"\"\"\n        self.stream = stream\n        self.errors = errors\n\n    def write(self, object):\n\n        \"\"\" Writes the object's contents encoded to self.stream.\n        \"\"\"\n        data, consumed = self.encode(object, self.errors)\n        self.stream.write(data)\n\n    def writelines(self, list):\n\n        \"\"\" Writes the concatenated list of strings to the stream\n            using .write().\n        \"\"\"\n        self.write(''.join(list))\n\n    def reset(self):\n\n        \"\"\" Flushes and resets the codec buffers used for keeping state.\n\n            Calling this method should ensure that the data on the\n            output is put into a clean state, that allows appending\n            of new fresh data without having to rescan the whole\n            stream to recover state.\n\n        \"\"\"\n        pass\n\n    def seek(self, offset, whence=0):\n        self.stream.seek(offset, whence)\n        if whence == 0 and offset == 0:\n            self.reset()\n\n    def __getattr__(self, name,\n                    getattr=getattr):\n\n        \"\"\" Inherit all other methods from the underlying stream.\n        \"\"\"\n        return getattr(self.stream, name)\n\n    def __enter__(self):\n        return self\n\n    def __exit__(self, type, value, tb):\n        self.stream.close()\n\n###\n\nclass StreamReader(Codec):\n\n    def __init__(self, stream, errors='strict'):\n\n        \"\"\" Creates a StreamReader instance.\n\n            stream must be a file-like object open for reading\n            (binary) data.\n\n            The StreamReader may use different error handling\n            schemes by providing the errors keyword argument. These\n            parameters are predefined:\n\n             'strict' - raise a ValueError (or a subclass)\n             'ignore' - ignore the character and continue with the next\n             'replace'- replace with a suitable replacement character;\n\n            The set of allowed parameter values can be extended via\n            register_error.\n        \"\"\"\n        self.stream = stream\n        self.errors = errors\n        self.bytebuffer = \"\"\n        # For str->str decoding this will stay a str\n        # For str->unicode decoding the first read will promote it to unicode\n        self.charbuffer = \"\"\n        self.linebuffer = None\n\n    def decode(self, input, errors='strict'):\n        raise NotImplementedError\n\n    def read(self, size=-1, chars=-1, firstline=False):\n\n        \"\"\" Decodes data from the stream self.stream and returns the\n            resulting object.\n\n            chars indicates the number of characters to read from the\n            stream. read() will never return more than chars\n            characters, but it might return less, if there are not enough\n            characters available.\n\n            size indicates the approximate maximum number of bytes to\n            read from the stream for decoding purposes. The decoder\n            can modify this setting as appropriate. The default value\n            -1 indicates to read and decode as much as possible.  size\n            is intended to prevent having to decode huge files in one\n            step.\n\n            If firstline is true, and a UnicodeDecodeError happens\n            after the first line terminator in the input only the first line\n            will be returned, the rest of the input will be kept until the\n            next call to read().\n\n            The method should use a greedy read strategy meaning that\n            it should read as much data as is allowed within the\n            definition of the encoding and the given size, e.g.  if\n            optional encoding endings or state markers are available\n            on the stream, these should be read too.\n        \"\"\"\n        # If we have lines cached, first merge them back into characters\n        if self.linebuffer:\n            self.charbuffer = \"\".join(self.linebuffer)\n            self.linebuffer = None\n\n        # read until we get the required number of characters (if available)\n        while True:\n            # can the request be satisfied from the character buffer?\n            if chars >= 0:\n                if len(self.charbuffer) >= chars:\n                    break\n            elif size >= 0:\n                if len(self.charbuffer) >= size:\n                    break\n            # we need more data\n            if size < 0:\n                newdata = self.stream.read()\n            else:\n                newdata = self.stream.read(size)\n            # decode bytes (those remaining from the last call included)\n            data = self.bytebuffer + newdata\n            try:\n                newchars, decodedbytes = self.decode(data, self.errors)\n            except UnicodeDecodeError, exc:\n                if firstline:\n                    newchars, decodedbytes = self.decode(data[:exc.start], self.errors)\n                    lines = newchars.splitlines(True)\n                    if len(lines)<=1:\n                        raise\n                else:\n                    raise\n            # keep undecoded bytes until the next call\n            self.bytebuffer = data[decodedbytes:]\n            # put new characters in the character buffer\n            self.charbuffer += newchars\n            # there was no data available\n            if not newdata:\n                break\n        if chars < 0:\n            # Return everything we've got\n            result = self.charbuffer\n            self.charbuffer = \"\"\n        else:\n            # Return the first chars characters\n            result = self.charbuffer[:chars]\n            self.charbuffer = self.charbuffer[chars:]\n        return result\n\n    def readline(self, size=None, keepends=True):\n\n        \"\"\" Read one line from the input stream and return the\n            decoded data.\n\n            size, if given, is passed as size argument to the\n            read() method.\n\n        \"\"\"\n        # If we have lines cached from an earlier read, return\n        # them unconditionally\n        if self.linebuffer:\n            line = self.linebuffer[0]\n            del self.linebuffer[0]\n            if len(self.linebuffer) == 1:\n                # revert to charbuffer mode; we might need more data\n                # next time\n                self.charbuffer = self.linebuffer[0]\n                self.linebuffer = None\n            if not keepends:\n                line = line.splitlines(False)[0]\n            return line\n\n        readsize = size or 72\n        line = \"\"\n        # If size is given, we call read() only once\n        while True:\n            data = self.read(readsize, firstline=True)\n            if data:\n                # If we're at a \"\\r\" read one extra character (which might\n                # be a \"\\n\") to get a proper line ending. If the stream is\n                # temporarily exhausted we return the wrong line ending.\n                if data.endswith(\"\\r\"):\n                    data += self.read(size=1, chars=1)\n\n            line += data\n            lines = line.splitlines(True)\n            if lines:\n                if len(lines) > 1:\n                    # More than one line result; the first line is a full line\n                    # to return\n                    line = lines[0]\n                    del lines[0]\n                    if len(lines) > 1:\n                        # cache the remaining lines\n                        lines[-1] += self.charbuffer\n                        self.linebuffer = lines\n                        self.charbuffer = None\n                    else:\n                        # only one remaining line, put it back into charbuffer\n                        self.charbuffer = lines[0] + self.charbuffer\n                    if not keepends:\n                        line = line.splitlines(False)[0]\n                    break\n                line0withend = lines[0]\n                line0withoutend = lines[0].splitlines(False)[0]\n                if line0withend != line0withoutend: # We really have a line end\n                    # Put the rest back together and keep it until the next call\n                    self.charbuffer = \"\".join(lines[1:]) + self.charbuffer\n                    if keepends:\n                        line = line0withend\n                    else:\n                        line = line0withoutend\n                    break\n            # we didn't get anything or this was our only try\n            if not data or size is not None:\n                if line and not keepends:\n                    line = line.splitlines(False)[0]\n                break\n            if readsize<8000:\n                readsize *= 2\n        return line\n\n    def readlines(self, sizehint=None, keepends=True):\n\n        \"\"\" Read all lines available on the input stream\n            and return them as list of lines.\n\n            Line breaks are implemented using the codec's decoder\n            method and are included in the list entries.\n\n            sizehint, if given, is ignored since there is no efficient\n            way to finding the true end-of-line.\n\n        \"\"\"\n        data = self.read()\n        return data.splitlines(keepends)\n\n    def reset(self):\n\n        \"\"\" Resets the codec buffers used for keeping state.\n\n            Note that no stream repositioning should take place.\n            This method is primarily intended to be able to recover\n            from decoding errors.\n\n        \"\"\"\n        self.bytebuffer = \"\"\n        self.charbuffer = u\"\"\n        self.linebuffer = None\n\n    def seek(self, offset, whence=0):\n        \"\"\" Set the input stream's current position.\n\n            Resets the codec buffers used for keeping state.\n        \"\"\"\n        self.stream.seek(offset, whence)\n        self.reset()\n\n    def next(self):\n\n        \"\"\" Return the next decoded line from the input stream.\"\"\"\n        line = self.readline()\n        if line:\n            return line\n        raise StopIteration\n\n    def __iter__(self):\n        return self\n\n    def __getattr__(self, name,\n                    getattr=getattr):\n\n        \"\"\" Inherit all other methods from the underlying stream.\n        \"\"\"\n        return getattr(self.stream, name)\n\n    def __enter__(self):\n        return self\n\n    def __exit__(self, type, value, tb):\n        self.stream.close()\n\n###\n\nclass StreamReaderWriter:\n\n    \"\"\" StreamReaderWriter instances allow wrapping streams which\n        work in both read and write modes.\n\n        The design is such that one can use the factory functions\n        returned by the codec.lookup() function to construct the\n        instance.\n\n    \"\"\"\n    # Optional attributes set by the file wrappers below\n    encoding = 'unknown'\n\n    def __init__(self, stream, Reader, Writer, errors='strict'):\n\n        \"\"\" Creates a StreamReaderWriter instance.\n\n            stream must be a Stream-like object.\n\n            Reader, Writer must be factory functions or classes\n            providing the StreamReader, StreamWriter interface resp.\n\n            Error handling is done in the same way as defined for the\n            StreamWriter/Readers.\n\n        \"\"\"\n        self.stream = stream\n        self.reader = Reader(stream, errors)\n        self.writer = Writer(stream, errors)\n        self.errors = errors\n\n    def read(self, size=-1):\n\n        return self.reader.read(size)\n\n    def readline(self, size=None):\n\n        return self.reader.readline(size)\n\n    def readlines(self, sizehint=None):\n\n        return self.reader.readlines(sizehint)\n\n    def next(self):\n\n        \"\"\" Return the next decoded line from the input stream.\"\"\"\n        return self.reader.next()\n\n    def __iter__(self):\n        return self\n\n    def write(self, data):\n\n        return self.writer.write(data)\n\n    def writelines(self, list):\n\n        return self.writer.writelines(list)\n\n    def reset(self):\n\n        self.reader.reset()\n        self.writer.reset()\n\n    def seek(self, offset, whence=0):\n        self.stream.seek(offset, whence)\n        self.reader.reset()\n        if whence == 0 and offset == 0:\n            self.writer.reset()\n\n    def __getattr__(self, name,\n                    getattr=getattr):\n\n        \"\"\" Inherit all other methods from the underlying stream.\n        \"\"\"\n        return getattr(self.stream, name)\n\n    # these are needed to make \"with codecs.open(...)\" work properly\n\n    def __enter__(self):\n        return self\n\n    def __exit__(self, type, value, tb):\n        self.stream.close()\n\n###\n\nclass StreamRecoder:\n\n    \"\"\" StreamRecoder instances provide a frontend - backend\n        view of encoding data.\n\n        They use the complete set of APIs returned by the\n        codecs.lookup() function to implement their task.\n\n        Data written to the stream is first decoded into an\n        intermediate format (which is dependent on the given codec\n        combination) and then written to the stream using an instance\n        of the provided Writer class.\n\n        In the other direction, data is read from the stream using a\n        Reader instance and then return encoded data to the caller.\n\n    \"\"\"\n    # Optional attributes set by the file wrappers below\n    data_encoding = 'unknown'\n    file_encoding = 'unknown'\n\n    def __init__(self, stream, encode, decode, Reader, Writer,\n                 errors='strict'):\n\n        \"\"\" Creates a StreamRecoder instance which implements a two-way\n            conversion: encode and decode work on the frontend (the\n            input to .read() and output of .write()) while\n            Reader and Writer work on the backend (reading and\n            writing to the stream).\n\n            You can use these objects to do transparent direct\n            recodings from e.g. latin-1 to utf-8 and back.\n\n            stream must be a file-like object.\n\n            encode, decode must adhere to the Codec interface, Reader,\n            Writer must be factory functions or classes providing the\n            StreamReader, StreamWriter interface resp.\n\n            encode and decode are needed for the frontend translation,\n            Reader and Writer for the backend translation. Unicode is\n            used as intermediate encoding.\n\n            Error handling is done in the same way as defined for the\n            StreamWriter/Readers.\n\n        \"\"\"\n        self.stream = stream\n        self.encode = encode\n        self.decode = decode\n        self.reader = Reader(stream, errors)\n        self.writer = Writer(stream, errors)\n        self.errors = errors\n\n    def read(self, size=-1):\n\n        data = self.reader.read(size)\n        data, bytesencoded = self.encode(data, self.errors)\n        return data\n\n    def readline(self, size=None):\n\n        if size is None:\n            data = self.reader.readline()\n        else:\n            data = self.reader.readline(size)\n        data, bytesencoded = self.encode(data, self.errors)\n        return data\n\n    def readlines(self, sizehint=None):\n\n        data = self.reader.read()\n        data, bytesencoded = self.encode(data, self.errors)\n        return data.splitlines(1)\n\n    def next(self):\n\n        \"\"\" Return the next decoded line from the input stream.\"\"\"\n        data = self.reader.next()\n        data, bytesencoded = self.encode(data, self.errors)\n        return data\n\n    def __iter__(self):\n        return self\n\n    def write(self, data):\n\n        data, bytesdecoded = self.decode(data, self.errors)\n        return self.writer.write(data)\n\n    def writelines(self, list):\n\n        data = ''.join(list)\n        data, bytesdecoded = self.decode(data, self.errors)\n        return self.writer.write(data)\n\n    def reset(self):\n\n        self.reader.reset()\n        self.writer.reset()\n\n    def __getattr__(self, name,\n                    getattr=getattr):\n\n        \"\"\" Inherit all other methods from the underlying stream.\n        \"\"\"\n        return getattr(self.stream, name)\n\n    def __enter__(self):\n        return self\n\n    def __exit__(self, type, value, tb):\n        self.stream.close()\n\n### Shortcuts\n\ndef open(filename, mode='rb', encoding=None, errors='strict', buffering=1):\n\n    \"\"\" Open an encoded file using the given mode and return\n        a wrapped version providing transparent encoding/decoding.\n\n        Note: The wrapped version will only accept the object format\n        defined by the codecs, i.e. Unicode objects for most builtin\n        codecs. Output is also codec dependent and will usually be\n        Unicode as well.\n\n        Files are always opened in binary mode, even if no binary mode\n        was specified. This is done to avoid data loss due to encodings\n        using 8-bit values. The default file mode is 'rb' meaning to\n        open the file in binary read mode.\n\n        encoding specifies the encoding which is to be used for the\n        file.\n\n        errors may be given to define the error handling. It defaults\n        to 'strict' which causes ValueErrors to be raised in case an\n        encoding error occurs.\n\n        buffering has the same meaning as for the builtin open() API.\n        It defaults to line buffered.\n\n        The returned wrapped file object provides an extra attribute\n        .encoding which allows querying the used encoding. This\n        attribute is only available if an encoding was specified as\n        parameter.\n\n    \"\"\"\n    if encoding is not None:\n        if 'U' in mode:\n            # No automatic conversion of '\\n' is done on reading and writing\n            mode = mode.strip().replace('U', '')\n            if mode[:1] not in set('rwa'):\n                mode = 'r' + mode\n        if 'b' not in mode:\n            # Force opening of the file in binary mode\n            mode = mode + 'b'\n    file = __builtin__.open(filename, mode, buffering)\n    if encoding is None:\n        return file\n    info = lookup(encoding)\n    srw = StreamReaderWriter(file, info.streamreader, info.streamwriter, errors)\n    # Add attributes to simplify introspection\n    srw.encoding = encoding\n    return srw\n\ndef EncodedFile(file, data_encoding, file_encoding=None, errors='strict'):\n\n    \"\"\" Return a wrapped version of file which provides transparent\n        encoding translation.\n\n        Strings written to the wrapped file are interpreted according\n        to the given data_encoding and then written to the original\n        file as string using file_encoding. The intermediate encoding\n        will usually be Unicode but depends on the specified codecs.\n\n        Strings are read from the file using file_encoding and then\n        passed back to the caller as string using data_encoding.\n\n        If file_encoding is not given, it defaults to data_encoding.\n\n        errors may be given to define the error handling. It defaults\n        to 'strict' which causes ValueErrors to be raised in case an\n        encoding error occurs.\n\n        The returned wrapped file object provides two extra attributes\n        .data_encoding and .file_encoding which reflect the given\n        parameters of the same name. The attributes can be used for\n        introspection by Python programs.\n\n    \"\"\"\n    if file_encoding is None:\n        file_encoding = data_encoding\n    data_info = lookup(data_encoding)\n    file_info = lookup(file_encoding)\n    sr = StreamRecoder(file, data_info.encode, data_info.decode,\n                       file_info.streamreader, file_info.streamwriter, errors)\n    # Add attributes to simplify introspection\n    sr.data_encoding = data_encoding\n    sr.file_encoding = file_encoding\n    return sr\n\n### Helpers for codec lookup\n\ndef getencoder(encoding):\n\n    \"\"\" Lookup up the codec for the given encoding and return\n        its encoder function.\n\n        Raises a LookupError in case the encoding cannot be found.\n\n    \"\"\"\n    return lookup(encoding).encode\n\ndef getdecoder(encoding):\n\n    \"\"\" Lookup up the codec for the given encoding and return\n        its decoder function.\n\n        Raises a LookupError in case the encoding cannot be found.\n\n    \"\"\"\n    return lookup(encoding).decode\n\ndef getincrementalencoder(encoding):\n\n    \"\"\" Lookup up the codec for the given encoding and return\n        its IncrementalEncoder class or factory function.\n\n        Raises a LookupError in case the encoding cannot be found\n        or the codecs doesn't provide an incremental encoder.\n\n    \"\"\"\n    encoder = lookup(encoding).incrementalencoder\n    if encoder is None:\n        raise LookupError(encoding)\n    return encoder\n\ndef getincrementaldecoder(encoding):\n\n    \"\"\" Lookup up the codec for the given encoding and return\n        its IncrementalDecoder class or factory function.\n\n        Raises a LookupError in case the encoding cannot be found\n        or the codecs doesn't provide an incremental decoder.\n\n    \"\"\"\n    decoder = lookup(encoding).incrementaldecoder\n    if decoder is None:\n        raise LookupError(encoding)\n    return decoder\n\ndef getreader(encoding):\n\n    \"\"\" Lookup up the codec for the given encoding and return\n        its StreamReader class or factory function.\n\n        Raises a LookupError in case the encoding cannot be found.\n\n    \"\"\"\n    return lookup(encoding).streamreader\n\ndef getwriter(encoding):\n\n    \"\"\" Lookup up the codec for the given encoding and return\n        its StreamWriter class or factory function.\n\n        Raises a LookupError in case the encoding cannot be found.\n\n    \"\"\"\n    return lookup(encoding).streamwriter\n\ndef iterencode(iterator, encoding, errors='strict', **kwargs):\n    \"\"\"\n    Encoding iterator.\n\n    Encodes the input strings from the iterator using a IncrementalEncoder.\n\n    errors and kwargs are passed through to the IncrementalEncoder\n    constructor.\n    \"\"\"\n    encoder = getincrementalencoder(encoding)(errors, **kwargs)\n    for input in iterator:\n        output = encoder.encode(input)\n        if output:\n            yield output\n    output = encoder.encode(\"\", True)\n    if output:\n        yield output\n\ndef iterdecode(iterator, encoding, errors='strict', **kwargs):\n    \"\"\"\n    Decoding iterator.\n\n    Decodes the input strings from the iterator using a IncrementalDecoder.\n\n    errors and kwargs are passed through to the IncrementalDecoder\n    constructor.\n    \"\"\"\n    decoder = getincrementaldecoder(encoding)(errors, **kwargs)\n    for input in iterator:\n        output = decoder.decode(input)\n        if output:\n            yield output\n    output = decoder.decode(\"\", True)\n    if output:\n        yield output\n\n### Helpers for charmap-based codecs\n\ndef make_identity_dict(rng):\n\n    \"\"\" make_identity_dict(rng) -> dict\n\n        Return a dictionary where elements of the rng sequence are\n        mapped to themselves.\n\n    \"\"\"\n    res = {}\n    for i in rng:\n        res[i]=i\n    return res\n\ndef make_encoding_map(decoding_map):\n\n    \"\"\" Creates an encoding map from a decoding map.\n\n        If a target mapping in the decoding map occurs multiple\n        times, then that target is mapped to None (undefined mapping),\n        causing an exception when encountered by the charmap codec\n        during translation.\n\n        One example where this happens is cp875.py which decodes\n        multiple character to \\u001a.\n\n    \"\"\"\n    m = {}\n    for k,v in decoding_map.items():\n        if not v in m:\n            m[v] = k\n        else:\n            m[v] = None\n    return m\n\n### error handlers\n\ntry:\n    strict_errors = lookup_error(\"strict\")\n    ignore_errors = lookup_error(\"ignore\")\n    replace_errors = lookup_error(\"replace\")\n    xmlcharrefreplace_errors = lookup_error(\"xmlcharrefreplace\")\n    backslashreplace_errors = lookup_error(\"backslashreplace\")\nexcept LookupError:\n    # In --disable-unicode builds, these error handler are missing\n    strict_errors = None\n    ignore_errors = None\n    replace_errors = None\n    xmlcharrefreplace_errors = None\n    backslashreplace_errors = None\n\n# Tell modulefinder that using codecs probably needs the encodings\n# package\n_false = 0\nif _false:\n    import encodings\n\n### Tests\n\nif __name__ == '__main__':\n\n    # Make stdout translate Latin-1 output into UTF-8 output\n    sys.stdout = EncodedFile(sys.stdout, 'latin-1', 'utf-8')\n\n    # Have stdin translate Latin-1 input into UTF-8 input\n    sys.stdin = EncodedFile(sys.stdin, 'utf-8', 'latin-1')\n", 
    "codeop": "r\"\"\"Utilities to compile possibly incomplete Python source code.\n\nThis module provides two interfaces, broadly similar to the builtin\nfunction compile(), which take program text, a filename and a 'mode'\nand:\n\n- Return code object if the command is complete and valid\n- Return None if the command is incomplete\n- Raise SyntaxError, ValueError or OverflowError if the command is a\n  syntax error (OverflowError and ValueError can be produced by\n  malformed literals).\n\nApproach:\n\nFirst, check if the source consists entirely of blank lines and\ncomments; if so, replace it with 'pass', because the built-in\nparser doesn't always do the right thing for these.\n\nCompile three times: as is, with \\n, and with \\n\\n appended.  If it\ncompiles as is, it's complete.  If it compiles with one \\n appended,\nwe expect more.  If it doesn't compile either way, we compare the\nerror we get when compiling with \\n or \\n\\n appended.  If the errors\nare the same, the code is broken.  But if the errors are different, we\nexpect more.  Not intuitive; not even guaranteed to hold in future\nreleases; but this matches the compiler's behavior from Python 1.4\nthrough 2.2, at least.\n\nCaveat:\n\nIt is possible (but not likely) that the parser stops parsing with a\nsuccessful outcome before reaching the end of the source; in this\ncase, trailing symbols may be ignored instead of causing an error.\nFor example, a backslash followed by two newlines may be followed by\narbitrary garbage.  This will be fixed once the API for the parser is\nbetter.\n\nThe two interfaces are:\n\ncompile_command(source, filename, symbol):\n\n    Compiles a single command in the manner described above.\n\nCommandCompiler():\n\n    Instances of this class have __call__ methods identical in\n    signature to compile_command; the difference is that if the\n    instance compiles program text containing a __future__ statement,\n    the instance 'remembers' and compiles all subsequent program texts\n    with the statement in force.\n\nThe module also provides another class:\n\nCompile():\n\n    Instances of this class act like the built-in function compile,\n    but with 'memory' in the sense described above.\n\"\"\"\n\nimport __future__\n\n_features = [getattr(__future__, fname)\n             for fname in __future__.all_feature_names]\n\n__all__ = [\"compile_command\", \"Compile\", \"CommandCompiler\"]\n\nPyCF_DONT_IMPLY_DEDENT = 0x200          # Matches pythonrun.h\n\ndef _maybe_compile(compiler, source, filename, symbol):\n    # Check for source consisting of only blank lines and comments\n    for line in source.split(\"\\n\"):\n        line = line.strip()\n        if line and line[0] != '#':\n            break               # Leave it alone\n    else:\n        if symbol != \"eval\":\n            source = \"pass\"     # Replace it with a 'pass' statement\n\n    err = err1 = err2 = None\n    code = code1 = code2 = None\n\n    try:\n        code = compiler(source, filename, symbol)\n    except SyntaxError, err:\n        pass\n\n    try:\n        code1 = compiler(source + \"\\n\", filename, symbol)\n    except SyntaxError, err1:\n        pass\n\n    try:\n        code2 = compiler(source + \"\\n\\n\", filename, symbol)\n    except SyntaxError, err2:\n        pass\n\n    if code:\n        return code\n    if not code1 and repr(err1) == repr(err2):\n        raise SyntaxError, err1\n\ndef _compile(source, filename, symbol):\n    return compile(source, filename, symbol, PyCF_DONT_IMPLY_DEDENT)\n\ndef compile_command(source, filename=\"<input>\", symbol=\"single\"):\n    r\"\"\"Compile a command and determine whether it is incomplete.\n\n    Arguments:\n\n    source -- the source string; may contain \\n characters\n    filename -- optional filename from which source was read; default\n                \"<input>\"\n    symbol -- optional grammar start symbol; \"single\" (default) or \"eval\"\n\n    Return value / exceptions raised:\n\n    - Return a code object if the command is complete and valid\n    - Return None if the command is incomplete\n    - Raise SyntaxError, ValueError or OverflowError if the command is a\n      syntax error (OverflowError and ValueError can be produced by\n      malformed literals).\n    \"\"\"\n    return _maybe_compile(_compile, source, filename, symbol)\n\nclass Compile:\n    \"\"\"Instances of this class behave much like the built-in compile\n    function, but if one is used to compile text containing a future\n    statement, it \"remembers\" and compiles all subsequent program texts\n    with the statement in force.\"\"\"\n    def __init__(self):\n        self.flags = PyCF_DONT_IMPLY_DEDENT\n\n    def __call__(self, source, filename, symbol):\n        codeob = compile(source, filename, symbol, self.flags, 1)\n        for feature in _features:\n            if codeob.co_flags & feature.compiler_flag:\n                self.flags |= feature.compiler_flag\n        return codeob\n\nclass CommandCompiler:\n    \"\"\"Instances of this class have __call__ methods identical in\n    signature to compile_command; the difference is that if the\n    instance compiles program text containing a __future__ statement,\n    the instance 'remembers' and compiles all subsequent program texts\n    with the statement in force.\"\"\"\n\n    def __init__(self,):\n        self.compiler = Compile()\n\n    def __call__(self, source, filename=\"<input>\", symbol=\"single\"):\n        r\"\"\"Compile a command and determine whether it is incomplete.\n\n        Arguments:\n\n        source -- the source string; may contain \\n characters\n        filename -- optional filename from which source was read;\n                    default \"<input>\"\n        symbol -- optional grammar start symbol; \"single\" (default) or\n                  \"eval\"\n\n        Return value / exceptions raised:\n\n        - Return a code object if the command is complete and valid\n        - Return None if the command is incomplete\n        - Raise SyntaxError, ValueError or OverflowError if the command is a\n          syntax error (OverflowError and ValueError can be produced by\n          malformed literals).\n        \"\"\"\n        return _maybe_compile(self.compiler, source, filename, symbol)\n", 
    "collections": "__all__ = ['Counter', 'deque', 'defaultdict', 'namedtuple', 'OrderedDict']\n# For bootstrapping reasons, the collection ABCs are defined in _abcoll.py.\n# They should however be considered an integral part of collections.py.\nfrom _abcoll import *\nimport _abcoll\n__all__ += _abcoll.__all__\n\nfrom _collections import deque, defaultdict\nfrom operator import itemgetter as _itemgetter, eq as _eq\nfrom keyword import iskeyword as _iskeyword\nimport sys as _sys\nimport heapq as _heapq\nfrom itertools import repeat as _repeat, chain as _chain, starmap as _starmap\nfrom itertools import imap as _imap\ntry:\n    from __pypy__ import newdict\nexcept ImportError:\n    assert '__pypy__' not in _sys.builtin_module_names\n    newdict = lambda _ : {}\ntry:\n    from __pypy__ import reversed_dict\nexcept ImportError:\n    reversed_dict = lambda d: reversed(d.keys())\n\ntry:\n    from thread import get_ident as _get_ident\nexcept ImportError:\n    from dummy_thread import get_ident as _get_ident\n\n\n################################################################################\n### OrderedDict\n################################################################################\n\nclass OrderedDict(dict):\n    '''Dictionary that remembers insertion order.\n\n    In PyPy all dicts are ordered anyway.  This is mostly useful as a\n    placeholder to mean \"this dict must be ordered even on CPython\".\n\n    Known difference: iterating over an OrderedDict which is being\n    concurrently modified raises RuntimeError in PyPy.  In CPython\n    instead we get some behavior that appears reasonable in some\n    cases but is nonsensical in other cases.  This is officially\n    forbidden by the CPython docs, so we forbid it explicitly for now.\n    '''\n\n    def __reversed__(self):\n        return reversed_dict(self)\n\n    def popitem(self, last=True):\n        '''od.popitem() -> (k, v), return and remove a (key, value) pair.\n        Pairs are returned in LIFO order if last is true or FIFO order if false.\n\n        '''\n        if last:\n            return dict.popitem(self)\n        else:\n            it = dict.__iter__(self)\n            try:\n                k = it.next()\n            except StopIteration:\n                raise KeyError('dictionary is empty')\n            return (k, self.pop(k))\n\n    def __repr__(self, _repr_running={}):\n        'od.__repr__() <==> repr(od)'\n        call_key = id(self), _get_ident()\n        if call_key in _repr_running:\n            return '...'\n        _repr_running[call_key] = 1\n        try:\n            if not self:\n                return '%s()' % (self.__class__.__name__,)\n            return '%s(%r)' % (self.__class__.__name__, self.items())\n        finally:\n            del _repr_running[call_key]\n\n    def __reduce__(self):\n        'Return state information for pickling'\n        items = [[k, self[k]] for k in self]\n        inst_dict = vars(self).copy()\n        if inst_dict:\n            return (self.__class__, (items,), inst_dict)\n        return self.__class__, (items,)\n\n    def copy(self):\n        'od.copy() -> a shallow copy of od'\n        return self.__class__(self)\n\n    def __eq__(self, other):\n        '''od.__eq__(y) <==> od==y.  Comparison to another OD is order-sensitive\n        while comparison to a regular mapping is order-insensitive.\n\n        '''\n        if isinstance(other, OrderedDict):\n            return dict.__eq__(self, other) and all(_imap(_eq, self, other))\n        return dict.__eq__(self, other)\n\n    def __ne__(self, other):\n        'od.__ne__(y) <==> od!=y'\n        return not self == other\n\n    # -- the following methods support python 3.x style dictionary views --\n\n    def viewkeys(self):\n        \"od.viewkeys() -> a set-like object providing a view on od's keys\"\n        return KeysView(self)\n\n    def viewvalues(self):\n        \"od.viewvalues() -> an object providing a view on od's values\"\n        return ValuesView(self)\n\n    def viewitems(self):\n        \"od.viewitems() -> a set-like object providing a view on od's items\"\n        return ItemsView(self)\n\n\n################################################################################\n### namedtuple\n################################################################################\n\n_class_template = '''\\\nclass {typename}(tuple):\n    '{typename}({arg_list})'\n\n    __slots__ = ()\n\n    _fields = {field_names!r}\n\n    def __new__(_cls, {arg_list}):\n        'Create new instance of {typename}({arg_list})'\n        return _tuple.__new__(_cls, ({arg_list}))\n\n    @classmethod\n    def _make(cls, iterable, new=tuple.__new__, len=len):\n        'Make a new {typename} object from a sequence or iterable'\n        result = new(cls, iterable)\n        if len(result) != {num_fields:d}:\n            raise TypeError('Expected {num_fields:d} arguments, got %d' % len(result))\n        return result\n\n    def __repr__(self):\n        'Return a nicely formatted representation string'\n        return '{typename}({repr_fmt})' % self\n\n    def _asdict(self):\n        'Return a new OrderedDict which maps field names to their values'\n        return OrderedDict(zip(self._fields, self))\n\n    def _replace(_self, **kwds):\n        'Return a new {typename} object replacing specified fields with new values'\n        result = _self._make(map(kwds.pop, {field_names!r}, _self))\n        if kwds:\n            raise ValueError('Got unexpected field names: %r' % kwds.keys())\n        return result\n\n    def __getnewargs__(self):\n        'Return self as a plain tuple.  Used by copy and pickle.'\n        return tuple(self)\n\n    __dict__ = _property(_asdict)\n\n    def __getstate__(self):\n        'Exclude the OrderedDict from pickling'\n        pass\n\n{field_defs}\n'''\n\n_repr_template = '{name}=%r'\n\n_field_template = '''\\\n    {name} = _property(lambda self: self[{index:d}], doc='Alias for field number {index:d}')\n'''\n\ndef namedtuple(typename, field_names, verbose=False, rename=False):\n    \"\"\"Returns a new subclass of tuple with named fields.\n\n    >>> Point = namedtuple('Point', ['x', 'y'])\n    >>> Point.__doc__                   # docstring for the new class\n    'Point(x, y)'\n    >>> p = Point(11, y=22)             # instantiate with positional args or keywords\n    >>> p[0] + p[1]                     # indexable like a plain tuple\n    33\n    >>> x, y = p                        # unpack like a regular tuple\n    >>> x, y\n    (11, 22)\n    >>> p.x + p.y                       # fields also accessable by name\n    33\n    >>> d = p._asdict()                 # convert to a dictionary\n    >>> d['x']\n    11\n    >>> Point(**d)                      # convert from a dictionary\n    Point(x=11, y=22)\n    >>> p._replace(x=100)               # _replace() is like str.replace() but targets named fields\n    Point(x=100, y=22)\n\n    \"\"\"\n\n    # Validate the field names.  At the user's option, either generate an error\n    # message or automatically replace the field name with a valid name.\n    if isinstance(field_names, basestring):\n        field_names = field_names.replace(',', ' ').split()\n    field_names = map(str, field_names)\n    typename = str(typename)\n    if rename:\n        seen = set()\n        for index, name in enumerate(field_names):\n            if (not all(c.isalnum() or c=='_' for c in name)\n                or _iskeyword(name)\n                or not name\n                or name[0].isdigit()\n                or name.startswith('_')\n                or name in seen):\n                field_names[index] = '_%d' % index\n            seen.add(name)\n    for name in [typename] + field_names:\n        if type(name) != str:\n            raise TypeError('Type names and field names must be strings')\n        if not all(c.isalnum() or c=='_' for c in name):\n            raise ValueError('Type names and field names can only contain '\n                             'alphanumeric characters and underscores: %r' % name)\n        if _iskeyword(name):\n            raise ValueError('Type names and field names cannot be a '\n                             'keyword: %r' % name)\n        if name[0].isdigit():\n            raise ValueError('Type names and field names cannot start with '\n                             'a number: %r' % name)\n    seen = set()\n    for name in field_names:\n        if name.startswith('_') and not rename:\n            raise ValueError('Field names cannot start with an underscore: '\n                             '%r' % name)\n        if name in seen:\n            raise ValueError('Encountered duplicate field name: %r' % name)\n        seen.add(name)\n\n    # Fill-in the class template\n    class_definition = _class_template.format(\n        typename = typename,\n        field_names = tuple(field_names),\n        num_fields = len(field_names),\n        arg_list = repr(tuple(field_names)).replace(\"'\", \"\")[1:-1],\n        repr_fmt = ', '.join(_repr_template.format(name=name)\n                             for name in field_names),\n        field_defs = '\\n'.join(_field_template.format(index=index, name=name)\n                               for index, name in enumerate(field_names))\n    )\n    if verbose:\n        print class_definition\n\n    # Execute the template string in a temporary namespace and support\n    # tracing utilities by setting a value for frame.f_globals['__name__']\n    namespace = newdict('module')\n    namespace['__name__'] = 'namedtuple_%s' % typename\n    namespace['OrderedDict'] = OrderedDict\n    namespace['_property'] = property\n    namespace['_tuple'] = tuple\n    try:\n        exec class_definition in namespace\n    except SyntaxError as e:\n        raise SyntaxError(e.message + ':\\n' + class_definition)\n    result = namespace[typename]\n\n    # For pickling to work, the __module__ variable needs to be set to the frame\n    # where the named tuple is created.  Bypass this step in environments where\n    # sys._getframe is not defined (Jython for example) or sys._getframe is not\n    # defined for arguments greater than 0 (IronPython).\n    try:\n        result.__module__ = _sys._getframe(1).f_globals.get('__name__', '__main__')\n    except (AttributeError, ValueError):\n        pass\n\n    return result\n\n\n########################################################################\n###  Counter\n########################################################################\n\nclass Counter(dict):\n    '''Dict subclass for counting hashable items.  Sometimes called a bag\n    or multiset.  Elements are stored as dictionary keys and their counts\n    are stored as dictionary values.\n\n    >>> c = Counter('abcdeabcdabcaba')  # count elements from a string\n\n    >>> c.most_common(3)                # three most common elements\n    [('a', 5), ('b', 4), ('c', 3)]\n    >>> sorted(c)                       # list all unique elements\n    ['a', 'b', 'c', 'd', 'e']\n    >>> ''.join(sorted(c.elements()))   # list elements with repetitions\n    'aaaaabbbbcccdde'\n    >>> sum(c.values())                 # total of all counts\n    15\n\n    >>> c['a']                          # count of letter 'a'\n    5\n    >>> for elem in 'shazam':           # update counts from an iterable\n    ...     c[elem] += 1                # by adding 1 to each element's count\n    >>> c['a']                          # now there are seven 'a'\n    7\n    >>> del c['b']                      # remove all 'b'\n    >>> c['b']                          # now there are zero 'b'\n    0\n\n    >>> d = Counter('simsalabim')       # make another counter\n    >>> c.update(d)                     # add in the second counter\n    >>> c['a']                          # now there are nine 'a'\n    9\n\n    >>> c.clear()                       # empty the counter\n    >>> c\n    Counter()\n\n    Note:  If a count is set to zero or reduced to zero, it will remain\n    in the counter until the entry is deleted or the counter is cleared:\n\n    >>> c = Counter('aaabbc')\n    >>> c['b'] -= 2                     # reduce the count of 'b' by two\n    >>> c.most_common()                 # 'b' is still in, but its count is zero\n    [('a', 3), ('c', 1), ('b', 0)]\n\n    '''\n    # References:\n    #   http://en.wikipedia.org/wiki/Multiset\n    #   http://www.gnu.org/software/smalltalk/manual-base/html_node/Bag.html\n    #   http://www.demo2s.com/Tutorial/Cpp/0380__set-multiset/Catalog0380__set-multiset.htm\n    #   http://code.activestate.com/recipes/259174/\n    #   Knuth, TAOCP Vol. II section 4.6.3\n\n    def __init__(self, iterable=None, **kwds):\n        '''Create a new, empty Counter object.  And if given, count elements\n        from an input iterable.  Or, initialize the count from another mapping\n        of elements to their counts.\n\n        >>> c = Counter()                           # a new, empty counter\n        >>> c = Counter('gallahad')                 # a new counter from an iterable\n        >>> c = Counter({'a': 4, 'b': 2})           # a new counter from a mapping\n        >>> c = Counter(a=4, b=2)                   # a new counter from keyword args\n\n        '''\n        super(Counter, self).__init__()\n        self.update(iterable, **kwds)\n\n    def __missing__(self, key):\n        'The count of elements not in the Counter is zero.'\n        # Needed so that self[missing_item] does not raise KeyError\n        return 0\n\n    def most_common(self, n=None):\n        '''List the n most common elements and their counts from the most\n        common to the least.  If n is None, then list all element counts.\n\n        >>> Counter('abcdeabcdabcaba').most_common(3)\n        [('a', 5), ('b', 4), ('c', 3)]\n\n        '''\n        # Emulate Bag.sortedByCount from Smalltalk\n        if n is None:\n            return sorted(self.iteritems(), key=_itemgetter(1), reverse=True)\n        return _heapq.nlargest(n, self.iteritems(), key=_itemgetter(1))\n\n    def elements(self):\n        '''Iterator over elements repeating each as many times as its count.\n\n        >>> c = Counter('ABCABC')\n        >>> sorted(c.elements())\n        ['A', 'A', 'B', 'B', 'C', 'C']\n\n        # Knuth's example for prime factors of 1836:  2**2 * 3**3 * 17**1\n        >>> prime_factors = Counter({2: 2, 3: 3, 17: 1})\n        >>> product = 1\n        >>> for factor in prime_factors.elements():     # loop over factors\n        ...     product *= factor                       # and multiply them\n        >>> product\n        1836\n\n        Note, if an element's count has been set to zero or is a negative\n        number, elements() will ignore it.\n\n        '''\n        # Emulate Bag.do from Smalltalk and Multiset.begin from C++.\n        return _chain.from_iterable(_starmap(_repeat, self.iteritems()))\n\n    # Override dict methods where necessary\n\n    @classmethod\n    def fromkeys(cls, iterable, v=None):\n        # There is no equivalent method for counters because setting v=1\n        # means that no element can have a count greater than one.\n        raise NotImplementedError(\n            'Counter.fromkeys() is undefined.  Use Counter(iterable) instead.')\n\n    def update(self, iterable=None, **kwds):\n        '''Like dict.update() but add counts instead of replacing them.\n\n        Source can be an iterable, a dictionary, or another Counter instance.\n\n        >>> c = Counter('which')\n        >>> c.update('witch')           # add elements from another iterable\n        >>> d = Counter('watch')\n        >>> c.update(d)                 # add elements from another counter\n        >>> c['h']                      # four 'h' in which, witch, and watch\n        4\n\n        '''\n        # The regular dict.update() operation makes no sense here because the\n        # replace behavior results in the some of original untouched counts\n        # being mixed-in with all of the other counts for a mismash that\n        # doesn't have a straight-forward interpretation in most counting\n        # contexts.  Instead, we implement straight-addition.  Both the inputs\n        # and outputs are allowed to contain zero and negative counts.\n\n        if iterable is not None:\n            if isinstance(iterable, Mapping):\n                if self:\n                    self_get = self.get\n                    for elem, count in iterable.iteritems():\n                        self[elem] = self_get(elem, 0) + count\n                else:\n                    super(Counter, self).update(iterable) # fast path when counter is empty\n            else:\n                self_get = self.get\n                for elem in iterable:\n                    self[elem] = self_get(elem, 0) + 1\n        if kwds:\n            self.update(kwds)\n\n    def subtract(self, iterable=None, **kwds):\n        '''Like dict.update() but subtracts counts instead of replacing them.\n        Counts can be reduced below zero.  Both the inputs and outputs are\n        allowed to contain zero and negative counts.\n\n        Source can be an iterable, a dictionary, or another Counter instance.\n\n        >>> c = Counter('which')\n        >>> c.subtract('witch')             # subtract elements from another iterable\n        >>> c.subtract(Counter('watch'))    # subtract elements from another counter\n        >>> c['h']                          # 2 in which, minus 1 in witch, minus 1 in watch\n        0\n        >>> c['w']                          # 1 in which, minus 1 in witch, minus 1 in watch\n        -1\n\n        '''\n        if iterable is not None:\n            self_get = self.get\n            if isinstance(iterable, Mapping):\n                for elem, count in iterable.items():\n                    self[elem] = self_get(elem, 0) - count\n            else:\n                for elem in iterable:\n                    self[elem] = self_get(elem, 0) - 1\n        if kwds:\n            self.subtract(kwds)\n\n    def copy(self):\n        'Return a shallow copy.'\n        return self.__class__(self)\n\n    def __reduce__(self):\n        return self.__class__, (dict(self),)\n\n    def __delitem__(self, elem):\n        'Like dict.__delitem__() but does not raise KeyError for missing values.'\n        if elem in self:\n            super(Counter, self).__delitem__(elem)\n\n    def __repr__(self):\n        if not self:\n            return '%s()' % self.__class__.__name__\n        items = ', '.join(map('%r: %r'.__mod__, self.most_common()))\n        return '%s({%s})' % (self.__class__.__name__, items)\n\n    # Multiset-style mathematical operations discussed in:\n    #       Knuth TAOCP Volume II section 4.6.3 exercise 19\n    #       and at http://en.wikipedia.org/wiki/Multiset\n    #\n    # Outputs guaranteed to only include positive counts.\n    #\n    # To strip negative and zero counts, add-in an empty counter:\n    #       c += Counter()\n\n    def __add__(self, other):\n        '''Add counts from two counters.\n\n        >>> Counter('abbb') + Counter('bcc')\n        Counter({'b': 4, 'c': 2, 'a': 1})\n\n        '''\n        if not isinstance(other, Counter):\n            return NotImplemented\n        result = Counter()\n        for elem, count in self.items():\n            newcount = count + other[elem]\n            if newcount > 0:\n                result[elem] = newcount\n        for elem, count in other.items():\n            if elem not in self and count > 0:\n                result[elem] = count\n        return result\n\n    def __sub__(self, other):\n        ''' Subtract count, but keep only results with positive counts.\n\n        >>> Counter('abbbc') - Counter('bccd')\n        Counter({'b': 2, 'a': 1})\n\n        '''\n        if not isinstance(other, Counter):\n            return NotImplemented\n        result = Counter()\n        for elem, count in self.items():\n            newcount = count - other[elem]\n            if newcount > 0:\n                result[elem] = newcount\n        for elem, count in other.items():\n            if elem not in self and count < 0:\n                result[elem] = 0 - count\n        return result\n\n    def __or__(self, other):\n        '''Union is the maximum of value in either of the input counters.\n\n        >>> Counter('abbb') | Counter('bcc')\n        Counter({'b': 3, 'c': 2, 'a': 1})\n\n        '''\n        if not isinstance(other, Counter):\n            return NotImplemented\n        result = Counter()\n        for elem, count in self.items():\n            other_count = other[elem]\n            newcount = other_count if count < other_count else count\n            if newcount > 0:\n                result[elem] = newcount\n        for elem, count in other.items():\n            if elem not in self and count > 0:\n                result[elem] = count\n        return result\n\n    def __and__(self, other):\n        ''' Intersection is the minimum of corresponding counts.\n\n        >>> Counter('abbb') & Counter('bcc')\n        Counter({'b': 1})\n\n        '''\n        if not isinstance(other, Counter):\n            return NotImplemented\n        result = Counter()\n        for elem, count in self.items():\n            other_count = other[elem]\n            newcount = count if count < other_count else other_count\n            if newcount > 0:\n                result[elem] = newcount\n        return result\n\n\nif __name__ == '__main__':\n    # verify that instances can be pickled\n    from cPickle import loads, dumps\n    Point = namedtuple('Point', 'x, y', True)\n    p = Point(x=10, y=20)\n    assert p == loads(dumps(p))\n\n    # test and demonstrate ability to override methods\n    class Point(namedtuple('Point', 'x y')):\n        __slots__ = ()\n        @property\n        def hypot(self):\n            return (self.x ** 2 + self.y ** 2) ** 0.5\n        def __str__(self):\n            return 'Point: x=%6.3f  y=%6.3f  hypot=%6.3f' % (self.x, self.y, self.hypot)\n\n    for p in Point(3, 4), Point(14, 5/7.):\n        print p\n\n    class Point(namedtuple('Point', 'x y')):\n        'Point class with optimized _make() and _replace() without error-checking'\n        __slots__ = ()\n        _make = classmethod(tuple.__new__)\n        def _replace(self, _map=map, **kwds):\n            return self._make(_map(kwds.get, ('x', 'y'), self))\n\n    print Point(11, 22)._replace(x=100)\n\n    Point3D = namedtuple('Point3D', Point._fields + ('z',))\n    print Point3D.__doc__\n\n    import doctest\n    TestResults = namedtuple('TestResults', 'failed attempted')\n    print TestResults(*doctest.testmod())\n", 
    "copy": "\"\"\"Generic (shallow and deep) copying operations.\n\nInterface summary:\n\n        import copy\n\n        x = copy.copy(y)        # make a shallow copy of y\n        x = copy.deepcopy(y)    # make a deep copy of y\n\nFor module specific errors, copy.Error is raised.\n\nThe difference between shallow and deep copying is only relevant for\ncompound objects (objects that contain other objects, like lists or\nclass instances).\n\n- A shallow copy constructs a new compound object and then (to the\n  extent possible) inserts *the same objects* into it that the\n  original contains.\n\n- A deep copy constructs a new compound object and then, recursively,\n  inserts *copies* into it of the objects found in the original.\n\nTwo problems often exist with deep copy operations that don't exist\nwith shallow copy operations:\n\n a) recursive objects (compound objects that, directly or indirectly,\n    contain a reference to themselves) may cause a recursive loop\n\n b) because deep copy copies *everything* it may copy too much, e.g.\n    administrative data structures that should be shared even between\n    copies\n\nPython's deep copy operation avoids these problems by:\n\n a) keeping a table of objects already copied during the current\n    copying pass\n\n b) letting user-defined classes override the copying operation or the\n    set of components copied\n\nThis version does not copy types like module, class, function, method,\nnor stack trace, stack frame, nor file, socket, window, nor array, nor\nany similar types.\n\nClasses can use the same interfaces to control copying that they use\nto control pickling: they can define methods called __getinitargs__(),\n__getstate__() and __setstate__().  See the documentation for module\n\"pickle\" for information on these methods.\n\"\"\"\n\nimport types\nimport weakref\nfrom copy_reg import dispatch_table\n\nclass Error(Exception):\n    pass\nerror = Error   # backward compatibility\n\ntry:\n    from org.python.core import PyStringMap\nexcept ImportError:\n    PyStringMap = None\n\n__all__ = [\"Error\", \"copy\", \"deepcopy\"]\n\ndef copy(x):\n    \"\"\"Shallow copy operation on arbitrary Python objects.\n\n    See the module's __doc__ string for more info.\n    \"\"\"\n\n    cls = type(x)\n\n    copier = _copy_dispatch.get(cls)\n    if copier:\n        return copier(x)\n\n    copier = getattr(cls, \"__copy__\", None)\n    if copier:\n        return copier(x)\n\n    reductor = dispatch_table.get(cls)\n    if reductor:\n        rv = reductor(x)\n    else:\n        reductor = getattr(x, \"__reduce_ex__\", None)\n        if reductor:\n            rv = reductor(2)\n        else:\n            reductor = getattr(x, \"__reduce__\", None)\n            if reductor:\n                rv = reductor()\n            else:\n                raise Error(\"un(shallow)copyable object of type %s\" % cls)\n\n    return _reconstruct(x, rv, 0)\n\n\n_copy_dispatch = d = {}\n\ndef _copy_immutable(x):\n    return x\nfor t in (type(None), int, long, float, bool, str, tuple,\n          frozenset, type, xrange, types.ClassType,\n          types.BuiltinFunctionType, type(Ellipsis),\n          types.FunctionType, weakref.ref):\n    d[t] = _copy_immutable\nfor name in (\"ComplexType\", \"UnicodeType\", \"CodeType\"):\n    t = getattr(types, name, None)\n    if t is not None:\n        d[t] = _copy_immutable\n\ndef _copy_with_constructor(x):\n    return type(x)(x)\nfor t in (list, dict, set):\n    d[t] = _copy_with_constructor\n\ndef _copy_with_copy_method(x):\n    return x.copy()\nif PyStringMap is not None:\n    d[PyStringMap] = _copy_with_copy_method\n\ndef _copy_inst(x):\n    if hasattr(x, '__copy__'):\n        return x.__copy__()\n    if hasattr(x, '__getinitargs__'):\n        args = x.__getinitargs__()\n        y = x.__class__(*args)\n    else:\n        y = _EmptyClass()\n        y.__class__ = x.__class__\n    if hasattr(x, '__getstate__'):\n        state = x.__getstate__()\n    else:\n        state = x.__dict__\n    if hasattr(y, '__setstate__'):\n        y.__setstate__(state)\n    else:\n        y.__dict__.update(state)\n    return y\nd[types.InstanceType] = _copy_inst\n\ndel d\n\ndef deepcopy(x, memo=None, _nil=[]):\n    \"\"\"Deep copy operation on arbitrary Python objects.\n\n    See the module's __doc__ string for more info.\n    \"\"\"\n\n    if memo is None:\n        memo = {}\n\n    d = id(x)\n    y = memo.get(d, _nil)\n    if y is not _nil:\n        return y\n\n    cls = type(x)\n\n    copier = _deepcopy_dispatch.get(cls)\n    if copier:\n        y = copier(x, memo)\n    else:\n        try:\n            issc = issubclass(cls, type)\n        except TypeError: # cls is not a class (old Boost; see SF #502085)\n            issc = 0\n        if issc:\n            y = _deepcopy_atomic(x, memo)\n        else:\n            copier = getattr(x, \"__deepcopy__\", None)\n            if copier:\n                y = copier(memo)\n            else:\n                reductor = dispatch_table.get(cls)\n                if reductor:\n                    rv = reductor(x)\n                else:\n                    reductor = getattr(x, \"__reduce_ex__\", None)\n                    if reductor:\n                        rv = reductor(2)\n                    else:\n                        reductor = getattr(x, \"__reduce__\", None)\n                        if reductor:\n                            rv = reductor()\n                        else:\n                            raise Error(\n                                \"un(deep)copyable object of type %s\" % cls)\n                y = _reconstruct(x, rv, 1, memo)\n\n    memo[d] = y\n    _keep_alive(x, memo) # Make sure x lives at least as long as d\n    return y\n\n_deepcopy_dispatch = d = {}\n\ndef _deepcopy_atomic(x, memo):\n    return x\nd[type(None)] = _deepcopy_atomic\nd[type(Ellipsis)] = _deepcopy_atomic\nd[int] = _deepcopy_atomic\nd[long] = _deepcopy_atomic\nd[float] = _deepcopy_atomic\nd[bool] = _deepcopy_atomic\ntry:\n    d[complex] = _deepcopy_atomic\nexcept NameError:\n    pass\nd[str] = _deepcopy_atomic\ntry:\n    d[unicode] = _deepcopy_atomic\nexcept NameError:\n    pass\ntry:\n    d[types.CodeType] = _deepcopy_atomic\nexcept AttributeError:\n    pass\nd[type] = _deepcopy_atomic\nd[xrange] = _deepcopy_atomic\nd[types.ClassType] = _deepcopy_atomic\nd[types.BuiltinFunctionType] = _deepcopy_atomic\nd[types.FunctionType] = _deepcopy_atomic\nd[weakref.ref] = _deepcopy_atomic\n\ndef _deepcopy_list(x, memo):\n    y = []\n    memo[id(x)] = y\n    for a in x:\n        y.append(deepcopy(a, memo))\n    return y\nd[list] = _deepcopy_list\n\ndef _deepcopy_tuple(x, memo):\n    y = []\n    for a in x:\n        y.append(deepcopy(a, memo))\n    d = id(x)\n    try:\n        return memo[d]\n    except KeyError:\n        pass\n    for i in range(len(x)):\n        if x[i] is not y[i]:\n            y = tuple(y)\n            break\n    else:\n        y = x\n    memo[d] = y\n    return y\nd[tuple] = _deepcopy_tuple\n\ndef _deepcopy_dict(x, memo):\n    y = {}\n    memo[id(x)] = y\n    for key, value in x.iteritems():\n        y[deepcopy(key, memo)] = deepcopy(value, memo)\n    return y\nd[dict] = _deepcopy_dict\nif PyStringMap is not None:\n    d[PyStringMap] = _deepcopy_dict\n\ndef _deepcopy_method(x, memo): # Copy instance methods\n    return type(x)(x.im_func, deepcopy(x.im_self, memo), x.im_class)\n_deepcopy_dispatch[types.MethodType] = _deepcopy_method\n\ndef _keep_alive(x, memo):\n    \"\"\"Keeps a reference to the object x in the memo.\n\n    Because we remember objects by their id, we have\n    to assure that possibly temporary objects are kept\n    alive by referencing them.\n    We store a reference at the id of the memo, which should\n    normally not be used unless someone tries to deepcopy\n    the memo itself...\n    \"\"\"\n    try:\n        memo[id(memo)].append(x)\n    except KeyError:\n        # aha, this is the first one :-)\n        memo[id(memo)]=[x]\n\ndef _deepcopy_inst(x, memo):\n    if hasattr(x, '__deepcopy__'):\n        return x.__deepcopy__(memo)\n    if hasattr(x, '__getinitargs__'):\n        args = x.__getinitargs__()\n        args = deepcopy(args, memo)\n        y = x.__class__(*args)\n    else:\n        y = _EmptyClass()\n        y.__class__ = x.__class__\n    memo[id(x)] = y\n    if hasattr(x, '__getstate__'):\n        state = x.__getstate__()\n    else:\n        state = x.__dict__\n    state = deepcopy(state, memo)\n    if hasattr(y, '__setstate__'):\n        y.__setstate__(state)\n    else:\n        y.__dict__.update(state)\n    return y\nd[types.InstanceType] = _deepcopy_inst\n\ndef _reconstruct(x, info, deep, memo=None):\n    if isinstance(info, str):\n        return x\n    assert isinstance(info, tuple)\n    if memo is None:\n        memo = {}\n    n = len(info)\n    assert n in (2, 3, 4, 5)\n    callable, args = info[:2]\n    if n > 2:\n        state = info[2]\n    else:\n        state = {}\n    if n > 3:\n        listiter = info[3]\n    else:\n        listiter = None\n    if n > 4:\n        dictiter = info[4]\n    else:\n        dictiter = None\n    if deep:\n        args = deepcopy(args, memo)\n    y = callable(*args)\n    memo[id(x)] = y\n\n    if state:\n        if deep:\n            state = deepcopy(state, memo)\n        if hasattr(y, '__setstate__'):\n            y.__setstate__(state)\n        else:\n            if isinstance(state, tuple) and len(state) == 2:\n                state, slotstate = state\n            else:\n                slotstate = None\n            if state is not None:\n                y.__dict__.update(state)\n            if slotstate is not None:\n                for key, value in slotstate.iteritems():\n                    setattr(y, key, value)\n\n    if listiter is not None:\n        for item in listiter:\n            if deep:\n                item = deepcopy(item, memo)\n            y.append(item)\n    if dictiter is not None:\n        for key, value in dictiter:\n            if deep:\n                key = deepcopy(key, memo)\n                value = deepcopy(value, memo)\n            y[key] = value\n    return y\n\ndel d\n\ndel types\n\n# Helper for instance creation without calling __init__\nclass _EmptyClass:\n    pass\n\ndef _test():\n    l = [None, 1, 2L, 3.14, 'xyzzy', (1, 2L), [3.14, 'abc'],\n         {'abc': 'ABC'}, (), [], {}]\n    l1 = copy(l)\n    print l1==l\n    l1 = map(copy, l)\n    print l1==l\n    l1 = deepcopy(l)\n    print l1==l\n    class C:\n        def __init__(self, arg=None):\n            self.a = 1\n            self.arg = arg\n            if __name__ == '__main__':\n                import sys\n                file = sys.argv[0]\n            else:\n                file = __file__\n            self.fp = open(file)\n            self.fp.close()\n        def __getstate__(self):\n            return {'a': self.a, 'arg': self.arg}\n        def __setstate__(self, state):\n            for key, value in state.iteritems():\n                setattr(self, key, value)\n        def __deepcopy__(self, memo=None):\n            new = self.__class__(deepcopy(self.arg, memo))\n            new.a = self.a\n            return new\n    c = C('argument sketch')\n    l.append(c)\n    l2 = copy(l)\n    print l == l2\n    print l\n    print l2\n    l2 = deepcopy(l)\n    print l == l2\n    print l\n    print l2\n    l.append({l[1]: l, 'xyz': l[2]})\n    l3 = copy(l)\n    import repr\n    print map(repr.repr, l)\n    print map(repr.repr, l1)\n    print map(repr.repr, l2)\n    print map(repr.repr, l3)\n    l3 = deepcopy(l)\n    import repr\n    print map(repr.repr, l)\n    print map(repr.repr, l1)\n    print map(repr.repr, l2)\n    print map(repr.repr, l3)\n    class odict(dict):\n        def __init__(self, d = {}):\n            self.a = 99\n            dict.__init__(self, d)\n        def __setitem__(self, k, i):\n            dict.__setitem__(self, k, i)\n            self.a\n    o = odict({\"A\" : \"B\"})\n    x = deepcopy(o)\n    print(o, x)\n\nif __name__ == '__main__':\n    _test()\n", 
    "copy_reg": "\"\"\"Helper to provide extensibility for pickle/cPickle.\n\nThis is only useful to add pickle support for extension types defined in\nC, not for instances of user-defined classes.\n\"\"\"\n\nfrom types import ClassType as _ClassType\n\n__all__ = [\"pickle\", \"constructor\",\n           \"add_extension\", \"remove_extension\", \"clear_extension_cache\"]\n\ndispatch_table = {}\n\ndef pickle(ob_type, pickle_function, constructor_ob=None):\n    if type(ob_type) is _ClassType:\n        raise TypeError(\"copy_reg is not intended for use with classes\")\n\n    if not hasattr(pickle_function, '__call__'):\n        raise TypeError(\"reduction functions must be callable\")\n    dispatch_table[ob_type] = pickle_function\n\n    # The constructor_ob function is a vestige of safe for unpickling.\n    # There is no reason for the caller to pass it anymore.\n    if constructor_ob is not None:\n        constructor(constructor_ob)\n\ndef constructor(object):\n    if not hasattr(object, '__call__'):\n        raise TypeError(\"constructors must be callable\")\n\n# Example: provide pickling support for complex numbers.\n\ntry:\n    complex\nexcept NameError:\n    pass\nelse:\n\n    def pickle_complex(c):\n        return complex, (c.real, c.imag)\n\n    pickle(complex, pickle_complex, complex)\n\n# Support for pickling new-style objects\n\ndef _reconstructor(cls, base, state):\n    if base is object:\n        obj = object.__new__(cls)\n    else:\n        obj = base.__new__(cls, state)\n        if base.__init__ != object.__init__:\n            base.__init__(obj, state)\n    return obj\n\n_HEAPTYPE = 1<<9\n\n# Python code for object.__reduce_ex__ for protocols 0 and 1\n\ndef _reduce_ex(self, proto):\n    assert proto < 2\n    for base in self.__class__.__mro__:\n        if hasattr(base, '__flags__') and not base.__flags__ & _HEAPTYPE:\n            break\n    else:\n        base = object # not really reachable\n    if base is object:\n        state = None\n    else:\n        if base is self.__class__:\n            raise TypeError, \"can't pickle %s objects\" % base.__name__\n        state = base(self)\n    args = (self.__class__, base, state)\n    try:\n        getstate = self.__getstate__\n    except AttributeError:\n        if getattr(self, \"__slots__\", None):\n            raise TypeError(\"a class that defines __slots__ without \"\n                            \"defining __getstate__ cannot be pickled\")\n        try:\n            dict = self.__dict__\n        except AttributeError:\n            dict = None\n    else:\n        dict = getstate()\n    if dict:\n        return _reconstructor, args, dict\n    else:\n        return _reconstructor, args\n\n# Helper for __reduce_ex__ protocol 2\n\ndef __newobj__(cls, *args):\n    return cls.__new__(cls, *args)\n\ndef _slotnames(cls):\n    \"\"\"Return a list of slot names for a given class.\n\n    This needs to find slots defined by the class and its bases, so we\n    can't simply return the __slots__ attribute.  We must walk down\n    the Method Resolution Order and concatenate the __slots__ of each\n    class found there.  (This assumes classes don't modify their\n    __slots__ attribute to misrepresent their slots after the class is\n    defined.)\n    \"\"\"\n\n    # Get the value from a cache in the class if possible\n    names = cls.__dict__.get(\"__slotnames__\")\n    if names is not None:\n        return names\n\n    # Not cached -- calculate the value\n    names = []\n    if not hasattr(cls, \"__slots__\"):\n        # This class has no slots\n        pass\n    else:\n        # Slots found -- gather slot names from all base classes\n        for c in cls.__mro__:\n            if \"__slots__\" in c.__dict__:\n                slots = c.__dict__['__slots__']\n                # if class has a single slot, it can be given as a string\n                if isinstance(slots, basestring):\n                    slots = (slots,)\n                for name in slots:\n                    # special descriptors\n                    if name in (\"__dict__\", \"__weakref__\"):\n                        continue\n                    # mangled names\n                    elif name.startswith('__') and not name.endswith('__'):\n                        names.append('_%s%s' % (c.__name__, name))\n                    else:\n                        names.append(name)\n\n    # Cache the outcome in the class if at all possible\n    try:\n        cls.__slotnames__ = names\n    except:\n        pass # But don't die if we can't\n\n    return names\n\n# A registry of extension codes.  This is an ad-hoc compression\n# mechanism.  Whenever a global reference to <module>, <name> is about\n# to be pickled, the (<module>, <name>) tuple is looked up here to see\n# if it is a registered extension code for it.  Extension codes are\n# universal, so that the meaning of a pickle does not depend on\n# context.  (There are also some codes reserved for local use that\n# don't have this restriction.)  Codes are positive ints; 0 is\n# reserved.\n\n_extension_registry = {}                # key -> code\n_inverted_registry = {}                 # code -> key\n_extension_cache = {}                   # code -> object\n# Don't ever rebind those names:  cPickle grabs a reference to them when\n# it's initialized, and won't see a rebinding.\n\ndef add_extension(module, name, code):\n    \"\"\"Register an extension code.\"\"\"\n    code = int(code)\n    if not 1 <= code <= 0x7fffffff:\n        raise ValueError, \"code out of range\"\n    key = (module, name)\n    if (_extension_registry.get(key) == code and\n        _inverted_registry.get(code) == key):\n        return # Redundant registrations are benign\n    if key in _extension_registry:\n        raise ValueError(\"key %s is already registered with code %s\" %\n                         (key, _extension_registry[key]))\n    if code in _inverted_registry:\n        raise ValueError(\"code %s is already in use for key %s\" %\n                         (code, _inverted_registry[code]))\n    _extension_registry[key] = code\n    _inverted_registry[code] = key\n\ndef remove_extension(module, name, code):\n    \"\"\"Unregister an extension code.  For testing only.\"\"\"\n    key = (module, name)\n    if (_extension_registry.get(key) != code or\n        _inverted_registry.get(code) != key):\n        raise ValueError(\"key %s is not registered with code %s\" %\n                         (key, code))\n    del _extension_registry[key]\n    del _inverted_registry[code]\n    if code in _extension_cache:\n        del _extension_cache[code]\n\ndef clear_extension_cache():\n    _extension_cache.clear()\n\n# Standard extension code assignments\n\n# Reserved ranges\n\n# First  Last Count  Purpose\n#     1   127   127  Reserved for Python standard library\n#   128   191    64  Reserved for Zope\n#   192   239    48  Reserved for 3rd parties\n#   240   255    16  Reserved for private use (will never be assigned)\n#   256   Inf   Inf  Reserved for future assignment\n\n# Extension codes are assigned by the Python Software Foundation.\n", 
    "difflib": "\"\"\"\nModule difflib -- helpers for computing deltas between objects.\n\nFunction get_close_matches(word, possibilities, n=3, cutoff=0.6):\n    Use SequenceMatcher to return list of the best \"good enough\" matches.\n\nFunction context_diff(a, b):\n    For two lists of strings, return a delta in context diff format.\n\nFunction ndiff(a, b):\n    Return a delta: the difference between `a` and `b` (lists of strings).\n\nFunction restore(delta, which):\n    Return one of the two sequences that generated an ndiff delta.\n\nFunction unified_diff(a, b):\n    For two lists of strings, return a delta in unified diff format.\n\nClass SequenceMatcher:\n    A flexible class for comparing pairs of sequences of any type.\n\nClass Differ:\n    For producing human-readable deltas from sequences of lines of text.\n\nClass HtmlDiff:\n    For producing HTML side by side comparison with change highlights.\n\"\"\"\n\n__all__ = ['get_close_matches', 'ndiff', 'restore', 'SequenceMatcher',\n           'Differ','IS_CHARACTER_JUNK', 'IS_LINE_JUNK', 'context_diff',\n           'unified_diff', 'HtmlDiff', 'Match']\n\nimport heapq\nfrom collections import namedtuple as _namedtuple\nfrom functools import reduce\n\nMatch = _namedtuple('Match', 'a b size')\n\ndef _calculate_ratio(matches, length):\n    if length:\n        return 2.0 * matches / length\n    return 1.0\n\nclass SequenceMatcher:\n\n    \"\"\"\n    SequenceMatcher is a flexible class for comparing pairs of sequences of\n    any type, so long as the sequence elements are hashable.  The basic\n    algorithm predates, and is a little fancier than, an algorithm\n    published in the late 1980's by Ratcliff and Obershelp under the\n    hyperbolic name \"gestalt pattern matching\".  The basic idea is to find\n    the longest contiguous matching subsequence that contains no \"junk\"\n    elements (R-O doesn't address junk).  The same idea is then applied\n    recursively to the pieces of the sequences to the left and to the right\n    of the matching subsequence.  This does not yield minimal edit\n    sequences, but does tend to yield matches that \"look right\" to people.\n\n    SequenceMatcher tries to compute a \"human-friendly diff\" between two\n    sequences.  Unlike e.g. UNIX(tm) diff, the fundamental notion is the\n    longest *contiguous* & junk-free matching subsequence.  That's what\n    catches peoples' eyes.  The Windows(tm) windiff has another interesting\n    notion, pairing up elements that appear uniquely in each sequence.\n    That, and the method here, appear to yield more intuitive difference\n    reports than does diff.  This method appears to be the least vulnerable\n    to synching up on blocks of \"junk lines\", though (like blank lines in\n    ordinary text files, or maybe \"<P>\" lines in HTML files).  That may be\n    because this is the only method of the 3 that has a *concept* of\n    \"junk\" <wink>.\n\n    Example, comparing two strings, and considering blanks to be \"junk\":\n\n    >>> s = SequenceMatcher(lambda x: x == \" \",\n    ...                     \"private Thread currentThread;\",\n    ...                     \"private volatile Thread currentThread;\")\n    >>>\n\n    .ratio() returns a float in [0, 1], measuring the \"similarity\" of the\n    sequences.  As a rule of thumb, a .ratio() value over 0.6 means the\n    sequences are close matches:\n\n    >>> print round(s.ratio(), 3)\n    0.866\n    >>>\n\n    If you're only interested in where the sequences match,\n    .get_matching_blocks() is handy:\n\n    >>> for block in s.get_matching_blocks():\n    ...     print \"a[%d] and b[%d] match for %d elements\" % block\n    a[0] and b[0] match for 8 elements\n    a[8] and b[17] match for 21 elements\n    a[29] and b[38] match for 0 elements\n\n    Note that the last tuple returned by .get_matching_blocks() is always a\n    dummy, (len(a), len(b), 0), and this is the only case in which the last\n    tuple element (number of elements matched) is 0.\n\n    If you want to know how to change the first sequence into the second,\n    use .get_opcodes():\n\n    >>> for opcode in s.get_opcodes():\n    ...     print \"%6s a[%d:%d] b[%d:%d]\" % opcode\n     equal a[0:8] b[0:8]\n    insert a[8:8] b[8:17]\n     equal a[8:29] b[17:38]\n\n    See the Differ class for a fancy human-friendly file differencer, which\n    uses SequenceMatcher both to compare sequences of lines, and to compare\n    sequences of characters within similar (near-matching) lines.\n\n    See also function get_close_matches() in this module, which shows how\n    simple code building on SequenceMatcher can be used to do useful work.\n\n    Timing:  Basic R-O is cubic time worst case and quadratic time expected\n    case.  SequenceMatcher is quadratic time for the worst case and has\n    expected-case behavior dependent in a complicated way on how many\n    elements the sequences have in common; best case time is linear.\n\n    Methods:\n\n    __init__(isjunk=None, a='', b='')\n        Construct a SequenceMatcher.\n\n    set_seqs(a, b)\n        Set the two sequences to be compared.\n\n    set_seq1(a)\n        Set the first sequence to be compared.\n\n    set_seq2(b)\n        Set the second sequence to be compared.\n\n    find_longest_match(alo, ahi, blo, bhi)\n        Find longest matching block in a[alo:ahi] and b[blo:bhi].\n\n    get_matching_blocks()\n        Return list of triples describing matching subsequences.\n\n    get_opcodes()\n        Return list of 5-tuples describing how to turn a into b.\n\n    ratio()\n        Return a measure of the sequences' similarity (float in [0,1]).\n\n    quick_ratio()\n        Return an upper bound on .ratio() relatively quickly.\n\n    real_quick_ratio()\n        Return an upper bound on ratio() very quickly.\n    \"\"\"\n\n    def __init__(self, isjunk=None, a='', b='', autojunk=True):\n        \"\"\"Construct a SequenceMatcher.\n\n        Optional arg isjunk is None (the default), or a one-argument\n        function that takes a sequence element and returns true iff the\n        element is junk.  None is equivalent to passing \"lambda x: 0\", i.e.\n        no elements are considered to be junk.  For example, pass\n            lambda x: x in \" \\\\t\"\n        if you're comparing lines as sequences of characters, and don't\n        want to synch up on blanks or hard tabs.\n\n        Optional arg a is the first of two sequences to be compared.  By\n        default, an empty string.  The elements of a must be hashable.  See\n        also .set_seqs() and .set_seq1().\n\n        Optional arg b is the second of two sequences to be compared.  By\n        default, an empty string.  The elements of b must be hashable. See\n        also .set_seqs() and .set_seq2().\n\n        Optional arg autojunk should be set to False to disable the\n        \"automatic junk heuristic\" that treats popular elements as junk\n        (see module documentation for more information).\n        \"\"\"\n\n        # Members:\n        # a\n        #      first sequence\n        # b\n        #      second sequence; differences are computed as \"what do\n        #      we need to do to 'a' to change it into 'b'?\"\n        # b2j\n        #      for x in b, b2j[x] is a list of the indices (into b)\n        #      at which x appears; junk elements do not appear\n        # fullbcount\n        #      for x in b, fullbcount[x] == the number of times x\n        #      appears in b; only materialized if really needed (used\n        #      only for computing quick_ratio())\n        # matching_blocks\n        #      a list of (i, j, k) triples, where a[i:i+k] == b[j:j+k];\n        #      ascending & non-overlapping in i and in j; terminated by\n        #      a dummy (len(a), len(b), 0) sentinel\n        # opcodes\n        #      a list of (tag, i1, i2, j1, j2) tuples, where tag is\n        #      one of\n        #          'replace'   a[i1:i2] should be replaced by b[j1:j2]\n        #          'delete'    a[i1:i2] should be deleted\n        #          'insert'    b[j1:j2] should be inserted\n        #          'equal'     a[i1:i2] == b[j1:j2]\n        # isjunk\n        #      a user-supplied function taking a sequence element and\n        #      returning true iff the element is \"junk\" -- this has\n        #      subtle but helpful effects on the algorithm, which I'll\n        #      get around to writing up someday <0.9 wink>.\n        #      DON'T USE!  Only __chain_b uses this.  Use isbjunk.\n        # isbjunk\n        #      for x in b, isbjunk(x) == isjunk(x) but much faster;\n        #      it's really the __contains__ method of a hidden dict.\n        #      DOES NOT WORK for x in a!\n        # isbpopular\n        #      for x in b, isbpopular(x) is true iff b is reasonably long\n        #      (at least 200 elements) and x accounts for more than 1 + 1% of\n        #      its elements (when autojunk is enabled).\n        #      DOES NOT WORK for x in a!\n\n        self.isjunk = isjunk\n        self.a = self.b = None\n        self.autojunk = autojunk\n        self.set_seqs(a, b)\n\n    def set_seqs(self, a, b):\n        \"\"\"Set the two sequences to be compared.\n\n        >>> s = SequenceMatcher()\n        >>> s.set_seqs(\"abcd\", \"bcde\")\n        >>> s.ratio()\n        0.75\n        \"\"\"\n\n        self.set_seq1(a)\n        self.set_seq2(b)\n\n    def set_seq1(self, a):\n        \"\"\"Set the first sequence to be compared.\n\n        The second sequence to be compared is not changed.\n\n        >>> s = SequenceMatcher(None, \"abcd\", \"bcde\")\n        >>> s.ratio()\n        0.75\n        >>> s.set_seq1(\"bcde\")\n        >>> s.ratio()\n        1.0\n        >>>\n\n        SequenceMatcher computes and caches detailed information about the\n        second sequence, so if you want to compare one sequence S against\n        many sequences, use .set_seq2(S) once and call .set_seq1(x)\n        repeatedly for each of the other sequences.\n\n        See also set_seqs() and set_seq2().\n        \"\"\"\n\n        if a is self.a:\n            return\n        self.a = a\n        self.matching_blocks = self.opcodes = None\n\n    def set_seq2(self, b):\n        \"\"\"Set the second sequence to be compared.\n\n        The first sequence to be compared is not changed.\n\n        >>> s = SequenceMatcher(None, \"abcd\", \"bcde\")\n        >>> s.ratio()\n        0.75\n        >>> s.set_seq2(\"abcd\")\n        >>> s.ratio()\n        1.0\n        >>>\n\n        SequenceMatcher computes and caches detailed information about the\n        second sequence, so if you want to compare one sequence S against\n        many sequences, use .set_seq2(S) once and call .set_seq1(x)\n        repeatedly for each of the other sequences.\n\n        See also set_seqs() and set_seq1().\n        \"\"\"\n\n        if b is self.b:\n            return\n        self.b = b\n        self.matching_blocks = self.opcodes = None\n        self.fullbcount = None\n        self.__chain_b()\n\n    # For each element x in b, set b2j[x] to a list of the indices in\n    # b where x appears; the indices are in increasing order; note that\n    # the number of times x appears in b is len(b2j[x]) ...\n    # when self.isjunk is defined, junk elements don't show up in this\n    # map at all, which stops the central find_longest_match method\n    # from starting any matching block at a junk element ...\n    # also creates the fast isbjunk function ...\n    # b2j also does not contain entries for \"popular\" elements, meaning\n    # elements that account for more than 1 + 1% of the total elements, and\n    # when the sequence is reasonably large (>= 200 elements); this can\n    # be viewed as an adaptive notion of semi-junk, and yields an enormous\n    # speedup when, e.g., comparing program files with hundreds of\n    # instances of \"return NULL;\" ...\n    # note that this is only called when b changes; so for cross-product\n    # kinds of matches, it's best to call set_seq2 once, then set_seq1\n    # repeatedly\n\n    def __chain_b(self):\n        # Because isjunk is a user-defined (not C) function, and we test\n        # for junk a LOT, it's important to minimize the number of calls.\n        # Before the tricks described here, __chain_b was by far the most\n        # time-consuming routine in the whole module!  If anyone sees\n        # Jim Roskind, thank him again for profile.py -- I never would\n        # have guessed that.\n        # The first trick is to build b2j ignoring the possibility\n        # of junk.  I.e., we don't call isjunk at all yet.  Throwing\n        # out the junk later is much cheaper than building b2j \"right\"\n        # from the start.\n        b = self.b\n        self.b2j = b2j = {}\n\n        for i, elt in enumerate(b):\n            indices = b2j.setdefault(elt, [])\n            indices.append(i)\n\n        # Purge junk elements\n        junk = set()\n        isjunk = self.isjunk\n        if isjunk:\n            for elt in list(b2j.keys()):  # using list() since b2j is modified\n                if isjunk(elt):\n                    junk.add(elt)\n                    del b2j[elt]\n\n        # Purge popular elements that are not junk\n        popular = set()\n        n = len(b)\n        if self.autojunk and n >= 200:\n            ntest = n // 100 + 1\n            for elt, idxs in list(b2j.items()):\n                if len(idxs) > ntest:\n                    popular.add(elt)\n                    del b2j[elt]\n\n        # Now for x in b, isjunk(x) == x in junk, but the latter is much faster.\n        # Sicne the number of *unique* junk elements is probably small, the\n        # memory burden of keeping this set alive is likely trivial compared to\n        # the size of b2j.\n        self.isbjunk = junk.__contains__\n        self.isbpopular = popular.__contains__\n\n    def find_longest_match(self, alo, ahi, blo, bhi):\n        \"\"\"Find longest matching block in a[alo:ahi] and b[blo:bhi].\n\n        If isjunk is not defined:\n\n        Return (i,j,k) such that a[i:i+k] is equal to b[j:j+k], where\n            alo <= i <= i+k <= ahi\n            blo <= j <= j+k <= bhi\n        and for all (i',j',k') meeting those conditions,\n            k >= k'\n            i <= i'\n            and if i == i', j <= j'\n\n        In other words, of all maximal matching blocks, return one that\n        starts earliest in a, and of all those maximal matching blocks that\n        start earliest in a, return the one that starts earliest in b.\n\n        >>> s = SequenceMatcher(None, \" abcd\", \"abcd abcd\")\n        >>> s.find_longest_match(0, 5, 0, 9)\n        Match(a=0, b=4, size=5)\n\n        If isjunk is defined, first the longest matching block is\n        determined as above, but with the additional restriction that no\n        junk element appears in the block.  Then that block is extended as\n        far as possible by matching (only) junk elements on both sides.  So\n        the resulting block never matches on junk except as identical junk\n        happens to be adjacent to an \"interesting\" match.\n\n        Here's the same example as before, but considering blanks to be\n        junk.  That prevents \" abcd\" from matching the \" abcd\" at the tail\n        end of the second sequence directly.  Instead only the \"abcd\" can\n        match, and matches the leftmost \"abcd\" in the second sequence:\n\n        >>> s = SequenceMatcher(lambda x: x==\" \", \" abcd\", \"abcd abcd\")\n        >>> s.find_longest_match(0, 5, 0, 9)\n        Match(a=1, b=0, size=4)\n\n        If no blocks match, return (alo, blo, 0).\n\n        >>> s = SequenceMatcher(None, \"ab\", \"c\")\n        >>> s.find_longest_match(0, 2, 0, 1)\n        Match(a=0, b=0, size=0)\n        \"\"\"\n\n        # CAUTION:  stripping common prefix or suffix would be incorrect.\n        # E.g.,\n        #    ab\n        #    acab\n        # Longest matching block is \"ab\", but if common prefix is\n        # stripped, it's \"a\" (tied with \"b\").  UNIX(tm) diff does so\n        # strip, so ends up claiming that ab is changed to acab by\n        # inserting \"ca\" in the middle.  That's minimal but unintuitive:\n        # \"it's obvious\" that someone inserted \"ac\" at the front.\n        # Windiff ends up at the same place as diff, but by pairing up\n        # the unique 'b's and then matching the first two 'a's.\n\n        a, b, b2j, isbjunk = self.a, self.b, self.b2j, self.isbjunk\n        besti, bestj, bestsize = alo, blo, 0\n        # find longest junk-free match\n        # during an iteration of the loop, j2len[j] = length of longest\n        # junk-free match ending with a[i-1] and b[j]\n        j2len = {}\n        nothing = []\n        for i in xrange(alo, ahi):\n            # look at all instances of a[i] in b; note that because\n            # b2j has no junk keys, the loop is skipped if a[i] is junk\n            j2lenget = j2len.get\n            newj2len = {}\n            for j in b2j.get(a[i], nothing):\n                # a[i] matches b[j]\n                if j < blo:\n                    continue\n                if j >= bhi:\n                    break\n                k = newj2len[j] = j2lenget(j-1, 0) + 1\n                if k > bestsize:\n                    besti, bestj, bestsize = i-k+1, j-k+1, k\n            j2len = newj2len\n\n        # Extend the best by non-junk elements on each end.  In particular,\n        # \"popular\" non-junk elements aren't in b2j, which greatly speeds\n        # the inner loop above, but also means \"the best\" match so far\n        # doesn't contain any junk *or* popular non-junk elements.\n        while besti > alo and bestj > blo and \\\n              not isbjunk(b[bestj-1]) and \\\n              a[besti-1] == b[bestj-1]:\n            besti, bestj, bestsize = besti-1, bestj-1, bestsize+1\n        while besti+bestsize < ahi and bestj+bestsize < bhi and \\\n              not isbjunk(b[bestj+bestsize]) and \\\n              a[besti+bestsize] == b[bestj+bestsize]:\n            bestsize += 1\n\n        # Now that we have a wholly interesting match (albeit possibly\n        # empty!), we may as well suck up the matching junk on each\n        # side of it too.  Can't think of a good reason not to, and it\n        # saves post-processing the (possibly considerable) expense of\n        # figuring out what to do with it.  In the case of an empty\n        # interesting match, this is clearly the right thing to do,\n        # because no other kind of match is possible in the regions.\n        while besti > alo and bestj > blo and \\\n              isbjunk(b[bestj-1]) and \\\n              a[besti-1] == b[bestj-1]:\n            besti, bestj, bestsize = besti-1, bestj-1, bestsize+1\n        while besti+bestsize < ahi and bestj+bestsize < bhi and \\\n              isbjunk(b[bestj+bestsize]) and \\\n              a[besti+bestsize] == b[bestj+bestsize]:\n            bestsize = bestsize + 1\n\n        return Match(besti, bestj, bestsize)\n\n    def get_matching_blocks(self):\n        \"\"\"Return list of triples describing matching subsequences.\n\n        Each triple is of the form (i, j, n), and means that\n        a[i:i+n] == b[j:j+n].  The triples are monotonically increasing in\n        i and in j.  New in Python 2.5, it's also guaranteed that if\n        (i, j, n) and (i', j', n') are adjacent triples in the list, and\n        the second is not the last triple in the list, then i+n != i' or\n        j+n != j'.  IOW, adjacent triples never describe adjacent equal\n        blocks.\n\n        The last triple is a dummy, (len(a), len(b), 0), and is the only\n        triple with n==0.\n\n        >>> s = SequenceMatcher(None, \"abxcd\", \"abcd\")\n        >>> s.get_matching_blocks()\n        [Match(a=0, b=0, size=2), Match(a=3, b=2, size=2), Match(a=5, b=4, size=0)]\n        \"\"\"\n\n        if self.matching_blocks is not None:\n            return self.matching_blocks\n        la, lb = len(self.a), len(self.b)\n\n        # This is most naturally expressed as a recursive algorithm, but\n        # at least one user bumped into extreme use cases that exceeded\n        # the recursion limit on their box.  So, now we maintain a list\n        # ('queue`) of blocks we still need to look at, and append partial\n        # results to `matching_blocks` in a loop; the matches are sorted\n        # at the end.\n        queue = [(0, la, 0, lb)]\n        matching_blocks = []\n        while queue:\n            alo, ahi, blo, bhi = queue.pop()\n            i, j, k = x = self.find_longest_match(alo, ahi, blo, bhi)\n            # a[alo:i] vs b[blo:j] unknown\n            # a[i:i+k] same as b[j:j+k]\n            # a[i+k:ahi] vs b[j+k:bhi] unknown\n            if k:   # if k is 0, there was no matching block\n                matching_blocks.append(x)\n                if alo < i and blo < j:\n                    queue.append((alo, i, blo, j))\n                if i+k < ahi and j+k < bhi:\n                    queue.append((i+k, ahi, j+k, bhi))\n        matching_blocks.sort()\n\n        # It's possible that we have adjacent equal blocks in the\n        # matching_blocks list now.  Starting with 2.5, this code was added\n        # to collapse them.\n        i1 = j1 = k1 = 0\n        non_adjacent = []\n        for i2, j2, k2 in matching_blocks:\n            # Is this block adjacent to i1, j1, k1?\n            if i1 + k1 == i2 and j1 + k1 == j2:\n                # Yes, so collapse them -- this just increases the length of\n                # the first block by the length of the second, and the first\n                # block so lengthened remains the block to compare against.\n                k1 += k2\n            else:\n                # Not adjacent.  Remember the first block (k1==0 means it's\n                # the dummy we started with), and make the second block the\n                # new block to compare against.\n                if k1:\n                    non_adjacent.append((i1, j1, k1))\n                i1, j1, k1 = i2, j2, k2\n        if k1:\n            non_adjacent.append((i1, j1, k1))\n\n        non_adjacent.append( (la, lb, 0) )\n        self.matching_blocks = map(Match._make, non_adjacent)\n        return self.matching_blocks\n\n    def get_opcodes(self):\n        \"\"\"Return list of 5-tuples describing how to turn a into b.\n\n        Each tuple is of the form (tag, i1, i2, j1, j2).  The first tuple\n        has i1 == j1 == 0, and remaining tuples have i1 == the i2 from the\n        tuple preceding it, and likewise for j1 == the previous j2.\n\n        The tags are strings, with these meanings:\n\n        'replace':  a[i1:i2] should be replaced by b[j1:j2]\n        'delete':   a[i1:i2] should be deleted.\n                    Note that j1==j2 in this case.\n        'insert':   b[j1:j2] should be inserted at a[i1:i1].\n                    Note that i1==i2 in this case.\n        'equal':    a[i1:i2] == b[j1:j2]\n\n        >>> a = \"qabxcd\"\n        >>> b = \"abycdf\"\n        >>> s = SequenceMatcher(None, a, b)\n        >>> for tag, i1, i2, j1, j2 in s.get_opcodes():\n        ...    print (\"%7s a[%d:%d] (%s) b[%d:%d] (%s)\" %\n        ...           (tag, i1, i2, a[i1:i2], j1, j2, b[j1:j2]))\n         delete a[0:1] (q) b[0:0] ()\n          equal a[1:3] (ab) b[0:2] (ab)\n        replace a[3:4] (x) b[2:3] (y)\n          equal a[4:6] (cd) b[3:5] (cd)\n         insert a[6:6] () b[5:6] (f)\n        \"\"\"\n\n        if self.opcodes is not None:\n            return self.opcodes\n        i = j = 0\n        self.opcodes = answer = []\n        for ai, bj, size in self.get_matching_blocks():\n            # invariant:  we've pumped out correct diffs to change\n            # a[:i] into b[:j], and the next matching block is\n            # a[ai:ai+size] == b[bj:bj+size].  So we need to pump\n            # out a diff to change a[i:ai] into b[j:bj], pump out\n            # the matching block, and move (i,j) beyond the match\n            tag = ''\n            if i < ai and j < bj:\n                tag = 'replace'\n            elif i < ai:\n                tag = 'delete'\n            elif j < bj:\n                tag = 'insert'\n            if tag:\n                answer.append( (tag, i, ai, j, bj) )\n            i, j = ai+size, bj+size\n            # the list of matching blocks is terminated by a\n            # sentinel with size 0\n            if size:\n                answer.append( ('equal', ai, i, bj, j) )\n        return answer\n\n    def get_grouped_opcodes(self, n=3):\n        \"\"\" Isolate change clusters by eliminating ranges with no changes.\n\n        Return a generator of groups with up to n lines of context.\n        Each group is in the same format as returned by get_opcodes().\n\n        >>> from pprint import pprint\n        >>> a = map(str, range(1,40))\n        >>> b = a[:]\n        >>> b[8:8] = ['i']     # Make an insertion\n        >>> b[20] += 'x'       # Make a replacement\n        >>> b[23:28] = []      # Make a deletion\n        >>> b[30] += 'y'       # Make another replacement\n        >>> pprint(list(SequenceMatcher(None,a,b).get_grouped_opcodes()))\n        [[('equal', 5, 8, 5, 8), ('insert', 8, 8, 8, 9), ('equal', 8, 11, 9, 12)],\n         [('equal', 16, 19, 17, 20),\n          ('replace', 19, 20, 20, 21),\n          ('equal', 20, 22, 21, 23),\n          ('delete', 22, 27, 23, 23),\n          ('equal', 27, 30, 23, 26)],\n         [('equal', 31, 34, 27, 30),\n          ('replace', 34, 35, 30, 31),\n          ('equal', 35, 38, 31, 34)]]\n        \"\"\"\n\n        codes = self.get_opcodes()\n        if not codes:\n            codes = [(\"equal\", 0, 1, 0, 1)]\n        # Fixup leading and trailing groups if they show no changes.\n        if codes[0][0] == 'equal':\n            tag, i1, i2, j1, j2 = codes[0]\n            codes[0] = tag, max(i1, i2-n), i2, max(j1, j2-n), j2\n        if codes[-1][0] == 'equal':\n            tag, i1, i2, j1, j2 = codes[-1]\n            codes[-1] = tag, i1, min(i2, i1+n), j1, min(j2, j1+n)\n\n        nn = n + n\n        group = []\n        for tag, i1, i2, j1, j2 in codes:\n            # End the current group and start a new one whenever\n            # there is a large range with no changes.\n            if tag == 'equal' and i2-i1 > nn:\n                group.append((tag, i1, min(i2, i1+n), j1, min(j2, j1+n)))\n                yield group\n                group = []\n                i1, j1 = max(i1, i2-n), max(j1, j2-n)\n            group.append((tag, i1, i2, j1 ,j2))\n        if group and not (len(group)==1 and group[0][0] == 'equal'):\n            yield group\n\n    def ratio(self):\n        \"\"\"Return a measure of the sequences' similarity (float in [0,1]).\n\n        Where T is the total number of elements in both sequences, and\n        M is the number of matches, this is 2.0*M / T.\n        Note that this is 1 if the sequences are identical, and 0 if\n        they have nothing in common.\n\n        .ratio() is expensive to compute if you haven't already computed\n        .get_matching_blocks() or .get_opcodes(), in which case you may\n        want to try .quick_ratio() or .real_quick_ratio() first to get an\n        upper bound.\n\n        >>> s = SequenceMatcher(None, \"abcd\", \"bcde\")\n        >>> s.ratio()\n        0.75\n        >>> s.quick_ratio()\n        0.75\n        >>> s.real_quick_ratio()\n        1.0\n        \"\"\"\n\n        matches = reduce(lambda sum, triple: sum + triple[-1],\n                         self.get_matching_blocks(), 0)\n        return _calculate_ratio(matches, len(self.a) + len(self.b))\n\n    def quick_ratio(self):\n        \"\"\"Return an upper bound on ratio() relatively quickly.\n\n        This isn't defined beyond that it is an upper bound on .ratio(), and\n        is faster to compute.\n        \"\"\"\n\n        # viewing a and b as multisets, set matches to the cardinality\n        # of their intersection; this counts the number of matches\n        # without regard to order, so is clearly an upper bound\n        if self.fullbcount is None:\n            self.fullbcount = fullbcount = {}\n            for elt in self.b:\n                fullbcount[elt] = fullbcount.get(elt, 0) + 1\n        fullbcount = self.fullbcount\n        # avail[x] is the number of times x appears in 'b' less the\n        # number of times we've seen it in 'a' so far ... kinda\n        avail = {}\n        availhas, matches = avail.__contains__, 0\n        for elt in self.a:\n            if availhas(elt):\n                numb = avail[elt]\n            else:\n                numb = fullbcount.get(elt, 0)\n            avail[elt] = numb - 1\n            if numb > 0:\n                matches = matches + 1\n        return _calculate_ratio(matches, len(self.a) + len(self.b))\n\n    def real_quick_ratio(self):\n        \"\"\"Return an upper bound on ratio() very quickly.\n\n        This isn't defined beyond that it is an upper bound on .ratio(), and\n        is faster to compute than either .ratio() or .quick_ratio().\n        \"\"\"\n\n        la, lb = len(self.a), len(self.b)\n        # can't have more matches than the number of elements in the\n        # shorter sequence\n        return _calculate_ratio(min(la, lb), la + lb)\n\ndef get_close_matches(word, possibilities, n=3, cutoff=0.6):\n    \"\"\"Use SequenceMatcher to return list of the best \"good enough\" matches.\n\n    word is a sequence for which close matches are desired (typically a\n    string).\n\n    possibilities is a list of sequences against which to match word\n    (typically a list of strings).\n\n    Optional arg n (default 3) is the maximum number of close matches to\n    return.  n must be > 0.\n\n    Optional arg cutoff (default 0.6) is a float in [0, 1].  Possibilities\n    that don't score at least that similar to word are ignored.\n\n    The best (no more than n) matches among the possibilities are returned\n    in a list, sorted by similarity score, most similar first.\n\n    >>> get_close_matches(\"appel\", [\"ape\", \"apple\", \"peach\", \"puppy\"])\n    ['apple', 'ape']\n    >>> import keyword as _keyword\n    >>> get_close_matches(\"wheel\", _keyword.kwlist)\n    ['while']\n    >>> get_close_matches(\"apple\", _keyword.kwlist)\n    []\n    >>> get_close_matches(\"accept\", _keyword.kwlist)\n    ['except']\n    \"\"\"\n\n    if not n >  0:\n        raise ValueError(\"n must be > 0: %r\" % (n,))\n    if not 0.0 <= cutoff <= 1.0:\n        raise ValueError(\"cutoff must be in [0.0, 1.0]: %r\" % (cutoff,))\n    result = []\n    s = SequenceMatcher()\n    s.set_seq2(word)\n    for x in possibilities:\n        s.set_seq1(x)\n        if s.real_quick_ratio() >= cutoff and \\\n           s.quick_ratio() >= cutoff and \\\n           s.ratio() >= cutoff:\n            result.append((s.ratio(), x))\n\n    # Move the best scorers to head of list\n    result = heapq.nlargest(n, result)\n    # Strip scores for the best n matches\n    return [x for score, x in result]\n\ndef _count_leading(line, ch):\n    \"\"\"\n    Return number of `ch` characters at the start of `line`.\n\n    Example:\n\n    >>> _count_leading('   abc', ' ')\n    3\n    \"\"\"\n\n    i, n = 0, len(line)\n    while i < n and line[i] == ch:\n        i += 1\n    return i\n\nclass Differ:\n    r\"\"\"\n    Differ is a class for comparing sequences of lines of text, and\n    producing human-readable differences or deltas.  Differ uses\n    SequenceMatcher both to compare sequences of lines, and to compare\n    sequences of characters within similar (near-matching) lines.\n\n    Each line of a Differ delta begins with a two-letter code:\n\n        '- '    line unique to sequence 1\n        '+ '    line unique to sequence 2\n        '  '    line common to both sequences\n        '? '    line not present in either input sequence\n\n    Lines beginning with '? ' attempt to guide the eye to intraline\n    differences, and were not present in either input sequence.  These lines\n    can be confusing if the sequences contain tab characters.\n\n    Note that Differ makes no claim to produce a *minimal* diff.  To the\n    contrary, minimal diffs are often counter-intuitive, because they synch\n    up anywhere possible, sometimes accidental matches 100 pages apart.\n    Restricting synch points to contiguous matches preserves some notion of\n    locality, at the occasional cost of producing a longer diff.\n\n    Example: Comparing two texts.\n\n    First we set up the texts, sequences of individual single-line strings\n    ending with newlines (such sequences can also be obtained from the\n    `readlines()` method of file-like objects):\n\n    >>> text1 = '''  1. Beautiful is better than ugly.\n    ...   2. Explicit is better than implicit.\n    ...   3. Simple is better than complex.\n    ...   4. Complex is better than complicated.\n    ... '''.splitlines(1)\n    >>> len(text1)\n    4\n    >>> text1[0][-1]\n    '\\n'\n    >>> text2 = '''  1. Beautiful is better than ugly.\n    ...   3.   Simple is better than complex.\n    ...   4. Complicated is better than complex.\n    ...   5. Flat is better than nested.\n    ... '''.splitlines(1)\n\n    Next we instantiate a Differ object:\n\n    >>> d = Differ()\n\n    Note that when instantiating a Differ object we may pass functions to\n    filter out line and character 'junk'.  See Differ.__init__ for details.\n\n    Finally, we compare the two:\n\n    >>> result = list(d.compare(text1, text2))\n\n    'result' is a list of strings, so let's pretty-print it:\n\n    >>> from pprint import pprint as _pprint\n    >>> _pprint(result)\n    ['    1. Beautiful is better than ugly.\\n',\n     '-   2. Explicit is better than implicit.\\n',\n     '-   3. Simple is better than complex.\\n',\n     '+   3.   Simple is better than complex.\\n',\n     '?     ++\\n',\n     '-   4. Complex is better than complicated.\\n',\n     '?            ^                     ---- ^\\n',\n     '+   4. Complicated is better than complex.\\n',\n     '?           ++++ ^                      ^\\n',\n     '+   5. Flat is better than nested.\\n']\n\n    As a single multi-line string it looks like this:\n\n    >>> print ''.join(result),\n        1. Beautiful is better than ugly.\n    -   2. Explicit is better than implicit.\n    -   3. Simple is better than complex.\n    +   3.   Simple is better than complex.\n    ?     ++\n    -   4. Complex is better than complicated.\n    ?            ^                     ---- ^\n    +   4. Complicated is better than complex.\n    ?           ++++ ^                      ^\n    +   5. Flat is better than nested.\n\n    Methods:\n\n    __init__(linejunk=None, charjunk=None)\n        Construct a text differencer, with optional filters.\n\n    compare(a, b)\n        Compare two sequences of lines; generate the resulting delta.\n    \"\"\"\n\n    def __init__(self, linejunk=None, charjunk=None):\n        \"\"\"\n        Construct a text differencer, with optional filters.\n\n        The two optional keyword parameters are for filter functions:\n\n        - `linejunk`: A function that should accept a single string argument,\n          and return true iff the string is junk. The module-level function\n          `IS_LINE_JUNK` may be used to filter out lines without visible\n          characters, except for at most one splat ('#').  It is recommended\n          to leave linejunk None; as of Python 2.3, the underlying\n          SequenceMatcher class has grown an adaptive notion of \"noise\" lines\n          that's better than any static definition the author has ever been\n          able to craft.\n\n        - `charjunk`: A function that should accept a string of length 1. The\n          module-level function `IS_CHARACTER_JUNK` may be used to filter out\n          whitespace characters (a blank or tab; **note**: bad idea to include\n          newline in this!).  Use of IS_CHARACTER_JUNK is recommended.\n        \"\"\"\n\n        self.linejunk = linejunk\n        self.charjunk = charjunk\n\n    def compare(self, a, b):\n        r\"\"\"\n        Compare two sequences of lines; generate the resulting delta.\n\n        Each sequence must contain individual single-line strings ending with\n        newlines. Such sequences can be obtained from the `readlines()` method\n        of file-like objects.  The delta generated also consists of newline-\n        terminated strings, ready to be printed as-is via the writeline()\n        method of a file-like object.\n\n        Example:\n\n        >>> print ''.join(Differ().compare('one\\ntwo\\nthree\\n'.splitlines(1),\n        ...                                'ore\\ntree\\nemu\\n'.splitlines(1))),\n        - one\n        ?  ^\n        + ore\n        ?  ^\n        - two\n        - three\n        ?  -\n        + tree\n        + emu\n        \"\"\"\n\n        cruncher = SequenceMatcher(self.linejunk, a, b)\n        for tag, alo, ahi, blo, bhi in cruncher.get_opcodes():\n            if tag == 'replace':\n                g = self._fancy_replace(a, alo, ahi, b, blo, bhi)\n            elif tag == 'delete':\n                g = self._dump('-', a, alo, ahi)\n            elif tag == 'insert':\n                g = self._dump('+', b, blo, bhi)\n            elif tag == 'equal':\n                g = self._dump(' ', a, alo, ahi)\n            else:\n                raise ValueError, 'unknown tag %r' % (tag,)\n\n            for line in g:\n                yield line\n\n    def _dump(self, tag, x, lo, hi):\n        \"\"\"Generate comparison results for a same-tagged range.\"\"\"\n        for i in xrange(lo, hi):\n            yield '%s %s' % (tag, x[i])\n\n    def _plain_replace(self, a, alo, ahi, b, blo, bhi):\n        assert alo < ahi and blo < bhi\n        # dump the shorter block first -- reduces the burden on short-term\n        # memory if the blocks are of very different sizes\n        if bhi - blo < ahi - alo:\n            first  = self._dump('+', b, blo, bhi)\n            second = self._dump('-', a, alo, ahi)\n        else:\n            first  = self._dump('-', a, alo, ahi)\n            second = self._dump('+', b, blo, bhi)\n\n        for g in first, second:\n            for line in g:\n                yield line\n\n    def _fancy_replace(self, a, alo, ahi, b, blo, bhi):\n        r\"\"\"\n        When replacing one block of lines with another, search the blocks\n        for *similar* lines; the best-matching pair (if any) is used as a\n        synch point, and intraline difference marking is done on the\n        similar pair. Lots of work, but often worth it.\n\n        Example:\n\n        >>> d = Differ()\n        >>> results = d._fancy_replace(['abcDefghiJkl\\n'], 0, 1,\n        ...                            ['abcdefGhijkl\\n'], 0, 1)\n        >>> print ''.join(results),\n        - abcDefghiJkl\n        ?    ^  ^  ^\n        + abcdefGhijkl\n        ?    ^  ^  ^\n        \"\"\"\n\n        # don't synch up unless the lines have a similarity score of at\n        # least cutoff; best_ratio tracks the best score seen so far\n        best_ratio, cutoff = 0.74, 0.75\n        cruncher = SequenceMatcher(self.charjunk)\n        eqi, eqj = None, None   # 1st indices of equal lines (if any)\n\n        # search for the pair that matches best without being identical\n        # (identical lines must be junk lines, & we don't want to synch up\n        # on junk -- unless we have to)\n        for j in xrange(blo, bhi):\n            bj = b[j]\n            cruncher.set_seq2(bj)\n            for i in xrange(alo, ahi):\n                ai = a[i]\n                if ai == bj:\n                    if eqi is None:\n                        eqi, eqj = i, j\n                    continue\n                cruncher.set_seq1(ai)\n                # computing similarity is expensive, so use the quick\n                # upper bounds first -- have seen this speed up messy\n                # compares by a factor of 3.\n                # note that ratio() is only expensive to compute the first\n                # time it's called on a sequence pair; the expensive part\n                # of the computation is cached by cruncher\n                if cruncher.real_quick_ratio() > best_ratio and \\\n                      cruncher.quick_ratio() > best_ratio and \\\n                      cruncher.ratio() > best_ratio:\n                    best_ratio, best_i, best_j = cruncher.ratio(), i, j\n        if best_ratio < cutoff:\n            # no non-identical \"pretty close\" pair\n            if eqi is None:\n                # no identical pair either -- treat it as a straight replace\n                for line in self._plain_replace(a, alo, ahi, b, blo, bhi):\n                    yield line\n                return\n            # no close pair, but an identical pair -- synch up on that\n            best_i, best_j, best_ratio = eqi, eqj, 1.0\n        else:\n            # there's a close pair, so forget the identical pair (if any)\n            eqi = None\n\n        # a[best_i] very similar to b[best_j]; eqi is None iff they're not\n        # identical\n\n        # pump out diffs from before the synch point\n        for line in self._fancy_helper(a, alo, best_i, b, blo, best_j):\n            yield line\n\n        # do intraline marking on the synch pair\n        aelt, belt = a[best_i], b[best_j]\n        if eqi is None:\n            # pump out a '-', '?', '+', '?' quad for the synched lines\n            atags = btags = \"\"\n            cruncher.set_seqs(aelt, belt)\n            for tag, ai1, ai2, bj1, bj2 in cruncher.get_opcodes():\n                la, lb = ai2 - ai1, bj2 - bj1\n                if tag == 'replace':\n                    atags += '^' * la\n                    btags += '^' * lb\n                elif tag == 'delete':\n                    atags += '-' * la\n                elif tag == 'insert':\n                    btags += '+' * lb\n                elif tag == 'equal':\n                    atags += ' ' * la\n                    btags += ' ' * lb\n                else:\n                    raise ValueError, 'unknown tag %r' % (tag,)\n            for line in self._qformat(aelt, belt, atags, btags):\n                yield line\n        else:\n            # the synch pair is identical\n            yield '  ' + aelt\n\n        # pump out diffs from after the synch point\n        for line in self._fancy_helper(a, best_i+1, ahi, b, best_j+1, bhi):\n            yield line\n\n    def _fancy_helper(self, a, alo, ahi, b, blo, bhi):\n        g = []\n        if alo < ahi:\n            if blo < bhi:\n                g = self._fancy_replace(a, alo, ahi, b, blo, bhi)\n            else:\n                g = self._dump('-', a, alo, ahi)\n        elif blo < bhi:\n            g = self._dump('+', b, blo, bhi)\n\n        for line in g:\n            yield line\n\n    def _qformat(self, aline, bline, atags, btags):\n        r\"\"\"\n        Format \"?\" output and deal with leading tabs.\n\n        Example:\n\n        >>> d = Differ()\n        >>> results = d._qformat('\\tabcDefghiJkl\\n', '\\tabcdefGhijkl\\n',\n        ...                      '  ^ ^  ^      ', '  ^ ^  ^      ')\n        >>> for line in results: print repr(line)\n        ...\n        '- \\tabcDefghiJkl\\n'\n        '? \\t ^ ^  ^\\n'\n        '+ \\tabcdefGhijkl\\n'\n        '? \\t ^ ^  ^\\n'\n        \"\"\"\n\n        # Can hurt, but will probably help most of the time.\n        common = min(_count_leading(aline, \"\\t\"),\n                     _count_leading(bline, \"\\t\"))\n        common = min(common, _count_leading(atags[:common], \" \"))\n        common = min(common, _count_leading(btags[:common], \" \"))\n        atags = atags[common:].rstrip()\n        btags = btags[common:].rstrip()\n\n        yield \"- \" + aline\n        if atags:\n            yield \"? %s%s\\n\" % (\"\\t\" * common, atags)\n\n        yield \"+ \" + bline\n        if btags:\n            yield \"? %s%s\\n\" % (\"\\t\" * common, btags)\n\n# With respect to junk, an earlier version of ndiff simply refused to\n# *start* a match with a junk element.  The result was cases like this:\n#     before: private Thread currentThread;\n#     after:  private volatile Thread currentThread;\n# If you consider whitespace to be junk, the longest contiguous match\n# not starting with junk is \"e Thread currentThread\".  So ndiff reported\n# that \"e volatil\" was inserted between the 't' and the 'e' in \"private\".\n# While an accurate view, to people that's absurd.  The current version\n# looks for matching blocks that are entirely junk-free, then extends the\n# longest one of those as far as possible but only with matching junk.\n# So now \"currentThread\" is matched, then extended to suck up the\n# preceding blank; then \"private\" is matched, and extended to suck up the\n# following blank; then \"Thread\" is matched; and finally ndiff reports\n# that \"volatile \" was inserted before \"Thread\".  The only quibble\n# remaining is that perhaps it was really the case that \" volatile\"\n# was inserted after \"private\".  I can live with that <wink>.\n\nimport re\n\ndef IS_LINE_JUNK(line, pat=re.compile(r\"\\s*#?\\s*$\").match):\n    r\"\"\"\n    Return 1 for ignorable line: iff `line` is blank or contains a single '#'.\n\n    Examples:\n\n    >>> IS_LINE_JUNK('\\n')\n    True\n    >>> IS_LINE_JUNK('  #   \\n')\n    True\n    >>> IS_LINE_JUNK('hello\\n')\n    False\n    \"\"\"\n\n    return pat(line) is not None\n\ndef IS_CHARACTER_JUNK(ch, ws=\" \\t\"):\n    r\"\"\"\n    Return 1 for ignorable character: iff `ch` is a space or tab.\n\n    Examples:\n\n    >>> IS_CHARACTER_JUNK(' ')\n    True\n    >>> IS_CHARACTER_JUNK('\\t')\n    True\n    >>> IS_CHARACTER_JUNK('\\n')\n    False\n    >>> IS_CHARACTER_JUNK('x')\n    False\n    \"\"\"\n\n    return ch in ws\n\n\n########################################################################\n###  Unified Diff\n########################################################################\n\ndef _format_range_unified(start, stop):\n    'Convert range to the \"ed\" format'\n    # Per the diff spec at http://www.unix.org/single_unix_specification/\n    beginning = start + 1     # lines start numbering with one\n    length = stop - start\n    if length == 1:\n        return '{}'.format(beginning)\n    if not length:\n        beginning -= 1        # empty ranges begin at line just before the range\n    return '{},{}'.format(beginning, length)\n\ndef unified_diff(a, b, fromfile='', tofile='', fromfiledate='',\n                 tofiledate='', n=3, lineterm='\\n'):\n    r\"\"\"\n    Compare two sequences of lines; generate the delta as a unified diff.\n\n    Unified diffs are a compact way of showing line changes and a few\n    lines of context.  The number of context lines is set by 'n' which\n    defaults to three.\n\n    By default, the diff control lines (those with ---, +++, or @@) are\n    created with a trailing newline.  This is helpful so that inputs\n    created from file.readlines() result in diffs that are suitable for\n    file.writelines() since both the inputs and outputs have trailing\n    newlines.\n\n    For inputs that do not have trailing newlines, set the lineterm\n    argument to \"\" so that the output will be uniformly newline free.\n\n    The unidiff format normally has a header for filenames and modification\n    times.  Any or all of these may be specified using strings for\n    'fromfile', 'tofile', 'fromfiledate', and 'tofiledate'.\n    The modification times are normally expressed in the ISO 8601 format.\n\n    Example:\n\n    >>> for line in unified_diff('one two three four'.split(),\n    ...             'zero one tree four'.split(), 'Original', 'Current',\n    ...             '2005-01-26 23:30:50', '2010-04-02 10:20:52',\n    ...             lineterm=''):\n    ...     print line                  # doctest: +NORMALIZE_WHITESPACE\n    --- Original        2005-01-26 23:30:50\n    +++ Current         2010-04-02 10:20:52\n    @@ -1,4 +1,4 @@\n    +zero\n     one\n    -two\n    -three\n    +tree\n     four\n    \"\"\"\n\n    started = False\n    for group in SequenceMatcher(None,a,b).get_grouped_opcodes(n):\n        if not started:\n            started = True\n            fromdate = '\\t{}'.format(fromfiledate) if fromfiledate else ''\n            todate = '\\t{}'.format(tofiledate) if tofiledate else ''\n            yield '--- {}{}{}'.format(fromfile, fromdate, lineterm)\n            yield '+++ {}{}{}'.format(tofile, todate, lineterm)\n\n        first, last = group[0], group[-1]\n        file1_range = _format_range_unified(first[1], last[2])\n        file2_range = _format_range_unified(first[3], last[4])\n        yield '@@ -{} +{} @@{}'.format(file1_range, file2_range, lineterm)\n\n        for tag, i1, i2, j1, j2 in group:\n            if tag == 'equal':\n                for line in a[i1:i2]:\n                    yield ' ' + line\n                continue\n            if tag in ('replace', 'delete'):\n                for line in a[i1:i2]:\n                    yield '-' + line\n            if tag in ('replace', 'insert'):\n                for line in b[j1:j2]:\n                    yield '+' + line\n\n\n########################################################################\n###  Context Diff\n########################################################################\n\ndef _format_range_context(start, stop):\n    'Convert range to the \"ed\" format'\n    # Per the diff spec at http://www.unix.org/single_unix_specification/\n    beginning = start + 1     # lines start numbering with one\n    length = stop - start\n    if not length:\n        beginning -= 1        # empty ranges begin at line just before the range\n    if length <= 1:\n        return '{}'.format(beginning)\n    return '{},{}'.format(beginning, beginning + length - 1)\n\n# See http://www.unix.org/single_unix_specification/\ndef context_diff(a, b, fromfile='', tofile='',\n                 fromfiledate='', tofiledate='', n=3, lineterm='\\n'):\n    r\"\"\"\n    Compare two sequences of lines; generate the delta as a context diff.\n\n    Context diffs are a compact way of showing line changes and a few\n    lines of context.  The number of context lines is set by 'n' which\n    defaults to three.\n\n    By default, the diff control lines (those with *** or ---) are\n    created with a trailing newline.  This is helpful so that inputs\n    created from file.readlines() result in diffs that are suitable for\n    file.writelines() since both the inputs and outputs have trailing\n    newlines.\n\n    For inputs that do not have trailing newlines, set the lineterm\n    argument to \"\" so that the output will be uniformly newline free.\n\n    The context diff format normally has a header for filenames and\n    modification times.  Any or all of these may be specified using\n    strings for 'fromfile', 'tofile', 'fromfiledate', and 'tofiledate'.\n    The modification times are normally expressed in the ISO 8601 format.\n    If not specified, the strings default to blanks.\n\n    Example:\n\n    >>> print ''.join(context_diff('one\\ntwo\\nthree\\nfour\\n'.splitlines(1),\n    ...       'zero\\none\\ntree\\nfour\\n'.splitlines(1), 'Original', 'Current')),\n    *** Original\n    --- Current\n    ***************\n    *** 1,4 ****\n      one\n    ! two\n    ! three\n      four\n    --- 1,4 ----\n    + zero\n      one\n    ! tree\n      four\n    \"\"\"\n\n    prefix = dict(insert='+ ', delete='- ', replace='! ', equal='  ')\n    started = False\n    for group in SequenceMatcher(None,a,b).get_grouped_opcodes(n):\n        if not started:\n            started = True\n            fromdate = '\\t{}'.format(fromfiledate) if fromfiledate else ''\n            todate = '\\t{}'.format(tofiledate) if tofiledate else ''\n            yield '*** {}{}{}'.format(fromfile, fromdate, lineterm)\n            yield '--- {}{}{}'.format(tofile, todate, lineterm)\n\n        first, last = group[0], group[-1]\n        yield '***************' + lineterm\n\n        file1_range = _format_range_context(first[1], last[2])\n        yield '*** {} ****{}'.format(file1_range, lineterm)\n\n        if any(tag in ('replace', 'delete') for tag, _, _, _, _ in group):\n            for tag, i1, i2, _, _ in group:\n                if tag != 'insert':\n                    for line in a[i1:i2]:\n                        yield prefix[tag] + line\n\n        file2_range = _format_range_context(first[3], last[4])\n        yield '--- {} ----{}'.format(file2_range, lineterm)\n\n        if any(tag in ('replace', 'insert') for tag, _, _, _, _ in group):\n            for tag, _, _, j1, j2 in group:\n                if tag != 'delete':\n                    for line in b[j1:j2]:\n                        yield prefix[tag] + line\n\ndef ndiff(a, b, linejunk=None, charjunk=IS_CHARACTER_JUNK):\n    r\"\"\"\n    Compare `a` and `b` (lists of strings); return a `Differ`-style delta.\n\n    Optional keyword parameters `linejunk` and `charjunk` are for filter\n    functions (or None):\n\n    - linejunk: A function that should accept a single string argument, and\n      return true iff the string is junk.  The default is None, and is\n      recommended; as of Python 2.3, an adaptive notion of \"noise\" lines is\n      used that does a good job on its own.\n\n    - charjunk: A function that should accept a string of length 1. The\n      default is module-level function IS_CHARACTER_JUNK, which filters out\n      whitespace characters (a blank or tab; note: bad idea to include newline\n      in this!).\n\n    Tools/scripts/ndiff.py is a command-line front-end to this function.\n\n    Example:\n\n    >>> diff = ndiff('one\\ntwo\\nthree\\n'.splitlines(1),\n    ...              'ore\\ntree\\nemu\\n'.splitlines(1))\n    >>> print ''.join(diff),\n    - one\n    ?  ^\n    + ore\n    ?  ^\n    - two\n    - three\n    ?  -\n    + tree\n    + emu\n    \"\"\"\n    return Differ(linejunk, charjunk).compare(a, b)\n\ndef _mdiff(fromlines, tolines, context=None, linejunk=None,\n           charjunk=IS_CHARACTER_JUNK):\n    r\"\"\"Returns generator yielding marked up from/to side by side differences.\n\n    Arguments:\n    fromlines -- list of text lines to compared to tolines\n    tolines -- list of text lines to be compared to fromlines\n    context -- number of context lines to display on each side of difference,\n               if None, all from/to text lines will be generated.\n    linejunk -- passed on to ndiff (see ndiff documentation)\n    charjunk -- passed on to ndiff (see ndiff documentation)\n\n    This function returns an iterator which returns a tuple:\n    (from line tuple, to line tuple, boolean flag)\n\n    from/to line tuple -- (line num, line text)\n        line num -- integer or None (to indicate a context separation)\n        line text -- original line text with following markers inserted:\n            '\\0+' -- marks start of added text\n            '\\0-' -- marks start of deleted text\n            '\\0^' -- marks start of changed text\n            '\\1' -- marks end of added/deleted/changed text\n\n    boolean flag -- None indicates context separation, True indicates\n        either \"from\" or \"to\" line contains a change, otherwise False.\n\n    This function/iterator was originally developed to generate side by side\n    file difference for making HTML pages (see HtmlDiff class for example\n    usage).\n\n    Note, this function utilizes the ndiff function to generate the side by\n    side difference markup.  Optional ndiff arguments may be passed to this\n    function and they in turn will be passed to ndiff.\n    \"\"\"\n    import re\n\n    # regular expression for finding intraline change indices\n    change_re = re.compile('(\\++|\\-+|\\^+)')\n\n    # create the difference iterator to generate the differences\n    diff_lines_iterator = ndiff(fromlines,tolines,linejunk,charjunk)\n\n    def _make_line(lines, format_key, side, num_lines=[0,0]):\n        \"\"\"Returns line of text with user's change markup and line formatting.\n\n        lines -- list of lines from the ndiff generator to produce a line of\n                 text from.  When producing the line of text to return, the\n                 lines used are removed from this list.\n        format_key -- '+' return first line in list with \"add\" markup around\n                          the entire line.\n                      '-' return first line in list with \"delete\" markup around\n                          the entire line.\n                      '?' return first line in list with add/delete/change\n                          intraline markup (indices obtained from second line)\n                      None return first line in list with no markup\n        side -- indice into the num_lines list (0=from,1=to)\n        num_lines -- from/to current line number.  This is NOT intended to be a\n                     passed parameter.  It is present as a keyword argument to\n                     maintain memory of the current line numbers between calls\n                     of this function.\n\n        Note, this function is purposefully not defined at the module scope so\n        that data it needs from its parent function (within whose context it\n        is defined) does not need to be of module scope.\n        \"\"\"\n        num_lines[side] += 1\n        # Handle case where no user markup is to be added, just return line of\n        # text with user's line format to allow for usage of the line number.\n        if format_key is None:\n            return (num_lines[side],lines.pop(0)[2:])\n        # Handle case of intraline changes\n        if format_key == '?':\n            text, markers = lines.pop(0), lines.pop(0)\n            # find intraline changes (store change type and indices in tuples)\n            sub_info = []\n            def record_sub_info(match_object,sub_info=sub_info):\n                sub_info.append([match_object.group(1)[0],match_object.span()])\n                return match_object.group(1)\n            change_re.sub(record_sub_info,markers)\n            # process each tuple inserting our special marks that won't be\n            # noticed by an xml/html escaper.\n            for key,(begin,end) in sub_info[::-1]:\n                text = text[0:begin]+'\\0'+key+text[begin:end]+'\\1'+text[end:]\n            text = text[2:]\n        # Handle case of add/delete entire line\n        else:\n            text = lines.pop(0)[2:]\n            # if line of text is just a newline, insert a space so there is\n            # something for the user to highlight and see.\n            if not text:\n                text = ' '\n            # insert marks that won't be noticed by an xml/html escaper.\n            text = '\\0' + format_key + text + '\\1'\n        # Return line of text, first allow user's line formatter to do its\n        # thing (such as adding the line number) then replace the special\n        # marks with what the user's change markup.\n        return (num_lines[side],text)\n\n    def _line_iterator():\n        \"\"\"Yields from/to lines of text with a change indication.\n\n        This function is an iterator.  It itself pulls lines from a\n        differencing iterator, processes them and yields them.  When it can\n        it yields both a \"from\" and a \"to\" line, otherwise it will yield one\n        or the other.  In addition to yielding the lines of from/to text, a\n        boolean flag is yielded to indicate if the text line(s) have\n        differences in them.\n\n        Note, this function is purposefully not defined at the module scope so\n        that data it needs from its parent function (within whose context it\n        is defined) does not need to be of module scope.\n        \"\"\"\n        lines = []\n        num_blanks_pending, num_blanks_to_yield = 0, 0\n        while True:\n            # Load up next 4 lines so we can look ahead, create strings which\n            # are a concatenation of the first character of each of the 4 lines\n            # so we can do some very readable comparisons.\n            while len(lines) < 4:\n                try:\n                    lines.append(diff_lines_iterator.next())\n                except StopIteration:\n                    lines.append('X')\n            s = ''.join([line[0] for line in lines])\n            if s.startswith('X'):\n                # When no more lines, pump out any remaining blank lines so the\n                # corresponding add/delete lines get a matching blank line so\n                # all line pairs get yielded at the next level.\n                num_blanks_to_yield = num_blanks_pending\n            elif s.startswith('-?+?'):\n                # simple intraline change\n                yield _make_line(lines,'?',0), _make_line(lines,'?',1), True\n                continue\n            elif s.startswith('--++'):\n                # in delete block, add block coming: we do NOT want to get\n                # caught up on blank lines yet, just process the delete line\n                num_blanks_pending -= 1\n                yield _make_line(lines,'-',0), None, True\n                continue\n            elif s.startswith(('--?+', '--+', '- ')):\n                # in delete block and see a intraline change or unchanged line\n                # coming: yield the delete line and then blanks\n                from_line,to_line = _make_line(lines,'-',0), None\n                num_blanks_to_yield,num_blanks_pending = num_blanks_pending-1,0\n            elif s.startswith('-+?'):\n                # intraline change\n                yield _make_line(lines,None,0), _make_line(lines,'?',1), True\n                continue\n            elif s.startswith('-?+'):\n                # intraline change\n                yield _make_line(lines,'?',0), _make_line(lines,None,1), True\n                continue\n            elif s.startswith('-'):\n                # delete FROM line\n                num_blanks_pending -= 1\n                yield _make_line(lines,'-',0), None, True\n                continue\n            elif s.startswith('+--'):\n                # in add block, delete block coming: we do NOT want to get\n                # caught up on blank lines yet, just process the add line\n                num_blanks_pending += 1\n                yield None, _make_line(lines,'+',1), True\n                continue\n            elif s.startswith(('+ ', '+-')):\n                # will be leaving an add block: yield blanks then add line\n                from_line, to_line = None, _make_line(lines,'+',1)\n                num_blanks_to_yield,num_blanks_pending = num_blanks_pending+1,0\n            elif s.startswith('+'):\n                # inside an add block, yield the add line\n                num_blanks_pending += 1\n                yield None, _make_line(lines,'+',1), True\n                continue\n            elif s.startswith(' '):\n                # unchanged text, yield it to both sides\n                yield _make_line(lines[:],None,0),_make_line(lines,None,1),False\n                continue\n            # Catch up on the blank lines so when we yield the next from/to\n            # pair, they are lined up.\n            while(num_blanks_to_yield < 0):\n                num_blanks_to_yield += 1\n                yield None,('','\\n'),True\n            while(num_blanks_to_yield > 0):\n                num_blanks_to_yield -= 1\n                yield ('','\\n'),None,True\n            if s.startswith('X'):\n                raise StopIteration\n            else:\n                yield from_line,to_line,True\n\n    def _line_pair_iterator():\n        \"\"\"Yields from/to lines of text with a change indication.\n\n        This function is an iterator.  It itself pulls lines from the line\n        iterator.  Its difference from that iterator is that this function\n        always yields a pair of from/to text lines (with the change\n        indication).  If necessary it will collect single from/to lines\n        until it has a matching pair from/to pair to yield.\n\n        Note, this function is purposefully not defined at the module scope so\n        that data it needs from its parent function (within whose context it\n        is defined) does not need to be of module scope.\n        \"\"\"\n        line_iterator = _line_iterator()\n        fromlines,tolines=[],[]\n        while True:\n            # Collecting lines of text until we have a from/to pair\n            while (len(fromlines)==0 or len(tolines)==0):\n                from_line, to_line, found_diff =line_iterator.next()\n                if from_line is not None:\n                    fromlines.append((from_line,found_diff))\n                if to_line is not None:\n                    tolines.append((to_line,found_diff))\n            # Once we have a pair, remove them from the collection and yield it\n            from_line, fromDiff = fromlines.pop(0)\n            to_line, to_diff = tolines.pop(0)\n            yield (from_line,to_line,fromDiff or to_diff)\n\n    # Handle case where user does not want context differencing, just yield\n    # them up without doing anything else with them.\n    line_pair_iterator = _line_pair_iterator()\n    if context is None:\n        while True:\n            yield line_pair_iterator.next()\n    # Handle case where user wants context differencing.  We must do some\n    # storage of lines until we know for sure that they are to be yielded.\n    else:\n        context += 1\n        lines_to_write = 0\n        while True:\n            # Store lines up until we find a difference, note use of a\n            # circular queue because we only need to keep around what\n            # we need for context.\n            index, contextLines = 0, [None]*(context)\n            found_diff = False\n            while(found_diff is False):\n                from_line, to_line, found_diff = line_pair_iterator.next()\n                i = index % context\n                contextLines[i] = (from_line, to_line, found_diff)\n                index += 1\n            # Yield lines that we have collected so far, but first yield\n            # the user's separator.\n            if index > context:\n                yield None, None, None\n                lines_to_write = context\n            else:\n                lines_to_write = index\n                index = 0\n            while(lines_to_write):\n                i = index % context\n                index += 1\n                yield contextLines[i]\n                lines_to_write -= 1\n            # Now yield the context lines after the change\n            lines_to_write = context-1\n            while(lines_to_write):\n                from_line, to_line, found_diff = line_pair_iterator.next()\n                # If another change within the context, extend the context\n                if found_diff:\n                    lines_to_write = context-1\n                else:\n                    lines_to_write -= 1\n                yield from_line, to_line, found_diff\n\n\n_file_template = \"\"\"\n<!DOCTYPE html PUBLIC \"-//W3C//DTD XHTML 1.0 Transitional//EN\"\n          \"http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd\">\n\n<html>\n\n<head>\n    <meta http-equiv=\"Content-Type\"\n          content=\"text/html; charset=ISO-8859-1\" />\n    <title></title>\n    <style type=\"text/css\">%(styles)s\n    </style>\n</head>\n\n<body>\n    %(table)s%(legend)s\n</body>\n\n</html>\"\"\"\n\n_styles = \"\"\"\n        table.diff {font-family:Courier; border:medium;}\n        .diff_header {background-color:#e0e0e0}\n        td.diff_header {text-align:right}\n        .diff_next {background-color:#c0c0c0}\n        .diff_add {background-color:#aaffaa}\n        .diff_chg {background-color:#ffff77}\n        .diff_sub {background-color:#ffaaaa}\"\"\"\n\n_table_template = \"\"\"\n    <table class=\"diff\" id=\"difflib_chg_%(prefix)s_top\"\n           cellspacing=\"0\" cellpadding=\"0\" rules=\"groups\" >\n        <colgroup></colgroup> <colgroup></colgroup> <colgroup></colgroup>\n        <colgroup></colgroup> <colgroup></colgroup> <colgroup></colgroup>\n        %(header_row)s\n        <tbody>\n%(data_rows)s        </tbody>\n    </table>\"\"\"\n\n_legend = \"\"\"\n    <table class=\"diff\" summary=\"Legends\">\n        <tr> <th colspan=\"2\"> Legends </th> </tr>\n        <tr> <td> <table border=\"\" summary=\"Colors\">\n                      <tr><th> Colors </th> </tr>\n                      <tr><td class=\"diff_add\">&nbsp;Added&nbsp;</td></tr>\n                      <tr><td class=\"diff_chg\">Changed</td> </tr>\n                      <tr><td class=\"diff_sub\">Deleted</td> </tr>\n                  </table></td>\n             <td> <table border=\"\" summary=\"Links\">\n                      <tr><th colspan=\"2\"> Links </th> </tr>\n                      <tr><td>(f)irst change</td> </tr>\n                      <tr><td>(n)ext change</td> </tr>\n                      <tr><td>(t)op</td> </tr>\n                  </table></td> </tr>\n    </table>\"\"\"\n\nclass HtmlDiff(object):\n    \"\"\"For producing HTML side by side comparison with change highlights.\n\n    This class can be used to create an HTML table (or a complete HTML file\n    containing the table) showing a side by side, line by line comparison\n    of text with inter-line and intra-line change highlights.  The table can\n    be generated in either full or contextual difference mode.\n\n    The following methods are provided for HTML generation:\n\n    make_table -- generates HTML for a single side by side table\n    make_file -- generates complete HTML file with a single side by side table\n\n    See tools/scripts/diff.py for an example usage of this class.\n    \"\"\"\n\n    _file_template = _file_template\n    _styles = _styles\n    _table_template = _table_template\n    _legend = _legend\n    _default_prefix = 0\n\n    def __init__(self,tabsize=8,wrapcolumn=None,linejunk=None,\n                 charjunk=IS_CHARACTER_JUNK):\n        \"\"\"HtmlDiff instance initializer\n\n        Arguments:\n        tabsize -- tab stop spacing, defaults to 8.\n        wrapcolumn -- column number where lines are broken and wrapped,\n            defaults to None where lines are not wrapped.\n        linejunk,charjunk -- keyword arguments passed into ndiff() (used to by\n            HtmlDiff() to generate the side by side HTML differences).  See\n            ndiff() documentation for argument default values and descriptions.\n        \"\"\"\n        self._tabsize = tabsize\n        self._wrapcolumn = wrapcolumn\n        self._linejunk = linejunk\n        self._charjunk = charjunk\n\n    def make_file(self,fromlines,tolines,fromdesc='',todesc='',context=False,\n                  numlines=5):\n        \"\"\"Returns HTML file of side by side comparison with change highlights\n\n        Arguments:\n        fromlines -- list of \"from\" lines\n        tolines -- list of \"to\" lines\n        fromdesc -- \"from\" file column header string\n        todesc -- \"to\" file column header string\n        context -- set to True for contextual differences (defaults to False\n            which shows full differences).\n        numlines -- number of context lines.  When context is set True,\n            controls number of lines displayed before and after the change.\n            When context is False, controls the number of lines to place\n            the \"next\" link anchors before the next change (so click of\n            \"next\" link jumps to just before the change).\n        \"\"\"\n\n        return self._file_template % dict(\n            styles = self._styles,\n            legend = self._legend,\n            table = self.make_table(fromlines,tolines,fromdesc,todesc,\n                                    context=context,numlines=numlines))\n\n    def _tab_newline_replace(self,fromlines,tolines):\n        \"\"\"Returns from/to line lists with tabs expanded and newlines removed.\n\n        Instead of tab characters being replaced by the number of spaces\n        needed to fill in to the next tab stop, this function will fill\n        the space with tab characters.  This is done so that the difference\n        algorithms can identify changes in a file when tabs are replaced by\n        spaces and vice versa.  At the end of the HTML generation, the tab\n        characters will be replaced with a nonbreakable space.\n        \"\"\"\n        def expand_tabs(line):\n            # hide real spaces\n            line = line.replace(' ','\\0')\n            # expand tabs into spaces\n            line = line.expandtabs(self._tabsize)\n            # replace spaces from expanded tabs back into tab characters\n            # (we'll replace them with markup after we do differencing)\n            line = line.replace(' ','\\t')\n            return line.replace('\\0',' ').rstrip('\\n')\n        fromlines = [expand_tabs(line) for line in fromlines]\n        tolines = [expand_tabs(line) for line in tolines]\n        return fromlines,tolines\n\n    def _split_line(self,data_list,line_num,text):\n        \"\"\"Builds list of text lines by splitting text lines at wrap point\n\n        This function will determine if the input text line needs to be\n        wrapped (split) into separate lines.  If so, the first wrap point\n        will be determined and the first line appended to the output\n        text line list.  This function is used recursively to handle\n        the second part of the split line to further split it.\n        \"\"\"\n        # if blank line or context separator, just add it to the output list\n        if not line_num:\n            data_list.append((line_num,text))\n            return\n\n        # if line text doesn't need wrapping, just add it to the output list\n        size = len(text)\n        max = self._wrapcolumn\n        if (size <= max) or ((size -(text.count('\\0')*3)) <= max):\n            data_list.append((line_num,text))\n            return\n\n        # scan text looking for the wrap point, keeping track if the wrap\n        # point is inside markers\n        i = 0\n        n = 0\n        mark = ''\n        while n < max and i < size:\n            if text[i] == '\\0':\n                i += 1\n                mark = text[i]\n                i += 1\n            elif text[i] == '\\1':\n                i += 1\n                mark = ''\n            else:\n                i += 1\n                n += 1\n\n        # wrap point is inside text, break it up into separate lines\n        line1 = text[:i]\n        line2 = text[i:]\n\n        # if wrap point is inside markers, place end marker at end of first\n        # line and start marker at beginning of second line because each\n        # line will have its own table tag markup around it.\n        if mark:\n            line1 = line1 + '\\1'\n            line2 = '\\0' + mark + line2\n\n        # tack on first line onto the output list\n        data_list.append((line_num,line1))\n\n        # use this routine again to wrap the remaining text\n        self._split_line(data_list,'>',line2)\n\n    def _line_wrapper(self,diffs):\n        \"\"\"Returns iterator that splits (wraps) mdiff text lines\"\"\"\n\n        # pull from/to data and flags from mdiff iterator\n        for fromdata,todata,flag in diffs:\n            # check for context separators and pass them through\n            if flag is None:\n                yield fromdata,todata,flag\n                continue\n            (fromline,fromtext),(toline,totext) = fromdata,todata\n            # for each from/to line split it at the wrap column to form\n            # list of text lines.\n            fromlist,tolist = [],[]\n            self._split_line(fromlist,fromline,fromtext)\n            self._split_line(tolist,toline,totext)\n            # yield from/to line in pairs inserting blank lines as\n            # necessary when one side has more wrapped lines\n            while fromlist or tolist:\n                if fromlist:\n                    fromdata = fromlist.pop(0)\n                else:\n                    fromdata = ('',' ')\n                if tolist:\n                    todata = tolist.pop(0)\n                else:\n                    todata = ('',' ')\n                yield fromdata,todata,flag\n\n    def _collect_lines(self,diffs):\n        \"\"\"Collects mdiff output into separate lists\n\n        Before storing the mdiff from/to data into a list, it is converted\n        into a single line of text with HTML markup.\n        \"\"\"\n\n        fromlist,tolist,flaglist = [],[],[]\n        # pull from/to data and flags from mdiff style iterator\n        for fromdata,todata,flag in diffs:\n            try:\n                # store HTML markup of the lines into the lists\n                fromlist.append(self._format_line(0,flag,*fromdata))\n                tolist.append(self._format_line(1,flag,*todata))\n            except TypeError:\n                # exceptions occur for lines where context separators go\n                fromlist.append(None)\n                tolist.append(None)\n            flaglist.append(flag)\n        return fromlist,tolist,flaglist\n\n    def _format_line(self,side,flag,linenum,text):\n        \"\"\"Returns HTML markup of \"from\" / \"to\" text lines\n\n        side -- 0 or 1 indicating \"from\" or \"to\" text\n        flag -- indicates if difference on line\n        linenum -- line number (used for line number column)\n        text -- line text to be marked up\n        \"\"\"\n        try:\n            linenum = '%d' % linenum\n            id = ' id=\"%s%s\"' % (self._prefix[side],linenum)\n        except TypeError:\n            # handle blank lines where linenum is '>' or ''\n            id = ''\n        # replace those things that would get confused with HTML symbols\n        text=text.replace(\"&\",\"&amp;\").replace(\">\",\"&gt;\").replace(\"<\",\"&lt;\")\n\n        # make space non-breakable so they don't get compressed or line wrapped\n        text = text.replace(' ','&nbsp;').rstrip()\n\n        return '<td class=\"diff_header\"%s>%s</td><td nowrap=\"nowrap\">%s</td>' \\\n               % (id,linenum,text)\n\n    def _make_prefix(self):\n        \"\"\"Create unique anchor prefixes\"\"\"\n\n        # Generate a unique anchor prefix so multiple tables\n        # can exist on the same HTML page without conflicts.\n        fromprefix = \"from%d_\" % HtmlDiff._default_prefix\n        toprefix = \"to%d_\" % HtmlDiff._default_prefix\n        HtmlDiff._default_prefix += 1\n        # store prefixes so line format method has access\n        self._prefix = [fromprefix,toprefix]\n\n    def _convert_flags(self,fromlist,tolist,flaglist,context,numlines):\n        \"\"\"Makes list of \"next\" links\"\"\"\n\n        # all anchor names will be generated using the unique \"to\" prefix\n        toprefix = self._prefix[1]\n\n        # process change flags, generating middle column of next anchors/links\n        next_id = ['']*len(flaglist)\n        next_href = ['']*len(flaglist)\n        num_chg, in_change = 0, False\n        last = 0\n        for i,flag in enumerate(flaglist):\n            if flag:\n                if not in_change:\n                    in_change = True\n                    last = i\n                    # at the beginning of a change, drop an anchor a few lines\n                    # (the context lines) before the change for the previous\n                    # link\n                    i = max([0,i-numlines])\n                    next_id[i] = ' id=\"difflib_chg_%s_%d\"' % (toprefix,num_chg)\n                    # at the beginning of a change, drop a link to the next\n                    # change\n                    num_chg += 1\n                    next_href[last] = '<a href=\"#difflib_chg_%s_%d\">n</a>' % (\n                         toprefix,num_chg)\n            else:\n                in_change = False\n        # check for cases where there is no content to avoid exceptions\n        if not flaglist:\n            flaglist = [False]\n            next_id = ['']\n            next_href = ['']\n            last = 0\n            if context:\n                fromlist = ['<td></td><td>&nbsp;No Differences Found&nbsp;</td>']\n                tolist = fromlist\n            else:\n                fromlist = tolist = ['<td></td><td>&nbsp;Empty File&nbsp;</td>']\n        # if not a change on first line, drop a link\n        if not flaglist[0]:\n            next_href[0] = '<a href=\"#difflib_chg_%s_0\">f</a>' % toprefix\n        # redo the last link to link to the top\n        next_href[last] = '<a href=\"#difflib_chg_%s_top\">t</a>' % (toprefix)\n\n        return fromlist,tolist,flaglist,next_href,next_id\n\n    def make_table(self,fromlines,tolines,fromdesc='',todesc='',context=False,\n                   numlines=5):\n        \"\"\"Returns HTML table of side by side comparison with change highlights\n\n        Arguments:\n        fromlines -- list of \"from\" lines\n        tolines -- list of \"to\" lines\n        fromdesc -- \"from\" file column header string\n        todesc -- \"to\" file column header string\n        context -- set to True for contextual differences (defaults to False\n            which shows full differences).\n        numlines -- number of context lines.  When context is set True,\n            controls number of lines displayed before and after the change.\n            When context is False, controls the number of lines to place\n            the \"next\" link anchors before the next change (so click of\n            \"next\" link jumps to just before the change).\n        \"\"\"\n\n        # make unique anchor prefixes so that multiple tables may exist\n        # on the same page without conflict.\n        self._make_prefix()\n\n        # change tabs to spaces before it gets more difficult after we insert\n        # markup\n        fromlines,tolines = self._tab_newline_replace(fromlines,tolines)\n\n        # create diffs iterator which generates side by side from/to data\n        if context:\n            context_lines = numlines\n        else:\n            context_lines = None\n        diffs = _mdiff(fromlines,tolines,context_lines,linejunk=self._linejunk,\n                      charjunk=self._charjunk)\n\n        # set up iterator to wrap lines that exceed desired width\n        if self._wrapcolumn:\n            diffs = self._line_wrapper(diffs)\n\n        # collect up from/to lines and flags into lists (also format the lines)\n        fromlist,tolist,flaglist = self._collect_lines(diffs)\n\n        # process change flags, generating middle column of next anchors/links\n        fromlist,tolist,flaglist,next_href,next_id = self._convert_flags(\n            fromlist,tolist,flaglist,context,numlines)\n\n        s = []\n        fmt = '            <tr><td class=\"diff_next\"%s>%s</td>%s' + \\\n              '<td class=\"diff_next\">%s</td>%s</tr>\\n'\n        for i in range(len(flaglist)):\n            if flaglist[i] is None:\n                # mdiff yields None on separator lines skip the bogus ones\n                # generated for the first line\n                if i > 0:\n                    s.append('        </tbody>        \\n        <tbody>\\n')\n            else:\n                s.append( fmt % (next_id[i],next_href[i],fromlist[i],\n                                           next_href[i],tolist[i]))\n        if fromdesc or todesc:\n            header_row = '<thead><tr>%s%s%s%s</tr></thead>' % (\n                '<th class=\"diff_next\"><br /></th>',\n                '<th colspan=\"2\" class=\"diff_header\">%s</th>' % fromdesc,\n                '<th class=\"diff_next\"><br /></th>',\n                '<th colspan=\"2\" class=\"diff_header\">%s</th>' % todesc)\n        else:\n            header_row = ''\n\n        table = self._table_template % dict(\n            data_rows=''.join(s),\n            header_row=header_row,\n            prefix=self._prefix[1])\n\n        return table.replace('\\0+','<span class=\"diff_add\">'). \\\n                     replace('\\0-','<span class=\"diff_sub\">'). \\\n                     replace('\\0^','<span class=\"diff_chg\">'). \\\n                     replace('\\1','</span>'). \\\n                     replace('\\t','&nbsp;')\n\ndel re\n\ndef restore(delta, which):\n    r\"\"\"\n    Generate one of the two sequences that generated a delta.\n\n    Given a `delta` produced by `Differ.compare()` or `ndiff()`, extract\n    lines originating from file 1 or 2 (parameter `which`), stripping off line\n    prefixes.\n\n    Examples:\n\n    >>> diff = ndiff('one\\ntwo\\nthree\\n'.splitlines(1),\n    ...              'ore\\ntree\\nemu\\n'.splitlines(1))\n    >>> diff = list(diff)\n    >>> print ''.join(restore(diff, 1)),\n    one\n    two\n    three\n    >>> print ''.join(restore(diff, 2)),\n    ore\n    tree\n    emu\n    \"\"\"\n    try:\n        tag = {1: \"- \", 2: \"+ \"}[int(which)]\n    except KeyError:\n        raise ValueError, ('unknown delta choice (must be 1 or 2): %r'\n                           % which)\n    prefixes = (\"  \", tag)\n    for line in delta:\n        if line[:2] in prefixes:\n            yield line[2:]\n\ndef _test():\n    import doctest, difflib\n    return doctest.testmod(difflib)\n\nif __name__ == \"__main__\":\n    _test()\n", 
    "dis": "\"\"\"Disassembler of Python byte code into mnemonics.\"\"\"\n\nimport sys\nimport types\n\nfrom opcode import *\nfrom opcode import __all__ as _opcodes_all\n\n__all__ = [\"dis\", \"disassemble\", \"distb\", \"disco\",\n           \"findlinestarts\", \"findlabels\"] + _opcodes_all\ndel _opcodes_all\n\n_have_code = (types.MethodType, types.FunctionType, types.CodeType,\n              types.ClassType, type)\n\ndef dis(x=None):\n    \"\"\"Disassemble classes, methods, functions, or code.\n\n    With no argument, disassemble the last traceback.\n\n    \"\"\"\n    if x is None:\n        distb()\n        return\n    if isinstance(x, types.InstanceType):\n        x = x.__class__\n    if hasattr(x, 'im_func'):\n        x = x.im_func\n    if hasattr(x, 'func_code'):\n        x = x.func_code\n    if hasattr(x, '__dict__'):\n        items = x.__dict__.items()\n        items.sort()\n        for name, x1 in items:\n            if isinstance(x1, _have_code):\n                print \"Disassembly of %s:\" % name\n                try:\n                    dis(x1)\n                except TypeError, msg:\n                    print \"Sorry:\", msg\n                print\n    elif hasattr(x, 'co_code'):\n        disassemble(x)\n    elif isinstance(x, str):\n        disassemble_string(x)\n    else:\n        raise TypeError, \\\n              \"don't know how to disassemble %s objects\" % \\\n              type(x).__name__\n\ndef distb(tb=None):\n    \"\"\"Disassemble a traceback (default: last traceback).\"\"\"\n    if tb is None:\n        try:\n            tb = sys.last_traceback\n        except AttributeError:\n            raise RuntimeError, \"no last traceback to disassemble\"\n        while tb.tb_next: tb = tb.tb_next\n    disassemble(tb.tb_frame.f_code, tb.tb_lasti)\n\ndef disassemble(co, lasti=-1):\n    \"\"\"Disassemble a code object.\"\"\"\n    code = co.co_code\n    labels = findlabels(code)\n    linestarts = dict(findlinestarts(co))\n    n = len(code)\n    i = 0\n    extended_arg = 0\n    free = None\n    while i < n:\n        c = code[i]\n        op = ord(c)\n        if i in linestarts:\n            if i > 0:\n                print\n            print \"%3d\" % linestarts[i],\n        else:\n            print '   ',\n\n        if i == lasti: print '-->',\n        else: print '   ',\n        if i in labels: print '>>',\n        else: print '  ',\n        print repr(i).rjust(4),\n        print opname[op].ljust(20),\n        i = i+1\n        if op >= HAVE_ARGUMENT:\n            oparg = ord(code[i]) + ord(code[i+1])*256 + extended_arg\n            extended_arg = 0\n            i = i+2\n            if op == EXTENDED_ARG:\n                extended_arg = oparg*65536L\n            print repr(oparg).rjust(5),\n            if op in hasconst:\n                print '(' + repr(co.co_consts[oparg]) + ')',\n            elif op in hasname:\n                print '(' + co.co_names[oparg] + ')',\n            elif op in hasjrel:\n                print '(to ' + repr(i + oparg) + ')',\n            elif op in haslocal:\n                print '(' + co.co_varnames[oparg] + ')',\n            elif op in hascompare:\n                print '(' + cmp_op[oparg] + ')',\n            elif op in hasfree:\n                if free is None:\n                    free = co.co_cellvars + co.co_freevars\n                print '(' + free[oparg] + ')',\n        print\n\ndef disassemble_string(code, lasti=-1, varnames=None, names=None,\n                       constants=None):\n    labels = findlabels(code)\n    n = len(code)\n    i = 0\n    while i < n:\n        c = code[i]\n        op = ord(c)\n        if i == lasti: print '-->',\n        else: print '   ',\n        if i in labels: print '>>',\n        else: print '  ',\n        print repr(i).rjust(4),\n        print opname[op].ljust(15),\n        i = i+1\n        if op >= HAVE_ARGUMENT:\n            oparg = ord(code[i]) + ord(code[i+1])*256\n            i = i+2\n            print repr(oparg).rjust(5),\n            if op in hasconst:\n                if constants:\n                    print '(' + repr(constants[oparg]) + ')',\n                else:\n                    print '(%d)'%oparg,\n            elif op in hasname:\n                if names is not None:\n                    print '(' + names[oparg] + ')',\n                else:\n                    print '(%d)'%oparg,\n            elif op in hasjrel:\n                print '(to ' + repr(i + oparg) + ')',\n            elif op in haslocal:\n                if varnames:\n                    print '(' + varnames[oparg] + ')',\n                else:\n                    print '(%d)' % oparg,\n            elif op in hascompare:\n                print '(' + cmp_op[oparg] + ')',\n        print\n\ndisco = disassemble                     # XXX For backwards compatibility\n\ndef findlabels(code):\n    \"\"\"Detect all offsets in a byte code which are jump targets.\n\n    Return the list of offsets.\n\n    \"\"\"\n    labels = []\n    n = len(code)\n    i = 0\n    while i < n:\n        c = code[i]\n        op = ord(c)\n        i = i+1\n        if op >= HAVE_ARGUMENT:\n            oparg = ord(code[i]) + ord(code[i+1])*256\n            i = i+2\n            label = -1\n            if op in hasjrel:\n                label = i+oparg\n            elif op in hasjabs:\n                label = oparg\n            if label >= 0:\n                if label not in labels:\n                    labels.append(label)\n    return labels\n\ndef findlinestarts(code):\n    \"\"\"Find the offsets in a byte code which are start of lines in the source.\n\n    Generate pairs (offset, lineno) as described in Python/compile.c.\n\n    \"\"\"\n    byte_increments = [ord(c) for c in code.co_lnotab[0::2]]\n    line_increments = [ord(c) for c in code.co_lnotab[1::2]]\n\n    lastlineno = None\n    lineno = code.co_firstlineno\n    addr = 0\n    for byte_incr, line_incr in zip(byte_increments, line_increments):\n        if byte_incr:\n            if lineno != lastlineno:\n                yield (addr, lineno)\n                lastlineno = lineno\n            addr += byte_incr\n        lineno += line_incr\n    if lineno != lastlineno:\n        yield (addr, lineno)\n\ndef _test():\n    \"\"\"Simple test program to disassemble a file.\"\"\"\n    if sys.argv[1:]:\n        if sys.argv[2:]:\n            sys.stderr.write(\"usage: python dis.py [-|file]\\n\")\n            sys.exit(2)\n        fn = sys.argv[1]\n        if not fn or fn == \"-\":\n            fn = None\n    else:\n        fn = None\n    if fn is None:\n        f = sys.stdin\n    else:\n        f = open(fn)\n    source = f.read()\n    if fn is not None:\n        f.close()\n    else:\n        fn = \"<stdin>\"\n    code = compile(source, fn, \"exec\")\n    dis(code)\n\nif __name__ == \"__main__\":\n    _test()\n", 
    "doctest": "# Module doctest.\n# Released to the public domain 16-Jan-2001, by Tim Peters (tim@python.org).\n# Major enhancements and refactoring by:\n#     Jim Fulton\n#     Edward Loper\n\n# Provided as-is; use at your own risk; no warranty; no promises; enjoy!\n\nr\"\"\"Module doctest -- a framework for running examples in docstrings.\n\nIn simplest use, end each module M to be tested with:\n\ndef _test():\n    import doctest\n    doctest.testmod()\n\nif __name__ == \"__main__\":\n    _test()\n\nThen running the module as a script will cause the examples in the\ndocstrings to get executed and verified:\n\npython M.py\n\nThis won't display anything unless an example fails, in which case the\nfailing example(s) and the cause(s) of the failure(s) are printed to stdout\n(why not stderr? because stderr is a lame hack <0.2 wink>), and the final\nline of output is \"Test failed.\".\n\nRun it with the -v switch instead:\n\npython M.py -v\n\nand a detailed report of all examples tried is printed to stdout, along\nwith assorted summaries at the end.\n\nYou can force verbose mode by passing \"verbose=True\" to testmod, or prohibit\nit by passing \"verbose=False\".  In either of those cases, sys.argv is not\nexamined by testmod.\n\nThere are a variety of other ways to run doctests, including integration\nwith the unittest framework, and support for running non-Python text\nfiles containing doctests.  There are also many ways to override parts\nof doctest's default behaviors.  See the Library Reference Manual for\ndetails.\n\"\"\"\n\n__docformat__ = 'reStructuredText en'\n\n__all__ = [\n    # 0, Option Flags\n    'register_optionflag',\n    'DONT_ACCEPT_TRUE_FOR_1',\n    'DONT_ACCEPT_BLANKLINE',\n    'NORMALIZE_WHITESPACE',\n    'ELLIPSIS',\n    'SKIP',\n    'IGNORE_EXCEPTION_DETAIL',\n    'COMPARISON_FLAGS',\n    'REPORT_UDIFF',\n    'REPORT_CDIFF',\n    'REPORT_NDIFF',\n    'REPORT_ONLY_FIRST_FAILURE',\n    'REPORTING_FLAGS',\n    # 1. Utility Functions\n    # 2. Example & DocTest\n    'Example',\n    'DocTest',\n    # 3. Doctest Parser\n    'DocTestParser',\n    # 4. Doctest Finder\n    'DocTestFinder',\n    # 5. Doctest Runner\n    'DocTestRunner',\n    'OutputChecker',\n    'DocTestFailure',\n    'UnexpectedException',\n    'DebugRunner',\n    # 6. Test Functions\n    'testmod',\n    'testfile',\n    'run_docstring_examples',\n    # 7. Tester\n    'Tester',\n    # 8. Unittest Support\n    'DocTestSuite',\n    'DocFileSuite',\n    'set_unittest_reportflags',\n    # 9. Debugging Support\n    'script_from_examples',\n    'testsource',\n    'debug_src',\n    'debug',\n]\n\nimport __future__\n\nimport sys, traceback, inspect, linecache, os, re\nimport unittest, difflib, pdb, tempfile\nimport warnings\nfrom StringIO import StringIO\nfrom collections import namedtuple\n\nTestResults = namedtuple('TestResults', 'failed attempted')\n\n# There are 4 basic classes:\n#  - Example: a <source, want> pair, plus an intra-docstring line number.\n#  - DocTest: a collection of examples, parsed from a docstring, plus\n#    info about where the docstring came from (name, filename, lineno).\n#  - DocTestFinder: extracts DocTests from a given object's docstring and\n#    its contained objects' docstrings.\n#  - DocTestRunner: runs DocTest cases, and accumulates statistics.\n#\n# So the basic picture is:\n#\n#                             list of:\n# +------+                   +---------+                   +-------+\n# |object| --DocTestFinder-> | DocTest | --DocTestRunner-> |results|\n# +------+                   +---------+                   +-------+\n#                            | Example |\n#                            |   ...   |\n#                            | Example |\n#                            +---------+\n\n# Option constants.\n\nOPTIONFLAGS_BY_NAME = {}\ndef register_optionflag(name):\n    # Create a new flag unless `name` is already known.\n    return OPTIONFLAGS_BY_NAME.setdefault(name, 1 << len(OPTIONFLAGS_BY_NAME))\n\nDONT_ACCEPT_TRUE_FOR_1 = register_optionflag('DONT_ACCEPT_TRUE_FOR_1')\nDONT_ACCEPT_BLANKLINE = register_optionflag('DONT_ACCEPT_BLANKLINE')\nNORMALIZE_WHITESPACE = register_optionflag('NORMALIZE_WHITESPACE')\nELLIPSIS = register_optionflag('ELLIPSIS')\nSKIP = register_optionflag('SKIP')\nIGNORE_EXCEPTION_DETAIL = register_optionflag('IGNORE_EXCEPTION_DETAIL')\n\nCOMPARISON_FLAGS = (DONT_ACCEPT_TRUE_FOR_1 |\n                    DONT_ACCEPT_BLANKLINE |\n                    NORMALIZE_WHITESPACE |\n                    ELLIPSIS |\n                    SKIP |\n                    IGNORE_EXCEPTION_DETAIL)\n\nREPORT_UDIFF = register_optionflag('REPORT_UDIFF')\nREPORT_CDIFF = register_optionflag('REPORT_CDIFF')\nREPORT_NDIFF = register_optionflag('REPORT_NDIFF')\nREPORT_ONLY_FIRST_FAILURE = register_optionflag('REPORT_ONLY_FIRST_FAILURE')\n\nREPORTING_FLAGS = (REPORT_UDIFF |\n                   REPORT_CDIFF |\n                   REPORT_NDIFF |\n                   REPORT_ONLY_FIRST_FAILURE)\n\n# Special string markers for use in `want` strings:\nBLANKLINE_MARKER = '<BLANKLINE>'\nELLIPSIS_MARKER = '...'\n\n######################################################################\n## Table of Contents\n######################################################################\n#  1. Utility Functions\n#  2. Example & DocTest -- store test cases\n#  3. DocTest Parser -- extracts examples from strings\n#  4. DocTest Finder -- extracts test cases from objects\n#  5. DocTest Runner -- runs test cases\n#  6. Test Functions -- convenient wrappers for testing\n#  7. Tester Class -- for backwards compatibility\n#  8. Unittest Support\n#  9. Debugging Support\n# 10. Example Usage\n\n######################################################################\n## 1. Utility Functions\n######################################################################\n\ndef _extract_future_flags(globs):\n    \"\"\"\n    Return the compiler-flags associated with the future features that\n    have been imported into the given namespace (globs).\n    \"\"\"\n    flags = 0\n    for fname in __future__.all_feature_names:\n        feature = globs.get(fname, None)\n        if feature is getattr(__future__, fname):\n            flags |= feature.compiler_flag\n    return flags\n\ndef _normalize_module(module, depth=2):\n    \"\"\"\n    Return the module specified by `module`.  In particular:\n      - If `module` is a module, then return module.\n      - If `module` is a string, then import and return the\n        module with that name.\n      - If `module` is None, then return the calling module.\n        The calling module is assumed to be the module of\n        the stack frame at the given depth in the call stack.\n    \"\"\"\n    if inspect.ismodule(module):\n        return module\n    elif isinstance(module, (str, unicode)):\n        return __import__(module, globals(), locals(), [\"*\"])\n    elif module is None:\n        return sys.modules[sys._getframe(depth).f_globals['__name__']]\n    else:\n        raise TypeError(\"Expected a module, string, or None\")\n\ndef _load_testfile(filename, package, module_relative):\n    if module_relative:\n        package = _normalize_module(package, 3)\n        filename = _module_relative_path(package, filename)\n        if hasattr(package, '__loader__'):\n            if hasattr(package.__loader__, 'get_data'):\n                file_contents = package.__loader__.get_data(filename)\n                # get_data() opens files as 'rb', so one must do the equivalent\n                # conversion as universal newlines would do.\n                return file_contents.replace(os.linesep, '\\n'), filename\n    with open(filename, 'U') as f:\n        return f.read(), filename\n\n# Use sys.stdout encoding for ouput.\n_encoding = getattr(sys.__stdout__, 'encoding', None) or 'utf-8'\n\ndef _indent(s, indent=4):\n    \"\"\"\n    Add the given number of space characters to the beginning of\n    every non-blank line in `s`, and return the result.\n    If the string `s` is Unicode, it is encoded using the stdout\n    encoding and the `backslashreplace` error handler.\n    \"\"\"\n    if isinstance(s, unicode):\n        s = s.encode(_encoding, 'backslashreplace')\n    # This regexp matches the start of non-blank lines:\n    return re.sub('(?m)^(?!$)', indent*' ', s)\n\ndef _exception_traceback(exc_info):\n    \"\"\"\n    Return a string containing a traceback message for the given\n    exc_info tuple (as returned by sys.exc_info()).\n    \"\"\"\n    # Get a traceback message.\n    excout = StringIO()\n    exc_type, exc_val, exc_tb = exc_info\n    traceback.print_exception(exc_type, exc_val, exc_tb, file=excout)\n    return excout.getvalue()\n\n# Override some StringIO methods.\nclass _SpoofOut(StringIO):\n    def getvalue(self):\n        result = StringIO.getvalue(self)\n        # If anything at all was written, make sure there's a trailing\n        # newline.  There's no way for the expected output to indicate\n        # that a trailing newline is missing.\n        if result and not result.endswith(\"\\n\"):\n            result += \"\\n\"\n        # Prevent softspace from screwing up the next test case, in\n        # case they used print with a trailing comma in an example.\n        if hasattr(self, \"softspace\"):\n            del self.softspace\n        return result\n\n    def truncate(self,   size=None):\n        StringIO.truncate(self, size)\n        if hasattr(self, \"softspace\"):\n            del self.softspace\n        if not self.buf:\n            # Reset it to an empty string, to make sure it's not unicode.\n            self.buf = ''\n\n# Worst-case linear-time ellipsis matching.\ndef _ellipsis_match(want, got):\n    \"\"\"\n    Essentially the only subtle case:\n    >>> _ellipsis_match('aa...aa', 'aaa')\n    False\n    \"\"\"\n    if ELLIPSIS_MARKER not in want:\n        return want == got\n\n    # Find \"the real\" strings.\n    ws = want.split(ELLIPSIS_MARKER)\n    assert len(ws) >= 2\n\n    # Deal with exact matches possibly needed at one or both ends.\n    startpos, endpos = 0, len(got)\n    w = ws[0]\n    if w:   # starts with exact match\n        if got.startswith(w):\n            startpos = len(w)\n            del ws[0]\n        else:\n            return False\n    w = ws[-1]\n    if w:   # ends with exact match\n        if got.endswith(w):\n            endpos -= len(w)\n            del ws[-1]\n        else:\n            return False\n\n    if startpos > endpos:\n        # Exact end matches required more characters than we have, as in\n        # _ellipsis_match('aa...aa', 'aaa')\n        return False\n\n    # For the rest, we only need to find the leftmost non-overlapping\n    # match for each piece.  If there's no overall match that way alone,\n    # there's no overall match period.\n    for w in ws:\n        # w may be '' at times, if there are consecutive ellipses, or\n        # due to an ellipsis at the start or end of `want`.  That's OK.\n        # Search for an empty string succeeds, and doesn't change startpos.\n        startpos = got.find(w, startpos, endpos)\n        if startpos < 0:\n            return False\n        startpos += len(w)\n\n    return True\n\ndef _comment_line(line):\n    \"Return a commented form of the given line\"\n    line = line.rstrip()\n    if line:\n        return '# '+line\n    else:\n        return '#'\n\ndef _strip_exception_details(msg):\n    # Support for IGNORE_EXCEPTION_DETAIL.\n    # Get rid of everything except the exception name; in particular, drop\n    # the possibly dotted module path (if any) and the exception message (if\n    # any).  We assume that a colon is never part of a dotted name, or of an\n    # exception name.\n    # E.g., given\n    #    \"foo.bar.MyError: la di da\"\n    # return \"MyError\"\n    # Or for \"abc.def\" or \"abc.def:\\n\" return \"def\".\n\n    start, end = 0, len(msg)\n    # The exception name must appear on the first line.\n    i = msg.find(\"\\n\")\n    if i >= 0:\n        end = i\n    # retain up to the first colon (if any)\n    i = msg.find(':', 0, end)\n    if i >= 0:\n        end = i\n    # retain just the exception name\n    i = msg.rfind('.', 0, end)\n    if i >= 0:\n        start = i+1\n    return msg[start: end]\n\nclass _OutputRedirectingPdb(pdb.Pdb):\n    \"\"\"\n    A specialized version of the python debugger that redirects stdout\n    to a given stream when interacting with the user.  Stdout is *not*\n    redirected when traced code is executed.\n    \"\"\"\n    def __init__(self, out):\n        self.__out = out\n        self.__debugger_used = False\n        pdb.Pdb.__init__(self, stdout=out)\n        # still use input() to get user input\n        self.use_rawinput = 1\n\n    def set_trace(self, frame=None):\n        self.__debugger_used = True\n        if frame is None:\n            frame = sys._getframe().f_back\n        pdb.Pdb.set_trace(self, frame)\n\n    def set_continue(self):\n        # Calling set_continue unconditionally would break unit test\n        # coverage reporting, as Bdb.set_continue calls sys.settrace(None).\n        if self.__debugger_used:\n            pdb.Pdb.set_continue(self)\n\n    def trace_dispatch(self, *args):\n        # Redirect stdout to the given stream.\n        save_stdout = sys.stdout\n        sys.stdout = self.__out\n        # Call Pdb's trace dispatch method.\n        try:\n            return pdb.Pdb.trace_dispatch(self, *args)\n        finally:\n            sys.stdout = save_stdout\n\n# [XX] Normalize with respect to os.path.pardir?\ndef _module_relative_path(module, path):\n    if not inspect.ismodule(module):\n        raise TypeError, 'Expected a module: %r' % module\n    if path.startswith('/'):\n        raise ValueError, 'Module-relative files may not have absolute paths'\n\n    # Find the base directory for the path.\n    if hasattr(module, '__file__'):\n        # A normal module/package\n        basedir = os.path.split(module.__file__)[0]\n    elif module.__name__ == '__main__':\n        # An interactive session.\n        if len(sys.argv)>0 and sys.argv[0] != '':\n            basedir = os.path.split(sys.argv[0])[0]\n        else:\n            basedir = os.curdir\n    else:\n        # A module w/o __file__ (this includes builtins)\n        raise ValueError(\"Can't resolve paths relative to the module \" +\n                         module + \" (it has no __file__)\")\n\n    # Combine the base directory and the path.\n    return os.path.join(basedir, *(path.split('/')))\n\n######################################################################\n## 2. Example & DocTest\n######################################################################\n## - An \"example\" is a <source, want> pair, where \"source\" is a\n##   fragment of source code, and \"want\" is the expected output for\n##   \"source.\"  The Example class also includes information about\n##   where the example was extracted from.\n##\n## - A \"doctest\" is a collection of examples, typically extracted from\n##   a string (such as an object's docstring).  The DocTest class also\n##   includes information about where the string was extracted from.\n\nclass Example:\n    \"\"\"\n    A single doctest example, consisting of source code and expected\n    output.  `Example` defines the following attributes:\n\n      - source: A single Python statement, always ending with a newline.\n        The constructor adds a newline if needed.\n\n      - want: The expected output from running the source code (either\n        from stdout, or a traceback in case of exception).  `want` ends\n        with a newline unless it's empty, in which case it's an empty\n        string.  The constructor adds a newline if needed.\n\n      - exc_msg: The exception message generated by the example, if\n        the example is expected to generate an exception; or `None` if\n        it is not expected to generate an exception.  This exception\n        message is compared against the return value of\n        `traceback.format_exception_only()`.  `exc_msg` ends with a\n        newline unless it's `None`.  The constructor adds a newline\n        if needed.\n\n      - lineno: The line number within the DocTest string containing\n        this Example where the Example begins.  This line number is\n        zero-based, with respect to the beginning of the DocTest.\n\n      - indent: The example's indentation in the DocTest string.\n        I.e., the number of space characters that precede the\n        example's first prompt.\n\n      - options: A dictionary mapping from option flags to True or\n        False, which is used to override default options for this\n        example.  Any option flags not contained in this dictionary\n        are left at their default value (as specified by the\n        DocTestRunner's optionflags).  By default, no options are set.\n    \"\"\"\n    def __init__(self, source, want, exc_msg=None, lineno=0, indent=0,\n                 options=None):\n        # Normalize inputs.\n        if not source.endswith('\\n'):\n            source += '\\n'\n        if want and not want.endswith('\\n'):\n            want += '\\n'\n        if exc_msg is not None and not exc_msg.endswith('\\n'):\n            exc_msg += '\\n'\n        # Store properties.\n        self.source = source\n        self.want = want\n        self.lineno = lineno\n        self.indent = indent\n        if options is None: options = {}\n        self.options = options\n        self.exc_msg = exc_msg\n\n    def __eq__(self, other):\n        if type(self) is not type(other):\n            return NotImplemented\n\n        return self.source == other.source and \\\n               self.want == other.want and \\\n               self.lineno == other.lineno and \\\n               self.indent == other.indent and \\\n               self.options == other.options and \\\n               self.exc_msg == other.exc_msg\n\n    def __ne__(self, other):\n        return not self == other\n\n    def __hash__(self):\n        return hash((self.source, self.want, self.lineno, self.indent,\n                     self.exc_msg))\n\n\nclass DocTest:\n    \"\"\"\n    A collection of doctest examples that should be run in a single\n    namespace.  Each `DocTest` defines the following attributes:\n\n      - examples: the list of examples.\n\n      - globs: The namespace (aka globals) that the examples should\n        be run in.\n\n      - name: A name identifying the DocTest (typically, the name of\n        the object whose docstring this DocTest was extracted from).\n\n      - filename: The name of the file that this DocTest was extracted\n        from, or `None` if the filename is unknown.\n\n      - lineno: The line number within filename where this DocTest\n        begins, or `None` if the line number is unavailable.  This\n        line number is zero-based, with respect to the beginning of\n        the file.\n\n      - docstring: The string that the examples were extracted from,\n        or `None` if the string is unavailable.\n    \"\"\"\n    def __init__(self, examples, globs, name, filename, lineno, docstring):\n        \"\"\"\n        Create a new DocTest containing the given examples.  The\n        DocTest's globals are initialized with a copy of `globs`.\n        \"\"\"\n        assert not isinstance(examples, basestring), \\\n               \"DocTest no longer accepts str; use DocTestParser instead\"\n        self.examples = examples\n        self.docstring = docstring\n        self.globs = globs.copy()\n        self.name = name\n        self.filename = filename\n        self.lineno = lineno\n\n    def __repr__(self):\n        if len(self.examples) == 0:\n            examples = 'no examples'\n        elif len(self.examples) == 1:\n            examples = '1 example'\n        else:\n            examples = '%d examples' % len(self.examples)\n        return ('<DocTest %s from %s:%s (%s)>' %\n                (self.name, self.filename, self.lineno, examples))\n\n    def __eq__(self, other):\n        if type(self) is not type(other):\n            return NotImplemented\n\n        return self.examples == other.examples and \\\n               self.docstring == other.docstring and \\\n               self.globs == other.globs and \\\n               self.name == other.name and \\\n               self.filename == other.filename and \\\n               self.lineno == other.lineno\n\n    def __ne__(self, other):\n        return not self == other\n\n    def __hash__(self):\n        return hash((self.docstring, self.name, self.filename, self.lineno))\n\n    # This lets us sort tests by name:\n    def __cmp__(self, other):\n        if not isinstance(other, DocTest):\n            return -1\n        return cmp((self.name, self.filename, self.lineno, id(self)),\n                   (other.name, other.filename, other.lineno, id(other)))\n\n######################################################################\n## 3. DocTestParser\n######################################################################\n\nclass DocTestParser:\n    \"\"\"\n    A class used to parse strings containing doctest examples.\n    \"\"\"\n    # This regular expression is used to find doctest examples in a\n    # string.  It defines three groups: `source` is the source code\n    # (including leading indentation and prompts); `indent` is the\n    # indentation of the first (PS1) line of the source code; and\n    # `want` is the expected output (including leading indentation).\n    _EXAMPLE_RE = re.compile(r'''\n        # Source consists of a PS1 line followed by zero or more PS2 lines.\n        (?P<source>\n            (?:^(?P<indent> [ ]*) >>>    .*)    # PS1 line\n            (?:\\n           [ ]*  \\.\\.\\. .*)*)  # PS2 lines\n        \\n?\n        # Want consists of any non-blank lines that do not start with PS1.\n        (?P<want> (?:(?![ ]*$)    # Not a blank line\n                     (?![ ]*>>>)  # Not a line starting with PS1\n                     .+$\\n?       # But any other line\n                  )*)\n        ''', re.MULTILINE | re.VERBOSE)\n\n    # A regular expression for handling `want` strings that contain\n    # expected exceptions.  It divides `want` into three pieces:\n    #    - the traceback header line (`hdr`)\n    #    - the traceback stack (`stack`)\n    #    - the exception message (`msg`), as generated by\n    #      traceback.format_exception_only()\n    # `msg` may have multiple lines.  We assume/require that the\n    # exception message is the first non-indented line starting with a word\n    # character following the traceback header line.\n    _EXCEPTION_RE = re.compile(r\"\"\"\n        # Grab the traceback header.  Different versions of Python have\n        # said different things on the first traceback line.\n        ^(?P<hdr> Traceback\\ \\(\n            (?: most\\ recent\\ call\\ last\n            |   innermost\\ last\n            ) \\) :\n        )\n        \\s* $                # toss trailing whitespace on the header.\n        (?P<stack> .*?)      # don't blink: absorb stuff until...\n        ^ (?P<msg> \\w+ .*)   #     a line *starts* with alphanum.\n        \"\"\", re.VERBOSE | re.MULTILINE | re.DOTALL)\n\n    # A callable returning a true value iff its argument is a blank line\n    # or contains a single comment.\n    _IS_BLANK_OR_COMMENT = re.compile(r'^[ ]*(#.*)?$').match\n\n    def parse(self, string, name='<string>'):\n        \"\"\"\n        Divide the given string into examples and intervening text,\n        and return them as a list of alternating Examples and strings.\n        Line numbers for the Examples are 0-based.  The optional\n        argument `name` is a name identifying this string, and is only\n        used for error messages.\n        \"\"\"\n        string = string.expandtabs()\n        # If all lines begin with the same indentation, then strip it.\n        min_indent = self._min_indent(string)\n        if min_indent > 0:\n            string = '\\n'.join([l[min_indent:] for l in string.split('\\n')])\n\n        output = []\n        charno, lineno = 0, 0\n        # Find all doctest examples in the string:\n        for m in self._EXAMPLE_RE.finditer(string):\n            # Add the pre-example text to `output`.\n            output.append(string[charno:m.start()])\n            # Update lineno (lines before this example)\n            lineno += string.count('\\n', charno, m.start())\n            # Extract info from the regexp match.\n            (source, options, want, exc_msg) = \\\n                     self._parse_example(m, name, lineno)\n            # Create an Example, and add it to the list.\n            if not self._IS_BLANK_OR_COMMENT(source):\n                output.append( Example(source, want, exc_msg,\n                                    lineno=lineno,\n                                    indent=min_indent+len(m.group('indent')),\n                                    options=options) )\n            # Update lineno (lines inside this example)\n            lineno += string.count('\\n', m.start(), m.end())\n            # Update charno.\n            charno = m.end()\n        # Add any remaining post-example text to `output`.\n        output.append(string[charno:])\n        return output\n\n    def get_doctest(self, string, globs, name, filename, lineno):\n        \"\"\"\n        Extract all doctest examples from the given string, and\n        collect them into a `DocTest` object.\n\n        `globs`, `name`, `filename`, and `lineno` are attributes for\n        the new `DocTest` object.  See the documentation for `DocTest`\n        for more information.\n        \"\"\"\n        return DocTest(self.get_examples(string, name), globs,\n                       name, filename, lineno, string)\n\n    def get_examples(self, string, name='<string>'):\n        \"\"\"\n        Extract all doctest examples from the given string, and return\n        them as a list of `Example` objects.  Line numbers are\n        0-based, because it's most common in doctests that nothing\n        interesting appears on the same line as opening triple-quote,\n        and so the first interesting line is called \\\"line 1\\\" then.\n\n        The optional argument `name` is a name identifying this\n        string, and is only used for error messages.\n        \"\"\"\n        return [x for x in self.parse(string, name)\n                if isinstance(x, Example)]\n\n    def _parse_example(self, m, name, lineno):\n        \"\"\"\n        Given a regular expression match from `_EXAMPLE_RE` (`m`),\n        return a pair `(source, want)`, where `source` is the matched\n        example's source code (with prompts and indentation stripped);\n        and `want` is the example's expected output (with indentation\n        stripped).\n\n        `name` is the string's name, and `lineno` is the line number\n        where the example starts; both are used for error messages.\n        \"\"\"\n        # Get the example's indentation level.\n        indent = len(m.group('indent'))\n\n        # Divide source into lines; check that they're properly\n        # indented; and then strip their indentation & prompts.\n        source_lines = m.group('source').split('\\n')\n        self._check_prompt_blank(source_lines, indent, name, lineno)\n        self._check_prefix(source_lines[1:], ' '*indent + '.', name, lineno)\n        source = '\\n'.join([sl[indent+4:] for sl in source_lines])\n\n        # Divide want into lines; check that it's properly indented; and\n        # then strip the indentation.  Spaces before the last newline should\n        # be preserved, so plain rstrip() isn't good enough.\n        want = m.group('want')\n        want_lines = want.split('\\n')\n        if len(want_lines) > 1 and re.match(r' *$', want_lines[-1]):\n            del want_lines[-1]  # forget final newline & spaces after it\n        self._check_prefix(want_lines, ' '*indent, name,\n                           lineno + len(source_lines))\n        want = '\\n'.join([wl[indent:] for wl in want_lines])\n\n        # If `want` contains a traceback message, then extract it.\n        m = self._EXCEPTION_RE.match(want)\n        if m:\n            exc_msg = m.group('msg')\n        else:\n            exc_msg = None\n\n        # Extract options from the source.\n        options = self._find_options(source, name, lineno)\n\n        return source, options, want, exc_msg\n\n    # This regular expression looks for option directives in the\n    # source code of an example.  Option directives are comments\n    # starting with \"doctest:\".  Warning: this may give false\n    # positives for string-literals that contain the string\n    # \"#doctest:\".  Eliminating these false positives would require\n    # actually parsing the string; but we limit them by ignoring any\n    # line containing \"#doctest:\" that is *followed* by a quote mark.\n    _OPTION_DIRECTIVE_RE = re.compile(r'#\\s*doctest:\\s*([^\\n\\'\"]*)$',\n                                      re.MULTILINE)\n\n    def _find_options(self, source, name, lineno):\n        \"\"\"\n        Return a dictionary containing option overrides extracted from\n        option directives in the given source string.\n\n        `name` is the string's name, and `lineno` is the line number\n        where the example starts; both are used for error messages.\n        \"\"\"\n        options = {}\n        # (note: with the current regexp, this will match at most once:)\n        for m in self._OPTION_DIRECTIVE_RE.finditer(source):\n            option_strings = m.group(1).replace(',', ' ').split()\n            for option in option_strings:\n                if (option[0] not in '+-' or\n                    option[1:] not in OPTIONFLAGS_BY_NAME):\n                    raise ValueError('line %r of the doctest for %s '\n                                     'has an invalid option: %r' %\n                                     (lineno+1, name, option))\n                flag = OPTIONFLAGS_BY_NAME[option[1:]]\n                options[flag] = (option[0] == '+')\n        if options and self._IS_BLANK_OR_COMMENT(source):\n            raise ValueError('line %r of the doctest for %s has an option '\n                             'directive on a line with no example: %r' %\n                             (lineno, name, source))\n        return options\n\n    # This regular expression finds the indentation of every non-blank\n    # line in a string.\n    _INDENT_RE = re.compile('^([ ]*)(?=\\S)', re.MULTILINE)\n\n    def _min_indent(self, s):\n        \"Return the minimum indentation of any non-blank line in `s`\"\n        indents = [len(indent) for indent in self._INDENT_RE.findall(s)]\n        if len(indents) > 0:\n            return min(indents)\n        else:\n            return 0\n\n    def _check_prompt_blank(self, lines, indent, name, lineno):\n        \"\"\"\n        Given the lines of a source string (including prompts and\n        leading indentation), check to make sure that every prompt is\n        followed by a space character.  If any line is not followed by\n        a space character, then raise ValueError.\n        \"\"\"\n        for i, line in enumerate(lines):\n            if len(line) >= indent+4 and line[indent+3] != ' ':\n                raise ValueError('line %r of the docstring for %s '\n                                 'lacks blank after %s: %r' %\n                                 (lineno+i+1, name,\n                                  line[indent:indent+3], line))\n\n    def _check_prefix(self, lines, prefix, name, lineno):\n        \"\"\"\n        Check that every line in the given list starts with the given\n        prefix; if any line does not, then raise a ValueError.\n        \"\"\"\n        for i, line in enumerate(lines):\n            if line and not line.startswith(prefix):\n                raise ValueError('line %r of the docstring for %s has '\n                                 'inconsistent leading whitespace: %r' %\n                                 (lineno+i+1, name, line))\n\n\n######################################################################\n## 4. DocTest Finder\n######################################################################\n\nclass DocTestFinder:\n    \"\"\"\n    A class used to extract the DocTests that are relevant to a given\n    object, from its docstring and the docstrings of its contained\n    objects.  Doctests can currently be extracted from the following\n    object types: modules, functions, classes, methods, staticmethods,\n    classmethods, and properties.\n    \"\"\"\n\n    def __init__(self, verbose=False, parser=DocTestParser(),\n                 recurse=True, exclude_empty=True):\n        \"\"\"\n        Create a new doctest finder.\n\n        The optional argument `parser` specifies a class or\n        function that should be used to create new DocTest objects (or\n        objects that implement the same interface as DocTest).  The\n        signature for this factory function should match the signature\n        of the DocTest constructor.\n\n        If the optional argument `recurse` is false, then `find` will\n        only examine the given object, and not any contained objects.\n\n        If the optional argument `exclude_empty` is false, then `find`\n        will include tests for objects with empty docstrings.\n        \"\"\"\n        self._parser = parser\n        self._verbose = verbose\n        self._recurse = recurse\n        self._exclude_empty = exclude_empty\n\n    def find(self, obj, name=None, module=None, globs=None, extraglobs=None):\n        \"\"\"\n        Return a list of the DocTests that are defined by the given\n        object's docstring, or by any of its contained objects'\n        docstrings.\n\n        The optional parameter `module` is the module that contains\n        the given object.  If the module is not specified or is None, then\n        the test finder will attempt to automatically determine the\n        correct module.  The object's module is used:\n\n            - As a default namespace, if `globs` is not specified.\n            - To prevent the DocTestFinder from extracting DocTests\n              from objects that are imported from other modules.\n            - To find the name of the file containing the object.\n            - To help find the line number of the object within its\n              file.\n\n        Contained objects whose module does not match `module` are ignored.\n\n        If `module` is False, no attempt to find the module will be made.\n        This is obscure, of use mostly in tests:  if `module` is False, or\n        is None but cannot be found automatically, then all objects are\n        considered to belong to the (non-existent) module, so all contained\n        objects will (recursively) be searched for doctests.\n\n        The globals for each DocTest is formed by combining `globs`\n        and `extraglobs` (bindings in `extraglobs` override bindings\n        in `globs`).  A new copy of the globals dictionary is created\n        for each DocTest.  If `globs` is not specified, then it\n        defaults to the module's `__dict__`, if specified, or {}\n        otherwise.  If `extraglobs` is not specified, then it defaults\n        to {}.\n\n        \"\"\"\n        # If name was not specified, then extract it from the object.\n        if name is None:\n            name = getattr(obj, '__name__', None)\n            if name is None:\n                raise ValueError(\"DocTestFinder.find: name must be given \"\n                        \"when obj.__name__ doesn't exist: %r\" %\n                                 (type(obj),))\n\n        # Find the module that contains the given object (if obj is\n        # a module, then module=obj.).  Note: this may fail, in which\n        # case module will be None.\n        if module is False:\n            module = None\n        elif module is None:\n            module = inspect.getmodule(obj)\n\n        # Read the module's source code.  This is used by\n        # DocTestFinder._find_lineno to find the line number for a\n        # given object's docstring.\n        try:\n            file = inspect.getsourcefile(obj) or inspect.getfile(obj)\n            if module is not None:\n                # Supply the module globals in case the module was\n                # originally loaded via a PEP 302 loader and\n                # file is not a valid filesystem path\n                source_lines = linecache.getlines(file, module.__dict__)\n            else:\n                # No access to a loader, so assume it's a normal\n                # filesystem path\n                source_lines = linecache.getlines(file)\n            if not source_lines:\n                source_lines = None\n        except TypeError:\n            source_lines = None\n\n        # Initialize globals, and merge in extraglobs.\n        if globs is None:\n            if module is None:\n                globs = {}\n            else:\n                globs = module.__dict__.copy()\n        else:\n            globs = globs.copy()\n        if extraglobs is not None:\n            globs.update(extraglobs)\n        if '__name__' not in globs:\n            globs['__name__'] = '__main__'  # provide a default module name\n\n        # Recursively explore `obj`, extracting DocTests.\n        tests = []\n        self._find(tests, obj, name, module, source_lines, globs, {})\n        # Sort the tests by alpha order of names, for consistency in\n        # verbose-mode output.  This was a feature of doctest in Pythons\n        # <= 2.3 that got lost by accident in 2.4.  It was repaired in\n        # 2.4.4 and 2.5.\n        tests.sort()\n        return tests\n\n    def _from_module(self, module, object):\n        \"\"\"\n        Return true if the given object is defined in the given\n        module.\n        \"\"\"\n        if module is None:\n            return True\n        elif inspect.getmodule(object) is not None:\n            return module is inspect.getmodule(object)\n        elif inspect.isfunction(object):\n            return module.__dict__ is object.func_globals\n        elif inspect.isclass(object):\n            return module.__name__ == object.__module__\n        elif hasattr(object, '__module__'):\n            return module.__name__ == object.__module__\n        elif isinstance(object, property):\n            return True # [XX] no way not be sure.\n        else:\n            raise ValueError(\"object must be a class or function\")\n\n    def _find(self, tests, obj, name, module, source_lines, globs, seen):\n        \"\"\"\n        Find tests for the given object and any contained objects, and\n        add them to `tests`.\n        \"\"\"\n        if self._verbose:\n            print 'Finding tests in %s' % name\n\n        # If we've already processed this object, then ignore it.\n        if id(obj) in seen:\n            return\n        seen[id(obj)] = 1\n\n        # Find a test for this object, and add it to the list of tests.\n        test = self._get_test(obj, name, module, globs, source_lines)\n        if test is not None:\n            tests.append(test)\n\n        # Look for tests in a module's contained objects.\n        if inspect.ismodule(obj) and self._recurse:\n            for valname, val in obj.__dict__.items():\n                valname = '%s.%s' % (name, valname)\n                # Recurse to functions & classes.\n                if ((inspect.isfunction(val) or inspect.isclass(val)) and\n                    self._from_module(module, val)):\n                    self._find(tests, val, valname, module, source_lines,\n                               globs, seen)\n\n        # Look for tests in a module's __test__ dictionary.\n        if inspect.ismodule(obj) and self._recurse:\n            for valname, val in getattr(obj, '__test__', {}).items():\n                if not isinstance(valname, basestring):\n                    raise ValueError(\"DocTestFinder.find: __test__ keys \"\n                                     \"must be strings: %r\" %\n                                     (type(valname),))\n                if not (inspect.isfunction(val) or inspect.isclass(val) or\n                        inspect.ismethod(val) or inspect.ismodule(val) or\n                        isinstance(val, basestring)):\n                    raise ValueError(\"DocTestFinder.find: __test__ values \"\n                                     \"must be strings, functions, methods, \"\n                                     \"classes, or modules: %r\" %\n                                     (type(val),))\n                valname = '%s.__test__.%s' % (name, valname)\n                self._find(tests, val, valname, module, source_lines,\n                           globs, seen)\n\n        # Look for tests in a class's contained objects.\n        if inspect.isclass(obj) and self._recurse:\n            for valname, val in obj.__dict__.items():\n                # Special handling for staticmethod/classmethod.\n                if isinstance(val, staticmethod):\n                    val = getattr(obj, valname)\n                if isinstance(val, classmethod):\n                    val = getattr(obj, valname).im_func\n\n                # Recurse to methods, properties, and nested classes.\n                if ((inspect.isfunction(val) or inspect.isclass(val) or\n                      isinstance(val, property)) and\n                      self._from_module(module, val)):\n                    valname = '%s.%s' % (name, valname)\n                    self._find(tests, val, valname, module, source_lines,\n                               globs, seen)\n\n    def _get_test(self, obj, name, module, globs, source_lines):\n        \"\"\"\n        Return a DocTest for the given object, if it defines a docstring;\n        otherwise, return None.\n        \"\"\"\n        # Extract the object's docstring.  If it doesn't have one,\n        # then return None (no test for this object).\n        if isinstance(obj, basestring):\n            docstring = obj\n        else:\n            try:\n                if obj.__doc__ is None:\n                    docstring = ''\n                else:\n                    docstring = obj.__doc__\n                    if not isinstance(docstring, basestring):\n                        docstring = str(docstring)\n            except (TypeError, AttributeError):\n                docstring = ''\n\n        # Find the docstring's location in the file.\n        lineno = self._find_lineno(obj, source_lines)\n\n        # Don't bother if the docstring is empty.\n        if self._exclude_empty and not docstring:\n            return None\n\n        # Return a DocTest for this object.\n        if module is None:\n            filename = None\n        else:\n            filename = getattr(module, '__file__', module.__name__)\n            if filename[-4:] in (\".pyc\", \".pyo\"):\n                filename = filename[:-1]\n        return self._parser.get_doctest(docstring, globs, name,\n                                        filename, lineno)\n\n    def _find_lineno(self, obj, source_lines):\n        \"\"\"\n        Return a line number of the given object's docstring.  Note:\n        this method assumes that the object has a docstring.\n        \"\"\"\n        lineno = None\n\n        # Find the line number for modules.\n        if inspect.ismodule(obj):\n            lineno = 0\n\n        # Find the line number for classes.\n        # Note: this could be fooled if a class is defined multiple\n        # times in a single file.\n        if inspect.isclass(obj):\n            if source_lines is None:\n                return None\n            pat = re.compile(r'^\\s*class\\s*%s\\b' %\n                             getattr(obj, '__name__', '-'))\n            for i, line in enumerate(source_lines):\n                if pat.match(line):\n                    lineno = i\n                    break\n\n        # Find the line number for functions & methods.\n        if inspect.ismethod(obj): obj = obj.im_func\n        if inspect.isfunction(obj): obj = obj.func_code\n        if inspect.istraceback(obj): obj = obj.tb_frame\n        if inspect.isframe(obj): obj = obj.f_code\n        if inspect.iscode(obj):\n            lineno = getattr(obj, 'co_firstlineno', None)-1\n\n        # Find the line number where the docstring starts.  Assume\n        # that it's the first line that begins with a quote mark.\n        # Note: this could be fooled by a multiline function\n        # signature, where a continuation line begins with a quote\n        # mark.\n        if lineno is not None:\n            if source_lines is None:\n                return lineno+1\n            pat = re.compile('(^|.*:)\\s*\\w*(\"|\\')')\n            for lineno in range(lineno, len(source_lines)):\n                if pat.match(source_lines[lineno]):\n                    return lineno\n\n        # We couldn't find the line number.\n        return None\n\n######################################################################\n## 5. DocTest Runner\n######################################################################\n\nclass DocTestRunner:\n    \"\"\"\n    A class used to run DocTest test cases, and accumulate statistics.\n    The `run` method is used to process a single DocTest case.  It\n    returns a tuple `(f, t)`, where `t` is the number of test cases\n    tried, and `f` is the number of test cases that failed.\n\n        >>> tests = DocTestFinder().find(_TestClass)\n        >>> runner = DocTestRunner(verbose=False)\n        >>> tests.sort(key = lambda test: test.name)\n        >>> for test in tests:\n        ...     print test.name, '->', runner.run(test)\n        _TestClass -> TestResults(failed=0, attempted=2)\n        _TestClass.__init__ -> TestResults(failed=0, attempted=2)\n        _TestClass.get -> TestResults(failed=0, attempted=2)\n        _TestClass.square -> TestResults(failed=0, attempted=1)\n\n    The `summarize` method prints a summary of all the test cases that\n    have been run by the runner, and returns an aggregated `(f, t)`\n    tuple:\n\n        >>> runner.summarize(verbose=1)\n        4 items passed all tests:\n           2 tests in _TestClass\n           2 tests in _TestClass.__init__\n           2 tests in _TestClass.get\n           1 tests in _TestClass.square\n        7 tests in 4 items.\n        7 passed and 0 failed.\n        Test passed.\n        TestResults(failed=0, attempted=7)\n\n    The aggregated number of tried examples and failed examples is\n    also available via the `tries` and `failures` attributes:\n\n        >>> runner.tries\n        7\n        >>> runner.failures\n        0\n\n    The comparison between expected outputs and actual outputs is done\n    by an `OutputChecker`.  This comparison may be customized with a\n    number of option flags; see the documentation for `testmod` for\n    more information.  If the option flags are insufficient, then the\n    comparison may also be customized by passing a subclass of\n    `OutputChecker` to the constructor.\n\n    The test runner's display output can be controlled in two ways.\n    First, an output function (`out) can be passed to\n    `TestRunner.run`; this function will be called with strings that\n    should be displayed.  It defaults to `sys.stdout.write`.  If\n    capturing the output is not sufficient, then the display output\n    can be also customized by subclassing DocTestRunner, and\n    overriding the methods `report_start`, `report_success`,\n    `report_unexpected_exception`, and `report_failure`.\n    \"\"\"\n    # This divider string is used to separate failure messages, and to\n    # separate sections of the summary.\n    DIVIDER = \"*\" * 70\n\n    def __init__(self, checker=None, verbose=None, optionflags=0):\n        \"\"\"\n        Create a new test runner.\n\n        Optional keyword arg `checker` is the `OutputChecker` that\n        should be used to compare the expected outputs and actual\n        outputs of doctest examples.\n\n        Optional keyword arg 'verbose' prints lots of stuff if true,\n        only failures if false; by default, it's true iff '-v' is in\n        sys.argv.\n\n        Optional argument `optionflags` can be used to control how the\n        test runner compares expected output to actual output, and how\n        it displays failures.  See the documentation for `testmod` for\n        more information.\n        \"\"\"\n        self._checker = checker or OutputChecker()\n        if verbose is None:\n            verbose = '-v' in sys.argv\n        self._verbose = verbose\n        self.optionflags = optionflags\n        self.original_optionflags = optionflags\n\n        # Keep track of the examples we've run.\n        self.tries = 0\n        self.failures = 0\n        self._name2ft = {}\n\n        # Create a fake output target for capturing doctest output.\n        self._fakeout = _SpoofOut()\n\n    #/////////////////////////////////////////////////////////////////\n    # Reporting methods\n    #/////////////////////////////////////////////////////////////////\n\n    def report_start(self, out, test, example):\n        \"\"\"\n        Report that the test runner is about to process the given\n        example.  (Only displays a message if verbose=True)\n        \"\"\"\n        if self._verbose:\n            if example.want:\n                out('Trying:\\n' + _indent(example.source) +\n                    'Expecting:\\n' + _indent(example.want))\n            else:\n                out('Trying:\\n' + _indent(example.source) +\n                    'Expecting nothing\\n')\n\n    def report_success(self, out, test, example, got):\n        \"\"\"\n        Report that the given example ran successfully.  (Only\n        displays a message if verbose=True)\n        \"\"\"\n        if self._verbose:\n            out(\"ok\\n\")\n\n    def report_failure(self, out, test, example, got):\n        \"\"\"\n        Report that the given example failed.\n        \"\"\"\n        out(self._failure_header(test, example) +\n            self._checker.output_difference(example, got, self.optionflags))\n\n    def report_unexpected_exception(self, out, test, example, exc_info):\n        \"\"\"\n        Report that the given example raised an unexpected exception.\n        \"\"\"\n        out(self._failure_header(test, example) +\n            'Exception raised:\\n' + _indent(_exception_traceback(exc_info)))\n\n    def _failure_header(self, test, example):\n        out = [self.DIVIDER]\n        if test.filename:\n            if test.lineno is not None and example.lineno is not None:\n                lineno = test.lineno + example.lineno + 1\n            else:\n                lineno = '?'\n            out.append('File \"%s\", line %s, in %s' %\n                       (test.filename, lineno, test.name))\n        else:\n            out.append('Line %s, in %s' % (example.lineno+1, test.name))\n        out.append('Failed example:')\n        source = example.source\n        out.append(_indent(source))\n        return '\\n'.join(out)\n\n    #/////////////////////////////////////////////////////////////////\n    # DocTest Running\n    #/////////////////////////////////////////////////////////////////\n\n    def __run(self, test, compileflags, out):\n        \"\"\"\n        Run the examples in `test`.  Write the outcome of each example\n        with one of the `DocTestRunner.report_*` methods, using the\n        writer function `out`.  `compileflags` is the set of compiler\n        flags that should be used to execute examples.  Return a tuple\n        `(f, t)`, where `t` is the number of examples tried, and `f`\n        is the number of examples that failed.  The examples are run\n        in the namespace `test.globs`.\n        \"\"\"\n        # Keep track of the number of failures and tries.\n        failures = tries = 0\n\n        # Save the option flags (since option directives can be used\n        # to modify them).\n        original_optionflags = self.optionflags\n\n        SUCCESS, FAILURE, BOOM = range(3) # `outcome` state\n\n        check = self._checker.check_output\n\n        # Process each example.\n        for examplenum, example in enumerate(test.examples):\n\n            # If REPORT_ONLY_FIRST_FAILURE is set, then suppress\n            # reporting after the first failure.\n            quiet = (self.optionflags & REPORT_ONLY_FIRST_FAILURE and\n                     failures > 0)\n\n            # Merge in the example's options.\n            self.optionflags = original_optionflags\n            if example.options:\n                for (optionflag, val) in example.options.items():\n                    if val:\n                        self.optionflags |= optionflag\n                    else:\n                        self.optionflags &= ~optionflag\n\n            # If 'SKIP' is set, then skip this example.\n            if self.optionflags & SKIP:\n                continue\n\n            # Record that we started this example.\n            tries += 1\n            if not quiet:\n                self.report_start(out, test, example)\n\n            # Use a special filename for compile(), so we can retrieve\n            # the source code during interactive debugging (see\n            # __patched_linecache_getlines).\n            filename = '<doctest %s[%d]>' % (test.name, examplenum)\n\n            # Run the example in the given context (globs), and record\n            # any exception that gets raised.  (But don't intercept\n            # keyboard interrupts.)\n            try:\n                # Don't blink!  This is where the user's code gets run.\n                exec compile(example.source, filename, \"single\",\n                             compileflags, 1) in test.globs\n                self.debugger.set_continue() # ==== Example Finished ====\n                exception = None\n            except KeyboardInterrupt:\n                raise\n            except:\n                exception = sys.exc_info()\n                self.debugger.set_continue() # ==== Example Finished ====\n\n            got = self._fakeout.getvalue()  # the actual output\n            self._fakeout.truncate(0)\n            outcome = FAILURE   # guilty until proved innocent or insane\n\n            # If the example executed without raising any exceptions,\n            # verify its output.\n            if exception is None:\n                if check(example.want, got, self.optionflags):\n                    outcome = SUCCESS\n\n            # The example raised an exception:  check if it was expected.\n            else:\n                exc_info = sys.exc_info()\n                exc_msg = traceback.format_exception_only(*exc_info[:2])[-1]\n                if not quiet:\n                    got += _exception_traceback(exc_info)\n\n                # If `example.exc_msg` is None, then we weren't expecting\n                # an exception.\n                if example.exc_msg is None:\n                    outcome = BOOM\n\n                # We expected an exception:  see whether it matches.\n                elif check(example.exc_msg, exc_msg, self.optionflags):\n                    outcome = SUCCESS\n\n                # Another chance if they didn't care about the detail.\n                elif self.optionflags & IGNORE_EXCEPTION_DETAIL:\n                    if check(_strip_exception_details(example.exc_msg),\n                             _strip_exception_details(exc_msg),\n                             self.optionflags):\n                        outcome = SUCCESS\n\n            # Report the outcome.\n            if outcome is SUCCESS:\n                if not quiet:\n                    self.report_success(out, test, example, got)\n            elif outcome is FAILURE:\n                if not quiet:\n                    self.report_failure(out, test, example, got)\n                failures += 1\n            elif outcome is BOOM:\n                if not quiet:\n                    self.report_unexpected_exception(out, test, example,\n                                                     exc_info)\n                failures += 1\n            else:\n                assert False, (\"unknown outcome\", outcome)\n\n        # Restore the option flags (in case they were modified)\n        self.optionflags = original_optionflags\n\n        # Record and return the number of failures and tries.\n        self.__record_outcome(test, failures, tries)\n        return TestResults(failures, tries)\n\n    def __record_outcome(self, test, f, t):\n        \"\"\"\n        Record the fact that the given DocTest (`test`) generated `f`\n        failures out of `t` tried examples.\n        \"\"\"\n        f2, t2 = self._name2ft.get(test.name, (0,0))\n        self._name2ft[test.name] = (f+f2, t+t2)\n        self.failures += f\n        self.tries += t\n\n    __LINECACHE_FILENAME_RE = re.compile(r'<doctest '\n                                         r'(?P<name>.+)'\n                                         r'\\[(?P<examplenum>\\d+)\\]>$')\n    def __patched_linecache_getlines(self, filename, module_globals=None):\n        m = self.__LINECACHE_FILENAME_RE.match(filename)\n        if m and m.group('name') == self.test.name:\n            example = self.test.examples[int(m.group('examplenum'))]\n            source = example.source\n            if isinstance(source, unicode):\n                source = source.encode('ascii', 'backslashreplace')\n            return source.splitlines(True)\n        else:\n            return self.save_linecache_getlines(filename, module_globals)\n\n    def run(self, test, compileflags=None, out=None, clear_globs=True):\n        \"\"\"\n        Run the examples in `test`, and display the results using the\n        writer function `out`.\n\n        The examples are run in the namespace `test.globs`.  If\n        `clear_globs` is true (the default), then this namespace will\n        be cleared after the test runs, to help with garbage\n        collection.  If you would like to examine the namespace after\n        the test completes, then use `clear_globs=False`.\n\n        `compileflags` gives the set of flags that should be used by\n        the Python compiler when running the examples.  If not\n        specified, then it will default to the set of future-import\n        flags that apply to `globs`.\n\n        The output of each example is checked using\n        `DocTestRunner.check_output`, and the results are formatted by\n        the `DocTestRunner.report_*` methods.\n        \"\"\"\n        self.test = test\n\n        if compileflags is None:\n            compileflags = _extract_future_flags(test.globs)\n\n        save_stdout = sys.stdout\n        if out is None:\n            out = save_stdout.write\n        sys.stdout = self._fakeout\n\n        # Patch pdb.set_trace to restore sys.stdout during interactive\n        # debugging (so it's not still redirected to self._fakeout).\n        # Note that the interactive output will go to *our*\n        # save_stdout, even if that's not the real sys.stdout; this\n        # allows us to write test cases for the set_trace behavior.\n        save_set_trace = pdb.set_trace\n        self.debugger = _OutputRedirectingPdb(save_stdout)\n        self.debugger.reset()\n        pdb.set_trace = self.debugger.set_trace\n\n        # Patch linecache.getlines, so we can see the example's source\n        # when we're inside the debugger.\n        self.save_linecache_getlines = linecache.getlines\n        linecache.getlines = self.__patched_linecache_getlines\n\n        # Make sure sys.displayhook just prints the value to stdout\n        save_displayhook = sys.displayhook\n        sys.displayhook = sys.__displayhook__\n\n        try:\n            return self.__run(test, compileflags, out)\n        finally:\n            sys.stdout = save_stdout\n            pdb.set_trace = save_set_trace\n            linecache.getlines = self.save_linecache_getlines\n            sys.displayhook = save_displayhook\n            if clear_globs:\n                test.globs.clear()\n\n    #/////////////////////////////////////////////////////////////////\n    # Summarization\n    #/////////////////////////////////////////////////////////////////\n    def summarize(self, verbose=None):\n        \"\"\"\n        Print a summary of all the test cases that have been run by\n        this DocTestRunner, and return a tuple `(f, t)`, where `f` is\n        the total number of failed examples, and `t` is the total\n        number of tried examples.\n\n        The optional `verbose` argument controls how detailed the\n        summary is.  If the verbosity is not specified, then the\n        DocTestRunner's verbosity is used.\n        \"\"\"\n        if verbose is None:\n            verbose = self._verbose\n        notests = []\n        passed = []\n        failed = []\n        totalt = totalf = 0\n        for x in self._name2ft.items():\n            name, (f, t) = x\n            assert f <= t\n            totalt += t\n            totalf += f\n            if t == 0:\n                notests.append(name)\n            elif f == 0:\n                passed.append( (name, t) )\n            else:\n                failed.append(x)\n        if verbose:\n            if notests:\n                print len(notests), \"items had no tests:\"\n                notests.sort()\n                for thing in notests:\n                    print \"   \", thing\n            if passed:\n                print len(passed), \"items passed all tests:\"\n                passed.sort()\n                for thing, count in passed:\n                    print \" %3d tests in %s\" % (count, thing)\n        if failed:\n            print self.DIVIDER\n            print len(failed), \"items had failures:\"\n            failed.sort()\n            for thing, (f, t) in failed:\n                print \" %3d of %3d in %s\" % (f, t, thing)\n        if verbose:\n            print totalt, \"tests in\", len(self._name2ft), \"items.\"\n            print totalt - totalf, \"passed and\", totalf, \"failed.\"\n        if totalf:\n            print \"***Test Failed***\", totalf, \"failures.\"\n        elif verbose:\n            print \"Test passed.\"\n        return TestResults(totalf, totalt)\n\n    #/////////////////////////////////////////////////////////////////\n    # Backward compatibility cruft to maintain doctest.master.\n    #/////////////////////////////////////////////////////////////////\n    def merge(self, other):\n        d = self._name2ft\n        for name, (f, t) in other._name2ft.items():\n            if name in d:\n                # Don't print here by default, since doing\n                #     so breaks some of the buildbots\n                #print \"*** DocTestRunner.merge: '\" + name + \"' in both\" \\\n                #    \" testers; summing outcomes.\"\n                f2, t2 = d[name]\n                f = f + f2\n                t = t + t2\n            d[name] = f, t\n\nclass OutputChecker:\n    \"\"\"\n    A class used to check the whether the actual output from a doctest\n    example matches the expected output.  `OutputChecker` defines two\n    methods: `check_output`, which compares a given pair of outputs,\n    and returns true if they match; and `output_difference`, which\n    returns a string describing the differences between two outputs.\n    \"\"\"\n    def check_output(self, want, got, optionflags):\n        \"\"\"\n        Return True iff the actual output from an example (`got`)\n        matches the expected output (`want`).  These strings are\n        always considered to match if they are identical; but\n        depending on what option flags the test runner is using,\n        several non-exact match types are also possible.  See the\n        documentation for `TestRunner` for more information about\n        option flags.\n        \"\"\"\n        # Handle the common case first, for efficiency:\n        # if they're string-identical, always return true.\n        if got == want:\n            return True\n\n        # The values True and False replaced 1 and 0 as the return\n        # value for boolean comparisons in Python 2.3.\n        if not (optionflags & DONT_ACCEPT_TRUE_FOR_1):\n            if (got,want) == (\"True\\n\", \"1\\n\"):\n                return True\n            if (got,want) == (\"False\\n\", \"0\\n\"):\n                return True\n\n        # <BLANKLINE> can be used as a special sequence to signify a\n        # blank line, unless the DONT_ACCEPT_BLANKLINE flag is used.\n        if not (optionflags & DONT_ACCEPT_BLANKLINE):\n            # Replace <BLANKLINE> in want with a blank line.\n            want = re.sub('(?m)^%s\\s*?$' % re.escape(BLANKLINE_MARKER),\n                          '', want)\n            # If a line in got contains only spaces, then remove the\n            # spaces.\n            got = re.sub('(?m)^\\s*?$', '', got)\n            if got == want:\n                return True\n\n        # This flag causes doctest to ignore any differences in the\n        # contents of whitespace strings.  Note that this can be used\n        # in conjunction with the ELLIPSIS flag.\n        if optionflags & NORMALIZE_WHITESPACE:\n            got = ' '.join(got.split())\n            want = ' '.join(want.split())\n            if got == want:\n                return True\n\n        # The ELLIPSIS flag says to let the sequence \"...\" in `want`\n        # match any substring in `got`.\n        if optionflags & ELLIPSIS:\n            if _ellipsis_match(want, got):\n                return True\n\n        # We didn't find any match; return false.\n        return False\n\n    # Should we do a fancy diff?\n    def _do_a_fancy_diff(self, want, got, optionflags):\n        # Not unless they asked for a fancy diff.\n        if not optionflags & (REPORT_UDIFF |\n                              REPORT_CDIFF |\n                              REPORT_NDIFF):\n            return False\n\n        # If expected output uses ellipsis, a meaningful fancy diff is\n        # too hard ... or maybe not.  In two real-life failures Tim saw,\n        # a diff was a major help anyway, so this is commented out.\n        # [todo] _ellipsis_match() knows which pieces do and don't match,\n        # and could be the basis for a kick-ass diff in this case.\n        ##if optionflags & ELLIPSIS and ELLIPSIS_MARKER in want:\n        ##    return False\n\n        # ndiff does intraline difference marking, so can be useful even\n        # for 1-line differences.\n        if optionflags & REPORT_NDIFF:\n            return True\n\n        # The other diff types need at least a few lines to be helpful.\n        return want.count('\\n') > 2 and got.count('\\n') > 2\n\n    def output_difference(self, example, got, optionflags):\n        \"\"\"\n        Return a string describing the differences between the\n        expected output for a given example (`example`) and the actual\n        output (`got`).  `optionflags` is the set of option flags used\n        to compare `want` and `got`.\n        \"\"\"\n        want = example.want\n        # If <BLANKLINE>s are being used, then replace blank lines\n        # with <BLANKLINE> in the actual output string.\n        if not (optionflags & DONT_ACCEPT_BLANKLINE):\n            got = re.sub('(?m)^[ ]*(?=\\n)', BLANKLINE_MARKER, got)\n\n        # Check if we should use diff.\n        if self._do_a_fancy_diff(want, got, optionflags):\n            # Split want & got into lines.\n            want_lines = want.splitlines(True)  # True == keep line ends\n            got_lines = got.splitlines(True)\n            # Use difflib to find their differences.\n            if optionflags & REPORT_UDIFF:\n                diff = difflib.unified_diff(want_lines, got_lines, n=2)\n                diff = list(diff)[2:] # strip the diff header\n                kind = 'unified diff with -expected +actual'\n            elif optionflags & REPORT_CDIFF:\n                diff = difflib.context_diff(want_lines, got_lines, n=2)\n                diff = list(diff)[2:] # strip the diff header\n                kind = 'context diff with expected followed by actual'\n            elif optionflags & REPORT_NDIFF:\n                engine = difflib.Differ(charjunk=difflib.IS_CHARACTER_JUNK)\n                diff = list(engine.compare(want_lines, got_lines))\n                kind = 'ndiff with -expected +actual'\n            else:\n                assert 0, 'Bad diff option'\n            # Remove trailing whitespace on diff output.\n            diff = [line.rstrip() + '\\n' for line in diff]\n            return 'Differences (%s):\\n' % kind + _indent(''.join(diff))\n\n        # If we're not using diff, then simply list the expected\n        # output followed by the actual output.\n        if want and got:\n            return 'Expected:\\n%sGot:\\n%s' % (_indent(want), _indent(got))\n        elif want:\n            return 'Expected:\\n%sGot nothing\\n' % _indent(want)\n        elif got:\n            return 'Expected nothing\\nGot:\\n%s' % _indent(got)\n        else:\n            return 'Expected nothing\\nGot nothing\\n'\n\nclass DocTestFailure(Exception):\n    \"\"\"A DocTest example has failed in debugging mode.\n\n    The exception instance has variables:\n\n    - test: the DocTest object being run\n\n    - example: the Example object that failed\n\n    - got: the actual output\n    \"\"\"\n    def __init__(self, test, example, got):\n        self.test = test\n        self.example = example\n        self.got = got\n\n    def __str__(self):\n        return str(self.test)\n\nclass UnexpectedException(Exception):\n    \"\"\"A DocTest example has encountered an unexpected exception\n\n    The exception instance has variables:\n\n    - test: the DocTest object being run\n\n    - example: the Example object that failed\n\n    - exc_info: the exception info\n    \"\"\"\n    def __init__(self, test, example, exc_info):\n        self.test = test\n        self.example = example\n        self.exc_info = exc_info\n\n    def __str__(self):\n        return str(self.test)\n\nclass DebugRunner(DocTestRunner):\n    r\"\"\"Run doc tests but raise an exception as soon as there is a failure.\n\n       If an unexpected exception occurs, an UnexpectedException is raised.\n       It contains the test, the example, and the original exception:\n\n         >>> runner = DebugRunner(verbose=False)\n         >>> test = DocTestParser().get_doctest('>>> raise KeyError\\n42',\n         ...                                    {}, 'foo', 'foo.py', 0)\n         >>> try:\n         ...     runner.run(test)\n         ... except UnexpectedException, failure:\n         ...     pass\n\n         >>> failure.test is test\n         True\n\n         >>> failure.example.want\n         '42\\n'\n\n         >>> exc_info = failure.exc_info\n         >>> raise exc_info[0], exc_info[1], exc_info[2]\n         Traceback (most recent call last):\n         ...\n         KeyError\n\n       We wrap the original exception to give the calling application\n       access to the test and example information.\n\n       If the output doesn't match, then a DocTestFailure is raised:\n\n         >>> test = DocTestParser().get_doctest('''\n         ...      >>> x = 1\n         ...      >>> x\n         ...      2\n         ...      ''', {}, 'foo', 'foo.py', 0)\n\n         >>> try:\n         ...    runner.run(test)\n         ... except DocTestFailure, failure:\n         ...    pass\n\n       DocTestFailure objects provide access to the test:\n\n         >>> failure.test is test\n         True\n\n       As well as to the example:\n\n         >>> failure.example.want\n         '2\\n'\n\n       and the actual output:\n\n         >>> failure.got\n         '1\\n'\n\n       If a failure or error occurs, the globals are left intact:\n\n         >>> del test.globs['__builtins__']\n         >>> test.globs\n         {'x': 1}\n\n         >>> test = DocTestParser().get_doctest('''\n         ...      >>> x = 2\n         ...      >>> raise KeyError\n         ...      ''', {}, 'foo', 'foo.py', 0)\n\n         >>> runner.run(test)\n         Traceback (most recent call last):\n         ...\n         UnexpectedException: <DocTest foo from foo.py:0 (2 examples)>\n\n         >>> del test.globs['__builtins__']\n         >>> test.globs\n         {'x': 2}\n\n       But the globals are cleared if there is no error:\n\n         >>> test = DocTestParser().get_doctest('''\n         ...      >>> x = 2\n         ...      ''', {}, 'foo', 'foo.py', 0)\n\n         >>> runner.run(test)\n         TestResults(failed=0, attempted=1)\n\n         >>> test.globs\n         {}\n\n       \"\"\"\n\n    def run(self, test, compileflags=None, out=None, clear_globs=True):\n        r = DocTestRunner.run(self, test, compileflags, out, False)\n        if clear_globs:\n            test.globs.clear()\n        return r\n\n    def report_unexpected_exception(self, out, test, example, exc_info):\n        raise UnexpectedException(test, example, exc_info)\n\n    def report_failure(self, out, test, example, got):\n        raise DocTestFailure(test, example, got)\n\n######################################################################\n## 6. Test Functions\n######################################################################\n# These should be backwards compatible.\n\n# For backward compatibility, a global instance of a DocTestRunner\n# class, updated by testmod.\nmaster = None\n\ndef testmod(m=None, name=None, globs=None, verbose=None,\n            report=True, optionflags=0, extraglobs=None,\n            raise_on_error=False, exclude_empty=False):\n    \"\"\"m=None, name=None, globs=None, verbose=None, report=True,\n       optionflags=0, extraglobs=None, raise_on_error=False,\n       exclude_empty=False\n\n    Test examples in docstrings in functions and classes reachable\n    from module m (or the current module if m is not supplied), starting\n    with m.__doc__.\n\n    Also test examples reachable from dict m.__test__ if it exists and is\n    not None.  m.__test__ maps names to functions, classes and strings;\n    function and class docstrings are tested even if the name is private;\n    strings are tested directly, as if they were docstrings.\n\n    Return (#failures, #tests).\n\n    See help(doctest) for an overview.\n\n    Optional keyword arg \"name\" gives the name of the module; by default\n    use m.__name__.\n\n    Optional keyword arg \"globs\" gives a dict to be used as the globals\n    when executing examples; by default, use m.__dict__.  A copy of this\n    dict is actually used for each docstring, so that each docstring's\n    examples start with a clean slate.\n\n    Optional keyword arg \"extraglobs\" gives a dictionary that should be\n    merged into the globals that are used to execute examples.  By\n    default, no extra globals are used.  This is new in 2.4.\n\n    Optional keyword arg \"verbose\" prints lots of stuff if true, prints\n    only failures if false; by default, it's true iff \"-v\" is in sys.argv.\n\n    Optional keyword arg \"report\" prints a summary at the end when true,\n    else prints nothing at the end.  In verbose mode, the summary is\n    detailed, else very brief (in fact, empty if all tests passed).\n\n    Optional keyword arg \"optionflags\" or's together module constants,\n    and defaults to 0.  This is new in 2.3.  Possible values (see the\n    docs for details):\n\n        DONT_ACCEPT_TRUE_FOR_1\n        DONT_ACCEPT_BLANKLINE\n        NORMALIZE_WHITESPACE\n        ELLIPSIS\n        SKIP\n        IGNORE_EXCEPTION_DETAIL\n        REPORT_UDIFF\n        REPORT_CDIFF\n        REPORT_NDIFF\n        REPORT_ONLY_FIRST_FAILURE\n\n    Optional keyword arg \"raise_on_error\" raises an exception on the\n    first unexpected exception or failure. This allows failures to be\n    post-mortem debugged.\n\n    Advanced tomfoolery:  testmod runs methods of a local instance of\n    class doctest.Tester, then merges the results into (or creates)\n    global Tester instance doctest.master.  Methods of doctest.master\n    can be called directly too, if you want to do something unusual.\n    Passing report=0 to testmod is especially useful then, to delay\n    displaying a summary.  Invoke doctest.master.summarize(verbose)\n    when you're done fiddling.\n    \"\"\"\n    global master\n\n    # If no module was given, then use __main__.\n    if m is None:\n        # DWA - m will still be None if this wasn't invoked from the command\n        # line, in which case the following TypeError is about as good an error\n        # as we should expect\n        m = sys.modules.get('__main__')\n\n    # Check that we were actually given a module.\n    if not inspect.ismodule(m):\n        raise TypeError(\"testmod: module required; %r\" % (m,))\n\n    # If no name was given, then use the module's name.\n    if name is None:\n        name = m.__name__\n\n    # Find, parse, and run all tests in the given module.\n    finder = DocTestFinder(exclude_empty=exclude_empty)\n\n    if raise_on_error:\n        runner = DebugRunner(verbose=verbose, optionflags=optionflags)\n    else:\n        runner = DocTestRunner(verbose=verbose, optionflags=optionflags)\n\n    for test in finder.find(m, name, globs=globs, extraglobs=extraglobs):\n        runner.run(test)\n\n    if report:\n        runner.summarize()\n\n    if master is None:\n        master = runner\n    else:\n        master.merge(runner)\n\n    return TestResults(runner.failures, runner.tries)\n\ndef testfile(filename, module_relative=True, name=None, package=None,\n             globs=None, verbose=None, report=True, optionflags=0,\n             extraglobs=None, raise_on_error=False, parser=DocTestParser(),\n             encoding=None):\n    \"\"\"\n    Test examples in the given file.  Return (#failures, #tests).\n\n    Optional keyword arg \"module_relative\" specifies how filenames\n    should be interpreted:\n\n      - If \"module_relative\" is True (the default), then \"filename\"\n         specifies a module-relative path.  By default, this path is\n         relative to the calling module's directory; but if the\n         \"package\" argument is specified, then it is relative to that\n         package.  To ensure os-independence, \"filename\" should use\n         \"/\" characters to separate path segments, and should not\n         be an absolute path (i.e., it may not begin with \"/\").\n\n      - If \"module_relative\" is False, then \"filename\" specifies an\n        os-specific path.  The path may be absolute or relative (to\n        the current working directory).\n\n    Optional keyword arg \"name\" gives the name of the test; by default\n    use the file's basename.\n\n    Optional keyword argument \"package\" is a Python package or the\n    name of a Python package whose directory should be used as the\n    base directory for a module relative filename.  If no package is\n    specified, then the calling module's directory is used as the base\n    directory for module relative filenames.  It is an error to\n    specify \"package\" if \"module_relative\" is False.\n\n    Optional keyword arg \"globs\" gives a dict to be used as the globals\n    when executing examples; by default, use {}.  A copy of this dict\n    is actually used for each docstring, so that each docstring's\n    examples start with a clean slate.\n\n    Optional keyword arg \"extraglobs\" gives a dictionary that should be\n    merged into the globals that are used to execute examples.  By\n    default, no extra globals are used.\n\n    Optional keyword arg \"verbose\" prints lots of stuff if true, prints\n    only failures if false; by default, it's true iff \"-v\" is in sys.argv.\n\n    Optional keyword arg \"report\" prints a summary at the end when true,\n    else prints nothing at the end.  In verbose mode, the summary is\n    detailed, else very brief (in fact, empty if all tests passed).\n\n    Optional keyword arg \"optionflags\" or's together module constants,\n    and defaults to 0.  Possible values (see the docs for details):\n\n        DONT_ACCEPT_TRUE_FOR_1\n        DONT_ACCEPT_BLANKLINE\n        NORMALIZE_WHITESPACE\n        ELLIPSIS\n        SKIP\n        IGNORE_EXCEPTION_DETAIL\n        REPORT_UDIFF\n        REPORT_CDIFF\n        REPORT_NDIFF\n        REPORT_ONLY_FIRST_FAILURE\n\n    Optional keyword arg \"raise_on_error\" raises an exception on the\n    first unexpected exception or failure. This allows failures to be\n    post-mortem debugged.\n\n    Optional keyword arg \"parser\" specifies a DocTestParser (or\n    subclass) that should be used to extract tests from the files.\n\n    Optional keyword arg \"encoding\" specifies an encoding that should\n    be used to convert the file to unicode.\n\n    Advanced tomfoolery:  testmod runs methods of a local instance of\n    class doctest.Tester, then merges the results into (or creates)\n    global Tester instance doctest.master.  Methods of doctest.master\n    can be called directly too, if you want to do something unusual.\n    Passing report=0 to testmod is especially useful then, to delay\n    displaying a summary.  Invoke doctest.master.summarize(verbose)\n    when you're done fiddling.\n    \"\"\"\n    global master\n\n    if package and not module_relative:\n        raise ValueError(\"Package may only be specified for module-\"\n                         \"relative paths.\")\n\n    # Relativize the path\n    text, filename = _load_testfile(filename, package, module_relative)\n\n    # If no name was given, then use the file's name.\n    if name is None:\n        name = os.path.basename(filename)\n\n    # Assemble the globals.\n    if globs is None:\n        globs = {}\n    else:\n        globs = globs.copy()\n    if extraglobs is not None:\n        globs.update(extraglobs)\n    if '__name__' not in globs:\n        globs['__name__'] = '__main__'\n\n    if raise_on_error:\n        runner = DebugRunner(verbose=verbose, optionflags=optionflags)\n    else:\n        runner = DocTestRunner(verbose=verbose, optionflags=optionflags)\n\n    if encoding is not None:\n        text = text.decode(encoding)\n\n    # Read the file, convert it to a test, and run it.\n    test = parser.get_doctest(text, globs, name, filename, 0)\n    runner.run(test)\n\n    if report:\n        runner.summarize()\n\n    if master is None:\n        master = runner\n    else:\n        master.merge(runner)\n\n    return TestResults(runner.failures, runner.tries)\n\ndef run_docstring_examples(f, globs, verbose=False, name=\"NoName\",\n                           compileflags=None, optionflags=0):\n    \"\"\"\n    Test examples in the given object's docstring (`f`), using `globs`\n    as globals.  Optional argument `name` is used in failure messages.\n    If the optional argument `verbose` is true, then generate output\n    even if there are no failures.\n\n    `compileflags` gives the set of flags that should be used by the\n    Python compiler when running the examples.  If not specified, then\n    it will default to the set of future-import flags that apply to\n    `globs`.\n\n    Optional keyword arg `optionflags` specifies options for the\n    testing and output.  See the documentation for `testmod` for more\n    information.\n    \"\"\"\n    # Find, parse, and run all tests in the given module.\n    finder = DocTestFinder(verbose=verbose, recurse=False)\n    runner = DocTestRunner(verbose=verbose, optionflags=optionflags)\n    for test in finder.find(f, name, globs=globs):\n        runner.run(test, compileflags=compileflags)\n\n######################################################################\n## 7. Tester\n######################################################################\n# This is provided only for backwards compatibility.  It's not\n# actually used in any way.\n\nclass Tester:\n    def __init__(self, mod=None, globs=None, verbose=None, optionflags=0):\n\n        warnings.warn(\"class Tester is deprecated; \"\n                      \"use class doctest.DocTestRunner instead\",\n                      DeprecationWarning, stacklevel=2)\n        if mod is None and globs is None:\n            raise TypeError(\"Tester.__init__: must specify mod or globs\")\n        if mod is not None and not inspect.ismodule(mod):\n            raise TypeError(\"Tester.__init__: mod must be a module; %r\" %\n                            (mod,))\n        if globs is None:\n            globs = mod.__dict__\n        self.globs = globs\n\n        self.verbose = verbose\n        self.optionflags = optionflags\n        self.testfinder = DocTestFinder()\n        self.testrunner = DocTestRunner(verbose=verbose,\n                                        optionflags=optionflags)\n\n    def runstring(self, s, name):\n        test = DocTestParser().get_doctest(s, self.globs, name, None, None)\n        if self.verbose:\n            print \"Running string\", name\n        (f,t) = self.testrunner.run(test)\n        if self.verbose:\n            print f, \"of\", t, \"examples failed in string\", name\n        return TestResults(f,t)\n\n    def rundoc(self, object, name=None, module=None):\n        f = t = 0\n        tests = self.testfinder.find(object, name, module=module,\n                                     globs=self.globs)\n        for test in tests:\n            (f2, t2) = self.testrunner.run(test)\n            (f,t) = (f+f2, t+t2)\n        return TestResults(f,t)\n\n    def rundict(self, d, name, module=None):\n        import types\n        m = types.ModuleType(name)\n        m.__dict__.update(d)\n        if module is None:\n            module = False\n        return self.rundoc(m, name, module)\n\n    def run__test__(self, d, name):\n        import types\n        m = types.ModuleType(name)\n        m.__test__ = d\n        return self.rundoc(m, name)\n\n    def summarize(self, verbose=None):\n        return self.testrunner.summarize(verbose)\n\n    def merge(self, other):\n        self.testrunner.merge(other.testrunner)\n\n######################################################################\n## 8. Unittest Support\n######################################################################\n\n_unittest_reportflags = 0\n\ndef set_unittest_reportflags(flags):\n    \"\"\"Sets the unittest option flags.\n\n    The old flag is returned so that a runner could restore the old\n    value if it wished to:\n\n      >>> import doctest\n      >>> old = doctest._unittest_reportflags\n      >>> doctest.set_unittest_reportflags(REPORT_NDIFF |\n      ...                          REPORT_ONLY_FIRST_FAILURE) == old\n      True\n\n      >>> doctest._unittest_reportflags == (REPORT_NDIFF |\n      ...                                   REPORT_ONLY_FIRST_FAILURE)\n      True\n\n    Only reporting flags can be set:\n\n      >>> doctest.set_unittest_reportflags(ELLIPSIS)\n      Traceback (most recent call last):\n      ...\n      ValueError: ('Only reporting flags allowed', 8)\n\n      >>> doctest.set_unittest_reportflags(old) == (REPORT_NDIFF |\n      ...                                   REPORT_ONLY_FIRST_FAILURE)\n      True\n    \"\"\"\n    global _unittest_reportflags\n\n    if (flags & REPORTING_FLAGS) != flags:\n        raise ValueError(\"Only reporting flags allowed\", flags)\n    old = _unittest_reportflags\n    _unittest_reportflags = flags\n    return old\n\n\nclass DocTestCase(unittest.TestCase):\n\n    def __init__(self, test, optionflags=0, setUp=None, tearDown=None,\n                 checker=None):\n\n        unittest.TestCase.__init__(self)\n        self._dt_optionflags = optionflags\n        self._dt_checker = checker\n        self._dt_test = test\n        self._dt_setUp = setUp\n        self._dt_tearDown = tearDown\n\n    def setUp(self):\n        test = self._dt_test\n\n        if self._dt_setUp is not None:\n            self._dt_setUp(test)\n\n    def tearDown(self):\n        test = self._dt_test\n\n        if self._dt_tearDown is not None:\n            self._dt_tearDown(test)\n\n        test.globs.clear()\n\n    def runTest(self):\n        test = self._dt_test\n        old = sys.stdout\n        new = StringIO()\n        optionflags = self._dt_optionflags\n\n        if not (optionflags & REPORTING_FLAGS):\n            # The option flags don't include any reporting flags,\n            # so add the default reporting flags\n            optionflags |= _unittest_reportflags\n\n        runner = DocTestRunner(optionflags=optionflags,\n                               checker=self._dt_checker, verbose=False)\n\n        try:\n            runner.DIVIDER = \"-\"*70\n            failures, tries = runner.run(\n                test, out=new.write, clear_globs=False)\n        finally:\n            sys.stdout = old\n\n        if failures:\n            raise self.failureException(self.format_failure(new.getvalue()))\n\n    def format_failure(self, err):\n        test = self._dt_test\n        if test.lineno is None:\n            lineno = 'unknown line number'\n        else:\n            lineno = '%s' % test.lineno\n        lname = '.'.join(test.name.split('.')[-1:])\n        return ('Failed doctest test for %s\\n'\n                '  File \"%s\", line %s, in %s\\n\\n%s'\n                % (test.name, test.filename, lineno, lname, err)\n                )\n\n    def debug(self):\n        r\"\"\"Run the test case without results and without catching exceptions\n\n           The unit test framework includes a debug method on test cases\n           and test suites to support post-mortem debugging.  The test code\n           is run in such a way that errors are not caught.  This way a\n           caller can catch the errors and initiate post-mortem debugging.\n\n           The DocTestCase provides a debug method that raises\n           UnexpectedException errors if there is an unexpected\n           exception:\n\n             >>> test = DocTestParser().get_doctest('>>> raise KeyError\\n42',\n             ...                {}, 'foo', 'foo.py', 0)\n             >>> case = DocTestCase(test)\n             >>> try:\n             ...     case.debug()\n             ... except UnexpectedException, failure:\n             ...     pass\n\n           The UnexpectedException contains the test, the example, and\n           the original exception:\n\n             >>> failure.test is test\n             True\n\n             >>> failure.example.want\n             '42\\n'\n\n             >>> exc_info = failure.exc_info\n             >>> raise exc_info[0], exc_info[1], exc_info[2]\n             Traceback (most recent call last):\n             ...\n             KeyError\n\n           If the output doesn't match, then a DocTestFailure is raised:\n\n             >>> test = DocTestParser().get_doctest('''\n             ...      >>> x = 1\n             ...      >>> x\n             ...      2\n             ...      ''', {}, 'foo', 'foo.py', 0)\n             >>> case = DocTestCase(test)\n\n             >>> try:\n             ...    case.debug()\n             ... except DocTestFailure, failure:\n             ...    pass\n\n           DocTestFailure objects provide access to the test:\n\n             >>> failure.test is test\n             True\n\n           As well as to the example:\n\n             >>> failure.example.want\n             '2\\n'\n\n           and the actual output:\n\n             >>> failure.got\n             '1\\n'\n\n           \"\"\"\n\n        self.setUp()\n        runner = DebugRunner(optionflags=self._dt_optionflags,\n                             checker=self._dt_checker, verbose=False)\n        runner.run(self._dt_test, clear_globs=False)\n        self.tearDown()\n\n    def id(self):\n        return self._dt_test.name\n\n    def __eq__(self, other):\n        if type(self) is not type(other):\n            return NotImplemented\n\n        return self._dt_test == other._dt_test and \\\n               self._dt_optionflags == other._dt_optionflags and \\\n               self._dt_setUp == other._dt_setUp and \\\n               self._dt_tearDown == other._dt_tearDown and \\\n               self._dt_checker == other._dt_checker\n\n    def __ne__(self, other):\n        return not self == other\n\n    def __hash__(self):\n        return hash((self._dt_optionflags, self._dt_setUp, self._dt_tearDown,\n                     self._dt_checker))\n\n    def __repr__(self):\n        name = self._dt_test.name.split('.')\n        return \"%s (%s)\" % (name[-1], '.'.join(name[:-1]))\n\n    __str__ = __repr__\n\n    def shortDescription(self):\n        return \"Doctest: \" + self._dt_test.name\n\nclass SkipDocTestCase(DocTestCase):\n    def __init__(self, module):\n        self.module = module\n        DocTestCase.__init__(self, None)\n\n    def setUp(self):\n        self.skipTest(\"DocTestSuite will not work with -O2 and above\")\n\n    def test_skip(self):\n        pass\n\n    def shortDescription(self):\n        return \"Skipping tests from %s\" % self.module.__name__\n\n    __str__ = shortDescription\n\n\ndef DocTestSuite(module=None, globs=None, extraglobs=None, test_finder=None,\n                 **options):\n    \"\"\"\n    Convert doctest tests for a module to a unittest test suite.\n\n    This converts each documentation string in a module that\n    contains doctest tests to a unittest test case.  If any of the\n    tests in a doc string fail, then the test case fails.  An exception\n    is raised showing the name of the file containing the test and a\n    (sometimes approximate) line number.\n\n    The `module` argument provides the module to be tested.  The argument\n    can be either a module or a module name.\n\n    If no argument is given, the calling module is used.\n\n    A number of options may be provided as keyword arguments:\n\n    setUp\n      A set-up function.  This is called before running the\n      tests in each file. The setUp function will be passed a DocTest\n      object.  The setUp function can access the test globals as the\n      globs attribute of the test passed.\n\n    tearDown\n      A tear-down function.  This is called after running the\n      tests in each file.  The tearDown function will be passed a DocTest\n      object.  The tearDown function can access the test globals as the\n      globs attribute of the test passed.\n\n    globs\n      A dictionary containing initial global variables for the tests.\n\n    optionflags\n       A set of doctest option flags expressed as an integer.\n    \"\"\"\n\n    if test_finder is None:\n        test_finder = DocTestFinder()\n\n    module = _normalize_module(module)\n    tests = test_finder.find(module, globs=globs, extraglobs=extraglobs)\n\n    if not tests and sys.flags.optimize >=2:\n        # Skip doctests when running with -O2\n        suite = unittest.TestSuite()\n        suite.addTest(SkipDocTestCase(module))\n        return suite\n    elif not tests:\n        # Why do we want to do this? Because it reveals a bug that might\n        # otherwise be hidden.\n        # It is probably a bug that this exception is not also raised if the\n        # number of doctest examples in tests is zero (i.e. if no doctest\n        # examples were found).  However, we should probably not be raising\n        # an exception at all here, though it is too late to make this change\n        # for a maintenance release.  See also issue #14649.\n        raise ValueError(module, \"has no docstrings\")\n\n    tests.sort()\n    suite = unittest.TestSuite()\n\n    for test in tests:\n        if len(test.examples) == 0:\n            continue\n        if not test.filename:\n            filename = module.__file__\n            if filename[-4:] in (\".pyc\", \".pyo\"):\n                filename = filename[:-1]\n            test.filename = filename\n        suite.addTest(DocTestCase(test, **options))\n\n    return suite\n\nclass DocFileCase(DocTestCase):\n\n    def id(self):\n        return '_'.join(self._dt_test.name.split('.'))\n\n    def __repr__(self):\n        return self._dt_test.filename\n    __str__ = __repr__\n\n    def format_failure(self, err):\n        return ('Failed doctest test for %s\\n  File \"%s\", line 0\\n\\n%s'\n                % (self._dt_test.name, self._dt_test.filename, err)\n                )\n\ndef DocFileTest(path, module_relative=True, package=None,\n                globs=None, parser=DocTestParser(),\n                encoding=None, **options):\n    if globs is None:\n        globs = {}\n    else:\n        globs = globs.copy()\n\n    if package and not module_relative:\n        raise ValueError(\"Package may only be specified for module-\"\n                         \"relative paths.\")\n\n    # Relativize the path.\n    doc, path = _load_testfile(path, package, module_relative)\n\n    if \"__file__\" not in globs:\n        globs[\"__file__\"] = path\n\n    # Find the file and read it.\n    name = os.path.basename(path)\n\n    # If an encoding is specified, use it to convert the file to unicode\n    if encoding is not None:\n        doc = doc.decode(encoding)\n\n    # Convert it to a test, and wrap it in a DocFileCase.\n    test = parser.get_doctest(doc, globs, name, path, 0)\n    return DocFileCase(test, **options)\n\ndef DocFileSuite(*paths, **kw):\n    \"\"\"A unittest suite for one or more doctest files.\n\n    The path to each doctest file is given as a string; the\n    interpretation of that string depends on the keyword argument\n    \"module_relative\".\n\n    A number of options may be provided as keyword arguments:\n\n    module_relative\n      If \"module_relative\" is True, then the given file paths are\n      interpreted as os-independent module-relative paths.  By\n      default, these paths are relative to the calling module's\n      directory; but if the \"package\" argument is specified, then\n      they are relative to that package.  To ensure os-independence,\n      \"filename\" should use \"/\" characters to separate path\n      segments, and may not be an absolute path (i.e., it may not\n      begin with \"/\").\n\n      If \"module_relative\" is False, then the given file paths are\n      interpreted as os-specific paths.  These paths may be absolute\n      or relative (to the current working directory).\n\n    package\n      A Python package or the name of a Python package whose directory\n      should be used as the base directory for module relative paths.\n      If \"package\" is not specified, then the calling module's\n      directory is used as the base directory for module relative\n      filenames.  It is an error to specify \"package\" if\n      \"module_relative\" is False.\n\n    setUp\n      A set-up function.  This is called before running the\n      tests in each file. The setUp function will be passed a DocTest\n      object.  The setUp function can access the test globals as the\n      globs attribute of the test passed.\n\n    tearDown\n      A tear-down function.  This is called after running the\n      tests in each file.  The tearDown function will be passed a DocTest\n      object.  The tearDown function can access the test globals as the\n      globs attribute of the test passed.\n\n    globs\n      A dictionary containing initial global variables for the tests.\n\n    optionflags\n      A set of doctest option flags expressed as an integer.\n\n    parser\n      A DocTestParser (or subclass) that should be used to extract\n      tests from the files.\n\n    encoding\n      An encoding that will be used to convert the files to unicode.\n    \"\"\"\n    suite = unittest.TestSuite()\n\n    # We do this here so that _normalize_module is called at the right\n    # level.  If it were called in DocFileTest, then this function\n    # would be the caller and we might guess the package incorrectly.\n    if kw.get('module_relative', True):\n        kw['package'] = _normalize_module(kw.get('package'))\n\n    for path in paths:\n        suite.addTest(DocFileTest(path, **kw))\n\n    return suite\n\n######################################################################\n## 9. Debugging Support\n######################################################################\n\ndef script_from_examples(s):\n    r\"\"\"Extract script from text with examples.\n\n       Converts text with examples to a Python script.  Example input is\n       converted to regular code.  Example output and all other words\n       are converted to comments:\n\n       >>> text = '''\n       ...       Here are examples of simple math.\n       ...\n       ...           Python has super accurate integer addition\n       ...\n       ...           >>> 2 + 2\n       ...           5\n       ...\n       ...           And very friendly error messages:\n       ...\n       ...           >>> 1/0\n       ...           To Infinity\n       ...           And\n       ...           Beyond\n       ...\n       ...           You can use logic if you want:\n       ...\n       ...           >>> if 0:\n       ...           ...    blah\n       ...           ...    blah\n       ...           ...\n       ...\n       ...           Ho hum\n       ...           '''\n\n       >>> print script_from_examples(text)\n       # Here are examples of simple math.\n       #\n       #     Python has super accurate integer addition\n       #\n       2 + 2\n       # Expected:\n       ## 5\n       #\n       #     And very friendly error messages:\n       #\n       1/0\n       # Expected:\n       ## To Infinity\n       ## And\n       ## Beyond\n       #\n       #     You can use logic if you want:\n       #\n       if 0:\n          blah\n          blah\n       #\n       #     Ho hum\n       <BLANKLINE>\n       \"\"\"\n    output = []\n    for piece in DocTestParser().parse(s):\n        if isinstance(piece, Example):\n            # Add the example's source code (strip trailing NL)\n            output.append(piece.source[:-1])\n            # Add the expected output:\n            want = piece.want\n            if want:\n                output.append('# Expected:')\n                output += ['## '+l for l in want.split('\\n')[:-1]]\n        else:\n            # Add non-example text.\n            output += [_comment_line(l)\n                       for l in piece.split('\\n')[:-1]]\n\n    # Trim junk on both ends.\n    while output and output[-1] == '#':\n        output.pop()\n    while output and output[0] == '#':\n        output.pop(0)\n    # Combine the output, and return it.\n    # Add a courtesy newline to prevent exec from choking (see bug #1172785)\n    return '\\n'.join(output) + '\\n'\n\ndef testsource(module, name):\n    \"\"\"Extract the test sources from a doctest docstring as a script.\n\n    Provide the module (or dotted name of the module) containing the\n    test to be debugged and the name (within the module) of the object\n    with the doc string with tests to be debugged.\n    \"\"\"\n    module = _normalize_module(module)\n    tests = DocTestFinder().find(module)\n    test = [t for t in tests if t.name == name]\n    if not test:\n        raise ValueError(name, \"not found in tests\")\n    test = test[0]\n    testsrc = script_from_examples(test.docstring)\n    return testsrc\n\ndef debug_src(src, pm=False, globs=None):\n    \"\"\"Debug a single doctest docstring, in argument `src`'\"\"\"\n    testsrc = script_from_examples(src)\n    debug_script(testsrc, pm, globs)\n\ndef debug_script(src, pm=False, globs=None):\n    \"Debug a test script.  `src` is the script, as a string.\"\n    import pdb\n\n    # Note that tempfile.NameTemporaryFile() cannot be used.  As the\n    # docs say, a file so created cannot be opened by name a second time\n    # on modern Windows boxes, and execfile() needs to open it.\n    srcfilename = tempfile.mktemp(\".py\", \"doctestdebug\")\n    f = open(srcfilename, 'w')\n    f.write(src)\n    f.close()\n\n    try:\n        if globs:\n            globs = globs.copy()\n        else:\n            globs = {}\n\n        if pm:\n            try:\n                execfile(srcfilename, globs, globs)\n            except:\n                print sys.exc_info()[1]\n                pdb.post_mortem(sys.exc_info()[2])\n        else:\n            # Note that %r is vital here.  '%s' instead can, e.g., cause\n            # backslashes to get treated as metacharacters on Windows.\n            pdb.run(\"execfile(%r)\" % srcfilename, globs, globs)\n\n    finally:\n        os.remove(srcfilename)\n\ndef debug(module, name, pm=False):\n    \"\"\"Debug a single doctest docstring.\n\n    Provide the module (or dotted name of the module) containing the\n    test to be debugged and the name (within the module) of the object\n    with the docstring with tests to be debugged.\n    \"\"\"\n    module = _normalize_module(module)\n    testsrc = testsource(module, name)\n    debug_script(testsrc, pm, module.__dict__)\n\n######################################################################\n## 10. Example Usage\n######################################################################\nclass _TestClass:\n    \"\"\"\n    A pointless class, for sanity-checking of docstring testing.\n\n    Methods:\n        square()\n        get()\n\n    >>> _TestClass(13).get() + _TestClass(-12).get()\n    1\n    >>> hex(_TestClass(13).square().get())\n    '0xa9'\n    \"\"\"\n\n    def __init__(self, val):\n        \"\"\"val -> _TestClass object with associated value val.\n\n        >>> t = _TestClass(123)\n        >>> print t.get()\n        123\n        \"\"\"\n\n        self.val = val\n\n    def square(self):\n        \"\"\"square() -> square TestClass's associated value\n\n        >>> _TestClass(13).square().get()\n        169\n        \"\"\"\n\n        self.val = self.val ** 2\n        return self\n\n    def get(self):\n        \"\"\"get() -> return TestClass's associated value.\n\n        >>> x = _TestClass(-42)\n        >>> print x.get()\n        -42\n        \"\"\"\n\n        return self.val\n\n__test__ = {\"_TestClass\": _TestClass,\n            \"string\": r\"\"\"\n                      Example of a string object, searched as-is.\n                      >>> x = 1; y = 2\n                      >>> x + y, x * y\n                      (3, 2)\n                      \"\"\",\n\n            \"bool-int equivalence\": r\"\"\"\n                                    In 2.2, boolean expressions displayed\n                                    0 or 1.  By default, we still accept\n                                    them.  This can be disabled by passing\n                                    DONT_ACCEPT_TRUE_FOR_1 to the new\n                                    optionflags argument.\n                                    >>> 4 == 4\n                                    1\n                                    >>> 4 == 4\n                                    True\n                                    >>> 4 > 4\n                                    0\n                                    >>> 4 > 4\n                                    False\n                                    \"\"\",\n\n            \"blank lines\": r\"\"\"\n                Blank lines can be marked with <BLANKLINE>:\n                    >>> print 'foo\\n\\nbar\\n'\n                    foo\n                    <BLANKLINE>\n                    bar\n                    <BLANKLINE>\n            \"\"\",\n\n            \"ellipsis\": r\"\"\"\n                If the ellipsis flag is used, then '...' can be used to\n                elide substrings in the desired output:\n                    >>> print range(1000) #doctest: +ELLIPSIS\n                    [0, 1, 2, ..., 999]\n            \"\"\",\n\n            \"whitespace normalization\": r\"\"\"\n                If the whitespace normalization flag is used, then\n                differences in whitespace are ignored.\n                    >>> print range(30) #doctest: +NORMALIZE_WHITESPACE\n                    [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14,\n                     15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26,\n                     27, 28, 29]\n            \"\"\",\n           }\n\n\ndef _test():\n    testfiles = [arg for arg in sys.argv[1:] if arg and arg[0] != '-']\n    if not testfiles:\n        name = os.path.basename(sys.argv[0])\n        if '__loader__' in globals():          # python -m\n            name, _ = os.path.splitext(name)\n        print(\"usage: {0} [-v] file ...\".format(name))\n        return 2\n    for filename in testfiles:\n        if filename.endswith(\".py\"):\n            # It is a module -- insert its dir into sys.path and try to\n            # import it. If it is part of a package, that possibly\n            # won't work because of package imports.\n            dirname, filename = os.path.split(filename)\n            sys.path.insert(0, dirname)\n            m = __import__(filename[:-3])\n            del sys.path[0]\n            failures, _ = testmod(m)\n        else:\n            failures, _ = testfile(filename, module_relative=False)\n        if failures:\n            return 1\n    return 0\n\n\nif __name__ == \"__main__\":\n    sys.exit(_test())\n", 
    "dummy_thread": "\"\"\"Drop-in replacement for the thread module.\n\nMeant to be used as a brain-dead substitute so that threaded code does\nnot need to be rewritten for when the thread module is not present.\n\nSuggested usage is::\n\n    try:\n        import thread\n    except ImportError:\n        import dummy_thread as thread\n\n\"\"\"\n# Exports only things specified by thread documentation;\n# skipping obsolete synonyms allocate(), start_new(), exit_thread().\n__all__ = ['error', 'start_new_thread', 'exit', 'get_ident', 'allocate_lock',\n           'interrupt_main', 'LockType']\n\nimport traceback as _traceback\n\nclass error(Exception):\n    \"\"\"Dummy implementation of thread.error.\"\"\"\n\n    def __init__(self, *args):\n        self.args = args\n\ndef start_new_thread(function, args, kwargs={}):\n    \"\"\"Dummy implementation of thread.start_new_thread().\n\n    Compatibility is maintained by making sure that ``args`` is a\n    tuple and ``kwargs`` is a dictionary.  If an exception is raised\n    and it is SystemExit (which can be done by thread.exit()) it is\n    caught and nothing is done; all other exceptions are printed out\n    by using traceback.print_exc().\n\n    If the executed function calls interrupt_main the KeyboardInterrupt will be\n    raised when the function returns.\n\n    \"\"\"\n    if type(args) != type(tuple()):\n        raise TypeError(\"2nd arg must be a tuple\")\n    if type(kwargs) != type(dict()):\n        raise TypeError(\"3rd arg must be a dict\")\n    global _main\n    _main = False\n    try:\n        function(*args, **kwargs)\n    except SystemExit:\n        pass\n    except:\n        _traceback.print_exc()\n    _main = True\n    global _interrupt\n    if _interrupt:\n        _interrupt = False\n        raise KeyboardInterrupt\n\ndef exit():\n    \"\"\"Dummy implementation of thread.exit().\"\"\"\n    raise SystemExit\n\ndef get_ident():\n    \"\"\"Dummy implementation of thread.get_ident().\n\n    Since this module should only be used when threadmodule is not\n    available, it is safe to assume that the current process is the\n    only thread.  Thus a constant can be safely returned.\n    \"\"\"\n    return -1\n\ndef allocate_lock():\n    \"\"\"Dummy implementation of thread.allocate_lock().\"\"\"\n    return LockType()\n\ndef stack_size(size=None):\n    \"\"\"Dummy implementation of thread.stack_size().\"\"\"\n    if size is not None:\n        raise error(\"setting thread stack size not supported\")\n    return 0\n\nclass LockType(object):\n    \"\"\"Class implementing dummy implementation of thread.LockType.\n\n    Compatibility is maintained by maintaining self.locked_status\n    which is a boolean that stores the state of the lock.  Pickling of\n    the lock, though, should not be done since if the thread module is\n    then used with an unpickled ``lock()`` from here problems could\n    occur from this class not having atomic methods.\n\n    \"\"\"\n\n    def __init__(self):\n        self.locked_status = False\n\n    def acquire(self, waitflag=None):\n        \"\"\"Dummy implementation of acquire().\n\n        For blocking calls, self.locked_status is automatically set to\n        True and returned appropriately based on value of\n        ``waitflag``.  If it is non-blocking, then the value is\n        actually checked and not set if it is already acquired.  This\n        is all done so that threading.Condition's assert statements\n        aren't triggered and throw a little fit.\n\n        \"\"\"\n        if waitflag is None or waitflag:\n            self.locked_status = True\n            return True\n        else:\n            if not self.locked_status:\n                self.locked_status = True\n                return True\n            else:\n                return False\n\n    __enter__ = acquire\n\n    def __exit__(self, typ, val, tb):\n        self.release()\n\n    def release(self):\n        \"\"\"Release the dummy lock.\"\"\"\n        # XXX Perhaps shouldn't actually bother to test?  Could lead\n        #     to problems for complex, threaded code.\n        if not self.locked_status:\n            raise error\n        self.locked_status = False\n        return True\n\n    def locked(self):\n        return self.locked_status\n\n# Used to signal that interrupt_main was called in a \"thread\"\n_interrupt = False\n# True when not executing in a \"thread\"\n_main = True\n\ndef interrupt_main():\n    \"\"\"Set _interrupt flag to True to have start_new_thread raise\n    KeyboardInterrupt upon exiting.\"\"\"\n    if _main:\n        raise KeyboardInterrupt\n    else:\n        global _interrupt\n        _interrupt = True\n", 
    "encodings.__init__": "\"\"\" Standard \"encodings\" Package\n\n    Standard Python encoding modules are stored in this package\n    directory.\n\n    Codec modules must have names corresponding to normalized encoding\n    names as defined in the normalize_encoding() function below, e.g.\n    'utf-8' must be implemented by the module 'utf_8.py'.\n\n    Each codec module must export the following interface:\n\n    * getregentry() -> codecs.CodecInfo object\n    The getregentry() API must a CodecInfo object with encoder, decoder,\n    incrementalencoder, incrementaldecoder, streamwriter and streamreader\n    atttributes which adhere to the Python Codec Interface Standard.\n\n    In addition, a module may optionally also define the following\n    APIs which are then used by the package's codec search function:\n\n    * getaliases() -> sequence of encoding name strings to use as aliases\n\n    Alias names returned by getaliases() must be normalized encoding\n    names as defined by normalize_encoding().\n\nWritten by Marc-Andre Lemburg (mal@lemburg.com).\n\n(c) Copyright CNRI, All Rights Reserved. NO WARRANTY.\n\n\"\"\"#\"\n\nimport codecs\nfrom encodings import aliases\nimport __builtin__\n\n_cache = {}\n_unknown = '--unknown--'\n_import_tail = ['*']\n_norm_encoding_map = ('                                              . '\n                      '0123456789       ABCDEFGHIJKLMNOPQRSTUVWXYZ     '\n                      ' abcdefghijklmnopqrstuvwxyz                     '\n                      '                                                '\n                      '                                                '\n                      '                ')\n_aliases = aliases.aliases\n\nclass CodecRegistryError(LookupError, SystemError):\n    pass\n\ndef normalize_encoding(encoding):\n\n    \"\"\" Normalize an encoding name.\n\n        Normalization works as follows: all non-alphanumeric\n        characters except the dot used for Python package names are\n        collapsed and replaced with a single underscore, e.g. '  -;#'\n        becomes '_'. Leading and trailing underscores are removed.\n\n        Note that encoding names should be ASCII only; if they do use\n        non-ASCII characters, these must be Latin-1 compatible.\n\n    \"\"\"\n    # Make sure we have an 8-bit string, because .translate() works\n    # differently for Unicode strings.\n    if hasattr(__builtin__, \"unicode\") and isinstance(encoding, unicode):\n        # Note that .encode('latin-1') does *not* use the codec\n        # registry, so this call doesn't recurse. (See unicodeobject.c\n        # PyUnicode_AsEncodedString() for details)\n        encoding = encoding.encode('latin-1')\n    return '_'.join(encoding.translate(_norm_encoding_map).split())\n\ndef search_function(encoding):\n\n    # Cache lookup\n    entry = _cache.get(encoding, _unknown)\n    if entry is not _unknown:\n        return entry\n\n    # Import the module:\n    #\n    # First try to find an alias for the normalized encoding\n    # name and lookup the module using the aliased name, then try to\n    # lookup the module using the standard import scheme, i.e. first\n    # try in the encodings package, then at top-level.\n    #\n    norm_encoding = normalize_encoding(encoding)\n    aliased_encoding = _aliases.get(norm_encoding) or \\\n                       _aliases.get(norm_encoding.replace('.', '_'))\n    if aliased_encoding is not None:\n        modnames = [aliased_encoding,\n                    norm_encoding]\n    else:\n        modnames = [norm_encoding]\n    for modname in modnames:\n        if not modname or '.' in modname:\n            continue\n        try:\n            # Import is absolute to prevent the possibly malicious import of a\n            # module with side-effects that is not in the 'encodings' package.\n            mod = __import__('encodings.' + modname, fromlist=_import_tail,\n                             level=0)\n        except ImportError:\n            pass\n        else:\n            break\n    else:\n        mod = None\n\n    try:\n        getregentry = mod.getregentry\n    except AttributeError:\n        # Not a codec module\n        mod = None\n\n    if mod is None:\n        # Cache misses\n        _cache[encoding] = None\n        return None\n\n    # Now ask the module for the registry entry\n    entry = getregentry()\n    if not isinstance(entry, codecs.CodecInfo):\n        if not 4 <= len(entry) <= 7:\n            raise CodecRegistryError,\\\n                 'module \"%s\" (%s) failed to register' % \\\n                  (mod.__name__, mod.__file__)\n        if not hasattr(entry[0], '__call__') or \\\n           not hasattr(entry[1], '__call__') or \\\n           (entry[2] is not None and not hasattr(entry[2], '__call__')) or \\\n           (entry[3] is not None and not hasattr(entry[3], '__call__')) or \\\n           (len(entry) > 4 and entry[4] is not None and not hasattr(entry[4], '__call__')) or \\\n           (len(entry) > 5 and entry[5] is not None and not hasattr(entry[5], '__call__')):\n            raise CodecRegistryError,\\\n                'incompatible codecs in module \"%s\" (%s)' % \\\n                (mod.__name__, mod.__file__)\n        if len(entry)<7 or entry[6] is None:\n            entry += (None,)*(6-len(entry)) + (mod.__name__.split(\".\", 1)[1],)\n        entry = codecs.CodecInfo(*entry)\n\n    # Cache the codec registry entry\n    _cache[encoding] = entry\n\n    # Register its aliases (without overwriting previously registered\n    # aliases)\n    try:\n        codecaliases = mod.getaliases()\n    except AttributeError:\n        pass\n    else:\n        for alias in codecaliases:\n            if alias not in _aliases:\n                _aliases[alias] = modname\n\n    # Return the registry entry\n    return entry\n\n# Register the search_function in the Python codec registry\ncodecs.register(search_function)\n", 
    "encodings.aliases": "\"\"\" Encoding Aliases Support\n\n    This module is used by the encodings package search function to\n    map encodings names to module names.\n\n    Note that the search function normalizes the encoding names before\n    doing the lookup, so the mapping will have to map normalized\n    encoding names to module names.\n\n    Contents:\n\n        The following aliases dictionary contains mappings of all IANA\n        character set names for which the Python core library provides\n        codecs. In addition to these, a few Python specific codec\n        aliases have also been added.\n\n\"\"\"\naliases = {\n\n    # Please keep this list sorted alphabetically by value !\n\n    # ascii codec\n    '646'                : 'ascii',\n    'ansi_x3.4_1968'     : 'ascii',\n    'ansi_x3_4_1968'     : 'ascii', # some email headers use this non-standard name\n    'ansi_x3.4_1986'     : 'ascii',\n    'cp367'              : 'ascii',\n    'csascii'            : 'ascii',\n    'ibm367'             : 'ascii',\n    'iso646_us'          : 'ascii',\n    'iso_646.irv_1991'   : 'ascii',\n    'iso_ir_6'           : 'ascii',\n    'us'                 : 'ascii',\n    'us_ascii'           : 'ascii',\n\n    # base64_codec codec\n    'base64'             : 'base64_codec',\n    'base_64'            : 'base64_codec',\n\n    # big5 codec\n    'big5_tw'            : 'big5',\n    'csbig5'             : 'big5',\n\n    # big5hkscs codec\n    'big5_hkscs'         : 'big5hkscs',\n    'hkscs'              : 'big5hkscs',\n\n    # bz2_codec codec\n    'bz2'                : 'bz2_codec',\n\n    # cp037 codec\n    '037'                : 'cp037',\n    'csibm037'           : 'cp037',\n    'ebcdic_cp_ca'       : 'cp037',\n    'ebcdic_cp_nl'       : 'cp037',\n    'ebcdic_cp_us'       : 'cp037',\n    'ebcdic_cp_wt'       : 'cp037',\n    'ibm037'             : 'cp037',\n    'ibm039'             : 'cp037',\n\n    # cp1026 codec\n    '1026'               : 'cp1026',\n    'csibm1026'          : 'cp1026',\n    'ibm1026'            : 'cp1026',\n\n    # cp1140 codec\n    '1140'               : 'cp1140',\n    'ibm1140'            : 'cp1140',\n\n    # cp1250 codec\n    '1250'               : 'cp1250',\n    'windows_1250'       : 'cp1250',\n\n    # cp1251 codec\n    '1251'               : 'cp1251',\n    'windows_1251'       : 'cp1251',\n\n    # cp1252 codec\n    '1252'               : 'cp1252',\n    'windows_1252'       : 'cp1252',\n\n    # cp1253 codec\n    '1253'               : 'cp1253',\n    'windows_1253'       : 'cp1253',\n\n    # cp1254 codec\n    '1254'               : 'cp1254',\n    'windows_1254'       : 'cp1254',\n\n    # cp1255 codec\n    '1255'               : 'cp1255',\n    'windows_1255'       : 'cp1255',\n\n    # cp1256 codec\n    '1256'               : 'cp1256',\n    'windows_1256'       : 'cp1256',\n\n    # cp1257 codec\n    '1257'               : 'cp1257',\n    'windows_1257'       : 'cp1257',\n\n    # cp1258 codec\n    '1258'               : 'cp1258',\n    'windows_1258'       : 'cp1258',\n\n    # cp424 codec\n    '424'                : 'cp424',\n    'csibm424'           : 'cp424',\n    'ebcdic_cp_he'       : 'cp424',\n    'ibm424'             : 'cp424',\n\n    # cp437 codec\n    '437'                : 'cp437',\n    'cspc8codepage437'   : 'cp437',\n    'ibm437'             : 'cp437',\n\n    # cp500 codec\n    '500'                : 'cp500',\n    'csibm500'           : 'cp500',\n    'ebcdic_cp_be'       : 'cp500',\n    'ebcdic_cp_ch'       : 'cp500',\n    'ibm500'             : 'cp500',\n\n    # cp775 codec\n    '775'                : 'cp775',\n    'cspc775baltic'      : 'cp775',\n    'ibm775'             : 'cp775',\n\n    # cp850 codec\n    '850'                : 'cp850',\n    'cspc850multilingual' : 'cp850',\n    'ibm850'             : 'cp850',\n\n    # cp852 codec\n    '852'                : 'cp852',\n    'cspcp852'           : 'cp852',\n    'ibm852'             : 'cp852',\n\n    # cp855 codec\n    '855'                : 'cp855',\n    'csibm855'           : 'cp855',\n    'ibm855'             : 'cp855',\n\n    # cp857 codec\n    '857'                : 'cp857',\n    'csibm857'           : 'cp857',\n    'ibm857'             : 'cp857',\n\n    # cp858 codec\n    '858'                : 'cp858',\n    'csibm858'           : 'cp858',\n    'ibm858'             : 'cp858',\n\n    # cp860 codec\n    '860'                : 'cp860',\n    'csibm860'           : 'cp860',\n    'ibm860'             : 'cp860',\n\n    # cp861 codec\n    '861'                : 'cp861',\n    'cp_is'              : 'cp861',\n    'csibm861'           : 'cp861',\n    'ibm861'             : 'cp861',\n\n    # cp862 codec\n    '862'                : 'cp862',\n    'cspc862latinhebrew' : 'cp862',\n    'ibm862'             : 'cp862',\n\n    # cp863 codec\n    '863'                : 'cp863',\n    'csibm863'           : 'cp863',\n    'ibm863'             : 'cp863',\n\n    # cp864 codec\n    '864'                : 'cp864',\n    'csibm864'           : 'cp864',\n    'ibm864'             : 'cp864',\n\n    # cp865 codec\n    '865'                : 'cp865',\n    'csibm865'           : 'cp865',\n    'ibm865'             : 'cp865',\n\n    # cp866 codec\n    '866'                : 'cp866',\n    'csibm866'           : 'cp866',\n    'ibm866'             : 'cp866',\n\n    # cp869 codec\n    '869'                : 'cp869',\n    'cp_gr'              : 'cp869',\n    'csibm869'           : 'cp869',\n    'ibm869'             : 'cp869',\n\n    # cp932 codec\n    '932'                : 'cp932',\n    'ms932'              : 'cp932',\n    'mskanji'            : 'cp932',\n    'ms_kanji'           : 'cp932',\n\n    # cp949 codec\n    '949'                : 'cp949',\n    'ms949'              : 'cp949',\n    'uhc'                : 'cp949',\n\n    # cp950 codec\n    '950'                : 'cp950',\n    'ms950'              : 'cp950',\n\n    # euc_jis_2004 codec\n    'jisx0213'           : 'euc_jis_2004',\n    'eucjis2004'         : 'euc_jis_2004',\n    'euc_jis2004'        : 'euc_jis_2004',\n\n    # euc_jisx0213 codec\n    'eucjisx0213'        : 'euc_jisx0213',\n\n    # euc_jp codec\n    'eucjp'              : 'euc_jp',\n    'ujis'               : 'euc_jp',\n    'u_jis'              : 'euc_jp',\n\n    # euc_kr codec\n    'euckr'              : 'euc_kr',\n    'korean'             : 'euc_kr',\n    'ksc5601'            : 'euc_kr',\n    'ks_c_5601'          : 'euc_kr',\n    'ks_c_5601_1987'     : 'euc_kr',\n    'ksx1001'            : 'euc_kr',\n    'ks_x_1001'          : 'euc_kr',\n\n    # gb18030 codec\n    'gb18030_2000'       : 'gb18030',\n\n    # gb2312 codec\n    'chinese'            : 'gb2312',\n    'csiso58gb231280'    : 'gb2312',\n    'euc_cn'             : 'gb2312',\n    'euccn'              : 'gb2312',\n    'eucgb2312_cn'       : 'gb2312',\n    'gb2312_1980'        : 'gb2312',\n    'gb2312_80'          : 'gb2312',\n    'iso_ir_58'          : 'gb2312',\n\n    # gbk codec\n    '936'                : 'gbk',\n    'cp936'              : 'gbk',\n    'ms936'              : 'gbk',\n\n    # hex_codec codec\n    'hex'                : 'hex_codec',\n\n    # hp_roman8 codec\n    'roman8'             : 'hp_roman8',\n    'r8'                 : 'hp_roman8',\n    'csHPRoman8'         : 'hp_roman8',\n\n    # hz codec\n    'hzgb'               : 'hz',\n    'hz_gb'              : 'hz',\n    'hz_gb_2312'         : 'hz',\n\n    # iso2022_jp codec\n    'csiso2022jp'        : 'iso2022_jp',\n    'iso2022jp'          : 'iso2022_jp',\n    'iso_2022_jp'        : 'iso2022_jp',\n\n    # iso2022_jp_1 codec\n    'iso2022jp_1'        : 'iso2022_jp_1',\n    'iso_2022_jp_1'      : 'iso2022_jp_1',\n\n    # iso2022_jp_2 codec\n    'iso2022jp_2'        : 'iso2022_jp_2',\n    'iso_2022_jp_2'      : 'iso2022_jp_2',\n\n    # iso2022_jp_2004 codec\n    'iso_2022_jp_2004'   : 'iso2022_jp_2004',\n    'iso2022jp_2004'     : 'iso2022_jp_2004',\n\n    # iso2022_jp_3 codec\n    'iso2022jp_3'        : 'iso2022_jp_3',\n    'iso_2022_jp_3'      : 'iso2022_jp_3',\n\n    # iso2022_jp_ext codec\n    'iso2022jp_ext'      : 'iso2022_jp_ext',\n    'iso_2022_jp_ext'    : 'iso2022_jp_ext',\n\n    # iso2022_kr codec\n    'csiso2022kr'        : 'iso2022_kr',\n    'iso2022kr'          : 'iso2022_kr',\n    'iso_2022_kr'        : 'iso2022_kr',\n\n    # iso8859_10 codec\n    'csisolatin6'        : 'iso8859_10',\n    'iso_8859_10'        : 'iso8859_10',\n    'iso_8859_10_1992'   : 'iso8859_10',\n    'iso_ir_157'         : 'iso8859_10',\n    'l6'                 : 'iso8859_10',\n    'latin6'             : 'iso8859_10',\n\n    # iso8859_11 codec\n    'thai'               : 'iso8859_11',\n    'iso_8859_11'        : 'iso8859_11',\n    'iso_8859_11_2001'   : 'iso8859_11',\n\n    # iso8859_13 codec\n    'iso_8859_13'        : 'iso8859_13',\n    'l7'                 : 'iso8859_13',\n    'latin7'             : 'iso8859_13',\n\n    # iso8859_14 codec\n    'iso_8859_14'        : 'iso8859_14',\n    'iso_8859_14_1998'   : 'iso8859_14',\n    'iso_celtic'         : 'iso8859_14',\n    'iso_ir_199'         : 'iso8859_14',\n    'l8'                 : 'iso8859_14',\n    'latin8'             : 'iso8859_14',\n\n    # iso8859_15 codec\n    'iso_8859_15'        : 'iso8859_15',\n    'l9'                 : 'iso8859_15',\n    'latin9'             : 'iso8859_15',\n\n    # iso8859_16 codec\n    'iso_8859_16'        : 'iso8859_16',\n    'iso_8859_16_2001'   : 'iso8859_16',\n    'iso_ir_226'         : 'iso8859_16',\n    'l10'                : 'iso8859_16',\n    'latin10'            : 'iso8859_16',\n\n    # iso8859_2 codec\n    'csisolatin2'        : 'iso8859_2',\n    'iso_8859_2'         : 'iso8859_2',\n    'iso_8859_2_1987'    : 'iso8859_2',\n    'iso_ir_101'         : 'iso8859_2',\n    'l2'                 : 'iso8859_2',\n    'latin2'             : 'iso8859_2',\n\n    # iso8859_3 codec\n    'csisolatin3'        : 'iso8859_3',\n    'iso_8859_3'         : 'iso8859_3',\n    'iso_8859_3_1988'    : 'iso8859_3',\n    'iso_ir_109'         : 'iso8859_3',\n    'l3'                 : 'iso8859_3',\n    'latin3'             : 'iso8859_3',\n\n    # iso8859_4 codec\n    'csisolatin4'        : 'iso8859_4',\n    'iso_8859_4'         : 'iso8859_4',\n    'iso_8859_4_1988'    : 'iso8859_4',\n    'iso_ir_110'         : 'iso8859_4',\n    'l4'                 : 'iso8859_4',\n    'latin4'             : 'iso8859_4',\n\n    # iso8859_5 codec\n    'csisolatincyrillic' : 'iso8859_5',\n    'cyrillic'           : 'iso8859_5',\n    'iso_8859_5'         : 'iso8859_5',\n    'iso_8859_5_1988'    : 'iso8859_5',\n    'iso_ir_144'         : 'iso8859_5',\n\n    # iso8859_6 codec\n    'arabic'             : 'iso8859_6',\n    'asmo_708'           : 'iso8859_6',\n    'csisolatinarabic'   : 'iso8859_6',\n    'ecma_114'           : 'iso8859_6',\n    'iso_8859_6'         : 'iso8859_6',\n    'iso_8859_6_1987'    : 'iso8859_6',\n    'iso_ir_127'         : 'iso8859_6',\n\n    # iso8859_7 codec\n    'csisolatingreek'    : 'iso8859_7',\n    'ecma_118'           : 'iso8859_7',\n    'elot_928'           : 'iso8859_7',\n    'greek'              : 'iso8859_7',\n    'greek8'             : 'iso8859_7',\n    'iso_8859_7'         : 'iso8859_7',\n    'iso_8859_7_1987'    : 'iso8859_7',\n    'iso_ir_126'         : 'iso8859_7',\n\n    # iso8859_8 codec\n    'csisolatinhebrew'   : 'iso8859_8',\n    'hebrew'             : 'iso8859_8',\n    'iso_8859_8'         : 'iso8859_8',\n    'iso_8859_8_1988'    : 'iso8859_8',\n    'iso_ir_138'         : 'iso8859_8',\n\n    # iso8859_9 codec\n    'csisolatin5'        : 'iso8859_9',\n    'iso_8859_9'         : 'iso8859_9',\n    'iso_8859_9_1989'    : 'iso8859_9',\n    'iso_ir_148'         : 'iso8859_9',\n    'l5'                 : 'iso8859_9',\n    'latin5'             : 'iso8859_9',\n\n    # johab codec\n    'cp1361'             : 'johab',\n    'ms1361'             : 'johab',\n\n    # koi8_r codec\n    'cskoi8r'            : 'koi8_r',\n\n    # latin_1 codec\n    #\n    # Note that the latin_1 codec is implemented internally in C and a\n    # lot faster than the charmap codec iso8859_1 which uses the same\n    # encoding. This is why we discourage the use of the iso8859_1\n    # codec and alias it to latin_1 instead.\n    #\n    '8859'               : 'latin_1',\n    'cp819'              : 'latin_1',\n    'csisolatin1'        : 'latin_1',\n    'ibm819'             : 'latin_1',\n    'iso8859'            : 'latin_1',\n    'iso8859_1'          : 'latin_1',\n    'iso_8859_1'         : 'latin_1',\n    'iso_8859_1_1987'    : 'latin_1',\n    'iso_ir_100'         : 'latin_1',\n    'l1'                 : 'latin_1',\n    'latin'              : 'latin_1',\n    'latin1'             : 'latin_1',\n\n    # mac_cyrillic codec\n    'maccyrillic'        : 'mac_cyrillic',\n\n    # mac_greek codec\n    'macgreek'           : 'mac_greek',\n\n    # mac_iceland codec\n    'maciceland'         : 'mac_iceland',\n\n    # mac_latin2 codec\n    'maccentraleurope'   : 'mac_latin2',\n    'maclatin2'          : 'mac_latin2',\n\n    # mac_roman codec\n    'macroman'           : 'mac_roman',\n\n    # mac_turkish codec\n    'macturkish'         : 'mac_turkish',\n\n    # mbcs codec\n    'dbcs'               : 'mbcs',\n\n    # ptcp154 codec\n    'csptcp154'          : 'ptcp154',\n    'pt154'              : 'ptcp154',\n    'cp154'              : 'ptcp154',\n    'cyrillic_asian'     : 'ptcp154',\n\n    # quopri_codec codec\n    'quopri'             : 'quopri_codec',\n    'quoted_printable'   : 'quopri_codec',\n    'quotedprintable'    : 'quopri_codec',\n\n    # rot_13 codec\n    'rot13'              : 'rot_13',\n\n    # shift_jis codec\n    'csshiftjis'         : 'shift_jis',\n    'shiftjis'           : 'shift_jis',\n    'sjis'               : 'shift_jis',\n    's_jis'              : 'shift_jis',\n\n    # shift_jis_2004 codec\n    'shiftjis2004'       : 'shift_jis_2004',\n    'sjis_2004'          : 'shift_jis_2004',\n    's_jis_2004'         : 'shift_jis_2004',\n\n    # shift_jisx0213 codec\n    'shiftjisx0213'      : 'shift_jisx0213',\n    'sjisx0213'          : 'shift_jisx0213',\n    's_jisx0213'         : 'shift_jisx0213',\n\n    # tactis codec\n    'tis260'             : 'tactis',\n\n    # tis_620 codec\n    'tis620'             : 'tis_620',\n    'tis_620_0'          : 'tis_620',\n    'tis_620_2529_0'     : 'tis_620',\n    'tis_620_2529_1'     : 'tis_620',\n    'iso_ir_166'         : 'tis_620',\n\n    # utf_16 codec\n    'u16'                : 'utf_16',\n    'utf16'              : 'utf_16',\n\n    # utf_16_be codec\n    'unicodebigunmarked' : 'utf_16_be',\n    'utf_16be'           : 'utf_16_be',\n\n    # utf_16_le codec\n    'unicodelittleunmarked' : 'utf_16_le',\n    'utf_16le'           : 'utf_16_le',\n\n    # utf_32 codec\n    'u32'                : 'utf_32',\n    'utf32'              : 'utf_32',\n\n    # utf_32_be codec\n    'utf_32be'           : 'utf_32_be',\n\n    # utf_32_le codec\n    'utf_32le'           : 'utf_32_le',\n\n    # utf_7 codec\n    'u7'                 : 'utf_7',\n    'utf7'               : 'utf_7',\n    'unicode_1_1_utf_7'  : 'utf_7',\n\n    # utf_8 codec\n    'u8'                 : 'utf_8',\n    'utf'                : 'utf_8',\n    'utf8'               : 'utf_8',\n    'utf8_ucs2'          : 'utf_8',\n    'utf8_ucs4'          : 'utf_8',\n\n    # uu_codec codec\n    'uu'                 : 'uu_codec',\n\n    # zlib_codec codec\n    'zip'                : 'zlib_codec',\n    'zlib'               : 'zlib_codec',\n\n}\n", 
    "encodings.ascii": "\"\"\" Python 'ascii' Codec\n\n\nWritten by Marc-Andre Lemburg (mal@lemburg.com).\n\n(c) Copyright CNRI, All Rights Reserved. NO WARRANTY.\n\n\"\"\"\nimport codecs\n\n### Codec APIs\n\nclass Codec(codecs.Codec):\n\n    # Note: Binding these as C functions will result in the class not\n    # converting them to methods. This is intended.\n    encode = codecs.ascii_encode\n    decode = codecs.ascii_decode\n\nclass IncrementalEncoder(codecs.IncrementalEncoder):\n    def encode(self, input, final=False):\n        return codecs.ascii_encode(input, self.errors)[0]\n\nclass IncrementalDecoder(codecs.IncrementalDecoder):\n    def decode(self, input, final=False):\n        return codecs.ascii_decode(input, self.errors)[0]\n\nclass StreamWriter(Codec,codecs.StreamWriter):\n    pass\n\nclass StreamReader(Codec,codecs.StreamReader):\n    pass\n\nclass StreamConverter(StreamWriter,StreamReader):\n\n    encode = codecs.ascii_decode\n    decode = codecs.ascii_encode\n\n### encodings module API\n\ndef getregentry():\n    return codecs.CodecInfo(\n        name='ascii',\n        encode=Codec.encode,\n        decode=Codec.decode,\n        incrementalencoder=IncrementalEncoder,\n        incrementaldecoder=IncrementalDecoder,\n        streamwriter=StreamWriter,\n        streamreader=StreamReader,\n    )\n", 
    "encodings.base64_codec": "\"\"\" Python 'base64_codec' Codec - base64 content transfer encoding\n\n    Unlike most of the other codecs which target Unicode, this codec\n    will return Python string objects for both encode and decode.\n\n    Written by Marc-Andre Lemburg (mal@lemburg.com).\n\n\"\"\"\nimport codecs, base64\n\n### Codec APIs\n\ndef base64_encode(input,errors='strict'):\n\n    \"\"\" Encodes the object input and returns a tuple (output\n        object, length consumed).\n\n        errors defines the error handling to apply. It defaults to\n        'strict' handling which is the only currently supported\n        error handling for this codec.\n\n    \"\"\"\n    assert errors == 'strict'\n    output = base64.encodestring(input)\n    return (output, len(input))\n\ndef base64_decode(input,errors='strict'):\n\n    \"\"\" Decodes the object input and returns a tuple (output\n        object, length consumed).\n\n        input must be an object which provides the bf_getreadbuf\n        buffer slot. Python strings, buffer objects and memory\n        mapped files are examples of objects providing this slot.\n\n        errors defines the error handling to apply. It defaults to\n        'strict' handling which is the only currently supported\n        error handling for this codec.\n\n    \"\"\"\n    assert errors == 'strict'\n    output = base64.decodestring(input)\n    return (output, len(input))\n\nclass Codec(codecs.Codec):\n\n    def encode(self, input,errors='strict'):\n        return base64_encode(input,errors)\n    def decode(self, input,errors='strict'):\n        return base64_decode(input,errors)\n\nclass IncrementalEncoder(codecs.IncrementalEncoder):\n    def encode(self, input, final=False):\n        assert self.errors == 'strict'\n        return base64.encodestring(input)\n\nclass IncrementalDecoder(codecs.IncrementalDecoder):\n    def decode(self, input, final=False):\n        assert self.errors == 'strict'\n        return base64.decodestring(input)\n\nclass StreamWriter(Codec,codecs.StreamWriter):\n    pass\n\nclass StreamReader(Codec,codecs.StreamReader):\n    pass\n\n### encodings module API\n\ndef getregentry():\n    return codecs.CodecInfo(\n        name='base64',\n        encode=base64_encode,\n        decode=base64_decode,\n        incrementalencoder=IncrementalEncoder,\n        incrementaldecoder=IncrementalDecoder,\n        streamwriter=StreamWriter,\n        streamreader=StreamReader,\n    )\n", 
    "encodings.hex_codec": "\"\"\" Python 'hex_codec' Codec - 2-digit hex content transfer encoding\n\n    Unlike most of the other codecs which target Unicode, this codec\n    will return Python string objects for both encode and decode.\n\n    Written by Marc-Andre Lemburg (mal@lemburg.com).\n\n\"\"\"\nimport codecs, binascii\n\n### Codec APIs\n\ndef hex_encode(input,errors='strict'):\n\n    \"\"\" Encodes the object input and returns a tuple (output\n        object, length consumed).\n\n        errors defines the error handling to apply. It defaults to\n        'strict' handling which is the only currently supported\n        error handling for this codec.\n\n    \"\"\"\n    assert errors == 'strict'\n    output = binascii.b2a_hex(input)\n    return (output, len(input))\n\ndef hex_decode(input,errors='strict'):\n\n    \"\"\" Decodes the object input and returns a tuple (output\n        object, length consumed).\n\n        input must be an object which provides the bf_getreadbuf\n        buffer slot. Python strings, buffer objects and memory\n        mapped files are examples of objects providing this slot.\n\n        errors defines the error handling to apply. It defaults to\n        'strict' handling which is the only currently supported\n        error handling for this codec.\n\n    \"\"\"\n    assert errors == 'strict'\n    output = binascii.a2b_hex(input)\n    return (output, len(input))\n\nclass Codec(codecs.Codec):\n\n    def encode(self, input,errors='strict'):\n        return hex_encode(input,errors)\n    def decode(self, input,errors='strict'):\n        return hex_decode(input,errors)\n\nclass IncrementalEncoder(codecs.IncrementalEncoder):\n    def encode(self, input, final=False):\n        assert self.errors == 'strict'\n        return binascii.b2a_hex(input)\n\nclass IncrementalDecoder(codecs.IncrementalDecoder):\n    def decode(self, input, final=False):\n        assert self.errors == 'strict'\n        return binascii.a2b_hex(input)\n\nclass StreamWriter(Codec,codecs.StreamWriter):\n    pass\n\nclass StreamReader(Codec,codecs.StreamReader):\n    pass\n\n### encodings module API\n\ndef getregentry():\n    return codecs.CodecInfo(\n        name='hex',\n        encode=hex_encode,\n        decode=hex_decode,\n        incrementalencoder=IncrementalEncoder,\n        incrementaldecoder=IncrementalDecoder,\n        streamwriter=StreamWriter,\n        streamreader=StreamReader,\n    )\n", 
    "encodings.latin_1": "\"\"\" Python 'latin-1' Codec\n\n\nWritten by Marc-Andre Lemburg (mal@lemburg.com).\n\n(c) Copyright CNRI, All Rights Reserved. NO WARRANTY.\n\n\"\"\"\nimport codecs\n\n### Codec APIs\n\nclass Codec(codecs.Codec):\n\n    # Note: Binding these as C functions will result in the class not\n    # converting them to methods. This is intended.\n    encode = codecs.latin_1_encode\n    decode = codecs.latin_1_decode\n\nclass IncrementalEncoder(codecs.IncrementalEncoder):\n    def encode(self, input, final=False):\n        return codecs.latin_1_encode(input,self.errors)[0]\n\nclass IncrementalDecoder(codecs.IncrementalDecoder):\n    def decode(self, input, final=False):\n        return codecs.latin_1_decode(input,self.errors)[0]\n\nclass StreamWriter(Codec,codecs.StreamWriter):\n    pass\n\nclass StreamReader(Codec,codecs.StreamReader):\n    pass\n\nclass StreamConverter(StreamWriter,StreamReader):\n\n    encode = codecs.latin_1_decode\n    decode = codecs.latin_1_encode\n\n### encodings module API\n\ndef getregentry():\n    return codecs.CodecInfo(\n        name='iso8859-1',\n        encode=Codec.encode,\n        decode=Codec.decode,\n        incrementalencoder=IncrementalEncoder,\n        incrementaldecoder=IncrementalDecoder,\n        streamreader=StreamReader,\n        streamwriter=StreamWriter,\n    )\n", 
    "encodings.raw_unicode_escape": "\"\"\" Python 'raw-unicode-escape' Codec\n\n\nWritten by Marc-Andre Lemburg (mal@lemburg.com).\n\n(c) Copyright CNRI, All Rights Reserved. NO WARRANTY.\n\n\"\"\"\nimport codecs\n\n### Codec APIs\n\nclass Codec(codecs.Codec):\n\n    # Note: Binding these as C functions will result in the class not\n    # converting them to methods. This is intended.\n    encode = codecs.raw_unicode_escape_encode\n    decode = codecs.raw_unicode_escape_decode\n\nclass IncrementalEncoder(codecs.IncrementalEncoder):\n    def encode(self, input, final=False):\n        return codecs.raw_unicode_escape_encode(input, self.errors)[0]\n\nclass IncrementalDecoder(codecs.IncrementalDecoder):\n    def decode(self, input, final=False):\n        return codecs.raw_unicode_escape_decode(input, self.errors)[0]\n\nclass StreamWriter(Codec,codecs.StreamWriter):\n    pass\n\nclass StreamReader(Codec,codecs.StreamReader):\n    pass\n\n### encodings module API\n\ndef getregentry():\n    return codecs.CodecInfo(\n        name='raw-unicode-escape',\n        encode=Codec.encode,\n        decode=Codec.decode,\n        incrementalencoder=IncrementalEncoder,\n        incrementaldecoder=IncrementalDecoder,\n        streamwriter=StreamWriter,\n        streamreader=StreamReader,\n    )\n", 
    "encodings.string_escape": "# -*- coding: utf-8 -*-\n\"\"\" Python 'escape' Codec\n\n\nWritten by Martin v. L\u00f6wis (martin@v.loewis.de).\n\n\"\"\"\nimport codecs\n\nclass Codec(codecs.Codec):\n\n    encode = codecs.escape_encode\n    decode = codecs.escape_decode\n\nclass IncrementalEncoder(codecs.IncrementalEncoder):\n    def encode(self, input, final=False):\n        return codecs.escape_encode(input, self.errors)[0]\n\nclass IncrementalDecoder(codecs.IncrementalDecoder):\n    def decode(self, input, final=False):\n        return codecs.escape_decode(input, self.errors)[0]\n\nclass StreamWriter(Codec,codecs.StreamWriter):\n    pass\n\nclass StreamReader(Codec,codecs.StreamReader):\n    pass\n\ndef getregentry():\n    return codecs.CodecInfo(\n        name='string-escape',\n        encode=Codec.encode,\n        decode=Codec.decode,\n        incrementalencoder=IncrementalEncoder,\n        incrementaldecoder=IncrementalDecoder,\n        streamwriter=StreamWriter,\n        streamreader=StreamReader,\n    )\n", 
    "encodings.unicode_escape": "\"\"\" Python 'unicode-escape' Codec\n\n\nWritten by Marc-Andre Lemburg (mal@lemburg.com).\n\n(c) Copyright CNRI, All Rights Reserved. NO WARRANTY.\n\n\"\"\"\nimport codecs\n\n### Codec APIs\n\nclass Codec(codecs.Codec):\n\n    # Note: Binding these as C functions will result in the class not\n    # converting them to methods. This is intended.\n    encode = codecs.unicode_escape_encode\n    decode = codecs.unicode_escape_decode\n\nclass IncrementalEncoder(codecs.IncrementalEncoder):\n    def encode(self, input, final=False):\n        return codecs.unicode_escape_encode(input, self.errors)[0]\n\nclass IncrementalDecoder(codecs.IncrementalDecoder):\n    def decode(self, input, final=False):\n        return codecs.unicode_escape_decode(input, self.errors)[0]\n\nclass StreamWriter(Codec,codecs.StreamWriter):\n    pass\n\nclass StreamReader(Codec,codecs.StreamReader):\n    pass\n\n### encodings module API\n\ndef getregentry():\n    return codecs.CodecInfo(\n        name='unicode-escape',\n        encode=Codec.encode,\n        decode=Codec.decode,\n        incrementalencoder=IncrementalEncoder,\n        incrementaldecoder=IncrementalDecoder,\n        streamwriter=StreamWriter,\n        streamreader=StreamReader,\n    )\n", 
    "encodings.unicode_internal": "\"\"\" Python 'unicode-internal' Codec\n\n\nWritten by Marc-Andre Lemburg (mal@lemburg.com).\n\n(c) Copyright CNRI, All Rights Reserved. NO WARRANTY.\n\n\"\"\"\nimport codecs\n\n### Codec APIs\n\nclass Codec(codecs.Codec):\n\n    # Note: Binding these as C functions will result in the class not\n    # converting them to methods. This is intended.\n    encode = codecs.unicode_internal_encode\n    decode = codecs.unicode_internal_decode\n\nclass IncrementalEncoder(codecs.IncrementalEncoder):\n    def encode(self, input, final=False):\n        return codecs.unicode_internal_encode(input, self.errors)[0]\n\nclass IncrementalDecoder(codecs.IncrementalDecoder):\n    def decode(self, input, final=False):\n        return codecs.unicode_internal_decode(input, self.errors)[0]\n\nclass StreamWriter(Codec,codecs.StreamWriter):\n    pass\n\nclass StreamReader(Codec,codecs.StreamReader):\n    pass\n\n### encodings module API\n\ndef getregentry():\n    return codecs.CodecInfo(\n        name='unicode-internal',\n        encode=Codec.encode,\n        decode=Codec.decode,\n        incrementalencoder=IncrementalEncoder,\n        incrementaldecoder=IncrementalDecoder,\n        streamwriter=StreamWriter,\n        streamreader=StreamReader,\n    )\n", 
    "encodings.utf_16": "\"\"\" Python 'utf-16' Codec\n\n\nWritten by Marc-Andre Lemburg (mal@lemburg.com).\n\n(c) Copyright CNRI, All Rights Reserved. NO WARRANTY.\n\n\"\"\"\nimport codecs, sys\n\n### Codec APIs\n\nencode = codecs.utf_16_encode\n\ndef decode(input, errors='strict'):\n    return codecs.utf_16_decode(input, errors, True)\n\nclass IncrementalEncoder(codecs.IncrementalEncoder):\n    def __init__(self, errors='strict'):\n        codecs.IncrementalEncoder.__init__(self, errors)\n        self.encoder = None\n\n    def encode(self, input, final=False):\n        if self.encoder is None:\n            result = codecs.utf_16_encode(input, self.errors)[0]\n            if sys.byteorder == 'little':\n                self.encoder = codecs.utf_16_le_encode\n            else:\n                self.encoder = codecs.utf_16_be_encode\n            return result\n        return self.encoder(input, self.errors)[0]\n\n    def reset(self):\n        codecs.IncrementalEncoder.reset(self)\n        self.encoder = None\n\n    def getstate(self):\n        # state info we return to the caller:\n        # 0: stream is in natural order for this platform\n        # 2: endianness hasn't been determined yet\n        # (we're never writing in unnatural order)\n        return (2 if self.encoder is None else 0)\n\n    def setstate(self, state):\n        if state:\n            self.encoder = None\n        else:\n            if sys.byteorder == 'little':\n                self.encoder = codecs.utf_16_le_encode\n            else:\n                self.encoder = codecs.utf_16_be_encode\n\nclass IncrementalDecoder(codecs.BufferedIncrementalDecoder):\n    def __init__(self, errors='strict'):\n        codecs.BufferedIncrementalDecoder.__init__(self, errors)\n        self.decoder = None\n\n    def _buffer_decode(self, input, errors, final):\n        if self.decoder is None:\n            (output, consumed, byteorder) = \\\n                codecs.utf_16_ex_decode(input, errors, 0, final)\n            if byteorder == -1:\n                self.decoder = codecs.utf_16_le_decode\n            elif byteorder == 1:\n                self.decoder = codecs.utf_16_be_decode\n            elif consumed >= 2:\n                raise UnicodeError(\"UTF-16 stream does not start with BOM\")\n            return (output, consumed)\n        return self.decoder(input, self.errors, final)\n\n    def reset(self):\n        codecs.BufferedIncrementalDecoder.reset(self)\n        self.decoder = None\n\nclass StreamWriter(codecs.StreamWriter):\n    def __init__(self, stream, errors='strict'):\n        codecs.StreamWriter.__init__(self, stream, errors)\n        self.encoder = None\n\n    def reset(self):\n        codecs.StreamWriter.reset(self)\n        self.encoder = None\n\n    def encode(self, input, errors='strict'):\n        if self.encoder is None:\n            result = codecs.utf_16_encode(input, errors)\n            if sys.byteorder == 'little':\n                self.encoder = codecs.utf_16_le_encode\n            else:\n                self.encoder = codecs.utf_16_be_encode\n            return result\n        else:\n            return self.encoder(input, errors)\n\nclass StreamReader(codecs.StreamReader):\n\n    def reset(self):\n        codecs.StreamReader.reset(self)\n        try:\n            del self.decode\n        except AttributeError:\n            pass\n\n    def decode(self, input, errors='strict'):\n        (object, consumed, byteorder) = \\\n            codecs.utf_16_ex_decode(input, errors, 0, False)\n        if byteorder == -1:\n            self.decode = codecs.utf_16_le_decode\n        elif byteorder == 1:\n            self.decode = codecs.utf_16_be_decode\n        elif consumed>=2:\n            raise UnicodeError,\"UTF-16 stream does not start with BOM\"\n        return (object, consumed)\n\n### encodings module API\n\ndef getregentry():\n    return codecs.CodecInfo(\n        name='utf-16',\n        encode=encode,\n        decode=decode,\n        incrementalencoder=IncrementalEncoder,\n        incrementaldecoder=IncrementalDecoder,\n        streamreader=StreamReader,\n        streamwriter=StreamWriter,\n    )\n", 
    "encodings.utf_8": "\"\"\" Python 'utf-8' Codec\n\n\nWritten by Marc-Andre Lemburg (mal@lemburg.com).\n\n(c) Copyright CNRI, All Rights Reserved. NO WARRANTY.\n\n\"\"\"\nimport codecs\n\n### Codec APIs\n\nencode = codecs.utf_8_encode\n\ndef decode(input, errors='strict'):\n    return codecs.utf_8_decode(input, errors, True)\n\nclass IncrementalEncoder(codecs.IncrementalEncoder):\n    def encode(self, input, final=False):\n        return codecs.utf_8_encode(input, self.errors)[0]\n\nclass IncrementalDecoder(codecs.BufferedIncrementalDecoder):\n    _buffer_decode = codecs.utf_8_decode\n\nclass StreamWriter(codecs.StreamWriter):\n    encode = codecs.utf_8_encode\n\nclass StreamReader(codecs.StreamReader):\n    decode = codecs.utf_8_decode\n\n### encodings module API\n\ndef getregentry():\n    return codecs.CodecInfo(\n        name='utf-8',\n        encode=encode,\n        decode=decode,\n        incrementalencoder=IncrementalEncoder,\n        incrementaldecoder=IncrementalDecoder,\n        streamreader=StreamReader,\n        streamwriter=StreamWriter,\n    )\n", 
    "fnmatch": "\"\"\"Filename matching with shell patterns.\n\nfnmatch(FILENAME, PATTERN) matches according to the local convention.\nfnmatchcase(FILENAME, PATTERN) always takes case in account.\n\nThe functions operate by translating the pattern into a regular\nexpression.  They cache the compiled regular expressions for speed.\n\nThe function translate(PATTERN) returns a regular expression\ncorresponding to PATTERN.  (It does not compile it.)\n\"\"\"\n\nimport re\n\n__all__ = [\"filter\", \"fnmatch\", \"fnmatchcase\", \"translate\"]\n\n_cache = {}\n_MAXCACHE = 100\n\ndef _purge():\n    \"\"\"Clear the pattern cache\"\"\"\n    _cache.clear()\n\ndef fnmatch(name, pat):\n    \"\"\"Test whether FILENAME matches PATTERN.\n\n    Patterns are Unix shell style:\n\n    *       matches everything\n    ?       matches any single character\n    [seq]   matches any character in seq\n    [!seq]  matches any char not in seq\n\n    An initial period in FILENAME is not special.\n    Both FILENAME and PATTERN are first case-normalized\n    if the operating system requires it.\n    If you don't want this, use fnmatchcase(FILENAME, PATTERN).\n    \"\"\"\n\n    import os\n    name = os.path.normcase(name)\n    pat = os.path.normcase(pat)\n    return fnmatchcase(name, pat)\n\ndef filter(names, pat):\n    \"\"\"Return the subset of the list NAMES that match PAT\"\"\"\n    import os,posixpath\n    result=[]\n    pat=os.path.normcase(pat)\n    if not pat in _cache:\n        res = translate(pat)\n        if len(_cache) >= _MAXCACHE:\n            _cache.clear()\n        _cache[pat] = re.compile(res)\n    match=_cache[pat].match\n    if os.path is posixpath:\n        # normcase on posix is NOP. Optimize it away from the loop.\n        for name in names:\n            if match(name):\n                result.append(name)\n    else:\n        for name in names:\n            if match(os.path.normcase(name)):\n                result.append(name)\n    return result\n\ndef fnmatchcase(name, pat):\n    \"\"\"Test whether FILENAME matches PATTERN, including case.\n\n    This is a version of fnmatch() which doesn't case-normalize\n    its arguments.\n    \"\"\"\n\n    if not pat in _cache:\n        res = translate(pat)\n        if len(_cache) >= _MAXCACHE:\n            _cache.clear()\n        _cache[pat] = re.compile(res)\n    return _cache[pat].match(name) is not None\n\ndef translate(pat):\n    \"\"\"Translate a shell PATTERN to a regular expression.\n\n    There is no way to quote meta-characters.\n    \"\"\"\n\n    i, n = 0, len(pat)\n    res = ''\n    while i < n:\n        c = pat[i]\n        i = i+1\n        if c == '*':\n            res = res + '.*'\n        elif c == '?':\n            res = res + '.'\n        elif c == '[':\n            j = i\n            if j < n and pat[j] == '!':\n                j = j+1\n            if j < n and pat[j] == ']':\n                j = j+1\n            while j < n and pat[j] != ']':\n                j = j+1\n            if j >= n:\n                res = res + '\\\\['\n            else:\n                stuff = pat[i:j].replace('\\\\','\\\\\\\\')\n                i = j+1\n                if stuff[0] == '!':\n                    stuff = '^' + stuff[1:]\n                elif stuff[0] == '^':\n                    stuff = '\\\\' + stuff\n                res = '%s[%s]' % (res, stuff)\n        else:\n            res = res + re.escape(c)\n    return res + '\\Z(?ms)'\n", 
    "functools": "\"\"\"functools.py - Tools for working with functions and callable objects\n\"\"\"\n# Python module wrapper for _functools C module\n# to allow utilities written in Python to be added\n# to the functools module.\n# Written by Nick Coghlan <ncoghlan at gmail.com>\n#   Copyright (C) 2006 Python Software Foundation.\n# See C source code for _functools credits/copyright\n\nfrom _functools import partial, reduce\n\n# update_wrapper() and wraps() are tools to help write\n# wrapper functions that can handle naive introspection\n\nWRAPPER_ASSIGNMENTS = ('__module__', '__name__', '__doc__')\nWRAPPER_UPDATES = ('__dict__',)\ndef update_wrapper(wrapper,\n                   wrapped,\n                   assigned = WRAPPER_ASSIGNMENTS,\n                   updated = WRAPPER_UPDATES):\n    \"\"\"Update a wrapper function to look like the wrapped function\n\n       wrapper is the function to be updated\n       wrapped is the original function\n       assigned is a tuple naming the attributes assigned directly\n       from the wrapped function to the wrapper function (defaults to\n       functools.WRAPPER_ASSIGNMENTS)\n       updated is a tuple naming the attributes of the wrapper that\n       are updated with the corresponding attribute from the wrapped\n       function (defaults to functools.WRAPPER_UPDATES)\n    \"\"\"\n    for attr in assigned:\n        setattr(wrapper, attr, getattr(wrapped, attr))\n    for attr in updated:\n        getattr(wrapper, attr).update(getattr(wrapped, attr, {}))\n    # Return the wrapper so this can be used as a decorator via partial()\n    return wrapper\n\ndef wraps(wrapped,\n          assigned = WRAPPER_ASSIGNMENTS,\n          updated = WRAPPER_UPDATES):\n    \"\"\"Decorator factory to apply update_wrapper() to a wrapper function\n\n       Returns a decorator that invokes update_wrapper() with the decorated\n       function as the wrapper argument and the arguments to wraps() as the\n       remaining arguments. Default arguments are as for update_wrapper().\n       This is a convenience function to simplify applying partial() to\n       update_wrapper().\n    \"\"\"\n    return partial(update_wrapper, wrapped=wrapped,\n                   assigned=assigned, updated=updated)\n\ndef total_ordering(cls):\n    \"\"\"Class decorator that fills in missing ordering methods\"\"\"\n    convert = {\n        '__lt__': [('__gt__', lambda self, other: not (self < other or self == other)),\n                   ('__le__', lambda self, other: self < other or self == other),\n                   ('__ge__', lambda self, other: not self < other)],\n        '__le__': [('__ge__', lambda self, other: not self <= other or self == other),\n                   ('__lt__', lambda self, other: self <= other and not self == other),\n                   ('__gt__', lambda self, other: not self <= other)],\n        '__gt__': [('__lt__', lambda self, other: not (self > other or self == other)),\n                   ('__ge__', lambda self, other: self > other or self == other),\n                   ('__le__', lambda self, other: not self > other)],\n        '__ge__': [('__le__', lambda self, other: (not self >= other) or self == other),\n                   ('__gt__', lambda self, other: self >= other and not self == other),\n                   ('__lt__', lambda self, other: not self >= other)]\n    }\n    roots = set(dir(cls)) & set(convert)\n    if not roots:\n        raise ValueError('must define at least one ordering operation: < > <= >=')\n    root = max(roots)       # prefer __lt__ to __le__ to __gt__ to __ge__\n    for opname, opfunc in convert[root]:\n        if opname not in roots:\n            opfunc.__name__ = opname\n            opfunc.__doc__ = getattr(int, opname).__doc__\n            setattr(cls, opname, opfunc)\n    return cls\n\ndef cmp_to_key(mycmp):\n    \"\"\"Convert a cmp= function into a key= function\"\"\"\n    class K(object):\n        __slots__ = ['obj']\n        def __init__(self, obj, *args):\n            self.obj = obj\n        def __lt__(self, other):\n            return mycmp(self.obj, other.obj) < 0\n        def __gt__(self, other):\n            return mycmp(self.obj, other.obj) > 0\n        def __eq__(self, other):\n            return mycmp(self.obj, other.obj) == 0\n        def __le__(self, other):\n            return mycmp(self.obj, other.obj) <= 0\n        def __ge__(self, other):\n            return mycmp(self.obj, other.obj) >= 0\n        def __ne__(self, other):\n            return mycmp(self.obj, other.obj) != 0\n        def __hash__(self):\n            raise TypeError('hash not implemented')\n    return K\n", 
    "genericpath": "\"\"\"\nPath operations common to more than one OS\nDo not use directly.  The OS specific modules import the appropriate\nfunctions from this module themselves.\n\"\"\"\nimport os\nimport stat\n\n__all__ = ['commonprefix', 'exists', 'getatime', 'getctime', 'getmtime',\n           'getsize', 'isdir', 'isfile']\n\n\n# Does a path exist?\n# This is false for dangling symbolic links on systems that support them.\ndef exists(path):\n    \"\"\"Test whether a path exists.  Returns False for broken symbolic links\"\"\"\n    try:\n        os.stat(path)\n    except os.error:\n        return False\n    return True\n\n\n# This follows symbolic links, so both islink() and isdir() can be true\n# for the same path on systems that support symlinks\ndef isfile(path):\n    \"\"\"Test whether a path is a regular file\"\"\"\n    try:\n        st = os.stat(path)\n    except os.error:\n        return False\n    return stat.S_ISREG(st.st_mode)\n\n\n# Is a path a directory?\n# This follows symbolic links, so both islink() and isdir()\n# can be true for the same path on systems that support symlinks\ndef isdir(s):\n    \"\"\"Return true if the pathname refers to an existing directory.\"\"\"\n    try:\n        st = os.stat(s)\n    except os.error:\n        return False\n    return stat.S_ISDIR(st.st_mode)\n\n\ndef getsize(filename):\n    \"\"\"Return the size of a file, reported by os.stat().\"\"\"\n    return os.stat(filename).st_size\n\n\ndef getmtime(filename):\n    \"\"\"Return the last modification time of a file, reported by os.stat().\"\"\"\n    return os.stat(filename).st_mtime\n\n\ndef getatime(filename):\n    \"\"\"Return the last access time of a file, reported by os.stat().\"\"\"\n    return os.stat(filename).st_atime\n\n\ndef getctime(filename):\n    \"\"\"Return the metadata change time of a file, reported by os.stat().\"\"\"\n    return os.stat(filename).st_ctime\n\n\n# Return the longest prefix of all list elements.\ndef commonprefix(m):\n    \"Given a list of pathnames, returns the longest common leading component\"\n    if not m: return ''\n    s1 = min(m)\n    s2 = max(m)\n    for i, c in enumerate(s1):\n        if c != s2[i]:\n            return s1[:i]\n    return s1\n\n# Split a path in root and extension.\n# The extension is everything starting at the last dot in the last\n# pathname component; the root is everything before that.\n# It is always true that root + ext == p.\n\n# Generic implementation of splitext, to be parametrized with\n# the separators\ndef _splitext(p, sep, altsep, extsep):\n    \"\"\"Split the extension from a pathname.\n\n    Extension is everything from the last dot to the end, ignoring\n    leading dots.  Returns \"(root, ext)\"; ext may be empty.\"\"\"\n\n    sepIndex = p.rfind(sep)\n    if altsep:\n        altsepIndex = p.rfind(altsep)\n        sepIndex = max(sepIndex, altsepIndex)\n\n    dotIndex = p.rfind(extsep)\n    if dotIndex > sepIndex:\n        # skip all leading dots\n        filenameIndex = sepIndex + 1\n        while filenameIndex < dotIndex:\n            if p[filenameIndex] != extsep:\n                return p[:dotIndex], p[dotIndex:]\n            filenameIndex += 1\n\n    return p, ''\n", 
    "getopt": "\"\"\"Parser for command line options.\n\nThis module helps scripts to parse the command line arguments in\nsys.argv.  It supports the same conventions as the Unix getopt()\nfunction (including the special meanings of arguments of the form `-'\nand `--').  Long options similar to those supported by GNU software\nmay be used as well via an optional third argument.  This module\nprovides two functions and an exception:\n\ngetopt() -- Parse command line options\ngnu_getopt() -- Like getopt(), but allow option and non-option arguments\nto be intermixed.\nGetoptError -- exception (class) raised with 'opt' attribute, which is the\noption involved with the exception.\n\"\"\"\n\n# Long option support added by Lars Wirzenius <liw@iki.fi>.\n#\n# Gerrit Holl <gerrit@nl.linux.org> moved the string-based exceptions\n# to class-based exceptions.\n#\n# Peter Astrand <astrand@lysator.liu.se> added gnu_getopt().\n#\n# TODO for gnu_getopt():\n#\n# - GNU getopt_long_only mechanism\n# - allow the caller to specify ordering\n# - RETURN_IN_ORDER option\n# - GNU extension with '-' as first character of option string\n# - optional arguments, specified by double colons\n# - a option string with a W followed by semicolon should\n#   treat \"-W foo\" as \"--foo\"\n\n__all__ = [\"GetoptError\",\"error\",\"getopt\",\"gnu_getopt\"]\n\nimport os\n\nclass GetoptError(Exception):\n    opt = ''\n    msg = ''\n    def __init__(self, msg, opt=''):\n        self.msg = msg\n        self.opt = opt\n        Exception.__init__(self, msg, opt)\n\n    def __str__(self):\n        return self.msg\n\nerror = GetoptError # backward compatibility\n\ndef getopt(args, shortopts, longopts = []):\n    \"\"\"getopt(args, options[, long_options]) -> opts, args\n\n    Parses command line options and parameter list.  args is the\n    argument list to be parsed, without the leading reference to the\n    running program.  Typically, this means \"sys.argv[1:]\".  shortopts\n    is the string of option letters that the script wants to\n    recognize, with options that require an argument followed by a\n    colon (i.e., the same format that Unix getopt() uses).  If\n    specified, longopts is a list of strings with the names of the\n    long options which should be supported.  The leading '--'\n    characters should not be included in the option name.  Options\n    which require an argument should be followed by an equal sign\n    ('=').\n\n    The return value consists of two elements: the first is a list of\n    (option, value) pairs; the second is the list of program arguments\n    left after the option list was stripped (this is a trailing slice\n    of the first argument).  Each option-and-value pair returned has\n    the option as its first element, prefixed with a hyphen (e.g.,\n    '-x'), and the option argument as its second element, or an empty\n    string if the option has no argument.  The options occur in the\n    list in the same order in which they were found, thus allowing\n    multiple occurrences.  Long and short options may be mixed.\n\n    \"\"\"\n\n    opts = []\n    if type(longopts) == type(\"\"):\n        longopts = [longopts]\n    else:\n        longopts = list(longopts)\n    while args and args[0].startswith('-') and args[0] != '-':\n        if args[0] == '--':\n            args = args[1:]\n            break\n        if args[0].startswith('--'):\n            opts, args = do_longs(opts, args[0][2:], longopts, args[1:])\n        else:\n            opts, args = do_shorts(opts, args[0][1:], shortopts, args[1:])\n\n    return opts, args\n\ndef gnu_getopt(args, shortopts, longopts = []):\n    \"\"\"getopt(args, options[, long_options]) -> opts, args\n\n    This function works like getopt(), except that GNU style scanning\n    mode is used by default. This means that option and non-option\n    arguments may be intermixed. The getopt() function stops\n    processing options as soon as a non-option argument is\n    encountered.\n\n    If the first character of the option string is `+', or if the\n    environment variable POSIXLY_CORRECT is set, then option\n    processing stops as soon as a non-option argument is encountered.\n\n    \"\"\"\n\n    opts = []\n    prog_args = []\n    if isinstance(longopts, str):\n        longopts = [longopts]\n    else:\n        longopts = list(longopts)\n\n    # Allow options after non-option arguments?\n    if shortopts.startswith('+'):\n        shortopts = shortopts[1:]\n        all_options_first = True\n    elif os.environ.get(\"POSIXLY_CORRECT\"):\n        all_options_first = True\n    else:\n        all_options_first = False\n\n    while args:\n        if args[0] == '--':\n            prog_args += args[1:]\n            break\n\n        if args[0][:2] == '--':\n            opts, args = do_longs(opts, args[0][2:], longopts, args[1:])\n        elif args[0][:1] == '-' and args[0] != '-':\n            opts, args = do_shorts(opts, args[0][1:], shortopts, args[1:])\n        else:\n            if all_options_first:\n                prog_args += args\n                break\n            else:\n                prog_args.append(args[0])\n                args = args[1:]\n\n    return opts, prog_args\n\ndef do_longs(opts, opt, longopts, args):\n    try:\n        i = opt.index('=')\n    except ValueError:\n        optarg = None\n    else:\n        opt, optarg = opt[:i], opt[i+1:]\n\n    has_arg, opt = long_has_args(opt, longopts)\n    if has_arg:\n        if optarg is None:\n            if not args:\n                raise GetoptError('option --%s requires argument' % opt, opt)\n            optarg, args = args[0], args[1:]\n    elif optarg is not None:\n        raise GetoptError('option --%s must not have an argument' % opt, opt)\n    opts.append(('--' + opt, optarg or ''))\n    return opts, args\n\n# Return:\n#   has_arg?\n#   full option name\ndef long_has_args(opt, longopts):\n    possibilities = [o for o in longopts if o.startswith(opt)]\n    if not possibilities:\n        raise GetoptError('option --%s not recognized' % opt, opt)\n    # Is there an exact match?\n    if opt in possibilities:\n        return False, opt\n    elif opt + '=' in possibilities:\n        return True, opt\n    # No exact match, so better be unique.\n    if len(possibilities) > 1:\n        # XXX since possibilities contains all valid continuations, might be\n        # nice to work them into the error msg\n        raise GetoptError('option --%s not a unique prefix' % opt, opt)\n    assert len(possibilities) == 1\n    unique_match = possibilities[0]\n    has_arg = unique_match.endswith('=')\n    if has_arg:\n        unique_match = unique_match[:-1]\n    return has_arg, unique_match\n\ndef do_shorts(opts, optstring, shortopts, args):\n    while optstring != '':\n        opt, optstring = optstring[0], optstring[1:]\n        if short_has_arg(opt, shortopts):\n            if optstring == '':\n                if not args:\n                    raise GetoptError('option -%s requires argument' % opt,\n                                      opt)\n                optstring, args = args[0], args[1:]\n            optarg, optstring = optstring, ''\n        else:\n            optarg = ''\n        opts.append(('-' + opt, optarg))\n    return opts, args\n\ndef short_has_arg(opt, shortopts):\n    for i in range(len(shortopts)):\n        if opt == shortopts[i] != ':':\n            return shortopts.startswith(':', i+1)\n    raise GetoptError('option -%s not recognized' % opt, opt)\n\nif __name__ == '__main__':\n    import sys\n    print getopt(sys.argv[1:], \"a:b\", [\"alpha=\", \"beta\"])\n", 
    "gettext": "\"\"\"Internationalization and localization support.\n\nThis module provides internationalization (I18N) and localization (L10N)\nsupport for your Python programs by providing an interface to the GNU gettext\nmessage catalog library.\n\nI18N refers to the operation by which a program is made aware of multiple\nlanguages.  L10N refers to the adaptation of your program, once\ninternationalized, to the local language and cultural habits.\n\n\"\"\"\n\n# This module represents the integration of work, contributions, feedback, and\n# suggestions from the following people:\n#\n# Martin von Loewis, who wrote the initial implementation of the underlying\n# C-based libintlmodule (later renamed _gettext), along with a skeletal\n# gettext.py implementation.\n#\n# Peter Funk, who wrote fintl.py, a fairly complete wrapper around intlmodule,\n# which also included a pure-Python implementation to read .mo files if\n# intlmodule wasn't available.\n#\n# James Henstridge, who also wrote a gettext.py module, which has some\n# interesting, but currently unsupported experimental features: the notion of\n# a Catalog class and instances, and the ability to add to a catalog file via\n# a Python API.\n#\n# Barry Warsaw integrated these modules, wrote the .install() API and code,\n# and conformed all C and Python code to Python's coding standards.\n#\n# Francois Pinard and Marc-Andre Lemburg also contributed valuably to this\n# module.\n#\n# J. David Ibanez implemented plural forms. Bruno Haible fixed some bugs.\n#\n# TODO:\n# - Lazy loading of .mo files.  Currently the entire catalog is loaded into\n#   memory, but that's probably bad for large translated programs.  Instead,\n#   the lexical sort of original strings in GNU .mo files should be exploited\n#   to do binary searches and lazy initializations.  Or you might want to use\n#   the undocumented double-hash algorithm for .mo files with hash tables, but\n#   you'll need to study the GNU gettext code to do this.\n#\n# - Support Solaris .mo file formats.  Unfortunately, we've been unable to\n#   find this format documented anywhere.\n\n\nimport locale, copy, os, re, struct, sys\nfrom errno import ENOENT\n\n\n__all__ = ['NullTranslations', 'GNUTranslations', 'Catalog',\n           'find', 'translation', 'install', 'textdomain', 'bindtextdomain',\n           'dgettext', 'dngettext', 'gettext', 'ngettext',\n           ]\n\n_default_localedir = os.path.join(sys.prefix, 'share', 'locale')\n\n\ndef test(condition, true, false):\n    \"\"\"\n    Implements the C expression:\n\n      condition ? true : false\n\n    Required to correctly interpret plural forms.\n    \"\"\"\n    if condition:\n        return true\n    else:\n        return false\n\n\ndef c2py(plural):\n    \"\"\"Gets a C expression as used in PO files for plural forms and returns a\n    Python lambda function that implements an equivalent expression.\n    \"\"\"\n    # Security check, allow only the \"n\" identifier\n    try:\n        from cStringIO import StringIO\n    except ImportError:\n        from StringIO import StringIO\n    import token, tokenize\n    tokens = tokenize.generate_tokens(StringIO(plural).readline)\n    try:\n        danger = [x for x in tokens if x[0] == token.NAME and x[1] != 'n']\n    except tokenize.TokenError:\n        raise ValueError, \\\n              'plural forms expression error, maybe unbalanced parenthesis'\n    else:\n        if danger:\n            raise ValueError, 'plural forms expression could be dangerous'\n\n    # Replace some C operators by their Python equivalents\n    plural = plural.replace('&&', ' and ')\n    plural = plural.replace('||', ' or ')\n\n    expr = re.compile(r'\\!([^=])')\n    plural = expr.sub(' not \\\\1', plural)\n\n    # Regular expression and replacement function used to transform\n    # \"a?b:c\" to \"test(a,b,c)\".\n    expr = re.compile(r'(.*?)\\?(.*?):(.*)')\n    def repl(x):\n        return \"test(%s, %s, %s)\" % (x.group(1), x.group(2),\n                                     expr.sub(repl, x.group(3)))\n\n    # Code to transform the plural expression, taking care of parentheses\n    stack = ['']\n    for c in plural:\n        if c == '(':\n            stack.append('')\n        elif c == ')':\n            if len(stack) == 1:\n                # Actually, we never reach this code, because unbalanced\n                # parentheses get caught in the security check at the\n                # beginning.\n                raise ValueError, 'unbalanced parenthesis in plural form'\n            s = expr.sub(repl, stack.pop())\n            stack[-1] += '(%s)' % s\n        else:\n            stack[-1] += c\n    plural = expr.sub(repl, stack.pop())\n\n    return eval('lambda n: int(%s)' % plural)\n\n\n\ndef _expand_lang(locale):\n    from locale import normalize\n    locale = normalize(locale)\n    COMPONENT_CODESET   = 1 << 0\n    COMPONENT_TERRITORY = 1 << 1\n    COMPONENT_MODIFIER  = 1 << 2\n    # split up the locale into its base components\n    mask = 0\n    pos = locale.find('@')\n    if pos >= 0:\n        modifier = locale[pos:]\n        locale = locale[:pos]\n        mask |= COMPONENT_MODIFIER\n    else:\n        modifier = ''\n    pos = locale.find('.')\n    if pos >= 0:\n        codeset = locale[pos:]\n        locale = locale[:pos]\n        mask |= COMPONENT_CODESET\n    else:\n        codeset = ''\n    pos = locale.find('_')\n    if pos >= 0:\n        territory = locale[pos:]\n        locale = locale[:pos]\n        mask |= COMPONENT_TERRITORY\n    else:\n        territory = ''\n    language = locale\n    ret = []\n    for i in range(mask+1):\n        if not (i & ~mask):  # if all components for this combo exist ...\n            val = language\n            if i & COMPONENT_TERRITORY: val += territory\n            if i & COMPONENT_CODESET:   val += codeset\n            if i & COMPONENT_MODIFIER:  val += modifier\n            ret.append(val)\n    ret.reverse()\n    return ret\n\n\n\nclass NullTranslations:\n    def __init__(self, fp=None):\n        self._info = {}\n        self._charset = None\n        self._output_charset = None\n        self._fallback = None\n        if fp is not None:\n            self._parse(fp)\n\n    def _parse(self, fp):\n        pass\n\n    def add_fallback(self, fallback):\n        if self._fallback:\n            self._fallback.add_fallback(fallback)\n        else:\n            self._fallback = fallback\n\n    def gettext(self, message):\n        if self._fallback:\n            return self._fallback.gettext(message)\n        return message\n\n    def lgettext(self, message):\n        if self._fallback:\n            return self._fallback.lgettext(message)\n        return message\n\n    def ngettext(self, msgid1, msgid2, n):\n        if self._fallback:\n            return self._fallback.ngettext(msgid1, msgid2, n)\n        if n == 1:\n            return msgid1\n        else:\n            return msgid2\n\n    def lngettext(self, msgid1, msgid2, n):\n        if self._fallback:\n            return self._fallback.lngettext(msgid1, msgid2, n)\n        if n == 1:\n            return msgid1\n        else:\n            return msgid2\n\n    def ugettext(self, message):\n        if self._fallback:\n            return self._fallback.ugettext(message)\n        return unicode(message)\n\n    def ungettext(self, msgid1, msgid2, n):\n        if self._fallback:\n            return self._fallback.ungettext(msgid1, msgid2, n)\n        if n == 1:\n            return unicode(msgid1)\n        else:\n            return unicode(msgid2)\n\n    def info(self):\n        return self._info\n\n    def charset(self):\n        return self._charset\n\n    def output_charset(self):\n        return self._output_charset\n\n    def set_output_charset(self, charset):\n        self._output_charset = charset\n\n    def install(self, unicode=False, names=None):\n        import __builtin__\n        __builtin__.__dict__['_'] = unicode and self.ugettext or self.gettext\n        if hasattr(names, \"__contains__\"):\n            if \"gettext\" in names:\n                __builtin__.__dict__['gettext'] = __builtin__.__dict__['_']\n            if \"ngettext\" in names:\n                __builtin__.__dict__['ngettext'] = (unicode and self.ungettext\n                                                             or self.ngettext)\n            if \"lgettext\" in names:\n                __builtin__.__dict__['lgettext'] = self.lgettext\n            if \"lngettext\" in names:\n                __builtin__.__dict__['lngettext'] = self.lngettext\n\n\nclass GNUTranslations(NullTranslations):\n    # Magic number of .mo files\n    LE_MAGIC = 0x950412deL\n    BE_MAGIC = 0xde120495L\n\n    def _parse(self, fp):\n        \"\"\"Override this method to support alternative .mo formats.\"\"\"\n        unpack = struct.unpack\n        filename = getattr(fp, 'name', '')\n        # Parse the .mo file header, which consists of 5 little endian 32\n        # bit words.\n        self._catalog = catalog = {}\n        self.plural = lambda n: int(n != 1) # germanic plural by default\n        buf = fp.read()\n        buflen = len(buf)\n        # Are we big endian or little endian?\n        magic = unpack('<I', buf[:4])[0]\n        if magic == self.LE_MAGIC:\n            version, msgcount, masteridx, transidx = unpack('<4I', buf[4:20])\n            ii = '<II'\n        elif magic == self.BE_MAGIC:\n            version, msgcount, masteridx, transidx = unpack('>4I', buf[4:20])\n            ii = '>II'\n        else:\n            raise IOError(0, 'Bad magic number', filename)\n        # Now put all messages from the .mo file buffer into the catalog\n        # dictionary.\n        for i in xrange(0, msgcount):\n            mlen, moff = unpack(ii, buf[masteridx:masteridx+8])\n            mend = moff + mlen\n            tlen, toff = unpack(ii, buf[transidx:transidx+8])\n            tend = toff + tlen\n            if mend < buflen and tend < buflen:\n                msg = buf[moff:mend]\n                tmsg = buf[toff:tend]\n            else:\n                raise IOError(0, 'File is corrupt', filename)\n            # See if we're looking at GNU .mo conventions for metadata\n            if mlen == 0:\n                # Catalog description\n                lastk = k = None\n                for item in tmsg.splitlines():\n                    item = item.strip()\n                    if not item:\n                        continue\n                    if ':' in item:\n                        k, v = item.split(':', 1)\n                        k = k.strip().lower()\n                        v = v.strip()\n                        self._info[k] = v\n                        lastk = k\n                    elif lastk:\n                        self._info[lastk] += '\\n' + item\n                    if k == 'content-type':\n                        self._charset = v.split('charset=')[1]\n                    elif k == 'plural-forms':\n                        v = v.split(';')\n                        plural = v[1].split('plural=')[1]\n                        self.plural = c2py(plural)\n            # Note: we unconditionally convert both msgids and msgstrs to\n            # Unicode using the character encoding specified in the charset\n            # parameter of the Content-Type header.  The gettext documentation\n            # strongly encourages msgids to be us-ascii, but some applications\n            # require alternative encodings (e.g. Zope's ZCML and ZPT).  For\n            # traditional gettext applications, the msgid conversion will\n            # cause no problems since us-ascii should always be a subset of\n            # the charset encoding.  We may want to fall back to 8-bit msgids\n            # if the Unicode conversion fails.\n            if '\\x00' in msg:\n                # Plural forms\n                msgid1, msgid2 = msg.split('\\x00')\n                tmsg = tmsg.split('\\x00')\n                if self._charset:\n                    msgid1 = unicode(msgid1, self._charset)\n                    tmsg = [unicode(x, self._charset) for x in tmsg]\n                for i in range(len(tmsg)):\n                    catalog[(msgid1, i)] = tmsg[i]\n            else:\n                if self._charset:\n                    msg = unicode(msg, self._charset)\n                    tmsg = unicode(tmsg, self._charset)\n                catalog[msg] = tmsg\n            # advance to next entry in the seek tables\n            masteridx += 8\n            transidx += 8\n\n    def gettext(self, message):\n        missing = object()\n        tmsg = self._catalog.get(message, missing)\n        if tmsg is missing:\n            if self._fallback:\n                return self._fallback.gettext(message)\n            return message\n        # Encode the Unicode tmsg back to an 8-bit string, if possible\n        if self._output_charset:\n            return tmsg.encode(self._output_charset)\n        elif self._charset:\n            return tmsg.encode(self._charset)\n        return tmsg\n\n    def lgettext(self, message):\n        missing = object()\n        tmsg = self._catalog.get(message, missing)\n        if tmsg is missing:\n            if self._fallback:\n                return self._fallback.lgettext(message)\n            return message\n        if self._output_charset:\n            return tmsg.encode(self._output_charset)\n        return tmsg.encode(locale.getpreferredencoding())\n\n    def ngettext(self, msgid1, msgid2, n):\n        try:\n            tmsg = self._catalog[(msgid1, self.plural(n))]\n            if self._output_charset:\n                return tmsg.encode(self._output_charset)\n            elif self._charset:\n                return tmsg.encode(self._charset)\n            return tmsg\n        except KeyError:\n            if self._fallback:\n                return self._fallback.ngettext(msgid1, msgid2, n)\n            if n == 1:\n                return msgid1\n            else:\n                return msgid2\n\n    def lngettext(self, msgid1, msgid2, n):\n        try:\n            tmsg = self._catalog[(msgid1, self.plural(n))]\n            if self._output_charset:\n                return tmsg.encode(self._output_charset)\n            return tmsg.encode(locale.getpreferredencoding())\n        except KeyError:\n            if self._fallback:\n                return self._fallback.lngettext(msgid1, msgid2, n)\n            if n == 1:\n                return msgid1\n            else:\n                return msgid2\n\n    def ugettext(self, message):\n        missing = object()\n        tmsg = self._catalog.get(message, missing)\n        if tmsg is missing:\n            if self._fallback:\n                return self._fallback.ugettext(message)\n            return unicode(message)\n        return tmsg\n\n    def ungettext(self, msgid1, msgid2, n):\n        try:\n            tmsg = self._catalog[(msgid1, self.plural(n))]\n        except KeyError:\n            if self._fallback:\n                return self._fallback.ungettext(msgid1, msgid2, n)\n            if n == 1:\n                tmsg = unicode(msgid1)\n            else:\n                tmsg = unicode(msgid2)\n        return tmsg\n\n\n# Locate a .mo file using the gettext strategy\ndef find(domain, localedir=None, languages=None, all=0):\n    # Get some reasonable defaults for arguments that were not supplied\n    if localedir is None:\n        localedir = _default_localedir\n    if languages is None:\n        languages = []\n        for envar in ('LANGUAGE', 'LC_ALL', 'LC_MESSAGES', 'LANG'):\n            val = os.environ.get(envar)\n            if val:\n                languages = val.split(':')\n                break\n        if 'C' not in languages:\n            languages.append('C')\n    # now normalize and expand the languages\n    nelangs = []\n    for lang in languages:\n        for nelang in _expand_lang(lang):\n            if nelang not in nelangs:\n                nelangs.append(nelang)\n    # select a language\n    if all:\n        result = []\n    else:\n        result = None\n    for lang in nelangs:\n        if lang == 'C':\n            break\n        mofile = os.path.join(localedir, lang, 'LC_MESSAGES', '%s.mo' % domain)\n        if os.path.exists(mofile):\n            if all:\n                result.append(mofile)\n            else:\n                return mofile\n    return result\n\n\n\n# a mapping between absolute .mo file path and Translation object\n_translations = {}\n\ndef translation(domain, localedir=None, languages=None,\n                class_=None, fallback=False, codeset=None):\n    if class_ is None:\n        class_ = GNUTranslations\n    mofiles = find(domain, localedir, languages, all=1)\n    if not mofiles:\n        if fallback:\n            return NullTranslations()\n        raise IOError(ENOENT, 'No translation file found for domain', domain)\n    # Avoid opening, reading, and parsing the .mo file after it's been done\n    # once.\n    result = None\n    for mofile in mofiles:\n        key = (class_, os.path.abspath(mofile))\n        t = _translations.get(key)\n        if t is None:\n            with open(mofile, 'rb') as fp:\n                t = _translations.setdefault(key, class_(fp))\n        # Copy the translation object to allow setting fallbacks and\n        # output charset. All other instance data is shared with the\n        # cached object.\n        t = copy.copy(t)\n        if codeset:\n            t.set_output_charset(codeset)\n        if result is None:\n            result = t\n        else:\n            result.add_fallback(t)\n    return result\n\n\ndef install(domain, localedir=None, unicode=False, codeset=None, names=None):\n    t = translation(domain, localedir, fallback=True, codeset=codeset)\n    t.install(unicode, names)\n\n\n\n# a mapping b/w domains and locale directories\n_localedirs = {}\n# a mapping b/w domains and codesets\n_localecodesets = {}\n# current global domain, `messages' used for compatibility w/ GNU gettext\n_current_domain = 'messages'\n\n\ndef textdomain(domain=None):\n    global _current_domain\n    if domain is not None:\n        _current_domain = domain\n    return _current_domain\n\n\ndef bindtextdomain(domain, localedir=None):\n    global _localedirs\n    if localedir is not None:\n        _localedirs[domain] = localedir\n    return _localedirs.get(domain, _default_localedir)\n\n\ndef bind_textdomain_codeset(domain, codeset=None):\n    global _localecodesets\n    if codeset is not None:\n        _localecodesets[domain] = codeset\n    return _localecodesets.get(domain)\n\n\ndef dgettext(domain, message):\n    try:\n        t = translation(domain, _localedirs.get(domain, None),\n                        codeset=_localecodesets.get(domain))\n    except IOError:\n        return message\n    return t.gettext(message)\n\ndef ldgettext(domain, message):\n    try:\n        t = translation(domain, _localedirs.get(domain, None),\n                        codeset=_localecodesets.get(domain))\n    except IOError:\n        return message\n    return t.lgettext(message)\n\ndef dngettext(domain, msgid1, msgid2, n):\n    try:\n        t = translation(domain, _localedirs.get(domain, None),\n                        codeset=_localecodesets.get(domain))\n    except IOError:\n        if n == 1:\n            return msgid1\n        else:\n            return msgid2\n    return t.ngettext(msgid1, msgid2, n)\n\ndef ldngettext(domain, msgid1, msgid2, n):\n    try:\n        t = translation(domain, _localedirs.get(domain, None),\n                        codeset=_localecodesets.get(domain))\n    except IOError:\n        if n == 1:\n            return msgid1\n        else:\n            return msgid2\n    return t.lngettext(msgid1, msgid2, n)\n\ndef gettext(message):\n    return dgettext(_current_domain, message)\n\ndef lgettext(message):\n    return ldgettext(_current_domain, message)\n\ndef ngettext(msgid1, msgid2, n):\n    return dngettext(_current_domain, msgid1, msgid2, n)\n\ndef lngettext(msgid1, msgid2, n):\n    return ldngettext(_current_domain, msgid1, msgid2, n)\n\n# dcgettext() has been deemed unnecessary and is not implemented.\n\n# James Henstridge's Catalog constructor from GNOME gettext.  Documented usage\n# was:\n#\n#    import gettext\n#    cat = gettext.Catalog(PACKAGE, localedir=LOCALEDIR)\n#    _ = cat.gettext\n#    print _('Hello World')\n\n# The resulting catalog object currently don't support access through a\n# dictionary API, which was supported (but apparently unused) in GNOME\n# gettext.\n\nCatalog = translation\n", 
    "hashlib": "# $Id$\n#\n#  Copyright (C) 2005   Gregory P. Smith (greg@krypto.org)\n#  Licensed to PSF under a Contributor Agreement.\n#\n\n__doc__ = \"\"\"hashlib module - A common interface to many hash functions.\n\nnew(name, string='') - returns a new hash object implementing the\n                       given hash function; initializing the hash\n                       using the given string data.\n\nNamed constructor functions are also available, these are much faster\nthan using new():\n\nmd5(), sha1(), sha224(), sha256(), sha384(), and sha512()\n\nMore algorithms may be available on your platform but the above are guaranteed\nto exist.  See the algorithms_guaranteed and algorithms_available attributes\nto find out what algorithm names can be passed to new().\n\nNOTE: If you want the adler32 or crc32 hash functions they are available in\nthe zlib module.\n\nChoose your hash function wisely.  Some have known collision weaknesses.\nsha384 and sha512 will be slow on 32 bit platforms.\n\nHash objects have these methods:\n - update(arg): Update the hash object with the string arg. Repeated calls\n                are equivalent to a single call with the concatenation of all\n                the arguments.\n - digest():    Return the digest of the strings passed to the update() method\n                so far. This may contain non-ASCII characters, including\n                NUL bytes.\n - hexdigest(): Like digest() except the digest is returned as a string of\n                double length, containing only hexadecimal digits.\n - copy():      Return a copy (clone) of the hash object. This can be used to\n                efficiently compute the digests of strings that share a common\n                initial substring.\n\nFor example, to obtain the digest of the string 'Nobody inspects the\nspammish repetition':\n\n    >>> import hashlib\n    >>> m = hashlib.md5()\n    >>> m.update(\"Nobody inspects\")\n    >>> m.update(\" the spammish repetition\")\n    >>> m.digest()\n    '\\\\xbbd\\\\x9c\\\\x83\\\\xdd\\\\x1e\\\\xa5\\\\xc9\\\\xd9\\\\xde\\\\xc9\\\\xa1\\\\x8d\\\\xf0\\\\xff\\\\xe9'\n\nMore condensed:\n\n    >>> hashlib.sha224(\"Nobody inspects the spammish repetition\").hexdigest()\n    'a4337bc45a8fc544c03f52dc550cd6e1e87021bc896588bd79e901e2'\n\n\"\"\"\n\n# This tuple and __get_builtin_constructor() must be modified if a new\n# always available algorithm is added.\n__always_supported = ('md5', 'sha1', 'sha224', 'sha256', 'sha384', 'sha512')\n\nalgorithms_guaranteed = set(__always_supported)\nalgorithms_available = set(__always_supported)\n\nalgorithms = __always_supported\n\n__all__ = __always_supported + ('new', 'algorithms_guaranteed',\n                                'algorithms_available', 'algorithms',\n                                'pbkdf2_hmac')\n\n\ndef __get_builtin_constructor(name):\n    try:\n        if name in ('SHA1', 'sha1'):\n            import _sha\n            return _sha.new\n        elif name in ('MD5', 'md5'):\n            import _md5\n            return _md5.new\n        elif name in ('SHA256', 'sha256', 'SHA224', 'sha224'):\n            import _sha256\n            bs = name[3:]\n            if bs == '256':\n                return _sha256.sha256\n            elif bs == '224':\n                return _sha256.sha224\n        elif name in ('SHA512', 'sha512', 'SHA384', 'sha384'):\n            import _sha512\n            bs = name[3:]\n            if bs == '512':\n                return _sha512.sha512\n            elif bs == '384':\n                return _sha512.sha384\n    except ImportError:\n        pass  # no extension module, this hash is unsupported.\n\n    raise ValueError('unsupported hash type ' + name)\n\n\ndef __get_openssl_constructor(name):\n    try:\n        f = getattr(_hashlib, 'openssl_' + name)\n        # Allow the C module to raise ValueError.  The function will be\n        # defined but the hash not actually available thanks to OpenSSL.\n        f()\n        # Use the C function directly (very fast)\n        return f\n    except (AttributeError, ValueError):\n        return __get_builtin_constructor(name)\n\n\ndef __py_new(name, string=''):\n    \"\"\"new(name, string='') - Return a new hashing object using the named algorithm;\n    optionally initialized with a string.\n    \"\"\"\n    return __get_builtin_constructor(name)(string)\n\n\ndef __hash_new(name, string=''):\n    \"\"\"new(name, string='') - Return a new hashing object using the named algorithm;\n    optionally initialized with a string.\n    \"\"\"\n    try:\n        return _hashlib.new(name, string)\n    except ValueError:\n        # If the _hashlib module (OpenSSL) doesn't support the named\n        # hash, try using our builtin implementations.\n        # This allows for SHA224/256 and SHA384/512 support even though\n        # the OpenSSL library prior to 0.9.8 doesn't provide them.\n        return __get_builtin_constructor(name)(string)\n\n\ntry:\n    import _hashlib\n    new = __hash_new\n    __get_hash = __get_openssl_constructor\n    algorithms_available = algorithms_available.union(\n        _hashlib.openssl_md_meth_names)\nexcept ImportError:\n    new = __py_new\n    __get_hash = __get_builtin_constructor\n\nfor __func_name in __always_supported:\n    # try them all, some may not work due to the OpenSSL\n    # version not supporting that algorithm.\n    try:\n        globals()[__func_name] = __get_hash(__func_name)\n    except ValueError:\n        import logging\n        logging.exception('code for hash %s was not found.', __func_name)\n\n\ntry:\n    # OpenSSL's PKCS5_PBKDF2_HMAC requires OpenSSL 1.0+ with HMAC and SHA\n    from _hashlib import pbkdf2_hmac\nexcept ImportError:\n    import binascii\n    import struct\n\n    _trans_5C = b\"\".join(chr(x ^ 0x5C) for x in range(256))\n    _trans_36 = b\"\".join(chr(x ^ 0x36) for x in range(256))\n\n    def pbkdf2_hmac(hash_name, password, salt, iterations, dklen=None):\n        \"\"\"Password based key derivation function 2 (PKCS #5 v2.0)\n\n        This Python implementations based on the hmac module about as fast\n        as OpenSSL's PKCS5_PBKDF2_HMAC for short passwords and much faster\n        for long passwords.\n        \"\"\"\n        if not isinstance(hash_name, str):\n            raise TypeError(hash_name)\n\n        if not isinstance(password, (bytes, bytearray)):\n            password = bytes(buffer(password))\n        if not isinstance(salt, (bytes, bytearray)):\n            salt = bytes(buffer(salt))\n\n        # Fast inline HMAC implementation\n        inner = new(hash_name)\n        outer = new(hash_name)\n        blocksize = getattr(inner, 'block_size', 64)\n        if len(password) > blocksize:\n            password = new(hash_name, password).digest()\n        password = password + b'\\x00' * (blocksize - len(password))\n        inner.update(password.translate(_trans_36))\n        outer.update(password.translate(_trans_5C))\n\n        def prf(msg, inner=inner, outer=outer):\n            # PBKDF2_HMAC uses the password as key. We can re-use the same\n            # digest objects and and just update copies to skip initialization.\n            icpy = inner.copy()\n            ocpy = outer.copy()\n            icpy.update(msg)\n            ocpy.update(icpy.digest())\n            return ocpy.digest()\n\n        if iterations < 1:\n            raise ValueError(iterations)\n        if dklen is None:\n            dklen = outer.digest_size\n        if dklen < 1:\n            raise ValueError(dklen)\n\n        hex_format_string = \"%%0%ix\" % (new(hash_name).digest_size * 2)\n\n        dkey = b''\n        loop = 1\n        while len(dkey) < dklen:\n            prev = prf(salt + struct.pack(b'>I', loop))\n            rkey = int(binascii.hexlify(prev), 16)\n            for i in xrange(iterations - 1):\n                prev = prf(prev)\n                rkey ^= int(binascii.hexlify(prev), 16)\n            loop += 1\n            dkey += binascii.unhexlify(hex_format_string % rkey)\n\n        return dkey[:dklen]\n\n# Cleanup locals()\ndel __always_supported, __func_name, __get_hash\ndel __py_new, __hash_new, __get_openssl_constructor\n", 
    "heapq": "# -*- coding: utf-8 -*-\n\n\"\"\"Heap queue algorithm (a.k.a. priority queue).\n\nHeaps are arrays for which a[k] <= a[2*k+1] and a[k] <= a[2*k+2] for\nall k, counting elements from 0.  For the sake of comparison,\nnon-existing elements are considered to be infinite.  The interesting\nproperty of a heap is that a[0] is always its smallest element.\n\nUsage:\n\nheap = []            # creates an empty heap\nheappush(heap, item) # pushes a new item on the heap\nitem = heappop(heap) # pops the smallest item from the heap\nitem = heap[0]       # smallest item on the heap without popping it\nheapify(x)           # transforms list into a heap, in-place, in linear time\nitem = heapreplace(heap, item) # pops and returns smallest item, and adds\n                               # new item; the heap size is unchanged\n\nOur API differs from textbook heap algorithms as follows:\n\n- We use 0-based indexing.  This makes the relationship between the\n  index for a node and the indexes for its children slightly less\n  obvious, but is more suitable since Python uses 0-based indexing.\n\n- Our heappop() method returns the smallest item, not the largest.\n\nThese two make it possible to view the heap as a regular Python list\nwithout surprises: heap[0] is the smallest item, and heap.sort()\nmaintains the heap invariant!\n\"\"\"\n\n# Original code by Kevin O'Connor, augmented by Tim Peters and Raymond Hettinger\n\n__about__ = \"\"\"Heap queues\n\n[explanation by Fran\u00e7ois Pinard]\n\nHeaps are arrays for which a[k] <= a[2*k+1] and a[k] <= a[2*k+2] for\nall k, counting elements from 0.  For the sake of comparison,\nnon-existing elements are considered to be infinite.  The interesting\nproperty of a heap is that a[0] is always its smallest element.\n\nThe strange invariant above is meant to be an efficient memory\nrepresentation for a tournament.  The numbers below are `k', not a[k]:\n\n                                   0\n\n                  1                                 2\n\n          3               4                5               6\n\n      7       8       9       10      11      12      13      14\n\n    15 16   17 18   19 20   21 22   23 24   25 26   27 28   29 30\n\n\nIn the tree above, each cell `k' is topping `2*k+1' and `2*k+2'.  In\nan usual binary tournament we see in sports, each cell is the winner\nover the two cells it tops, and we can trace the winner down the tree\nto see all opponents s/he had.  However, in many computer applications\nof such tournaments, we do not need to trace the history of a winner.\nTo be more memory efficient, when a winner is promoted, we try to\nreplace it by something else at a lower level, and the rule becomes\nthat a cell and the two cells it tops contain three different items,\nbut the top cell \"wins\" over the two topped cells.\n\nIf this heap invariant is protected at all time, index 0 is clearly\nthe overall winner.  The simplest algorithmic way to remove it and\nfind the \"next\" winner is to move some loser (let's say cell 30 in the\ndiagram above) into the 0 position, and then percolate this new 0 down\nthe tree, exchanging values, until the invariant is re-established.\nThis is clearly logarithmic on the total number of items in the tree.\nBy iterating over all items, you get an O(n ln n) sort.\n\nA nice feature of this sort is that you can efficiently insert new\nitems while the sort is going on, provided that the inserted items are\nnot \"better\" than the last 0'th element you extracted.  This is\nespecially useful in simulation contexts, where the tree holds all\nincoming events, and the \"win\" condition means the smallest scheduled\ntime.  When an event schedule other events for execution, they are\nscheduled into the future, so they can easily go into the heap.  So, a\nheap is a good structure for implementing schedulers (this is what I\nused for my MIDI sequencer :-).\n\nVarious structures for implementing schedulers have been extensively\nstudied, and heaps are good for this, as they are reasonably speedy,\nthe speed is almost constant, and the worst case is not much different\nthan the average case.  However, there are other representations which\nare more efficient overall, yet the worst cases might be terrible.\n\nHeaps are also very useful in big disk sorts.  You most probably all\nknow that a big sort implies producing \"runs\" (which are pre-sorted\nsequences, which size is usually related to the amount of CPU memory),\nfollowed by a merging passes for these runs, which merging is often\nvery cleverly organised[1].  It is very important that the initial\nsort produces the longest runs possible.  Tournaments are a good way\nto that.  If, using all the memory available to hold a tournament, you\nreplace and percolate items that happen to fit the current run, you'll\nproduce runs which are twice the size of the memory for random input,\nand much better for input fuzzily ordered.\n\nMoreover, if you output the 0'th item on disk and get an input which\nmay not fit in the current tournament (because the value \"wins\" over\nthe last output value), it cannot fit in the heap, so the size of the\nheap decreases.  The freed memory could be cleverly reused immediately\nfor progressively building a second heap, which grows at exactly the\nsame rate the first heap is melting.  When the first heap completely\nvanishes, you switch heaps and start a new run.  Clever and quite\neffective!\n\nIn a word, heaps are useful memory structures to know.  I use them in\na few applications, and I think it is good to keep a `heap' module\naround. :-)\n\n--------------------\n[1] The disk balancing algorithms which are current, nowadays, are\nmore annoying than clever, and this is a consequence of the seeking\ncapabilities of the disks.  On devices which cannot seek, like big\ntape drives, the story was quite different, and one had to be very\nclever to ensure (far in advance) that each tape movement will be the\nmost effective possible (that is, will best participate at\n\"progressing\" the merge).  Some tapes were even able to read\nbackwards, and this was also used to avoid the rewinding time.\nBelieve me, real good tape sorts were quite spectacular to watch!\nFrom all times, sorting has always been a Great Art! :-)\n\"\"\"\n\n__all__ = ['heappush', 'heappop', 'heapify', 'heapreplace', 'merge',\n           'nlargest', 'nsmallest', 'heappushpop']\n\nfrom itertools import islice, count, imap, izip, tee, chain\nfrom operator import itemgetter\n\ndef cmp_lt(x, y):\n    # Use __lt__ if available; otherwise, try __le__.\n    # In Py3.x, only __lt__ will be called.\n    return (x < y) if hasattr(x, '__lt__') else (not y <= x)\n\ndef heappush(heap, item):\n    \"\"\"Push item onto heap, maintaining the heap invariant.\"\"\"\n    heap.append(item)\n    _siftdown(heap, 0, len(heap)-1)\n\ndef heappop(heap):\n    \"\"\"Pop the smallest item off the heap, maintaining the heap invariant.\"\"\"\n    lastelt = heap.pop()    # raises appropriate IndexError if heap is empty\n    if heap:\n        returnitem = heap[0]\n        heap[0] = lastelt\n        _siftup(heap, 0)\n    else:\n        returnitem = lastelt\n    return returnitem\n\ndef heapreplace(heap, item):\n    \"\"\"Pop and return the current smallest value, and add the new item.\n\n    This is more efficient than heappop() followed by heappush(), and can be\n    more appropriate when using a fixed-size heap.  Note that the value\n    returned may be larger than item!  That constrains reasonable uses of\n    this routine unless written as part of a conditional replacement:\n\n        if item > heap[0]:\n            item = heapreplace(heap, item)\n    \"\"\"\n    returnitem = heap[0]    # raises appropriate IndexError if heap is empty\n    heap[0] = item\n    _siftup(heap, 0)\n    return returnitem\n\ndef heappushpop(heap, item):\n    \"\"\"Fast version of a heappush followed by a heappop.\"\"\"\n    if heap and cmp_lt(heap[0], item):\n        item, heap[0] = heap[0], item\n        _siftup(heap, 0)\n    return item\n\ndef heapify(x):\n    \"\"\"Transform list into a heap, in-place, in O(len(x)) time.\"\"\"\n    n = len(x)\n    # Transform bottom-up.  The largest index there's any point to looking at\n    # is the largest with a child index in-range, so must have 2*i + 1 < n,\n    # or i < (n-1)/2.  If n is even = 2*j, this is (2*j-1)/2 = j-1/2 so\n    # j-1 is the largest, which is n//2 - 1.  If n is odd = 2*j+1, this is\n    # (2*j+1-1)/2 = j so j-1 is the largest, and that's again n//2-1.\n    for i in reversed(xrange(n//2)):\n        _siftup(x, i)\n\ndef _heappushpop_max(heap, item):\n    \"\"\"Maxheap version of a heappush followed by a heappop.\"\"\"\n    if heap and cmp_lt(item, heap[0]):\n        item, heap[0] = heap[0], item\n        _siftup_max(heap, 0)\n    return item\n\ndef _heapify_max(x):\n    \"\"\"Transform list into a maxheap, in-place, in O(len(x)) time.\"\"\"\n    n = len(x)\n    for i in reversed(range(n//2)):\n        _siftup_max(x, i)\n\ndef nlargest(n, iterable):\n    \"\"\"Find the n largest elements in a dataset.\n\n    Equivalent to:  sorted(iterable, reverse=True)[:n]\n    \"\"\"\n    if n < 0:\n        return []\n    it = iter(iterable)\n    result = list(islice(it, n))\n    if not result:\n        return result\n    heapify(result)\n    _heappushpop = heappushpop\n    for elem in it:\n        _heappushpop(result, elem)\n    result.sort(reverse=True)\n    return result\n\ndef nsmallest(n, iterable):\n    \"\"\"Find the n smallest elements in a dataset.\n\n    Equivalent to:  sorted(iterable)[:n]\n    \"\"\"\n    if n < 0:\n        return []\n    it = iter(iterable)\n    result = list(islice(it, n))\n    if not result:\n        return result\n    _heapify_max(result)\n    _heappushpop = _heappushpop_max\n    for elem in it:\n        _heappushpop(result, elem)\n    result.sort()\n    return result\n\n# 'heap' is a heap at all indices >= startpos, except possibly for pos.  pos\n# is the index of a leaf with a possibly out-of-order value.  Restore the\n# heap invariant.\ndef _siftdown(heap, startpos, pos):\n    newitem = heap[pos]\n    # Follow the path to the root, moving parents down until finding a place\n    # newitem fits.\n    while pos > startpos:\n        parentpos = (pos - 1) >> 1\n        parent = heap[parentpos]\n        if cmp_lt(newitem, parent):\n            heap[pos] = parent\n            pos = parentpos\n            continue\n        break\n    heap[pos] = newitem\n\n# The child indices of heap index pos are already heaps, and we want to make\n# a heap at index pos too.  We do this by bubbling the smaller child of\n# pos up (and so on with that child's children, etc) until hitting a leaf,\n# then using _siftdown to move the oddball originally at index pos into place.\n#\n# We *could* break out of the loop as soon as we find a pos where newitem <=\n# both its children, but turns out that's not a good idea, and despite that\n# many books write the algorithm that way.  During a heap pop, the last array\n# element is sifted in, and that tends to be large, so that comparing it\n# against values starting from the root usually doesn't pay (= usually doesn't\n# get us out of the loop early).  See Knuth, Volume 3, where this is\n# explained and quantified in an exercise.\n#\n# Cutting the # of comparisons is important, since these routines have no\n# way to extract \"the priority\" from an array element, so that intelligence\n# is likely to be hiding in custom __cmp__ methods, or in array elements\n# storing (priority, record) tuples.  Comparisons are thus potentially\n# expensive.\n#\n# On random arrays of length 1000, making this change cut the number of\n# comparisons made by heapify() a little, and those made by exhaustive\n# heappop() a lot, in accord with theory.  Here are typical results from 3\n# runs (3 just to demonstrate how small the variance is):\n#\n# Compares needed by heapify     Compares needed by 1000 heappops\n# --------------------------     --------------------------------\n# 1837 cut to 1663               14996 cut to 8680\n# 1855 cut to 1659               14966 cut to 8678\n# 1847 cut to 1660               15024 cut to 8703\n#\n# Building the heap by using heappush() 1000 times instead required\n# 2198, 2148, and 2219 compares:  heapify() is more efficient, when\n# you can use it.\n#\n# The total compares needed by list.sort() on the same lists were 8627,\n# 8627, and 8632 (this should be compared to the sum of heapify() and\n# heappop() compares):  list.sort() is (unsurprisingly!) more efficient\n# for sorting.\n\ndef _siftup(heap, pos):\n    endpos = len(heap)\n    startpos = pos\n    newitem = heap[pos]\n    # Bubble up the smaller child until hitting a leaf.\n    childpos = 2*pos + 1    # leftmost child position\n    while childpos < endpos:\n        # Set childpos to index of smaller child.\n        rightpos = childpos + 1\n        if rightpos < endpos and not cmp_lt(heap[childpos], heap[rightpos]):\n            childpos = rightpos\n        # Move the smaller child up.\n        heap[pos] = heap[childpos]\n        pos = childpos\n        childpos = 2*pos + 1\n    # The leaf at pos is empty now.  Put newitem there, and bubble it up\n    # to its final resting place (by sifting its parents down).\n    heap[pos] = newitem\n    _siftdown(heap, startpos, pos)\n\ndef _siftdown_max(heap, startpos, pos):\n    'Maxheap variant of _siftdown'\n    newitem = heap[pos]\n    # Follow the path to the root, moving parents down until finding a place\n    # newitem fits.\n    while pos > startpos:\n        parentpos = (pos - 1) >> 1\n        parent = heap[parentpos]\n        if cmp_lt(parent, newitem):\n            heap[pos] = parent\n            pos = parentpos\n            continue\n        break\n    heap[pos] = newitem\n\ndef _siftup_max(heap, pos):\n    'Maxheap variant of _siftup'\n    endpos = len(heap)\n    startpos = pos\n    newitem = heap[pos]\n    # Bubble up the larger child until hitting a leaf.\n    childpos = 2*pos + 1    # leftmost child position\n    while childpos < endpos:\n        # Set childpos to index of larger child.\n        rightpos = childpos + 1\n        if rightpos < endpos and not cmp_lt(heap[rightpos], heap[childpos]):\n            childpos = rightpos\n        # Move the larger child up.\n        heap[pos] = heap[childpos]\n        pos = childpos\n        childpos = 2*pos + 1\n    # The leaf at pos is empty now.  Put newitem there, and bubble it up\n    # to its final resting place (by sifting its parents down).\n    heap[pos] = newitem\n    _siftdown_max(heap, startpos, pos)\n\n# If available, use C implementation\ntry:\n    from _heapq import *\nexcept ImportError:\n    pass\n\ndef merge(*iterables):\n    '''Merge multiple sorted inputs into a single sorted output.\n\n    Similar to sorted(itertools.chain(*iterables)) but returns a generator,\n    does not pull the data into memory all at once, and assumes that each of\n    the input streams is already sorted (smallest to largest).\n\n    >>> list(merge([1,3,5,7], [0,2,4,8], [5,10,15,20], [], [25]))\n    [0, 1, 2, 3, 4, 5, 5, 7, 8, 10, 15, 20, 25]\n\n    '''\n    _heappop, _heapreplace, _StopIteration = heappop, heapreplace, StopIteration\n    _len = len\n\n    h = []\n    h_append = h.append\n    for itnum, it in enumerate(map(iter, iterables)):\n        try:\n            next = it.next\n            h_append([next(), itnum, next])\n        except _StopIteration:\n            pass\n    heapify(h)\n\n    while _len(h) > 1:\n        try:\n            while 1:\n                v, itnum, next = s = h[0]\n                yield v\n                s[0] = next()               # raises StopIteration when exhausted\n                _heapreplace(h, s)          # restore heap condition\n        except _StopIteration:\n            _heappop(h)                     # remove empty iterator\n    if h:\n        # fast case when only a single iterator remains\n        v, itnum, next = h[0]\n        yield v\n        for v in next.__self__:\n            yield v\n\n# Extend the implementations of nsmallest and nlargest to use a key= argument\n_nsmallest = nsmallest\ndef nsmallest(n, iterable, key=None):\n    \"\"\"Find the n smallest elements in a dataset.\n\n    Equivalent to:  sorted(iterable, key=key)[:n]\n    \"\"\"\n    # Short-cut for n==1 is to use min() when len(iterable)>0\n    if n == 1:\n        it = iter(iterable)\n        head = list(islice(it, 1))\n        if not head:\n            return []\n        if key is None:\n            return [min(chain(head, it))]\n        return [min(chain(head, it), key=key)]\n\n    # When n>=size, it's faster to use sorted()\n    try:\n        size = len(iterable)\n    except (TypeError, AttributeError):\n        pass\n    else:\n        if n >= size:\n            return sorted(iterable, key=key)[:n]\n\n    # When key is none, use simpler decoration\n    if key is None:\n        it = izip(iterable, count())                        # decorate\n        result = _nsmallest(n, it)\n        return map(itemgetter(0), result)                   # undecorate\n\n    # General case, slowest method\n    in1, in2 = tee(iterable)\n    it = izip(imap(key, in1), count(), in2)                 # decorate\n    result = _nsmallest(n, it)\n    return map(itemgetter(2), result)                       # undecorate\n\n_nlargest = nlargest\ndef nlargest(n, iterable, key=None):\n    \"\"\"Find the n largest elements in a dataset.\n\n    Equivalent to:  sorted(iterable, key=key, reverse=True)[:n]\n    \"\"\"\n\n    # Short-cut for n==1 is to use max() when len(iterable)>0\n    if n == 1:\n        it = iter(iterable)\n        head = list(islice(it, 1))\n        if not head:\n            return []\n        if key is None:\n            return [max(chain(head, it))]\n        return [max(chain(head, it), key=key)]\n\n    # When n>=size, it's faster to use sorted()\n    try:\n        size = len(iterable)\n    except (TypeError, AttributeError):\n        pass\n    else:\n        if n >= size:\n            return sorted(iterable, key=key, reverse=True)[:n]\n\n    # When key is none, use simpler decoration\n    if key is None:\n        it = izip(iterable, count(0,-1))                    # decorate\n        result = _nlargest(n, it)\n        return map(itemgetter(0), result)                   # undecorate\n\n    # General case, slowest method\n    in1, in2 = tee(iterable)\n    it = izip(imap(key, in1), count(0,-1), in2)             # decorate\n    result = _nlargest(n, it)\n    return map(itemgetter(2), result)                       # undecorate\n\nif __name__ == \"__main__\":\n    # Simple sanity test\n    heap = []\n    data = [1, 3, 5, 7, 9, 2, 4, 6, 8, 0]\n    for item in data:\n        heappush(heap, item)\n    sort = []\n    while heap:\n        sort.append(heappop(heap))\n    print sort\n\n    import doctest\n    doctest.testmod()\n", 
    "inspect": "# -*- coding: utf-8 -*-\n\"\"\"Get useful information from live Python objects.\n\nThis module encapsulates the interface provided by the internal special\nattributes (func_*, co_*, im_*, tb_*, etc.) in a friendlier fashion.\nIt also provides some help for examining source code and class layout.\n\nHere are some of the useful functions provided by this module:\n\n    ismodule(), isclass(), ismethod(), isfunction(), isgeneratorfunction(),\n        isgenerator(), istraceback(), isframe(), iscode(), isbuiltin(),\n        isroutine() - check object types\n    getmembers() - get members of an object that satisfy a given condition\n\n    getfile(), getsourcefile(), getsource() - find an object's source code\n    getdoc(), getcomments() - get documentation on an object\n    getmodule() - determine the module that an object came from\n    getclasstree() - arrange classes so as to represent their hierarchy\n\n    getargspec(), getargvalues(), getcallargs() - get info about function arguments\n    formatargspec(), formatargvalues() - format an argument spec\n    getouterframes(), getinnerframes() - get info about frames\n    currentframe() - get the current stack frame\n    stack(), trace() - get info about frames on the stack or in a traceback\n\"\"\"\n\n# This module is in the public domain.  No warranties.\n\n__author__ = 'Ka-Ping Yee <ping@lfw.org>'\n__date__ = '1 Jan 2001'\n\nimport sys\nimport os\nimport types\nimport string\nimport re\nimport dis\nimport imp\nimport tokenize\nimport linecache\nfrom operator import attrgetter\nfrom collections import namedtuple\n\n# These constants are from Include/code.h.\nCO_OPTIMIZED, CO_NEWLOCALS, CO_VARARGS, CO_VARKEYWORDS = 0x1, 0x2, 0x4, 0x8\nCO_NESTED, CO_GENERATOR, CO_NOFREE = 0x10, 0x20, 0x40\n# See Include/object.h\nTPFLAGS_IS_ABSTRACT = 1 << 20\n\n# ----------------------------------------------------------- type-checking\ndef ismodule(object):\n    \"\"\"Return true if the object is a module.\n\n    Module objects provide these attributes:\n        __doc__         documentation string\n        __file__        filename (missing for built-in modules)\"\"\"\n    return isinstance(object, types.ModuleType)\n\ndef isclass(object):\n    \"\"\"Return true if the object is a class.\n\n    Class objects provide these attributes:\n        __doc__         documentation string\n        __module__      name of module in which this class was defined\"\"\"\n    return isinstance(object, (type, types.ClassType))\n\ndef ismethod(object):\n    \"\"\"Return true if the object is an instance method.\n\n    Instance method objects provide these attributes:\n        __doc__         documentation string\n        __name__        name with which this method was defined\n        im_class        class object in which this method belongs\n        im_func         function object containing implementation of method\n        im_self         instance to which this method is bound, or None\"\"\"\n    return isinstance(object, types.MethodType)\n\ndef ismethoddescriptor(object):\n    \"\"\"Return true if the object is a method descriptor.\n\n    But not if ismethod() or isclass() or isfunction() are true.\n\n    This is new in Python 2.2, and, for example, is true of int.__add__.\n    An object passing this test has a __get__ attribute but not a __set__\n    attribute, but beyond that the set of attributes varies.  __name__ is\n    usually sensible, and __doc__ often is.\n\n    Methods implemented via descriptors that also pass one of the other\n    tests return false from the ismethoddescriptor() test, simply because\n    the other tests promise more -- you can, e.g., count on having the\n    im_func attribute (etc) when an object passes ismethod().\"\"\"\n    return (hasattr(object, \"__get__\")\n            and not hasattr(object, \"__set__\") # else it's a data descriptor\n            and not ismethod(object)           # mutual exclusion\n            and not isfunction(object)\n            and not isclass(object))\n\ndef isdatadescriptor(object):\n    \"\"\"Return true if the object is a data descriptor.\n\n    Data descriptors have both a __get__ and a __set__ attribute.  Examples are\n    properties (defined in Python) and getsets and members (defined in C).\n    Typically, data descriptors will also have __name__ and __doc__ attributes\n    (properties, getsets, and members have both of these attributes), but this\n    is not guaranteed.\"\"\"\n    return (hasattr(object, \"__set__\") and hasattr(object, \"__get__\"))\n\nif hasattr(types, 'MemberDescriptorType'):\n    # CPython and equivalent\n    def ismemberdescriptor(object):\n        \"\"\"Return true if the object is a member descriptor.\n\n        Member descriptors are specialized descriptors defined in extension\n        modules.\"\"\"\n        return isinstance(object, types.MemberDescriptorType)\nelse:\n    # Other implementations\n    def ismemberdescriptor(object):\n        \"\"\"Return true if the object is a member descriptor.\n\n        Member descriptors are specialized descriptors defined in extension\n        modules.\"\"\"\n        return False\n\nif hasattr(types, 'GetSetDescriptorType'):\n    # CPython and equivalent\n    def isgetsetdescriptor(object):\n        \"\"\"Return true if the object is a getset descriptor.\n\n        getset descriptors are specialized descriptors defined in extension\n        modules.\"\"\"\n        return isinstance(object, types.GetSetDescriptorType)\nelse:\n    # Other implementations\n    def isgetsetdescriptor(object):\n        \"\"\"Return true if the object is a getset descriptor.\n\n        getset descriptors are specialized descriptors defined in extension\n        modules.\"\"\"\n        return False\n\ndef isfunction(object):\n    \"\"\"Return true if the object is a user-defined function.\n\n    Function objects provide these attributes:\n        __doc__         documentation string\n        __name__        name with which this function was defined\n        func_code       code object containing compiled function bytecode\n        func_defaults   tuple of any default values for arguments\n        func_doc        (same as __doc__)\n        func_globals    global namespace in which this function was defined\n        func_name       (same as __name__)\"\"\"\n    return isinstance(object, types.FunctionType)\n\ndef isgeneratorfunction(object):\n    \"\"\"Return true if the object is a user-defined generator function.\n\n    Generator function objects provides same attributes as functions.\n\n    See help(isfunction) for attributes listing.\"\"\"\n    return bool((isfunction(object) or ismethod(object)) and\n                object.func_code.co_flags & CO_GENERATOR)\n\ndef isgenerator(object):\n    \"\"\"Return true if the object is a generator.\n\n    Generator objects provide these attributes:\n        __iter__        defined to support iteration over container\n        close           raises a new GeneratorExit exception inside the\n                        generator to terminate the iteration\n        gi_code         code object\n        gi_frame        frame object or possibly None once the generator has\n                        been exhausted\n        gi_running      set to 1 when generator is executing, 0 otherwise\n        next            return the next item from the container\n        send            resumes the generator and \"sends\" a value that becomes\n                        the result of the current yield-expression\n        throw           used to raise an exception inside the generator\"\"\"\n    return isinstance(object, types.GeneratorType)\n\ndef istraceback(object):\n    \"\"\"Return true if the object is a traceback.\n\n    Traceback objects provide these attributes:\n        tb_frame        frame object at this level\n        tb_lasti        index of last attempted instruction in bytecode\n        tb_lineno       current line number in Python source code\n        tb_next         next inner traceback object (called by this level)\"\"\"\n    return isinstance(object, types.TracebackType)\n\ndef isframe(object):\n    \"\"\"Return true if the object is a frame object.\n\n    Frame objects provide these attributes:\n        f_back          next outer frame object (this frame's caller)\n        f_builtins      built-in namespace seen by this frame\n        f_code          code object being executed in this frame\n        f_exc_traceback traceback if raised in this frame, or None\n        f_exc_type      exception type if raised in this frame, or None\n        f_exc_value     exception value if raised in this frame, or None\n        f_globals       global namespace seen by this frame\n        f_lasti         index of last attempted instruction in bytecode\n        f_lineno        current line number in Python source code\n        f_locals        local namespace seen by this frame\n        f_restricted    0 or 1 if frame is in restricted execution mode\n        f_trace         tracing function for this frame, or None\"\"\"\n    return isinstance(object, types.FrameType)\n\ndef iscode(object):\n    \"\"\"Return true if the object is a code object.\n\n    Code objects provide these attributes:\n        co_argcount     number of arguments (not including * or ** args)\n        co_code         string of raw compiled bytecode\n        co_consts       tuple of constants used in the bytecode\n        co_filename     name of file in which this code object was created\n        co_firstlineno  number of first line in Python source code\n        co_flags        bitmap: 1=optimized | 2=newlocals | 4=*arg | 8=**arg\n        co_lnotab       encoded mapping of line numbers to bytecode indices\n        co_name         name with which this code object was defined\n        co_names        tuple of names of local variables\n        co_nlocals      number of local variables\n        co_stacksize    virtual machine stack space required\n        co_varnames     tuple of names of arguments and local variables\"\"\"\n    return isinstance(object, types.CodeType)\n\ndef isbuiltin(object):\n    \"\"\"Return true if the object is a built-in function or method.\n\n    Built-in functions and methods provide these attributes:\n        __doc__         documentation string\n        __name__        original name of this function or method\n        __self__        instance to which a method is bound, or None\"\"\"\n    return isinstance(object, types.BuiltinFunctionType)\n\ndef isroutine(object):\n    \"\"\"Return true if the object is any kind of function or method.\"\"\"\n    return (isbuiltin(object)\n            or isfunction(object)\n            or ismethod(object)\n            or ismethoddescriptor(object))\n\ndef isabstract(object):\n    \"\"\"Return true if the object is an abstract base class (ABC).\"\"\"\n    return bool(isinstance(object, type) and object.__flags__ & TPFLAGS_IS_ABSTRACT)\n\ndef getmembers(object, predicate=None):\n    \"\"\"Return all members of an object as (name, value) pairs sorted by name.\n    Optionally, only return members that satisfy a given predicate.\"\"\"\n    results = []\n    for key in dir(object):\n        try:\n            value = getattr(object, key)\n        except AttributeError:\n            continue\n        if not predicate or predicate(value):\n            results.append((key, value))\n    results.sort()\n    return results\n\nAttribute = namedtuple('Attribute', 'name kind defining_class object')\n\ndef classify_class_attrs(cls):\n    \"\"\"Return list of attribute-descriptor tuples.\n\n    For each name in dir(cls), the return list contains a 4-tuple\n    with these elements:\n\n        0. The name (a string).\n\n        1. The kind of attribute this is, one of these strings:\n               'class method'    created via classmethod()\n               'static method'   created via staticmethod()\n               'property'        created via property()\n               'method'          any other flavor of method\n               'data'            not a method\n\n        2. The class which defined this attribute (a class).\n\n        3. The object as obtained directly from the defining class's\n           __dict__, not via getattr.  This is especially important for\n           data attributes:  C.data is just a data object, but\n           C.__dict__['data'] may be a data descriptor with additional\n           info, like a __doc__ string.\n    \"\"\"\n\n    mro = getmro(cls)\n    names = dir(cls)\n    result = []\n    for name in names:\n        # Get the object associated with the name, and where it was defined.\n        # Getting an obj from the __dict__ sometimes reveals more than\n        # using getattr.  Static and class methods are dramatic examples.\n        # Furthermore, some objects may raise an Exception when fetched with\n        # getattr(). This is the case with some descriptors (bug #1785).\n        # Thus, we only use getattr() as a last resort.\n        homecls = None\n        for base in (cls,) + mro:\n            if name in base.__dict__:\n                obj = base.__dict__[name]\n                homecls = base\n                break\n        else:\n            obj = getattr(cls, name)\n            homecls = getattr(obj, \"__objclass__\", homecls)\n\n        # Classify the object.\n        if isinstance(obj, staticmethod):\n            kind = \"static method\"\n        elif isinstance(obj, classmethod):\n            kind = \"class method\"\n        elif isinstance(obj, property):\n            kind = \"property\"\n        elif ismethoddescriptor(obj):\n            kind = \"method\"\n        elif isdatadescriptor(obj):\n            kind = \"data\"\n        else:\n            obj_via_getattr = getattr(cls, name)\n            if (ismethod(obj_via_getattr) or\n                ismethoddescriptor(obj_via_getattr)):\n                kind = \"method\"\n            else:\n                kind = \"data\"\n            obj = obj_via_getattr\n\n        result.append(Attribute(name, kind, homecls, obj))\n\n    return result\n\n# ----------------------------------------------------------- class helpers\ndef _searchbases(cls, accum):\n    # Simulate the \"classic class\" search order.\n    if cls in accum:\n        return\n    accum.append(cls)\n    for base in cls.__bases__:\n        _searchbases(base, accum)\n\ndef getmro(cls):\n    \"Return tuple of base classes (including cls) in method resolution order.\"\n    if hasattr(cls, \"__mro__\"):\n        return cls.__mro__\n    else:\n        result = []\n        _searchbases(cls, result)\n        return tuple(result)\n\n# -------------------------------------------------- source code extraction\ndef indentsize(line):\n    \"\"\"Return the indent size, in spaces, at the start of a line of text.\"\"\"\n    expline = string.expandtabs(line)\n    return len(expline) - len(string.lstrip(expline))\n\ndef getdoc(object):\n    \"\"\"Get the documentation string for an object.\n\n    All tabs are expanded to spaces.  To clean up docstrings that are\n    indented to line up with blocks of code, any whitespace than can be\n    uniformly removed from the second line onwards is removed.\"\"\"\n    try:\n        doc = object.__doc__\n    except AttributeError:\n        return None\n    if not isinstance(doc, types.StringTypes):\n        return None\n    return cleandoc(doc)\n\ndef cleandoc(doc):\n    \"\"\"Clean up indentation from docstrings.\n\n    Any whitespace that can be uniformly removed from the second line\n    onwards is removed.\"\"\"\n    try:\n        lines = string.split(string.expandtabs(doc), '\\n')\n    except UnicodeError:\n        return None\n    else:\n        # Find minimum indentation of any non-blank lines after first line.\n        margin = sys.maxint\n        for line in lines[1:]:\n            content = len(string.lstrip(line))\n            if content:\n                indent = len(line) - content\n                margin = min(margin, indent)\n        # Remove indentation.\n        if lines:\n            lines[0] = lines[0].lstrip()\n        if margin < sys.maxint:\n            for i in range(1, len(lines)): lines[i] = lines[i][margin:]\n        # Remove any trailing or leading blank lines.\n        while lines and not lines[-1]:\n            lines.pop()\n        while lines and not lines[0]:\n            lines.pop(0)\n        return string.join(lines, '\\n')\n\ndef getfile(object):\n    \"\"\"Work out which source or compiled file an object was defined in.\"\"\"\n    if ismodule(object):\n        if hasattr(object, '__file__'):\n            return object.__file__\n        raise TypeError('{!r} is a built-in module'.format(object))\n    if isclass(object):\n        object = sys.modules.get(object.__module__)\n        if hasattr(object, '__file__'):\n            return object.__file__\n        raise TypeError('{!r} is a built-in class'.format(object))\n    if ismethod(object):\n        object = object.im_func\n    if isfunction(object):\n        object = object.func_code\n    if istraceback(object):\n        object = object.tb_frame\n    if isframe(object):\n        object = object.f_code\n    if iscode(object):\n        return object.co_filename\n    raise TypeError('{!r} is not a module, class, method, '\n                    'function, traceback, frame, or code object'.format(object))\n\nModuleInfo = namedtuple('ModuleInfo', 'name suffix mode module_type')\n\ndef getmoduleinfo(path):\n    \"\"\"Get the module name, suffix, mode, and module type for a given file.\"\"\"\n    filename = os.path.basename(path)\n    suffixes = map(lambda info:\n                   (-len(info[0]), info[0], info[1], info[2]),\n                    imp.get_suffixes())\n    suffixes.sort() # try longest suffixes first, in case they overlap\n    for neglen, suffix, mode, mtype in suffixes:\n        if filename[neglen:] == suffix:\n            return ModuleInfo(filename[:neglen], suffix, mode, mtype)\n\ndef getmodulename(path):\n    \"\"\"Return the module name for a given file, or None.\"\"\"\n    info = getmoduleinfo(path)\n    if info: return info[0]\n\ndef getsourcefile(object):\n    \"\"\"Return the filename that can be used to locate an object's source.\n    Return None if no way can be identified to get the source.\n    \"\"\"\n    filename = getfile(object)\n    if string.lower(filename[-4:]) in ('.pyc', '.pyo'):\n        filename = filename[:-4] + '.py'\n    for suffix, mode, kind in imp.get_suffixes():\n        if 'b' in mode and string.lower(filename[-len(suffix):]) == suffix:\n            # Looks like a binary file.  We want to only return a text file.\n            return None\n    if os.path.exists(filename):\n        return filename\n    # only return a non-existent filename if the module has a PEP 302 loader\n    if hasattr(getmodule(object, filename), '__loader__'):\n        return filename\n    # or it is in the linecache\n    if filename in linecache.cache:\n        return filename\n\ndef getabsfile(object, _filename=None):\n    \"\"\"Return an absolute path to the source or compiled file for an object.\n\n    The idea is for each object to have a unique origin, so this routine\n    normalizes the result as much as possible.\"\"\"\n    if _filename is None:\n        _filename = getsourcefile(object) or getfile(object)\n    return os.path.normcase(os.path.abspath(_filename))\n\nmodulesbyfile = {}\n_filesbymodname = {}\n\ndef getmodule(object, _filename=None):\n    \"\"\"Return the module an object was defined in, or None if not found.\"\"\"\n    if ismodule(object):\n        return object\n    if hasattr(object, '__module__'):\n        return sys.modules.get(object.__module__)\n    # Try the filename to modulename cache\n    if _filename is not None and _filename in modulesbyfile:\n        return sys.modules.get(modulesbyfile[_filename])\n    # Try the cache again with the absolute file name\n    try:\n        file = getabsfile(object, _filename)\n    except TypeError:\n        return None\n    if file in modulesbyfile:\n        return sys.modules.get(modulesbyfile[file])\n    # Update the filename to module name cache and check yet again\n    # Copy sys.modules in order to cope with changes while iterating\n    for modname, module in sys.modules.items():\n        if ismodule(module) and hasattr(module, '__file__'):\n            f = module.__file__\n            if f == _filesbymodname.get(modname, None):\n                # Have already mapped this module, so skip it\n                continue\n            _filesbymodname[modname] = f\n            f = getabsfile(module)\n            # Always map to the name the module knows itself by\n            modulesbyfile[f] = modulesbyfile[\n                os.path.realpath(f)] = module.__name__\n    if file in modulesbyfile:\n        return sys.modules.get(modulesbyfile[file])\n    # Check the main module\n    main = sys.modules['__main__']\n    if not hasattr(object, '__name__'):\n        return None\n    if hasattr(main, object.__name__):\n        mainobject = getattr(main, object.__name__)\n        if mainobject is object:\n            return main\n    # Check builtins\n    builtin = sys.modules['__builtin__']\n    if hasattr(builtin, object.__name__):\n        builtinobject = getattr(builtin, object.__name__)\n        if builtinobject is object:\n            return builtin\n\ndef findsource(object):\n    \"\"\"Return the entire source file and starting line number for an object.\n\n    The argument may be a module, class, method, function, traceback, frame,\n    or code object.  The source code is returned as a list of all the lines\n    in the file and the line number indexes a line in that list.  An IOError\n    is raised if the source code cannot be retrieved.\"\"\"\n\n    file = getfile(object)\n    sourcefile = getsourcefile(object)\n    if not sourcefile and file[:1] + file[-1:] != '<>':\n        raise IOError('source code not available')\n    file = sourcefile if sourcefile else file\n\n    module = getmodule(object, file)\n    if module:\n        lines = linecache.getlines(file, module.__dict__)\n    else:\n        lines = linecache.getlines(file)\n    if not lines:\n        raise IOError('could not get source code')\n\n    if ismodule(object):\n        return lines, 0\n\n    if isclass(object):\n        name = object.__name__\n        pat = re.compile(r'^(\\s*)class\\s*' + name + r'\\b')\n        # make some effort to find the best matching class definition:\n        # use the one with the least indentation, which is the one\n        # that's most probably not inside a function definition.\n        candidates = []\n        for i in range(len(lines)):\n            match = pat.match(lines[i])\n            if match:\n                # if it's at toplevel, it's already the best one\n                if lines[i][0] == 'c':\n                    return lines, i\n                # else add whitespace to candidate list\n                candidates.append((match.group(1), i))\n        if candidates:\n            # this will sort by whitespace, and by line number,\n            # less whitespace first\n            candidates.sort()\n            return lines, candidates[0][1]\n        else:\n            raise IOError('could not find class definition')\n\n    if ismethod(object):\n        object = object.im_func\n    if isfunction(object):\n        object = object.func_code\n    if istraceback(object):\n        object = object.tb_frame\n    if isframe(object):\n        object = object.f_code\n    if iscode(object):\n        if not hasattr(object, 'co_firstlineno'):\n            raise IOError('could not find function definition')\n        lnum = object.co_firstlineno - 1\n        pat = re.compile(r'^(\\s*def\\s)|(.*(?<!\\w)lambda(:|\\s))|^(\\s*@)')\n        while lnum > 0:\n            if pat.match(lines[lnum]): break\n            lnum = lnum - 1\n        return lines, lnum\n    raise IOError('could not find code object')\n\ndef getcomments(object):\n    \"\"\"Get lines of comments immediately preceding an object's source code.\n\n    Returns None when source can't be found.\n    \"\"\"\n    try:\n        lines, lnum = findsource(object)\n    except (IOError, TypeError):\n        return None\n\n    if ismodule(object):\n        # Look for a comment block at the top of the file.\n        start = 0\n        if lines and lines[0][:2] == '#!': start = 1\n        while start < len(lines) and string.strip(lines[start]) in ('', '#'):\n            start = start + 1\n        if start < len(lines) and lines[start][:1] == '#':\n            comments = []\n            end = start\n            while end < len(lines) and lines[end][:1] == '#':\n                comments.append(string.expandtabs(lines[end]))\n                end = end + 1\n            return string.join(comments, '')\n\n    # Look for a preceding block of comments at the same indentation.\n    elif lnum > 0:\n        indent = indentsize(lines[lnum])\n        end = lnum - 1\n        if end >= 0 and string.lstrip(lines[end])[:1] == '#' and \\\n            indentsize(lines[end]) == indent:\n            comments = [string.lstrip(string.expandtabs(lines[end]))]\n            if end > 0:\n                end = end - 1\n                comment = string.lstrip(string.expandtabs(lines[end]))\n                while comment[:1] == '#' and indentsize(lines[end]) == indent:\n                    comments[:0] = [comment]\n                    end = end - 1\n                    if end < 0: break\n                    comment = string.lstrip(string.expandtabs(lines[end]))\n            while comments and string.strip(comments[0]) == '#':\n                comments[:1] = []\n            while comments and string.strip(comments[-1]) == '#':\n                comments[-1:] = []\n            return string.join(comments, '')\n\nclass EndOfBlock(Exception): pass\n\nclass BlockFinder:\n    \"\"\"Provide a tokeneater() method to detect the end of a code block.\"\"\"\n    def __init__(self):\n        self.indent = 0\n        self.islambda = False\n        self.started = False\n        self.passline = False\n        self.last = 1\n\n    def tokeneater(self, type, token, srow_scol, erow_ecol, line):\n        srow, scol = srow_scol\n        erow, ecol = erow_ecol\n        if not self.started:\n            # look for the first \"def\", \"class\" or \"lambda\"\n            if token in (\"def\", \"class\", \"lambda\"):\n                if token == \"lambda\":\n                    self.islambda = True\n                self.started = True\n            self.passline = True    # skip to the end of the line\n        elif type == tokenize.NEWLINE:\n            self.passline = False   # stop skipping when a NEWLINE is seen\n            self.last = srow\n            if self.islambda:       # lambdas always end at the first NEWLINE\n                raise EndOfBlock\n        elif self.passline:\n            pass\n        elif type == tokenize.INDENT:\n            self.indent = self.indent + 1\n            self.passline = True\n        elif type == tokenize.DEDENT:\n            self.indent = self.indent - 1\n            # the end of matching indent/dedent pairs end a block\n            # (note that this only works for \"def\"/\"class\" blocks,\n            #  not e.g. for \"if: else:\" or \"try: finally:\" blocks)\n            if self.indent <= 0:\n                raise EndOfBlock\n        elif self.indent == 0 and type not in (tokenize.COMMENT, tokenize.NL):\n            # any other token on the same indentation level end the previous\n            # block as well, except the pseudo-tokens COMMENT and NL.\n            raise EndOfBlock\n\ndef getblock(lines):\n    \"\"\"Extract the block of code at the top of the given list of lines.\"\"\"\n    blockfinder = BlockFinder()\n    try:\n        tokenize.tokenize(iter(lines).next, blockfinder.tokeneater)\n    except (EndOfBlock, IndentationError):\n        pass\n    return lines[:blockfinder.last]\n\ndef getsourcelines(object):\n    \"\"\"Return a list of source lines and starting line number for an object.\n\n    The argument may be a module, class, method, function, traceback, frame,\n    or code object.  The source code is returned as a list of the lines\n    corresponding to the object and the line number indicates where in the\n    original source file the first line of code was found.  An IOError is\n    raised if the source code cannot be retrieved.\"\"\"\n    lines, lnum = findsource(object)\n\n    if ismodule(object): return lines, 0\n    else: return getblock(lines[lnum:]), lnum + 1\n\ndef getsource(object):\n    \"\"\"Return the text of the source code for an object.\n\n    The argument may be a module, class, method, function, traceback, frame,\n    or code object.  The source code is returned as a single string.  An\n    IOError is raised if the source code cannot be retrieved.\"\"\"\n    lines, lnum = getsourcelines(object)\n    return string.join(lines, '')\n\n# --------------------------------------------------- class tree extraction\ndef walktree(classes, children, parent):\n    \"\"\"Recursive helper function for getclasstree().\"\"\"\n    results = []\n    classes.sort(key=attrgetter('__module__', '__name__'))\n    for c in classes:\n        results.append((c, c.__bases__))\n        if c in children:\n            results.append(walktree(children[c], children, c))\n    return results\n\ndef getclasstree(classes, unique=0):\n    \"\"\"Arrange the given list of classes into a hierarchy of nested lists.\n\n    Where a nested list appears, it contains classes derived from the class\n    whose entry immediately precedes the list.  Each entry is a 2-tuple\n    containing a class and a tuple of its base classes.  If the 'unique'\n    argument is true, exactly one entry appears in the returned structure\n    for each class in the given list.  Otherwise, classes using multiple\n    inheritance and their descendants will appear multiple times.\"\"\"\n    children = {}\n    roots = []\n    for c in classes:\n        if c.__bases__:\n            for parent in c.__bases__:\n                if not parent in children:\n                    children[parent] = []\n                if c not in children[parent]:\n                    children[parent].append(c)\n                if unique and parent in classes: break\n        elif c not in roots:\n            roots.append(c)\n    for parent in children:\n        if parent not in classes:\n            roots.append(parent)\n    return walktree(roots, children, None)\n\n# ------------------------------------------------ argument list extraction\nArguments = namedtuple('Arguments', 'args varargs keywords')\n\ndef getargs(co):\n    \"\"\"Get information about the arguments accepted by a code object.\n\n    Three things are returned: (args, varargs, varkw), where 'args' is\n    a list of argument names (possibly containing nested lists), and\n    'varargs' and 'varkw' are the names of the * and ** arguments or None.\"\"\"\n\n    if not iscode(co):\n        if hasattr(len, 'func_code') and type(co) is type(len.func_code):\n            # PyPy extension: built-in function objects have a func_code too.\n            # There is no co_code on it, but co_argcount and co_varnames and\n            # co_flags are present.\n            pass\n        else:\n            raise TypeError('{!r} is not a code object'.format(co))\n\n    code = getattr(co, 'co_code', '')\n    nargs = co.co_argcount\n    names = co.co_varnames\n    args = list(names[:nargs])\n    step = 0\n\n    # The following acrobatics are for anonymous (tuple) arguments.\n    for i in range(nargs):\n        if args[i][:1] in ('', '.'):\n            stack, remain, count = [], [], []\n            while step < len(code):\n                op = ord(code[step])\n                step = step + 1\n                if op >= dis.HAVE_ARGUMENT:\n                    opname = dis.opname[op]\n                    value = ord(code[step]) + ord(code[step+1])*256\n                    step = step + 2\n                    if opname in ('UNPACK_TUPLE', 'UNPACK_SEQUENCE'):\n                        remain.append(value)\n                        count.append(value)\n                    elif opname == 'STORE_FAST':\n                        stack.append(names[value])\n\n                        # Special case for sublists of length 1: def foo((bar))\n                        # doesn't generate the UNPACK_TUPLE bytecode, so if\n                        # `remain` is empty here, we have such a sublist.\n                        if not remain:\n                            stack[0] = [stack[0]]\n                            break\n                        else:\n                            remain[-1] = remain[-1] - 1\n                            while remain[-1] == 0:\n                                remain.pop()\n                                size = count.pop()\n                                stack[-size:] = [stack[-size:]]\n                                if not remain: break\n                                remain[-1] = remain[-1] - 1\n                            if not remain: break\n            args[i] = stack[0]\n\n    varargs = None\n    if co.co_flags & CO_VARARGS:\n        varargs = co.co_varnames[nargs]\n        nargs = nargs + 1\n    varkw = None\n    if co.co_flags & CO_VARKEYWORDS:\n        varkw = co.co_varnames[nargs]\n    return Arguments(args, varargs, varkw)\n\nArgSpec = namedtuple('ArgSpec', 'args varargs keywords defaults')\n\ndef getargspec(func):\n    \"\"\"Get the names and default values of a function's arguments.\n\n    A tuple of four things is returned: (args, varargs, varkw, defaults).\n    'args' is a list of the argument names (it may contain nested lists).\n    'varargs' and 'varkw' are the names of the * and ** arguments or None.\n    'defaults' is an n-tuple of the default values of the last n arguments.\n    \"\"\"\n\n    if ismethod(func):\n        func = func.im_func\n    if not (isfunction(func) or\n            isbuiltin(func) and hasattr(func, 'func_code')):\n            # PyPy extension: this works for built-in functions too\n        raise TypeError('{!r} is not a Python function'.format(func))\n    args, varargs, varkw = getargs(func.func_code)\n    return ArgSpec(args, varargs, varkw, func.func_defaults)\n\nArgInfo = namedtuple('ArgInfo', 'args varargs keywords locals')\n\ndef getargvalues(frame):\n    \"\"\"Get information about arguments passed into a particular frame.\n\n    A tuple of four things is returned: (args, varargs, varkw, locals).\n    'args' is a list of the argument names (it may contain nested lists).\n    'varargs' and 'varkw' are the names of the * and ** arguments or None.\n    'locals' is the locals dictionary of the given frame.\"\"\"\n    args, varargs, varkw = getargs(frame.f_code)\n    return ArgInfo(args, varargs, varkw, frame.f_locals)\n\ndef joinseq(seq):\n    if len(seq) == 1:\n        return '(' + seq[0] + ',)'\n    else:\n        return '(' + string.join(seq, ', ') + ')'\n\ndef strseq(object, convert, join=joinseq):\n    \"\"\"Recursively walk a sequence, stringifying each element.\"\"\"\n    if type(object) in (list, tuple):\n        return join(map(lambda o, c=convert, j=join: strseq(o, c, j), object))\n    else:\n        return convert(object)\n\ndef formatargspec(args, varargs=None, varkw=None, defaults=None,\n                  formatarg=str,\n                  formatvarargs=lambda name: '*' + name,\n                  formatvarkw=lambda name: '**' + name,\n                  formatvalue=lambda value: '=' + repr(value),\n                  join=joinseq):\n    \"\"\"Format an argument spec from the 4 values returned by getargspec.\n\n    The first four arguments are (args, varargs, varkw, defaults).  The\n    other four arguments are the corresponding optional formatting functions\n    that are called to turn names and values into strings.  The ninth\n    argument is an optional function to format the sequence of arguments.\"\"\"\n    specs = []\n    if defaults:\n        firstdefault = len(args) - len(defaults)\n    for i, arg in enumerate(args):\n        spec = strseq(arg, formatarg, join)\n        if defaults and i >= firstdefault:\n            spec = spec + formatvalue(defaults[i - firstdefault])\n        specs.append(spec)\n    if varargs is not None:\n        specs.append(formatvarargs(varargs))\n    if varkw is not None:\n        specs.append(formatvarkw(varkw))\n    return '(' + string.join(specs, ', ') + ')'\n\ndef formatargvalues(args, varargs, varkw, locals,\n                    formatarg=str,\n                    formatvarargs=lambda name: '*' + name,\n                    formatvarkw=lambda name: '**' + name,\n                    formatvalue=lambda value: '=' + repr(value),\n                    join=joinseq):\n    \"\"\"Format an argument spec from the 4 values returned by getargvalues.\n\n    The first four arguments are (args, varargs, varkw, locals).  The\n    next four arguments are the corresponding optional formatting functions\n    that are called to turn names and values into strings.  The ninth\n    argument is an optional function to format the sequence of arguments.\"\"\"\n    def convert(name, locals=locals,\n                formatarg=formatarg, formatvalue=formatvalue):\n        return formatarg(name) + formatvalue(locals[name])\n    specs = []\n    for i in range(len(args)):\n        specs.append(strseq(args[i], convert, join))\n    if varargs:\n        specs.append(formatvarargs(varargs) + formatvalue(locals[varargs]))\n    if varkw:\n        specs.append(formatvarkw(varkw) + formatvalue(locals[varkw]))\n    return '(' + string.join(specs, ', ') + ')'\n\ndef getcallargs(func, *positional, **named):\n    \"\"\"Get the mapping of arguments to values.\n\n    A dict is returned, with keys the function argument names (including the\n    names of the * and ** arguments, if any), and values the respective bound\n    values from 'positional' and 'named'.\"\"\"\n    args, varargs, varkw, defaults = getargspec(func)\n    f_name = func.__name__\n    arg2value = {}\n\n    # The following closures are basically because of tuple parameter unpacking.\n    assigned_tuple_params = []\n    def assign(arg, value):\n        if isinstance(arg, str):\n            arg2value[arg] = value\n        else:\n            assigned_tuple_params.append(arg)\n            value = iter(value)\n            for i, subarg in enumerate(arg):\n                try:\n                    subvalue = next(value)\n                except StopIteration:\n                    raise ValueError('need more than %d %s to unpack' %\n                                     (i, 'values' if i > 1 else 'value'))\n                assign(subarg,subvalue)\n            try:\n                next(value)\n            except StopIteration:\n                pass\n            else:\n                raise ValueError('too many values to unpack')\n    def is_assigned(arg):\n        if isinstance(arg,str):\n            return arg in arg2value\n        return arg in assigned_tuple_params\n    if ismethod(func) and func.im_self is not None:\n        # implicit 'self' (or 'cls' for classmethods) argument\n        positional = (func.im_self,) + positional\n    num_pos = len(positional)\n    num_total = num_pos + len(named)\n    num_args = len(args)\n    num_defaults = len(defaults) if defaults else 0\n    for arg, value in zip(args, positional):\n        assign(arg, value)\n    if varargs:\n        if num_pos > num_args:\n            assign(varargs, positional[-(num_pos-num_args):])\n        else:\n            assign(varargs, ())\n    elif 0 < num_args < num_pos:\n        raise TypeError('%s() takes %s %d %s (%d given)' % (\n            f_name, 'at most' if defaults else 'exactly', num_args,\n            'arguments' if num_args > 1 else 'argument', num_total))\n    elif num_args == 0 and num_total:\n        if varkw:\n            if num_pos:\n                # XXX: We should use num_pos, but Python also uses num_total:\n                raise TypeError('%s() takes exactly 0 arguments '\n                                '(%d given)' % (f_name, num_total))\n        else:\n            raise TypeError('%s() takes no arguments (%d given)' %\n                            (f_name, num_total))\n    for arg in args:\n        if isinstance(arg, str) and arg in named:\n            if is_assigned(arg):\n                raise TypeError(\"%s() got multiple values for keyword \"\n                                \"argument '%s'\" % (f_name, arg))\n            else:\n                assign(arg, named.pop(arg))\n    if defaults:    # fill in any missing values with the defaults\n        for arg, value in zip(args[-num_defaults:], defaults):\n            if not is_assigned(arg):\n                assign(arg, value)\n    if varkw:\n        assign(varkw, named)\n    elif named:\n        unexpected = next(iter(named))\n        if isinstance(unexpected, unicode):\n            unexpected = unexpected.encode(sys.getdefaultencoding(), 'replace')\n        raise TypeError(\"%s() got an unexpected keyword argument '%s'\" %\n                        (f_name, unexpected))\n    unassigned = num_args - len([arg for arg in args if is_assigned(arg)])\n    if unassigned:\n        num_required = num_args - num_defaults\n        raise TypeError('%s() takes %s %d %s (%d given)' % (\n            f_name, 'at least' if defaults else 'exactly', num_required,\n            'arguments' if num_required > 1 else 'argument', num_total))\n    return arg2value\n\n# -------------------------------------------------- stack frame extraction\n\nTraceback = namedtuple('Traceback', 'filename lineno function code_context index')\n\ndef getframeinfo(frame, context=1):\n    \"\"\"Get information about a frame or traceback object.\n\n    A tuple of five things is returned: the filename, the line number of\n    the current line, the function name, a list of lines of context from\n    the source code, and the index of the current line within that list.\n    The optional second argument specifies the number of lines of context\n    to return, which are centered around the current line.\"\"\"\n    if istraceback(frame):\n        lineno = frame.tb_lineno\n        frame = frame.tb_frame\n    else:\n        lineno = frame.f_lineno\n    if not isframe(frame):\n        raise TypeError('{!r} is not a frame or traceback object'.format(frame))\n\n    filename = getsourcefile(frame) or getfile(frame)\n    if context > 0:\n        start = lineno - 1 - context//2\n        try:\n            lines, lnum = findsource(frame)\n        except IOError:\n            lines = index = None\n        else:\n            start = max(start, 1)\n            start = max(0, min(start, len(lines) - context))\n            lines = lines[start:start+context]\n            index = lineno - 1 - start\n    else:\n        lines = index = None\n\n    return Traceback(filename, lineno, frame.f_code.co_name, lines, index)\n\ndef getlineno(frame):\n    \"\"\"Get the line number from a frame object, allowing for optimization.\"\"\"\n    # FrameType.f_lineno is now a descriptor that grovels co_lnotab\n    return frame.f_lineno\n\ndef getouterframes(frame, context=1):\n    \"\"\"Get a list of records for a frame and all higher (calling) frames.\n\n    Each record contains a frame object, filename, line number, function\n    name, a list of lines of context, and index within the context.\"\"\"\n    framelist = []\n    while frame:\n        framelist.append((frame,) + getframeinfo(frame, context))\n        frame = frame.f_back\n    return framelist\n\ndef getinnerframes(tb, context=1):\n    \"\"\"Get a list of records for a traceback's frame and all lower frames.\n\n    Each record contains a frame object, filename, line number, function\n    name, a list of lines of context, and index within the context.\"\"\"\n    framelist = []\n    while tb:\n        framelist.append((tb.tb_frame,) + getframeinfo(tb, context))\n        tb = tb.tb_next\n    return framelist\n\nif hasattr(sys, '_getframe'):\n    currentframe = sys._getframe\nelse:\n    currentframe = lambda _=None: None\n\ndef stack(context=1):\n    \"\"\"Return a list of records for the stack above the caller's frame.\"\"\"\n    return getouterframes(sys._getframe(1), context)\n\ndef trace(context=1):\n    \"\"\"Return a list of records for the stack below the current exception.\"\"\"\n    return getinnerframes(sys.exc_info()[2], context)\n", 
    "io": "\"\"\"The io module provides the Python interfaces to stream handling. The\nbuiltin open function is defined in this module.\n\nAt the top of the I/O hierarchy is the abstract base class IOBase. It\ndefines the basic interface to a stream. Note, however, that there is no\nseparation between reading and writing to streams; implementations are\nallowed to raise an IOError if they do not support a given operation.\n\nExtending IOBase is RawIOBase which deals simply with the reading and\nwriting of raw bytes to a stream. FileIO subclasses RawIOBase to provide\nan interface to OS files.\n\nBufferedIOBase deals with buffering on a raw byte stream (RawIOBase). Its\nsubclasses, BufferedWriter, BufferedReader, and BufferedRWPair buffer\nstreams that are readable, writable, and both respectively.\nBufferedRandom provides a buffered interface to random access\nstreams. BytesIO is a simple stream of in-memory bytes.\n\nAnother IOBase subclass, TextIOBase, deals with the encoding and decoding\nof streams into text. TextIOWrapper, which extends it, is a buffered text\ninterface to a buffered raw stream (`BufferedIOBase`). Finally, StringIO\nis a in-memory stream for text.\n\nArgument names are not part of the specification, and only the arguments\nof open() are intended to be used as keyword arguments.\n\ndata:\n\nDEFAULT_BUFFER_SIZE\n\n   An int containing the default buffer size used by the module's buffered\n   I/O classes. open() uses the file's blksize (as obtained by os.stat) if\n   possible.\n\"\"\"\n# New I/O library conforming to PEP 3116.\n\n__author__ = (\"Guido van Rossum <guido@python.org>, \"\n              \"Mike Verdone <mike.verdone@gmail.com>, \"\n              \"Mark Russell <mark.russell@zen.co.uk>, \"\n              \"Antoine Pitrou <solipsis@pitrou.net>, \"\n              \"Amaury Forgeot d'Arc <amauryfa@gmail.com>, \"\n              \"Benjamin Peterson <benjamin@python.org>\")\n\n__all__ = [\"BlockingIOError\", \"open\", \"IOBase\", \"RawIOBase\", \"FileIO\",\n           \"BytesIO\", \"StringIO\", \"BufferedIOBase\",\n           \"BufferedReader\", \"BufferedWriter\", \"BufferedRWPair\",\n           \"BufferedRandom\", \"TextIOBase\", \"TextIOWrapper\",\n           \"UnsupportedOperation\", \"SEEK_SET\", \"SEEK_CUR\", \"SEEK_END\"]\n\n\nimport _io\nimport abc\n\nfrom _io import (DEFAULT_BUFFER_SIZE, BlockingIOError, UnsupportedOperation,\n                 open, FileIO, BytesIO, StringIO, BufferedReader,\n                 BufferedWriter, BufferedRWPair, BufferedRandom,\n                 IncrementalNewlineDecoder, TextIOWrapper)\n\nOpenWrapper = _io.open # for compatibility with _pyio\n\n# for seek()\nSEEK_SET = 0\nSEEK_CUR = 1\nSEEK_END = 2\n\n# Declaring ABCs in C is tricky so we do it here.\n# Method descriptions and default implementations are inherited from the C\n# version however.\nclass IOBase(_io._IOBase):\n    __metaclass__ = abc.ABCMeta\n    __doc__ = _io._IOBase.__doc__\n\nclass RawIOBase(_io._RawIOBase, IOBase):\n    __doc__ = _io._RawIOBase.__doc__\n\nclass BufferedIOBase(_io._BufferedIOBase, IOBase):\n    __doc__ = _io._BufferedIOBase.__doc__\n\nclass TextIOBase(_io._TextIOBase, IOBase):\n    __doc__ = _io._TextIOBase.__doc__\n\nRawIOBase.register(FileIO)\n\nfor klass in (BytesIO, BufferedReader, BufferedWriter, BufferedRandom,\n              BufferedRWPair):\n    BufferedIOBase.register(klass)\n\nfor klass in (StringIO, TextIOWrapper):\n    TextIOBase.register(klass)\ndel klass\n", 
    "keyword": "#! /usr/bin/env python\n\n\"\"\"Keywords (from \"graminit.c\")\n\nThis file is automatically generated; please don't muck it up!\n\nTo update the symbols in this file, 'cd' to the top directory of\nthe python source tree after building the interpreter and run:\n\n    ./python Lib/keyword.py\n\"\"\"\n\n__all__ = [\"iskeyword\", \"kwlist\"]\n\nkwlist = [\n#--start keywords--\n        'and',\n        'as',\n        'assert',\n        'break',\n        'class',\n        'continue',\n        'def',\n        'del',\n        'elif',\n        'else',\n        'except',\n        'exec',\n        'finally',\n        'for',\n        'from',\n        'global',\n        'if',\n        'import',\n        'in',\n        'is',\n        'lambda',\n        'not',\n        'or',\n        'pass',\n        'print',\n        'raise',\n        'return',\n        'try',\n        'while',\n        'with',\n        'yield',\n#--end keywords--\n        ]\n\niskeyword = frozenset(kwlist).__contains__\n\ndef main():\n    import sys, re\n\n    args = sys.argv[1:]\n    iptfile = args and args[0] or \"Python/graminit.c\"\n    if len(args) > 1: optfile = args[1]\n    else: optfile = \"Lib/keyword.py\"\n\n    # scan the source file for keywords\n    fp = open(iptfile)\n    strprog = re.compile('\"([^\"]+)\"')\n    lines = []\n    for line in fp:\n        if '{1, \"' in line:\n            match = strprog.search(line)\n            if match:\n                lines.append(\"        '\" + match.group(1) + \"',\\n\")\n    fp.close()\n    lines.sort()\n\n    # load the output skeleton from the target\n    fp = open(optfile)\n    format = fp.readlines()\n    fp.close()\n\n    # insert the lines of keywords\n    try:\n        start = format.index(\"#--start keywords--\\n\") + 1\n        end = format.index(\"#--end keywords--\\n\")\n        format[start:end] = lines\n    except ValueError:\n        sys.stderr.write(\"target does not contain format markers\\n\")\n        sys.exit(1)\n\n    # write the output file\n    fp = open(optfile, 'w')\n    fp.write(''.join(format))\n    fp.close()\n\nif __name__ == \"__main__\":\n    main()\n", 
    "linecache": "\"\"\"Cache lines from files.\n\nThis is intended to read lines from modules imported -- hence if a filename\nis not found, it will look down the module search path for a file by\nthat name.\n\"\"\"\n\nimport sys\nimport os\n\n__all__ = [\"getline\", \"clearcache\", \"checkcache\"]\n\ndef getline(filename, lineno, module_globals=None):\n    lines = getlines(filename, module_globals)\n    if 1 <= lineno <= len(lines):\n        return lines[lineno-1]\n    else:\n        return ''\n\n\n# The cache\n\ncache = {} # The cache\n\n\ndef clearcache():\n    \"\"\"Clear the cache entirely.\"\"\"\n\n    global cache\n    cache = {}\n\n\ndef getlines(filename, module_globals=None):\n    \"\"\"Get the lines for a file from the cache.\n    Update the cache if it doesn't contain an entry for this file already.\"\"\"\n\n    if filename in cache:\n        return cache[filename][2]\n    else:\n        return updatecache(filename, module_globals)\n\n\ndef checkcache(filename=None):\n    \"\"\"Discard cache entries that are out of date.\n    (This is not checked upon each call!)\"\"\"\n\n    if filename is None:\n        filenames = cache.keys()\n    else:\n        if filename in cache:\n            filenames = [filename]\n        else:\n            return\n\n    for filename in filenames:\n        size, mtime, lines, fullname = cache[filename]\n        if mtime is None:\n            continue   # no-op for files loaded via a __loader__\n        try:\n            stat = os.stat(fullname)\n        except os.error:\n            del cache[filename]\n            continue\n        if size != stat.st_size or mtime != stat.st_mtime:\n            del cache[filename]\n\n\ndef updatecache(filename, module_globals=None):\n    \"\"\"Update a cache entry and return its list of lines.\n    If something's wrong, print a message, discard the cache entry,\n    and return an empty list.\"\"\"\n\n    if filename in cache:\n        del cache[filename]\n    if not filename or (filename.startswith('<') and filename.endswith('>')):\n        return []\n\n    fullname = filename\n    try:\n        stat = os.stat(fullname)\n    except OSError:\n        basename = filename\n\n        # Try for a __loader__, if available\n        if module_globals and '__loader__' in module_globals:\n            name = module_globals.get('__name__')\n            loader = module_globals['__loader__']\n            get_source = getattr(loader, 'get_source', None)\n\n            if name and get_source:\n                try:\n                    data = get_source(name)\n                except (ImportError, IOError):\n                    pass\n                else:\n                    if data is None:\n                        # No luck, the PEP302 loader cannot find the source\n                        # for this module.\n                        return []\n                    cache[filename] = (\n                        len(data), None,\n                        [line+'\\n' for line in data.splitlines()], fullname\n                    )\n                    return cache[filename][2]\n\n        # Try looking through the module search path, which is only useful\n        # when handling a relative filename.\n        if os.path.isabs(filename):\n            return []\n\n        for dirname in sys.path:\n            # When using imputil, sys.path may contain things other than\n            # strings; ignore them when it happens.\n            try:\n                fullname = os.path.join(dirname, basename)\n            except (TypeError, AttributeError):\n                # Not sufficiently string-like to do anything useful with.\n                continue\n            try:\n                stat = os.stat(fullname)\n                break\n            except os.error:\n                pass\n        else:\n            return []\n    try:\n        with open(fullname, 'rU') as fp:\n            lines = fp.readlines()\n    except IOError:\n        return []\n    if lines and not lines[-1].endswith('\\n'):\n        lines[-1] += '\\n'\n    size, mtime = stat.st_size, stat.st_mtime\n    cache[filename] = size, mtime, lines, fullname\n    return lines\n", 
    "locale": "\"\"\" Locale support.\n\n    The module provides low-level access to the C lib's locale APIs\n    and adds high level number formatting APIs as well as a locale\n    aliasing engine to complement these.\n\n    The aliasing engine includes support for many commonly used locale\n    names and maps them to values suitable for passing to the C lib's\n    setlocale() function. It also includes default encodings for all\n    supported locale names.\n\n\"\"\"\n\nimport sys\nimport encodings\nimport encodings.aliases\nimport re\nimport operator\nimport functools\n\ntry:\n    _unicode = unicode\nexcept NameError:\n    # If Python is built without Unicode support, the unicode type\n    # will not exist. Fake one.\n    class _unicode(object):\n        pass\n\n# Try importing the _locale module.\n#\n# If this fails, fall back on a basic 'C' locale emulation.\n\n# Yuck:  LC_MESSAGES is non-standard:  can't tell whether it exists before\n# trying the import.  So __all__ is also fiddled at the end of the file.\n__all__ = [\"getlocale\", \"getdefaultlocale\", \"getpreferredencoding\", \"Error\",\n           \"setlocale\", \"resetlocale\", \"localeconv\", \"strcoll\", \"strxfrm\",\n           \"str\", \"atof\", \"atoi\", \"format\", \"format_string\", \"currency\",\n           \"normalize\", \"LC_CTYPE\", \"LC_COLLATE\", \"LC_TIME\", \"LC_MONETARY\",\n           \"LC_NUMERIC\", \"LC_ALL\", \"CHAR_MAX\"]\n\ntry:\n\n    from _locale import *\n\nexcept ImportError:\n\n    # Locale emulation\n\n    CHAR_MAX = 127\n    LC_ALL = 6\n    LC_COLLATE = 3\n    LC_CTYPE = 0\n    LC_MESSAGES = 5\n    LC_MONETARY = 4\n    LC_NUMERIC = 1\n    LC_TIME = 2\n    Error = ValueError\n\n    def localeconv():\n        \"\"\" localeconv() -> dict.\n            Returns numeric and monetary locale-specific parameters.\n        \"\"\"\n        # 'C' locale default values\n        return {'grouping': [127],\n                'currency_symbol': '',\n                'n_sign_posn': 127,\n                'p_cs_precedes': 127,\n                'n_cs_precedes': 127,\n                'mon_grouping': [],\n                'n_sep_by_space': 127,\n                'decimal_point': '.',\n                'negative_sign': '',\n                'positive_sign': '',\n                'p_sep_by_space': 127,\n                'int_curr_symbol': '',\n                'p_sign_posn': 127,\n                'thousands_sep': '',\n                'mon_thousands_sep': '',\n                'frac_digits': 127,\n                'mon_decimal_point': '',\n                'int_frac_digits': 127}\n\n    def setlocale(category, value=None):\n        \"\"\" setlocale(integer,string=None) -> string.\n            Activates/queries locale processing.\n        \"\"\"\n        if value not in (None, '', 'C'):\n            raise Error, '_locale emulation only supports \"C\" locale'\n        return 'C'\n\n    def strcoll(a,b):\n        \"\"\" strcoll(string,string) -> int.\n            Compares two strings according to the locale.\n        \"\"\"\n        return cmp(a,b)\n\n    def strxfrm(s):\n        \"\"\" strxfrm(string) -> string.\n            Returns a string that behaves for cmp locale-aware.\n        \"\"\"\n        return s\n\n\n_localeconv = localeconv\n\n# With this dict, you can override some items of localeconv's return value.\n# This is useful for testing purposes.\n_override_localeconv = {}\n\n@functools.wraps(_localeconv)\ndef localeconv():\n    d = _localeconv()\n    if _override_localeconv:\n        d.update(_override_localeconv)\n    return d\n\n\n### Number formatting APIs\n\n# Author: Martin von Loewis\n# improved by Georg Brandl\n\n# Iterate over grouping intervals\ndef _grouping_intervals(grouping):\n    last_interval = None\n    for interval in grouping:\n        # if grouping is -1, we are done\n        if interval == CHAR_MAX:\n            return\n        # 0: re-use last group ad infinitum\n        if interval == 0:\n            if last_interval is None:\n                raise ValueError(\"invalid grouping\")\n            while True:\n                yield last_interval\n        yield interval\n        last_interval = interval\n\n#perform the grouping from right to left\ndef _group(s, monetary=False):\n    conv = localeconv()\n    thousands_sep = conv[monetary and 'mon_thousands_sep' or 'thousands_sep']\n    grouping = conv[monetary and 'mon_grouping' or 'grouping']\n    if not grouping:\n        return (s, 0)\n    if s[-1] == ' ':\n        stripped = s.rstrip()\n        right_spaces = s[len(stripped):]\n        s = stripped\n    else:\n        right_spaces = ''\n    left_spaces = ''\n    groups = []\n    for interval in _grouping_intervals(grouping):\n        if not s or s[-1] not in \"0123456789\":\n            # only non-digit characters remain (sign, spaces)\n            left_spaces = s\n            s = ''\n            break\n        groups.append(s[-interval:])\n        s = s[:-interval]\n    if s:\n        groups.append(s)\n    groups.reverse()\n    return (\n        left_spaces + thousands_sep.join(groups) + right_spaces,\n        len(thousands_sep) * (len(groups) - 1)\n    )\n\n# Strip a given amount of excess padding from the given string\ndef _strip_padding(s, amount):\n    lpos = 0\n    while amount and s[lpos] == ' ':\n        lpos += 1\n        amount -= 1\n    rpos = len(s) - 1\n    while amount and s[rpos] == ' ':\n        rpos -= 1\n        amount -= 1\n    return s[lpos:rpos+1]\n\n_percent_re = re.compile(r'%(?:\\((?P<key>.*?)\\))?'\n                         r'(?P<modifiers>[-#0-9 +*.hlL]*?)[eEfFgGdiouxXcrs%]')\n\ndef format(percent, value, grouping=False, monetary=False, *additional):\n    \"\"\"Returns the locale-aware substitution of a %? specifier\n    (percent).\n\n    additional is for format strings which contain one or more\n    '*' modifiers.\"\"\"\n    # this is only for one-percent-specifier strings and this should be checked\n    match = _percent_re.match(percent)\n    if not match or len(match.group())!= len(percent):\n        raise ValueError((\"format() must be given exactly one %%char \"\n                         \"format specifier, %s not valid\") % repr(percent))\n    return _format(percent, value, grouping, monetary, *additional)\n\ndef _format(percent, value, grouping=False, monetary=False, *additional):\n    if additional:\n        formatted = percent % ((value,) + additional)\n    else:\n        formatted = percent % value\n    # floats and decimal ints need special action!\n    if percent[-1] in 'eEfFgG':\n        seps = 0\n        parts = formatted.split('.')\n        if grouping:\n            parts[0], seps = _group(parts[0], monetary=monetary)\n        decimal_point = localeconv()[monetary and 'mon_decimal_point'\n                                              or 'decimal_point']\n        formatted = decimal_point.join(parts)\n        if seps:\n            formatted = _strip_padding(formatted, seps)\n    elif percent[-1] in 'diu':\n        seps = 0\n        if grouping:\n            formatted, seps = _group(formatted, monetary=monetary)\n        if seps:\n            formatted = _strip_padding(formatted, seps)\n    return formatted\n\ndef format_string(f, val, grouping=False):\n    \"\"\"Formats a string in the same way that the % formatting would use,\n    but takes the current locale into account.\n    Grouping is applied if the third parameter is true.\"\"\"\n    percents = list(_percent_re.finditer(f))\n    new_f = _percent_re.sub('%s', f)\n\n    if operator.isMappingType(val):\n        new_val = []\n        for perc in percents:\n            if perc.group()[-1]=='%':\n                new_val.append('%')\n            else:\n                new_val.append(format(perc.group(), val, grouping))\n    else:\n        if not isinstance(val, tuple):\n            val = (val,)\n        new_val = []\n        i = 0\n        for perc in percents:\n            if perc.group()[-1]=='%':\n                new_val.append('%')\n            else:\n                starcount = perc.group('modifiers').count('*')\n                new_val.append(_format(perc.group(),\n                                      val[i],\n                                      grouping,\n                                      False,\n                                      *val[i+1:i+1+starcount]))\n                i += (1 + starcount)\n    val = tuple(new_val)\n\n    return new_f % val\n\ndef currency(val, symbol=True, grouping=False, international=False):\n    \"\"\"Formats val according to the currency settings\n    in the current locale.\"\"\"\n    conv = localeconv()\n\n    # check for illegal values\n    digits = conv[international and 'int_frac_digits' or 'frac_digits']\n    if digits == 127:\n        raise ValueError(\"Currency formatting is not possible using \"\n                         \"the 'C' locale.\")\n\n    s = format('%%.%if' % digits, abs(val), grouping, monetary=True)\n    # '<' and '>' are markers if the sign must be inserted between symbol and value\n    s = '<' + s + '>'\n\n    if symbol:\n        smb = conv[international and 'int_curr_symbol' or 'currency_symbol']\n        precedes = conv[val<0 and 'n_cs_precedes' or 'p_cs_precedes']\n        separated = conv[val<0 and 'n_sep_by_space' or 'p_sep_by_space']\n\n        if precedes:\n            s = smb + (separated and ' ' or '') + s\n        else:\n            s = s + (separated and ' ' or '') + smb\n\n    sign_pos = conv[val<0 and 'n_sign_posn' or 'p_sign_posn']\n    sign = conv[val<0 and 'negative_sign' or 'positive_sign']\n\n    if sign_pos == 0:\n        s = '(' + s + ')'\n    elif sign_pos == 1:\n        s = sign + s\n    elif sign_pos == 2:\n        s = s + sign\n    elif sign_pos == 3:\n        s = s.replace('<', sign)\n    elif sign_pos == 4:\n        s = s.replace('>', sign)\n    else:\n        # the default if nothing specified;\n        # this should be the most fitting sign position\n        s = sign + s\n\n    return s.replace('<', '').replace('>', '')\n\ndef str(val):\n    \"\"\"Convert float to integer, taking the locale into account.\"\"\"\n    return format(\"%.12g\", val)\n\ndef atof(string, func=float):\n    \"Parses a string as a float according to the locale settings.\"\n    #First, get rid of the grouping\n    ts = localeconv()['thousands_sep']\n    if ts:\n        string = string.replace(ts, '')\n    #next, replace the decimal point with a dot\n    dd = localeconv()['decimal_point']\n    if dd:\n        string = string.replace(dd, '.')\n    #finally, parse the string\n    return func(string)\n\ndef atoi(str):\n    \"Converts a string to an integer according to the locale settings.\"\n    return atof(str, int)\n\ndef _test():\n    setlocale(LC_ALL, \"\")\n    #do grouping\n    s1 = format(\"%d\", 123456789,1)\n    print s1, \"is\", atoi(s1)\n    #standard formatting\n    s1 = str(3.14)\n    print s1, \"is\", atof(s1)\n\n### Locale name aliasing engine\n\n# Author: Marc-Andre Lemburg, mal@lemburg.com\n# Various tweaks by Fredrik Lundh <fredrik@pythonware.com>\n\n# store away the low-level version of setlocale (it's\n# overridden below)\n_setlocale = setlocale\n\n# Avoid relying on the locale-dependent .lower() method\n# (see issue #1813).\n_ascii_lower_map = ''.join(\n    chr(x + 32 if x >= ord('A') and x <= ord('Z') else x)\n    for x in range(256)\n)\n\ndef _replace_encoding(code, encoding):\n    if '.' in code:\n        langname = code[:code.index('.')]\n    else:\n        langname = code\n    # Convert the encoding to a C lib compatible encoding string\n    norm_encoding = encodings.normalize_encoding(encoding)\n    #print('norm encoding: %r' % norm_encoding)\n    norm_encoding = encodings.aliases.aliases.get(norm_encoding,\n                                                  norm_encoding)\n    #print('aliased encoding: %r' % norm_encoding)\n    encoding = locale_encoding_alias.get(norm_encoding,\n                                         norm_encoding)\n    #print('found encoding %r' % encoding)\n    return langname + '.' + encoding\n\ndef normalize(localename):\n\n    \"\"\" Returns a normalized locale code for the given locale\n        name.\n\n        The returned locale code is formatted for use with\n        setlocale().\n\n        If normalization fails, the original name is returned\n        unchanged.\n\n        If the given encoding is not known, the function defaults to\n        the default encoding for the locale code just like setlocale()\n        does.\n\n    \"\"\"\n    # Normalize the locale name and extract the encoding and modifier\n    if isinstance(localename, _unicode):\n        localename = localename.encode('ascii')\n    code = localename.translate(_ascii_lower_map)\n    if ':' in code:\n        # ':' is sometimes used as encoding delimiter.\n        code = code.replace(':', '.')\n    if '@' in code:\n        code, modifier = code.split('@', 1)\n    else:\n        modifier = ''\n    if '.' in code:\n        langname, encoding = code.split('.')[:2]\n    else:\n        langname = code\n        encoding = ''\n\n    # First lookup: fullname (possibly with encoding and modifier)\n    lang_enc = langname\n    if encoding:\n        norm_encoding = encoding.replace('-', '')\n        norm_encoding = norm_encoding.replace('_', '')\n        lang_enc += '.' + norm_encoding\n    lookup_name = lang_enc\n    if modifier:\n        lookup_name += '@' + modifier\n    code = locale_alias.get(lookup_name, None)\n    if code is not None:\n        return code\n    #print('first lookup failed')\n\n    if modifier:\n        # Second try: fullname without modifier (possibly with encoding)\n        code = locale_alias.get(lang_enc, None)\n        if code is not None:\n            #print('lookup without modifier succeeded')\n            if '@' not in code:\n                return code + '@' + modifier\n            if code.split('@', 1)[1].translate(_ascii_lower_map) == modifier:\n                return code\n        #print('second lookup failed')\n\n    if encoding:\n        # Third try: langname (without encoding, possibly with modifier)\n        lookup_name = langname\n        if modifier:\n            lookup_name += '@' + modifier\n        code = locale_alias.get(lookup_name, None)\n        if code is not None:\n            #print('lookup without encoding succeeded')\n            if '@' not in code:\n                return _replace_encoding(code, encoding)\n            code, modifier = code.split('@', 1)\n            return _replace_encoding(code, encoding) + '@' + modifier\n\n        if modifier:\n            # Fourth try: langname (without encoding and modifier)\n            code = locale_alias.get(langname, None)\n            if code is not None:\n                #print('lookup without modifier and encoding succeeded')\n                if '@' not in code:\n                    return _replace_encoding(code, encoding) + '@' + modifier\n                code, defmod = code.split('@', 1)\n                if defmod.translate(_ascii_lower_map) == modifier:\n                    return _replace_encoding(code, encoding) + '@' + defmod\n\n    return localename\n\ndef _parse_localename(localename):\n\n    \"\"\" Parses the locale code for localename and returns the\n        result as tuple (language code, encoding).\n\n        The localename is normalized and passed through the locale\n        alias engine. A ValueError is raised in case the locale name\n        cannot be parsed.\n\n        The language code corresponds to RFC 1766.  code and encoding\n        can be None in case the values cannot be determined or are\n        unknown to this implementation.\n\n    \"\"\"\n    code = normalize(localename)\n    if '@' in code:\n        # Deal with locale modifiers\n        code, modifier = code.split('@', 1)\n        if modifier == 'euro' and '.' not in code:\n            # Assume Latin-9 for @euro locales. This is bogus,\n            # since some systems may use other encodings for these\n            # locales. Also, we ignore other modifiers.\n            return code, 'iso-8859-15'\n\n    if '.' in code:\n        return tuple(code.split('.')[:2])\n    elif code == 'C':\n        return None, None\n    raise ValueError, 'unknown locale: %s' % localename\n\ndef _build_localename(localetuple):\n\n    \"\"\" Builds a locale code from the given tuple (language code,\n        encoding).\n\n        No aliasing or normalizing takes place.\n\n    \"\"\"\n    language, encoding = localetuple\n    if language is None:\n        language = 'C'\n    if encoding is None:\n        return language\n    else:\n        return language + '.' + encoding\n\ndef getdefaultlocale(envvars=('LC_ALL', 'LC_CTYPE', 'LANG', 'LANGUAGE')):\n\n    \"\"\" Tries to determine the default locale settings and returns\n        them as tuple (language code, encoding).\n\n        According to POSIX, a program which has not called\n        setlocale(LC_ALL, \"\") runs using the portable 'C' locale.\n        Calling setlocale(LC_ALL, \"\") lets it use the default locale as\n        defined by the LANG variable. Since we don't want to interfere\n        with the current locale setting we thus emulate the behavior\n        in the way described above.\n\n        To maintain compatibility with other platforms, not only the\n        LANG variable is tested, but a list of variables given as\n        envvars parameter. The first found to be defined will be\n        used. envvars defaults to the search path used in GNU gettext;\n        it must always contain the variable name 'LANG'.\n\n        Except for the code 'C', the language code corresponds to RFC\n        1766.  code and encoding can be None in case the values cannot\n        be determined.\n\n    \"\"\"\n\n    try:\n        # check if it's supported by the _locale module\n        import _locale\n        code, encoding = _locale._getdefaultlocale()\n    except (ImportError, AttributeError):\n        pass\n    else:\n        # make sure the code/encoding values are valid\n        if sys.platform == \"win32\" and code and code[:2] == \"0x\":\n            # map windows language identifier to language name\n            code = windows_locale.get(int(code, 0))\n        # ...add other platform-specific processing here, if\n        # necessary...\n        return code, encoding\n\n    # fall back on POSIX behaviour\n    import os\n    lookup = os.environ.get\n    for variable in envvars:\n        localename = lookup(variable,None)\n        if localename:\n            if variable == 'LANGUAGE':\n                localename = localename.split(':')[0]\n            break\n    else:\n        localename = 'C'\n    return _parse_localename(localename)\n\n\ndef getlocale(category=LC_CTYPE):\n\n    \"\"\" Returns the current setting for the given locale category as\n        tuple (language code, encoding).\n\n        category may be one of the LC_* value except LC_ALL. It\n        defaults to LC_CTYPE.\n\n        Except for the code 'C', the language code corresponds to RFC\n        1766.  code and encoding can be None in case the values cannot\n        be determined.\n\n    \"\"\"\n    localename = _setlocale(category)\n    if category == LC_ALL and ';' in localename:\n        raise TypeError, 'category LC_ALL is not supported'\n    return _parse_localename(localename)\n\ndef setlocale(category, locale=None):\n\n    \"\"\" Set the locale for the given category.  The locale can be\n        a string, an iterable of two strings (language code and encoding),\n        or None.\n\n        Iterables are converted to strings using the locale aliasing\n        engine.  Locale strings are passed directly to the C lib.\n\n        category may be given as one of the LC_* values.\n\n    \"\"\"\n    if locale and type(locale) is not type(\"\"):\n        # convert to string\n        locale = normalize(_build_localename(locale))\n    return _setlocale(category, locale)\n\ndef resetlocale(category=LC_ALL):\n\n    \"\"\" Sets the locale for category to the default setting.\n\n        The default setting is determined by calling\n        getdefaultlocale(). category defaults to LC_ALL.\n\n    \"\"\"\n    _setlocale(category, _build_localename(getdefaultlocale()))\n\nif sys.platform.startswith(\"win\"):\n    # On Win32, this will return the ANSI code page\n    def getpreferredencoding(do_setlocale = True):\n        \"\"\"Return the charset that the user is likely using.\"\"\"\n        import _locale\n        return _locale._getdefaultlocale()[1]\nelse:\n    # On Unix, if CODESET is available, use that.\n    try:\n        CODESET\n    except NameError:\n        # Fall back to parsing environment variables :-(\n        def getpreferredencoding(do_setlocale = True):\n            \"\"\"Return the charset that the user is likely using,\n            by looking at environment variables.\"\"\"\n            return getdefaultlocale()[1]\n    else:\n        def getpreferredencoding(do_setlocale = True):\n            \"\"\"Return the charset that the user is likely using,\n            according to the system configuration.\"\"\"\n            if do_setlocale:\n                oldloc = setlocale(LC_CTYPE)\n                try:\n                    setlocale(LC_CTYPE, \"\")\n                except Error:\n                    pass\n                result = nl_langinfo(CODESET)\n                setlocale(LC_CTYPE, oldloc)\n                return result\n            else:\n                return nl_langinfo(CODESET)\n\n\n### Database\n#\n# The following data was extracted from the locale.alias file which\n# comes with X11 and then hand edited removing the explicit encoding\n# definitions and adding some more aliases. The file is usually\n# available as /usr/lib/X11/locale/locale.alias.\n#\n\n#\n# The local_encoding_alias table maps lowercase encoding alias names\n# to C locale encoding names (case-sensitive). Note that normalize()\n# first looks up the encoding in the encodings.aliases dictionary and\n# then applies this mapping to find the correct C lib name for the\n# encoding.\n#\nlocale_encoding_alias = {\n\n    # Mappings for non-standard encoding names used in locale names\n    '437':                          'C',\n    'c':                            'C',\n    'en':                           'ISO8859-1',\n    'jis':                          'JIS7',\n    'jis7':                         'JIS7',\n    'ajec':                         'eucJP',\n\n    # Mappings from Python codec names to C lib encoding names\n    'ascii':                        'ISO8859-1',\n    'latin_1':                      'ISO8859-1',\n    'iso8859_1':                    'ISO8859-1',\n    'iso8859_10':                   'ISO8859-10',\n    'iso8859_11':                   'ISO8859-11',\n    'iso8859_13':                   'ISO8859-13',\n    'iso8859_14':                   'ISO8859-14',\n    'iso8859_15':                   'ISO8859-15',\n    'iso8859_16':                   'ISO8859-16',\n    'iso8859_2':                    'ISO8859-2',\n    'iso8859_3':                    'ISO8859-3',\n    'iso8859_4':                    'ISO8859-4',\n    'iso8859_5':                    'ISO8859-5',\n    'iso8859_6':                    'ISO8859-6',\n    'iso8859_7':                    'ISO8859-7',\n    'iso8859_8':                    'ISO8859-8',\n    'iso8859_9':                    'ISO8859-9',\n    'iso2022_jp':                   'JIS7',\n    'shift_jis':                    'SJIS',\n    'tactis':                       'TACTIS',\n    'euc_jp':                       'eucJP',\n    'euc_kr':                       'eucKR',\n    'utf_8':                        'UTF-8',\n    'koi8_r':                       'KOI8-R',\n    'koi8_u':                       'KOI8-U',\n    # XXX This list is still incomplete. If you know more\n    # mappings, please file a bug report. Thanks.\n}\n\n#\n# The locale_alias table maps lowercase alias names to C locale names\n# (case-sensitive). Encodings are always separated from the locale\n# name using a dot ('.'); they should only be given in case the\n# language name is needed to interpret the given encoding alias\n# correctly (CJK codes often have this need).\n#\n# Note that the normalize() function which uses this tables\n# removes '_' and '-' characters from the encoding part of the\n# locale name before doing the lookup. This saves a lot of\n# space in the table.\n#\n# MAL 2004-12-10:\n# Updated alias mapping to most recent locale.alias file\n# from X.org distribution using makelocalealias.py.\n#\n# These are the differences compared to the old mapping (Python 2.4\n# and older):\n#\n#    updated 'bg' -> 'bg_BG.ISO8859-5' to 'bg_BG.CP1251'\n#    updated 'bg_bg' -> 'bg_BG.ISO8859-5' to 'bg_BG.CP1251'\n#    updated 'bulgarian' -> 'bg_BG.ISO8859-5' to 'bg_BG.CP1251'\n#    updated 'cz' -> 'cz_CZ.ISO8859-2' to 'cs_CZ.ISO8859-2'\n#    updated 'cz_cz' -> 'cz_CZ.ISO8859-2' to 'cs_CZ.ISO8859-2'\n#    updated 'czech' -> 'cs_CS.ISO8859-2' to 'cs_CZ.ISO8859-2'\n#    updated 'dutch' -> 'nl_BE.ISO8859-1' to 'nl_NL.ISO8859-1'\n#    updated 'et' -> 'et_EE.ISO8859-4' to 'et_EE.ISO8859-15'\n#    updated 'et_ee' -> 'et_EE.ISO8859-4' to 'et_EE.ISO8859-15'\n#    updated 'fi' -> 'fi_FI.ISO8859-1' to 'fi_FI.ISO8859-15'\n#    updated 'fi_fi' -> 'fi_FI.ISO8859-1' to 'fi_FI.ISO8859-15'\n#    updated 'iw' -> 'iw_IL.ISO8859-8' to 'he_IL.ISO8859-8'\n#    updated 'iw_il' -> 'iw_IL.ISO8859-8' to 'he_IL.ISO8859-8'\n#    updated 'japanese' -> 'ja_JP.SJIS' to 'ja_JP.eucJP'\n#    updated 'lt' -> 'lt_LT.ISO8859-4' to 'lt_LT.ISO8859-13'\n#    updated 'lv' -> 'lv_LV.ISO8859-4' to 'lv_LV.ISO8859-13'\n#    updated 'sl' -> 'sl_CS.ISO8859-2' to 'sl_SI.ISO8859-2'\n#    updated 'slovene' -> 'sl_CS.ISO8859-2' to 'sl_SI.ISO8859-2'\n#    updated 'th_th' -> 'th_TH.TACTIS' to 'th_TH.ISO8859-11'\n#    updated 'zh_cn' -> 'zh_CN.eucCN' to 'zh_CN.gb2312'\n#    updated 'zh_cn.big5' -> 'zh_TW.eucTW' to 'zh_TW.big5'\n#    updated 'zh_tw' -> 'zh_TW.eucTW' to 'zh_TW.big5'\n#\n# MAL 2008-05-30:\n# Updated alias mapping to most recent locale.alias file\n# from X.org distribution using makelocalealias.py.\n#\n# These are the differences compared to the old mapping (Python 2.5\n# and older):\n#\n#    updated 'cs_cs.iso88592' -> 'cs_CZ.ISO8859-2' to 'cs_CS.ISO8859-2'\n#    updated 'serbocroatian' -> 'sh_YU.ISO8859-2' to 'sr_CS.ISO8859-2'\n#    updated 'sh' -> 'sh_YU.ISO8859-2' to 'sr_CS.ISO8859-2'\n#    updated 'sh_hr.iso88592' -> 'sh_HR.ISO8859-2' to 'hr_HR.ISO8859-2'\n#    updated 'sh_sp' -> 'sh_YU.ISO8859-2' to 'sr_CS.ISO8859-2'\n#    updated 'sh_yu' -> 'sh_YU.ISO8859-2' to 'sr_CS.ISO8859-2'\n#    updated 'sp' -> 'sp_YU.ISO8859-5' to 'sr_CS.ISO8859-5'\n#    updated 'sp_yu' -> 'sp_YU.ISO8859-5' to 'sr_CS.ISO8859-5'\n#    updated 'sr' -> 'sr_YU.ISO8859-5' to 'sr_CS.ISO8859-5'\n#    updated 'sr@cyrillic' -> 'sr_YU.ISO8859-5' to 'sr_CS.ISO8859-5'\n#    updated 'sr_sp' -> 'sr_SP.ISO8859-2' to 'sr_CS.ISO8859-2'\n#    updated 'sr_yu' -> 'sr_YU.ISO8859-5' to 'sr_CS.ISO8859-5'\n#    updated 'sr_yu.cp1251@cyrillic' -> 'sr_YU.CP1251' to 'sr_CS.CP1251'\n#    updated 'sr_yu.iso88592' -> 'sr_YU.ISO8859-2' to 'sr_CS.ISO8859-2'\n#    updated 'sr_yu.iso88595' -> 'sr_YU.ISO8859-5' to 'sr_CS.ISO8859-5'\n#    updated 'sr_yu.iso88595@cyrillic' -> 'sr_YU.ISO8859-5' to 'sr_CS.ISO8859-5'\n#    updated 'sr_yu.microsoftcp1251@cyrillic' -> 'sr_YU.CP1251' to 'sr_CS.CP1251'\n#    updated 'sr_yu.utf8@cyrillic' -> 'sr_YU.UTF-8' to 'sr_CS.UTF-8'\n#    updated 'sr_yu@cyrillic' -> 'sr_YU.ISO8859-5' to 'sr_CS.ISO8859-5'\n#\n# AP 2010-04-12:\n# Updated alias mapping to most recent locale.alias file\n# from X.org distribution using makelocalealias.py.\n#\n# These are the differences compared to the old mapping (Python 2.6.5\n# and older):\n#\n#    updated 'ru' -> 'ru_RU.ISO8859-5' to 'ru_RU.UTF-8'\n#    updated 'ru_ru' -> 'ru_RU.ISO8859-5' to 'ru_RU.UTF-8'\n#    updated 'serbocroatian' -> 'sr_CS.ISO8859-2' to 'sr_RS.UTF-8@latin'\n#    updated 'sh' -> 'sr_CS.ISO8859-2' to 'sr_RS.UTF-8@latin'\n#    updated 'sh_yu' -> 'sr_CS.ISO8859-2' to 'sr_RS.UTF-8@latin'\n#    updated 'sr' -> 'sr_CS.ISO8859-5' to 'sr_RS.UTF-8'\n#    updated 'sr@cyrillic' -> 'sr_CS.ISO8859-5' to 'sr_RS.UTF-8'\n#    updated 'sr@latn' -> 'sr_CS.ISO8859-2' to 'sr_RS.UTF-8@latin'\n#    updated 'sr_cs.utf8@latn' -> 'sr_CS.UTF-8' to 'sr_RS.UTF-8@latin'\n#    updated 'sr_cs@latn' -> 'sr_CS.ISO8859-2' to 'sr_RS.UTF-8@latin'\n#    updated 'sr_yu' -> 'sr_CS.ISO8859-5' to 'sr_RS.UTF-8@latin'\n#    updated 'sr_yu.utf8@cyrillic' -> 'sr_CS.UTF-8' to 'sr_RS.UTF-8'\n#    updated 'sr_yu@cyrillic' -> 'sr_CS.ISO8859-5' to 'sr_RS.UTF-8'\n#\n# SS 2013-12-20:\n# Updated alias mapping to most recent locale.alias file\n# from X.org distribution using makelocalealias.py.\n#\n# These are the differences compared to the old mapping (Python 2.7.6\n# and older):\n#\n#    updated 'a3' -> 'a3_AZ.KOI8-C' to 'az_AZ.KOI8-C'\n#    updated 'a3_az' -> 'a3_AZ.KOI8-C' to 'az_AZ.KOI8-C'\n#    updated 'a3_az.koi8c' -> 'a3_AZ.KOI8-C' to 'az_AZ.KOI8-C'\n#    updated 'cs_cs.iso88592' -> 'cs_CS.ISO8859-2' to 'cs_CZ.ISO8859-2'\n#    updated 'hebrew' -> 'iw_IL.ISO8859-8' to 'he_IL.ISO8859-8'\n#    updated 'hebrew.iso88598' -> 'iw_IL.ISO8859-8' to 'he_IL.ISO8859-8'\n#    updated 'sd' -> 'sd_IN@devanagari.UTF-8' to 'sd_IN.UTF-8'\n#    updated 'sr@latn' -> 'sr_RS.UTF-8@latin' to 'sr_CS.UTF-8@latin'\n#    updated 'sr_cs' -> 'sr_RS.UTF-8' to 'sr_CS.UTF-8'\n#    updated 'sr_cs.utf8@latn' -> 'sr_RS.UTF-8@latin' to 'sr_CS.UTF-8@latin'\n#    updated 'sr_cs@latn' -> 'sr_RS.UTF-8@latin' to 'sr_CS.UTF-8@latin'\n#\n# SS 2014-10-01:\n# Updated alias mapping with glibc 2.19 supported locales.\n\nlocale_alias = {\n    'a3':                                   'az_AZ.KOI8-C',\n    'a3_az':                                'az_AZ.KOI8-C',\n    'a3_az.koi8c':                          'az_AZ.KOI8-C',\n    'a3_az.koic':                           'az_AZ.KOI8-C',\n    'aa_dj':                                'aa_DJ.ISO8859-1',\n    'aa_er':                                'aa_ER.UTF-8',\n    'aa_et':                                'aa_ET.UTF-8',\n    'af':                                   'af_ZA.ISO8859-1',\n    'af_za':                                'af_ZA.ISO8859-1',\n    'af_za.iso88591':                       'af_ZA.ISO8859-1',\n    'am':                                   'am_ET.UTF-8',\n    'am_et':                                'am_ET.UTF-8',\n    'american':                             'en_US.ISO8859-1',\n    'american.iso88591':                    'en_US.ISO8859-1',\n    'an_es':                                'an_ES.ISO8859-15',\n    'ar':                                   'ar_AA.ISO8859-6',\n    'ar_aa':                                'ar_AA.ISO8859-6',\n    'ar_aa.iso88596':                       'ar_AA.ISO8859-6',\n    'ar_ae':                                'ar_AE.ISO8859-6',\n    'ar_ae.iso88596':                       'ar_AE.ISO8859-6',\n    'ar_bh':                                'ar_BH.ISO8859-6',\n    'ar_bh.iso88596':                       'ar_BH.ISO8859-6',\n    'ar_dz':                                'ar_DZ.ISO8859-6',\n    'ar_dz.iso88596':                       'ar_DZ.ISO8859-6',\n    'ar_eg':                                'ar_EG.ISO8859-6',\n    'ar_eg.iso88596':                       'ar_EG.ISO8859-6',\n    'ar_in':                                'ar_IN.UTF-8',\n    'ar_iq':                                'ar_IQ.ISO8859-6',\n    'ar_iq.iso88596':                       'ar_IQ.ISO8859-6',\n    'ar_jo':                                'ar_JO.ISO8859-6',\n    'ar_jo.iso88596':                       'ar_JO.ISO8859-6',\n    'ar_kw':                                'ar_KW.ISO8859-6',\n    'ar_kw.iso88596':                       'ar_KW.ISO8859-6',\n    'ar_lb':                                'ar_LB.ISO8859-6',\n    'ar_lb.iso88596':                       'ar_LB.ISO8859-6',\n    'ar_ly':                                'ar_LY.ISO8859-6',\n    'ar_ly.iso88596':                       'ar_LY.ISO8859-6',\n    'ar_ma':                                'ar_MA.ISO8859-6',\n    'ar_ma.iso88596':                       'ar_MA.ISO8859-6',\n    'ar_om':                                'ar_OM.ISO8859-6',\n    'ar_om.iso88596':                       'ar_OM.ISO8859-6',\n    'ar_qa':                                'ar_QA.ISO8859-6',\n    'ar_qa.iso88596':                       'ar_QA.ISO8859-6',\n    'ar_sa':                                'ar_SA.ISO8859-6',\n    'ar_sa.iso88596':                       'ar_SA.ISO8859-6',\n    'ar_sd':                                'ar_SD.ISO8859-6',\n    'ar_sd.iso88596':                       'ar_SD.ISO8859-6',\n    'ar_sy':                                'ar_SY.ISO8859-6',\n    'ar_sy.iso88596':                       'ar_SY.ISO8859-6',\n    'ar_tn':                                'ar_TN.ISO8859-6',\n    'ar_tn.iso88596':                       'ar_TN.ISO8859-6',\n    'ar_ye':                                'ar_YE.ISO8859-6',\n    'ar_ye.iso88596':                       'ar_YE.ISO8859-6',\n    'arabic':                               'ar_AA.ISO8859-6',\n    'arabic.iso88596':                      'ar_AA.ISO8859-6',\n    'as':                                   'as_IN.UTF-8',\n    'as_in':                                'as_IN.UTF-8',\n    'ast_es':                               'ast_ES.ISO8859-15',\n    'ayc_pe':                               'ayc_PE.UTF-8',\n    'az':                                   'az_AZ.ISO8859-9E',\n    'az_az':                                'az_AZ.ISO8859-9E',\n    'az_az.iso88599e':                      'az_AZ.ISO8859-9E',\n    'be':                                   'be_BY.CP1251',\n    'be@latin':                             'be_BY.UTF-8@latin',\n    'be_bg.utf8':                           'bg_BG.UTF-8',\n    'be_by':                                'be_BY.CP1251',\n    'be_by.cp1251':                         'be_BY.CP1251',\n    'be_by.microsoftcp1251':                'be_BY.CP1251',\n    'be_by.utf8@latin':                     'be_BY.UTF-8@latin',\n    'be_by@latin':                          'be_BY.UTF-8@latin',\n    'bem_zm':                               'bem_ZM.UTF-8',\n    'ber_dz':                               'ber_DZ.UTF-8',\n    'ber_ma':                               'ber_MA.UTF-8',\n    'bg':                                   'bg_BG.CP1251',\n    'bg_bg':                                'bg_BG.CP1251',\n    'bg_bg.cp1251':                         'bg_BG.CP1251',\n    'bg_bg.iso88595':                       'bg_BG.ISO8859-5',\n    'bg_bg.koi8r':                          'bg_BG.KOI8-R',\n    'bg_bg.microsoftcp1251':                'bg_BG.CP1251',\n    'bho_in':                               'bho_IN.UTF-8',\n    'bn_bd':                                'bn_BD.UTF-8',\n    'bn_in':                                'bn_IN.UTF-8',\n    'bo_cn':                                'bo_CN.UTF-8',\n    'bo_in':                                'bo_IN.UTF-8',\n    'bokmal':                               'nb_NO.ISO8859-1',\n    'bokm\\xe5l':                            'nb_NO.ISO8859-1',\n    'br':                                   'br_FR.ISO8859-1',\n    'br_fr':                                'br_FR.ISO8859-1',\n    'br_fr.iso88591':                       'br_FR.ISO8859-1',\n    'br_fr.iso885914':                      'br_FR.ISO8859-14',\n    'br_fr.iso885915':                      'br_FR.ISO8859-15',\n    'br_fr.iso885915@euro':                 'br_FR.ISO8859-15',\n    'br_fr.utf8@euro':                      'br_FR.UTF-8',\n    'br_fr@euro':                           'br_FR.ISO8859-15',\n    'brx_in':                               'brx_IN.UTF-8',\n    'bs':                                   'bs_BA.ISO8859-2',\n    'bs_ba':                                'bs_BA.ISO8859-2',\n    'bs_ba.iso88592':                       'bs_BA.ISO8859-2',\n    'bulgarian':                            'bg_BG.CP1251',\n    'byn_er':                               'byn_ER.UTF-8',\n    'c':                                    'C',\n    'c-french':                             'fr_CA.ISO8859-1',\n    'c-french.iso88591':                    'fr_CA.ISO8859-1',\n    'c.ascii':                              'C',\n    'c.en':                                 'C',\n    'c.iso88591':                           'en_US.ISO8859-1',\n    'c.utf8':                               'en_US.UTF-8',\n    'c_c':                                  'C',\n    'c_c.c':                                'C',\n    'ca':                                   'ca_ES.ISO8859-1',\n    'ca_ad':                                'ca_AD.ISO8859-1',\n    'ca_ad.iso88591':                       'ca_AD.ISO8859-1',\n    'ca_ad.iso885915':                      'ca_AD.ISO8859-15',\n    'ca_ad.iso885915@euro':                 'ca_AD.ISO8859-15',\n    'ca_ad.utf8@euro':                      'ca_AD.UTF-8',\n    'ca_ad@euro':                           'ca_AD.ISO8859-15',\n    'ca_es':                                'ca_ES.ISO8859-1',\n    'ca_es.iso88591':                       'ca_ES.ISO8859-1',\n    'ca_es.iso885915':                      'ca_ES.ISO8859-15',\n    'ca_es.iso885915@euro':                 'ca_ES.ISO8859-15',\n    'ca_es.utf8@euro':                      'ca_ES.UTF-8',\n    'ca_es@valencia':                       'ca_ES.ISO8859-15@valencia',\n    'ca_es@euro':                           'ca_ES.ISO8859-15',\n    'ca_fr':                                'ca_FR.ISO8859-1',\n    'ca_fr.iso88591':                       'ca_FR.ISO8859-1',\n    'ca_fr.iso885915':                      'ca_FR.ISO8859-15',\n    'ca_fr.iso885915@euro':                 'ca_FR.ISO8859-15',\n    'ca_fr.utf8@euro':                      'ca_FR.UTF-8',\n    'ca_fr@euro':                           'ca_FR.ISO8859-15',\n    'ca_it':                                'ca_IT.ISO8859-1',\n    'ca_it.iso88591':                       'ca_IT.ISO8859-1',\n    'ca_it.iso885915':                      'ca_IT.ISO8859-15',\n    'ca_it.iso885915@euro':                 'ca_IT.ISO8859-15',\n    'ca_it.utf8@euro':                      'ca_IT.UTF-8',\n    'ca_it@euro':                           'ca_IT.ISO8859-15',\n    'catalan':                              'ca_ES.ISO8859-1',\n    'cextend':                              'en_US.ISO8859-1',\n    'cextend.en':                           'en_US.ISO8859-1',\n    'chinese-s':                            'zh_CN.eucCN',\n    'chinese-t':                            'zh_TW.eucTW',\n    'crh_ua':                               'crh_UA.UTF-8',\n    'croatian':                             'hr_HR.ISO8859-2',\n    'cs':                                   'cs_CZ.ISO8859-2',\n    'cs_cs':                                'cs_CZ.ISO8859-2',\n    'cs_cs.iso88592':                       'cs_CZ.ISO8859-2',\n    'cs_cz':                                'cs_CZ.ISO8859-2',\n    'cs_cz.iso88592':                       'cs_CZ.ISO8859-2',\n    'csb_pl':                               'csb_PL.UTF-8',\n    'cv_ru':                                'cv_RU.UTF-8',\n    'cy':                                   'cy_GB.ISO8859-1',\n    'cy_gb':                                'cy_GB.ISO8859-1',\n    'cy_gb.iso88591':                       'cy_GB.ISO8859-1',\n    'cy_gb.iso885914':                      'cy_GB.ISO8859-14',\n    'cy_gb.iso885915':                      'cy_GB.ISO8859-15',\n    'cy_gb@euro':                           'cy_GB.ISO8859-15',\n    'cz':                                   'cs_CZ.ISO8859-2',\n    'cz_cz':                                'cs_CZ.ISO8859-2',\n    'czech':                                'cs_CZ.ISO8859-2',\n    'da':                                   'da_DK.ISO8859-1',\n    'da.iso885915':                         'da_DK.ISO8859-15',\n    'da_dk':                                'da_DK.ISO8859-1',\n    'da_dk.88591':                          'da_DK.ISO8859-1',\n    'da_dk.885915':                         'da_DK.ISO8859-15',\n    'da_dk.iso88591':                       'da_DK.ISO8859-1',\n    'da_dk.iso885915':                      'da_DK.ISO8859-15',\n    'da_dk@euro':                           'da_DK.ISO8859-15',\n    'danish':                               'da_DK.ISO8859-1',\n    'danish.iso88591':                      'da_DK.ISO8859-1',\n    'dansk':                                'da_DK.ISO8859-1',\n    'de':                                   'de_DE.ISO8859-1',\n    'de.iso885915':                         'de_DE.ISO8859-15',\n    'de_at':                                'de_AT.ISO8859-1',\n    'de_at.iso88591':                       'de_AT.ISO8859-1',\n    'de_at.iso885915':                      'de_AT.ISO8859-15',\n    'de_at.iso885915@euro':                 'de_AT.ISO8859-15',\n    'de_at.utf8@euro':                      'de_AT.UTF-8',\n    'de_at@euro':                           'de_AT.ISO8859-15',\n    'de_be':                                'de_BE.ISO8859-1',\n    'de_be.iso88591':                       'de_BE.ISO8859-1',\n    'de_be.iso885915':                      'de_BE.ISO8859-15',\n    'de_be.iso885915@euro':                 'de_BE.ISO8859-15',\n    'de_be.utf8@euro':                      'de_BE.UTF-8',\n    'de_be@euro':                           'de_BE.ISO8859-15',\n    'de_ch':                                'de_CH.ISO8859-1',\n    'de_ch.iso88591':                       'de_CH.ISO8859-1',\n    'de_ch.iso885915':                      'de_CH.ISO8859-15',\n    'de_ch@euro':                           'de_CH.ISO8859-15',\n    'de_de':                                'de_DE.ISO8859-1',\n    'de_de.88591':                          'de_DE.ISO8859-1',\n    'de_de.885915':                         'de_DE.ISO8859-15',\n    'de_de.885915@euro':                    'de_DE.ISO8859-15',\n    'de_de.iso88591':                       'de_DE.ISO8859-1',\n    'de_de.iso885915':                      'de_DE.ISO8859-15',\n    'de_de.iso885915@euro':                 'de_DE.ISO8859-15',\n    'de_de.utf8@euro':                      'de_DE.UTF-8',\n    'de_de@euro':                           'de_DE.ISO8859-15',\n    'de_li.utf8':                           'de_LI.UTF-8',\n    'de_lu':                                'de_LU.ISO8859-1',\n    'de_lu.iso88591':                       'de_LU.ISO8859-1',\n    'de_lu.iso885915':                      'de_LU.ISO8859-15',\n    'de_lu.iso885915@euro':                 'de_LU.ISO8859-15',\n    'de_lu.utf8@euro':                      'de_LU.UTF-8',\n    'de_lu@euro':                           'de_LU.ISO8859-15',\n    'deutsch':                              'de_DE.ISO8859-1',\n    'doi_in':                               'doi_IN.UTF-8',\n    'dutch':                                'nl_NL.ISO8859-1',\n    'dutch.iso88591':                       'nl_BE.ISO8859-1',\n    'dv_mv':                                'dv_MV.UTF-8',\n    'dz_bt':                                'dz_BT.UTF-8',\n    'ee':                                   'ee_EE.ISO8859-4',\n    'ee_ee':                                'ee_EE.ISO8859-4',\n    'ee_ee.iso88594':                       'ee_EE.ISO8859-4',\n    'eesti':                                'et_EE.ISO8859-1',\n    'el':                                   'el_GR.ISO8859-7',\n    'el_cy':                                'el_CY.ISO8859-7',\n    'el_gr':                                'el_GR.ISO8859-7',\n    'el_gr.iso88597':                       'el_GR.ISO8859-7',\n    'el_gr@euro':                           'el_GR.ISO8859-15',\n    'en':                                   'en_US.ISO8859-1',\n    'en.iso88591':                          'en_US.ISO8859-1',\n    'en_ag':                                'en_AG.UTF-8',\n    'en_au':                                'en_AU.ISO8859-1',\n    'en_au.iso88591':                       'en_AU.ISO8859-1',\n    'en_be':                                'en_BE.ISO8859-1',\n    'en_be@euro':                           'en_BE.ISO8859-15',\n    'en_bw':                                'en_BW.ISO8859-1',\n    'en_bw.iso88591':                       'en_BW.ISO8859-1',\n    'en_ca':                                'en_CA.ISO8859-1',\n    'en_ca.iso88591':                       'en_CA.ISO8859-1',\n    'en_dk':                                'en_DK.ISO8859-1',\n    'en_dl.utf8':                           'en_DL.UTF-8',\n    'en_gb':                                'en_GB.ISO8859-1',\n    'en_gb.88591':                          'en_GB.ISO8859-1',\n    'en_gb.iso88591':                       'en_GB.ISO8859-1',\n    'en_gb.iso885915':                      'en_GB.ISO8859-15',\n    'en_gb@euro':                           'en_GB.ISO8859-15',\n    'en_hk':                                'en_HK.ISO8859-1',\n    'en_hk.iso88591':                       'en_HK.ISO8859-1',\n    'en_ie':                                'en_IE.ISO8859-1',\n    'en_ie.iso88591':                       'en_IE.ISO8859-1',\n    'en_ie.iso885915':                      'en_IE.ISO8859-15',\n    'en_ie.iso885915@euro':                 'en_IE.ISO8859-15',\n    'en_ie.utf8@euro':                      'en_IE.UTF-8',\n    'en_ie@euro':                           'en_IE.ISO8859-15',\n    'en_in':                                'en_IN.ISO8859-1',\n    'en_ng':                                'en_NG.UTF-8',\n    'en_nz':                                'en_NZ.ISO8859-1',\n    'en_nz.iso88591':                       'en_NZ.ISO8859-1',\n    'en_ph':                                'en_PH.ISO8859-1',\n    'en_ph.iso88591':                       'en_PH.ISO8859-1',\n    'en_sg':                                'en_SG.ISO8859-1',\n    'en_sg.iso88591':                       'en_SG.ISO8859-1',\n    'en_uk':                                'en_GB.ISO8859-1',\n    'en_us':                                'en_US.ISO8859-1',\n    'en_us.88591':                          'en_US.ISO8859-1',\n    'en_us.885915':                         'en_US.ISO8859-15',\n    'en_us.iso88591':                       'en_US.ISO8859-1',\n    'en_us.iso885915':                      'en_US.ISO8859-15',\n    'en_us.iso885915@euro':                 'en_US.ISO8859-15',\n    'en_us@euro':                           'en_US.ISO8859-15',\n    'en_us@euro@euro':                      'en_US.ISO8859-15',\n    'en_za':                                'en_ZA.ISO8859-1',\n    'en_za.88591':                          'en_ZA.ISO8859-1',\n    'en_za.iso88591':                       'en_ZA.ISO8859-1',\n    'en_za.iso885915':                      'en_ZA.ISO8859-15',\n    'en_za@euro':                           'en_ZA.ISO8859-15',\n    'en_zm':                                'en_ZM.UTF-8',\n    'en_zw':                                'en_ZW.ISO8859-1',\n    'en_zw.iso88591':                       'en_ZW.ISO8859-1',\n    'en_zw.utf8':                           'en_ZS.UTF-8',\n    'eng_gb':                               'en_GB.ISO8859-1',\n    'eng_gb.8859':                          'en_GB.ISO8859-1',\n    'english':                              'en_EN.ISO8859-1',\n    'english.iso88591':                     'en_EN.ISO8859-1',\n    'english_uk':                           'en_GB.ISO8859-1',\n    'english_uk.8859':                      'en_GB.ISO8859-1',\n    'english_united-states':                'en_US.ISO8859-1',\n    'english_united-states.437':            'C',\n    'english_us':                           'en_US.ISO8859-1',\n    'english_us.8859':                      'en_US.ISO8859-1',\n    'english_us.ascii':                     'en_US.ISO8859-1',\n    'eo':                                   'eo_XX.ISO8859-3',\n    'eo.utf8':                              'eo.UTF-8',\n    'eo_eo':                                'eo_EO.ISO8859-3',\n    'eo_eo.iso88593':                       'eo_EO.ISO8859-3',\n    'eo_us.utf8':                           'eo_US.UTF-8',\n    'eo_xx':                                'eo_XX.ISO8859-3',\n    'eo_xx.iso88593':                       'eo_XX.ISO8859-3',\n    'es':                                   'es_ES.ISO8859-1',\n    'es_ar':                                'es_AR.ISO8859-1',\n    'es_ar.iso88591':                       'es_AR.ISO8859-1',\n    'es_bo':                                'es_BO.ISO8859-1',\n    'es_bo.iso88591':                       'es_BO.ISO8859-1',\n    'es_cl':                                'es_CL.ISO8859-1',\n    'es_cl.iso88591':                       'es_CL.ISO8859-1',\n    'es_co':                                'es_CO.ISO8859-1',\n    'es_co.iso88591':                       'es_CO.ISO8859-1',\n    'es_cr':                                'es_CR.ISO8859-1',\n    'es_cr.iso88591':                       'es_CR.ISO8859-1',\n    'es_cu':                                'es_CU.UTF-8',\n    'es_do':                                'es_DO.ISO8859-1',\n    'es_do.iso88591':                       'es_DO.ISO8859-1',\n    'es_ec':                                'es_EC.ISO8859-1',\n    'es_ec.iso88591':                       'es_EC.ISO8859-1',\n    'es_es':                                'es_ES.ISO8859-1',\n    'es_es.88591':                          'es_ES.ISO8859-1',\n    'es_es.iso88591':                       'es_ES.ISO8859-1',\n    'es_es.iso885915':                      'es_ES.ISO8859-15',\n    'es_es.iso885915@euro':                 'es_ES.ISO8859-15',\n    'es_es.utf8@euro':                      'es_ES.UTF-8',\n    'es_es@euro':                           'es_ES.ISO8859-15',\n    'es_gt':                                'es_GT.ISO8859-1',\n    'es_gt.iso88591':                       'es_GT.ISO8859-1',\n    'es_hn':                                'es_HN.ISO8859-1',\n    'es_hn.iso88591':                       'es_HN.ISO8859-1',\n    'es_mx':                                'es_MX.ISO8859-1',\n    'es_mx.iso88591':                       'es_MX.ISO8859-1',\n    'es_ni':                                'es_NI.ISO8859-1',\n    'es_ni.iso88591':                       'es_NI.ISO8859-1',\n    'es_pa':                                'es_PA.ISO8859-1',\n    'es_pa.iso88591':                       'es_PA.ISO8859-1',\n    'es_pa.iso885915':                      'es_PA.ISO8859-15',\n    'es_pa@euro':                           'es_PA.ISO8859-15',\n    'es_pe':                                'es_PE.ISO8859-1',\n    'es_pe.iso88591':                       'es_PE.ISO8859-1',\n    'es_pe.iso885915':                      'es_PE.ISO8859-15',\n    'es_pe@euro':                           'es_PE.ISO8859-15',\n    'es_pr':                                'es_PR.ISO8859-1',\n    'es_pr.iso88591':                       'es_PR.ISO8859-1',\n    'es_py':                                'es_PY.ISO8859-1',\n    'es_py.iso88591':                       'es_PY.ISO8859-1',\n    'es_py.iso885915':                      'es_PY.ISO8859-15',\n    'es_py@euro':                           'es_PY.ISO8859-15',\n    'es_sv':                                'es_SV.ISO8859-1',\n    'es_sv.iso88591':                       'es_SV.ISO8859-1',\n    'es_sv.iso885915':                      'es_SV.ISO8859-15',\n    'es_sv@euro':                           'es_SV.ISO8859-15',\n    'es_us':                                'es_US.ISO8859-1',\n    'es_us.iso88591':                       'es_US.ISO8859-1',\n    'es_uy':                                'es_UY.ISO8859-1',\n    'es_uy.iso88591':                       'es_UY.ISO8859-1',\n    'es_uy.iso885915':                      'es_UY.ISO8859-15',\n    'es_uy@euro':                           'es_UY.ISO8859-15',\n    'es_ve':                                'es_VE.ISO8859-1',\n    'es_ve.iso88591':                       'es_VE.ISO8859-1',\n    'es_ve.iso885915':                      'es_VE.ISO8859-15',\n    'es_ve@euro':                           'es_VE.ISO8859-15',\n    'estonian':                             'et_EE.ISO8859-1',\n    'et':                                   'et_EE.ISO8859-15',\n    'et_ee':                                'et_EE.ISO8859-15',\n    'et_ee.iso88591':                       'et_EE.ISO8859-1',\n    'et_ee.iso885913':                      'et_EE.ISO8859-13',\n    'et_ee.iso885915':                      'et_EE.ISO8859-15',\n    'et_ee.iso88594':                       'et_EE.ISO8859-4',\n    'et_ee@euro':                           'et_EE.ISO8859-15',\n    'eu':                                   'eu_ES.ISO8859-1',\n    'eu_es':                                'eu_ES.ISO8859-1',\n    'eu_es.iso88591':                       'eu_ES.ISO8859-1',\n    'eu_es.iso885915':                      'eu_ES.ISO8859-15',\n    'eu_es.iso885915@euro':                 'eu_ES.ISO8859-15',\n    'eu_es.utf8@euro':                      'eu_ES.UTF-8',\n    'eu_es@euro':                           'eu_ES.ISO8859-15',\n    'eu_fr':                                'eu_FR.ISO8859-1',\n    'fa':                                   'fa_IR.UTF-8',\n    'fa_ir':                                'fa_IR.UTF-8',\n    'fa_ir.isiri3342':                      'fa_IR.ISIRI-3342',\n    'ff_sn':                                'ff_SN.UTF-8',\n    'fi':                                   'fi_FI.ISO8859-15',\n    'fi.iso885915':                         'fi_FI.ISO8859-15',\n    'fi_fi':                                'fi_FI.ISO8859-15',\n    'fi_fi.88591':                          'fi_FI.ISO8859-1',\n    'fi_fi.iso88591':                       'fi_FI.ISO8859-1',\n    'fi_fi.iso885915':                      'fi_FI.ISO8859-15',\n    'fi_fi.iso885915@euro':                 'fi_FI.ISO8859-15',\n    'fi_fi.utf8@euro':                      'fi_FI.UTF-8',\n    'fi_fi@euro':                           'fi_FI.ISO8859-15',\n    'fil_ph':                               'fil_PH.UTF-8',\n    'finnish':                              'fi_FI.ISO8859-1',\n    'finnish.iso88591':                     'fi_FI.ISO8859-1',\n    'fo':                                   'fo_FO.ISO8859-1',\n    'fo_fo':                                'fo_FO.ISO8859-1',\n    'fo_fo.iso88591':                       'fo_FO.ISO8859-1',\n    'fo_fo.iso885915':                      'fo_FO.ISO8859-15',\n    'fo_fo@euro':                           'fo_FO.ISO8859-15',\n    'fr':                                   'fr_FR.ISO8859-1',\n    'fr.iso885915':                         'fr_FR.ISO8859-15',\n    'fr_be':                                'fr_BE.ISO8859-1',\n    'fr_be.88591':                          'fr_BE.ISO8859-1',\n    'fr_be.iso88591':                       'fr_BE.ISO8859-1',\n    'fr_be.iso885915':                      'fr_BE.ISO8859-15',\n    'fr_be.iso885915@euro':                 'fr_BE.ISO8859-15',\n    'fr_be.utf8@euro':                      'fr_BE.UTF-8',\n    'fr_be@euro':                           'fr_BE.ISO8859-15',\n    'fr_ca':                                'fr_CA.ISO8859-1',\n    'fr_ca.88591':                          'fr_CA.ISO8859-1',\n    'fr_ca.iso88591':                       'fr_CA.ISO8859-1',\n    'fr_ca.iso885915':                      'fr_CA.ISO8859-15',\n    'fr_ca@euro':                           'fr_CA.ISO8859-15',\n    'fr_ch':                                'fr_CH.ISO8859-1',\n    'fr_ch.88591':                          'fr_CH.ISO8859-1',\n    'fr_ch.iso88591':                       'fr_CH.ISO8859-1',\n    'fr_ch.iso885915':                      'fr_CH.ISO8859-15',\n    'fr_ch@euro':                           'fr_CH.ISO8859-15',\n    'fr_fr':                                'fr_FR.ISO8859-1',\n    'fr_fr.88591':                          'fr_FR.ISO8859-1',\n    'fr_fr.iso88591':                       'fr_FR.ISO8859-1',\n    'fr_fr.iso885915':                      'fr_FR.ISO8859-15',\n    'fr_fr.iso885915@euro':                 'fr_FR.ISO8859-15',\n    'fr_fr.utf8@euro':                      'fr_FR.UTF-8',\n    'fr_fr@euro':                           'fr_FR.ISO8859-15',\n    'fr_lu':                                'fr_LU.ISO8859-1',\n    'fr_lu.88591':                          'fr_LU.ISO8859-1',\n    'fr_lu.iso88591':                       'fr_LU.ISO8859-1',\n    'fr_lu.iso885915':                      'fr_LU.ISO8859-15',\n    'fr_lu.iso885915@euro':                 'fr_LU.ISO8859-15',\n    'fr_lu.utf8@euro':                      'fr_LU.UTF-8',\n    'fr_lu@euro':                           'fr_LU.ISO8859-15',\n    'fran\\xe7ais':                          'fr_FR.ISO8859-1',\n    'fre_fr':                               'fr_FR.ISO8859-1',\n    'fre_fr.8859':                          'fr_FR.ISO8859-1',\n    'french':                               'fr_FR.ISO8859-1',\n    'french.iso88591':                      'fr_CH.ISO8859-1',\n    'french_france':                        'fr_FR.ISO8859-1',\n    'french_france.8859':                   'fr_FR.ISO8859-1',\n    'fur_it':                               'fur_IT.UTF-8',\n    'fy_de':                                'fy_DE.UTF-8',\n    'fy_nl':                                'fy_NL.UTF-8',\n    'ga':                                   'ga_IE.ISO8859-1',\n    'ga_ie':                                'ga_IE.ISO8859-1',\n    'ga_ie.iso88591':                       'ga_IE.ISO8859-1',\n    'ga_ie.iso885914':                      'ga_IE.ISO8859-14',\n    'ga_ie.iso885915':                      'ga_IE.ISO8859-15',\n    'ga_ie.iso885915@euro':                 'ga_IE.ISO8859-15',\n    'ga_ie.utf8@euro':                      'ga_IE.UTF-8',\n    'ga_ie@euro':                           'ga_IE.ISO8859-15',\n    'galego':                               'gl_ES.ISO8859-1',\n    'galician':                             'gl_ES.ISO8859-1',\n    'gd':                                   'gd_GB.ISO8859-1',\n    'gd_gb':                                'gd_GB.ISO8859-1',\n    'gd_gb.iso88591':                       'gd_GB.ISO8859-1',\n    'gd_gb.iso885914':                      'gd_GB.ISO8859-14',\n    'gd_gb.iso885915':                      'gd_GB.ISO8859-15',\n    'gd_gb@euro':                           'gd_GB.ISO8859-15',\n    'ger_de':                               'de_DE.ISO8859-1',\n    'ger_de.8859':                          'de_DE.ISO8859-1',\n    'german':                               'de_DE.ISO8859-1',\n    'german.iso88591':                      'de_CH.ISO8859-1',\n    'german_germany':                       'de_DE.ISO8859-1',\n    'german_germany.8859':                  'de_DE.ISO8859-1',\n    'gez_er':                               'gez_ER.UTF-8',\n    'gez_et':                               'gez_ET.UTF-8',\n    'gl':                                   'gl_ES.ISO8859-1',\n    'gl_es':                                'gl_ES.ISO8859-1',\n    'gl_es.iso88591':                       'gl_ES.ISO8859-1',\n    'gl_es.iso885915':                      'gl_ES.ISO8859-15',\n    'gl_es.iso885915@euro':                 'gl_ES.ISO8859-15',\n    'gl_es.utf8@euro':                      'gl_ES.UTF-8',\n    'gl_es@euro':                           'gl_ES.ISO8859-15',\n    'greek':                                'el_GR.ISO8859-7',\n    'greek.iso88597':                       'el_GR.ISO8859-7',\n    'gu_in':                                'gu_IN.UTF-8',\n    'gv':                                   'gv_GB.ISO8859-1',\n    'gv_gb':                                'gv_GB.ISO8859-1',\n    'gv_gb.iso88591':                       'gv_GB.ISO8859-1',\n    'gv_gb.iso885914':                      'gv_GB.ISO8859-14',\n    'gv_gb.iso885915':                      'gv_GB.ISO8859-15',\n    'gv_gb@euro':                           'gv_GB.ISO8859-15',\n    'ha_ng':                                'ha_NG.UTF-8',\n    'he':                                   'he_IL.ISO8859-8',\n    'he_il':                                'he_IL.ISO8859-8',\n    'he_il.cp1255':                         'he_IL.CP1255',\n    'he_il.iso88598':                       'he_IL.ISO8859-8',\n    'he_il.microsoftcp1255':                'he_IL.CP1255',\n    'hebrew':                               'he_IL.ISO8859-8',\n    'hebrew.iso88598':                      'he_IL.ISO8859-8',\n    'hi':                                   'hi_IN.ISCII-DEV',\n    'hi_in':                                'hi_IN.ISCII-DEV',\n    'hi_in.isciidev':                       'hi_IN.ISCII-DEV',\n    'hne':                                  'hne_IN.UTF-8',\n    'hne_in':                               'hne_IN.UTF-8',\n    'hr':                                   'hr_HR.ISO8859-2',\n    'hr_hr':                                'hr_HR.ISO8859-2',\n    'hr_hr.iso88592':                       'hr_HR.ISO8859-2',\n    'hrvatski':                             'hr_HR.ISO8859-2',\n    'hsb_de':                               'hsb_DE.ISO8859-2',\n    'ht_ht':                                'ht_HT.UTF-8',\n    'hu':                                   'hu_HU.ISO8859-2',\n    'hu_hu':                                'hu_HU.ISO8859-2',\n    'hu_hu.iso88592':                       'hu_HU.ISO8859-2',\n    'hungarian':                            'hu_HU.ISO8859-2',\n    'hy_am':                                'hy_AM.UTF-8',\n    'hy_am.armscii8':                       'hy_AM.ARMSCII_8',\n    'ia':                                   'ia.UTF-8',\n    'ia_fr':                                'ia_FR.UTF-8',\n    'icelandic':                            'is_IS.ISO8859-1',\n    'icelandic.iso88591':                   'is_IS.ISO8859-1',\n    'id':                                   'id_ID.ISO8859-1',\n    'id_id':                                'id_ID.ISO8859-1',\n    'ig_ng':                                'ig_NG.UTF-8',\n    'ik_ca':                                'ik_CA.UTF-8',\n    'in':                                   'id_ID.ISO8859-1',\n    'in_id':                                'id_ID.ISO8859-1',\n    'is':                                   'is_IS.ISO8859-1',\n    'is_is':                                'is_IS.ISO8859-1',\n    'is_is.iso88591':                       'is_IS.ISO8859-1',\n    'is_is.iso885915':                      'is_IS.ISO8859-15',\n    'is_is@euro':                           'is_IS.ISO8859-15',\n    'iso-8859-1':                           'en_US.ISO8859-1',\n    'iso-8859-15':                          'en_US.ISO8859-15',\n    'iso8859-1':                            'en_US.ISO8859-1',\n    'iso8859-15':                           'en_US.ISO8859-15',\n    'iso_8859_1':                           'en_US.ISO8859-1',\n    'iso_8859_15':                          'en_US.ISO8859-15',\n    'it':                                   'it_IT.ISO8859-1',\n    'it.iso885915':                         'it_IT.ISO8859-15',\n    'it_ch':                                'it_CH.ISO8859-1',\n    'it_ch.iso88591':                       'it_CH.ISO8859-1',\n    'it_ch.iso885915':                      'it_CH.ISO8859-15',\n    'it_ch@euro':                           'it_CH.ISO8859-15',\n    'it_it':                                'it_IT.ISO8859-1',\n    'it_it.88591':                          'it_IT.ISO8859-1',\n    'it_it.iso88591':                       'it_IT.ISO8859-1',\n    'it_it.iso885915':                      'it_IT.ISO8859-15',\n    'it_it.iso885915@euro':                 'it_IT.ISO8859-15',\n    'it_it.utf8@euro':                      'it_IT.UTF-8',\n    'it_it@euro':                           'it_IT.ISO8859-15',\n    'italian':                              'it_IT.ISO8859-1',\n    'italian.iso88591':                     'it_IT.ISO8859-1',\n    'iu':                                   'iu_CA.NUNACOM-8',\n    'iu_ca':                                'iu_CA.NUNACOM-8',\n    'iu_ca.nunacom8':                       'iu_CA.NUNACOM-8',\n    'iw':                                   'he_IL.ISO8859-8',\n    'iw_il':                                'he_IL.ISO8859-8',\n    'iw_il.iso88598':                       'he_IL.ISO8859-8',\n    'iw_il.utf8':                           'iw_IL.UTF-8',\n    'ja':                                   'ja_JP.eucJP',\n    'ja.jis':                               'ja_JP.JIS7',\n    'ja.sjis':                              'ja_JP.SJIS',\n    'ja_jp':                                'ja_JP.eucJP',\n    'ja_jp.ajec':                           'ja_JP.eucJP',\n    'ja_jp.euc':                            'ja_JP.eucJP',\n    'ja_jp.eucjp':                          'ja_JP.eucJP',\n    'ja_jp.iso-2022-jp':                    'ja_JP.JIS7',\n    'ja_jp.iso2022jp':                      'ja_JP.JIS7',\n    'ja_jp.jis':                            'ja_JP.JIS7',\n    'ja_jp.jis7':                           'ja_JP.JIS7',\n    'ja_jp.mscode':                         'ja_JP.SJIS',\n    'ja_jp.pck':                            'ja_JP.SJIS',\n    'ja_jp.sjis':                           'ja_JP.SJIS',\n    'ja_jp.ujis':                           'ja_JP.eucJP',\n    'japan':                                'ja_JP.eucJP',\n    'japanese':                             'ja_JP.eucJP',\n    'japanese-euc':                         'ja_JP.eucJP',\n    'japanese.euc':                         'ja_JP.eucJP',\n    'japanese.sjis':                        'ja_JP.SJIS',\n    'jp_jp':                                'ja_JP.eucJP',\n    'ka':                                   'ka_GE.GEORGIAN-ACADEMY',\n    'ka_ge':                                'ka_GE.GEORGIAN-ACADEMY',\n    'ka_ge.georgianacademy':                'ka_GE.GEORGIAN-ACADEMY',\n    'ka_ge.georgianps':                     'ka_GE.GEORGIAN-PS',\n    'ka_ge.georgianrs':                     'ka_GE.GEORGIAN-ACADEMY',\n    'kk_kz':                                'kk_KZ.RK1048',\n    'kl':                                   'kl_GL.ISO8859-1',\n    'kl_gl':                                'kl_GL.ISO8859-1',\n    'kl_gl.iso88591':                       'kl_GL.ISO8859-1',\n    'kl_gl.iso885915':                      'kl_GL.ISO8859-15',\n    'kl_gl@euro':                           'kl_GL.ISO8859-15',\n    'km_kh':                                'km_KH.UTF-8',\n    'kn':                                   'kn_IN.UTF-8',\n    'kn_in':                                'kn_IN.UTF-8',\n    'ko':                                   'ko_KR.eucKR',\n    'ko_kr':                                'ko_KR.eucKR',\n    'ko_kr.euc':                            'ko_KR.eucKR',\n    'ko_kr.euckr':                          'ko_KR.eucKR',\n    'kok_in':                               'kok_IN.UTF-8',\n    'korean':                               'ko_KR.eucKR',\n    'korean.euc':                           'ko_KR.eucKR',\n    'ks':                                   'ks_IN.UTF-8',\n    'ks_in':                                'ks_IN.UTF-8',\n    'ks_in@devanagari':                     'ks_IN.UTF-8@devanagari',\n    'ks_in@devanagari.utf8':                'ks_IN.UTF-8@devanagari',\n    'ku_tr':                                'ku_TR.ISO8859-9',\n    'kw':                                   'kw_GB.ISO8859-1',\n    'kw_gb':                                'kw_GB.ISO8859-1',\n    'kw_gb.iso88591':                       'kw_GB.ISO8859-1',\n    'kw_gb.iso885914':                      'kw_GB.ISO8859-14',\n    'kw_gb.iso885915':                      'kw_GB.ISO8859-15',\n    'kw_gb@euro':                           'kw_GB.ISO8859-15',\n    'ky':                                   'ky_KG.UTF-8',\n    'ky_kg':                                'ky_KG.UTF-8',\n    'lb_lu':                                'lb_LU.UTF-8',\n    'lg_ug':                                'lg_UG.ISO8859-10',\n    'li_be':                                'li_BE.UTF-8',\n    'li_nl':                                'li_NL.UTF-8',\n    'lij_it':                               'lij_IT.UTF-8',\n    'lithuanian':                           'lt_LT.ISO8859-13',\n    'lo':                                   'lo_LA.MULELAO-1',\n    'lo_la':                                'lo_LA.MULELAO-1',\n    'lo_la.cp1133':                         'lo_LA.IBM-CP1133',\n    'lo_la.ibmcp1133':                      'lo_LA.IBM-CP1133',\n    'lo_la.mulelao1':                       'lo_LA.MULELAO-1',\n    'lt':                                   'lt_LT.ISO8859-13',\n    'lt_lt':                                'lt_LT.ISO8859-13',\n    'lt_lt.iso885913':                      'lt_LT.ISO8859-13',\n    'lt_lt.iso88594':                       'lt_LT.ISO8859-4',\n    'lv':                                   'lv_LV.ISO8859-13',\n    'lv_lv':                                'lv_LV.ISO8859-13',\n    'lv_lv.iso885913':                      'lv_LV.ISO8859-13',\n    'lv_lv.iso88594':                       'lv_LV.ISO8859-4',\n    'mag_in':                               'mag_IN.UTF-8',\n    'mai':                                  'mai_IN.UTF-8',\n    'mai_in':                               'mai_IN.UTF-8',\n    'mg_mg':                                'mg_MG.ISO8859-15',\n    'mhr_ru':                               'mhr_RU.UTF-8',\n    'mi':                                   'mi_NZ.ISO8859-1',\n    'mi_nz':                                'mi_NZ.ISO8859-1',\n    'mi_nz.iso88591':                       'mi_NZ.ISO8859-1',\n    'mk':                                   'mk_MK.ISO8859-5',\n    'mk_mk':                                'mk_MK.ISO8859-5',\n    'mk_mk.cp1251':                         'mk_MK.CP1251',\n    'mk_mk.iso88595':                       'mk_MK.ISO8859-5',\n    'mk_mk.microsoftcp1251':                'mk_MK.CP1251',\n    'ml':                                   'ml_IN.UTF-8',\n    'ml_in':                                'ml_IN.UTF-8',\n    'mn_mn':                                'mn_MN.UTF-8',\n    'mni_in':                               'mni_IN.UTF-8',\n    'mr':                                   'mr_IN.UTF-8',\n    'mr_in':                                'mr_IN.UTF-8',\n    'ms':                                   'ms_MY.ISO8859-1',\n    'ms_my':                                'ms_MY.ISO8859-1',\n    'ms_my.iso88591':                       'ms_MY.ISO8859-1',\n    'mt':                                   'mt_MT.ISO8859-3',\n    'mt_mt':                                'mt_MT.ISO8859-3',\n    'mt_mt.iso88593':                       'mt_MT.ISO8859-3',\n    'my_mm':                                'my_MM.UTF-8',\n    'nan_tw@latin':                         'nan_TW.UTF-8@latin',\n    'nb':                                   'nb_NO.ISO8859-1',\n    'nb_no':                                'nb_NO.ISO8859-1',\n    'nb_no.88591':                          'nb_NO.ISO8859-1',\n    'nb_no.iso88591':                       'nb_NO.ISO8859-1',\n    'nb_no.iso885915':                      'nb_NO.ISO8859-15',\n    'nb_no@euro':                           'nb_NO.ISO8859-15',\n    'nds_de':                               'nds_DE.UTF-8',\n    'nds_nl':                               'nds_NL.UTF-8',\n    'ne_np':                                'ne_NP.UTF-8',\n    'nhn_mx':                               'nhn_MX.UTF-8',\n    'niu_nu':                               'niu_NU.UTF-8',\n    'niu_nz':                               'niu_NZ.UTF-8',\n    'nl':                                   'nl_NL.ISO8859-1',\n    'nl.iso885915':                         'nl_NL.ISO8859-15',\n    'nl_aw':                                'nl_AW.UTF-8',\n    'nl_be':                                'nl_BE.ISO8859-1',\n    'nl_be.88591':                          'nl_BE.ISO8859-1',\n    'nl_be.iso88591':                       'nl_BE.ISO8859-1',\n    'nl_be.iso885915':                      'nl_BE.ISO8859-15',\n    'nl_be.iso885915@euro':                 'nl_BE.ISO8859-15',\n    'nl_be.utf8@euro':                      'nl_BE.UTF-8',\n    'nl_be@euro':                           'nl_BE.ISO8859-15',\n    'nl_nl':                                'nl_NL.ISO8859-1',\n    'nl_nl.88591':                          'nl_NL.ISO8859-1',\n    'nl_nl.iso88591':                       'nl_NL.ISO8859-1',\n    'nl_nl.iso885915':                      'nl_NL.ISO8859-15',\n    'nl_nl.iso885915@euro':                 'nl_NL.ISO8859-15',\n    'nl_nl.utf8@euro':                      'nl_NL.UTF-8',\n    'nl_nl@euro':                           'nl_NL.ISO8859-15',\n    'nn':                                   'nn_NO.ISO8859-1',\n    'nn_no':                                'nn_NO.ISO8859-1',\n    'nn_no.88591':                          'nn_NO.ISO8859-1',\n    'nn_no.iso88591':                       'nn_NO.ISO8859-1',\n    'nn_no.iso885915':                      'nn_NO.ISO8859-15',\n    'nn_no@euro':                           'nn_NO.ISO8859-15',\n    'no':                                   'no_NO.ISO8859-1',\n    'no@nynorsk':                           'ny_NO.ISO8859-1',\n    'no_no':                                'no_NO.ISO8859-1',\n    'no_no.88591':                          'no_NO.ISO8859-1',\n    'no_no.iso88591':                       'no_NO.ISO8859-1',\n    'no_no.iso885915':                      'no_NO.ISO8859-15',\n    'no_no.iso88591@bokmal':                'no_NO.ISO8859-1',\n    'no_no.iso88591@nynorsk':               'no_NO.ISO8859-1',\n    'no_no@euro':                           'no_NO.ISO8859-15',\n    'norwegian':                            'no_NO.ISO8859-1',\n    'norwegian.iso88591':                   'no_NO.ISO8859-1',\n    'nr':                                   'nr_ZA.ISO8859-1',\n    'nr_za':                                'nr_ZA.ISO8859-1',\n    'nr_za.iso88591':                       'nr_ZA.ISO8859-1',\n    'nso':                                  'nso_ZA.ISO8859-15',\n    'nso_za':                               'nso_ZA.ISO8859-15',\n    'nso_za.iso885915':                     'nso_ZA.ISO8859-15',\n    'ny':                                   'ny_NO.ISO8859-1',\n    'ny_no':                                'ny_NO.ISO8859-1',\n    'ny_no.88591':                          'ny_NO.ISO8859-1',\n    'ny_no.iso88591':                       'ny_NO.ISO8859-1',\n    'ny_no.iso885915':                      'ny_NO.ISO8859-15',\n    'ny_no@euro':                           'ny_NO.ISO8859-15',\n    'nynorsk':                              'nn_NO.ISO8859-1',\n    'oc':                                   'oc_FR.ISO8859-1',\n    'oc_fr':                                'oc_FR.ISO8859-1',\n    'oc_fr.iso88591':                       'oc_FR.ISO8859-1',\n    'oc_fr.iso885915':                      'oc_FR.ISO8859-15',\n    'oc_fr@euro':                           'oc_FR.ISO8859-15',\n    'om_et':                                'om_ET.UTF-8',\n    'om_ke':                                'om_KE.ISO8859-1',\n    'or':                                   'or_IN.UTF-8',\n    'or_in':                                'or_IN.UTF-8',\n    'os_ru':                                'os_RU.UTF-8',\n    'pa':                                   'pa_IN.UTF-8',\n    'pa_in':                                'pa_IN.UTF-8',\n    'pa_pk':                                'pa_PK.UTF-8',\n    'pap_an':                               'pap_AN.UTF-8',\n    'pd':                                   'pd_US.ISO8859-1',\n    'pd_de':                                'pd_DE.ISO8859-1',\n    'pd_de.iso88591':                       'pd_DE.ISO8859-1',\n    'pd_de.iso885915':                      'pd_DE.ISO8859-15',\n    'pd_de@euro':                           'pd_DE.ISO8859-15',\n    'pd_us':                                'pd_US.ISO8859-1',\n    'pd_us.iso88591':                       'pd_US.ISO8859-1',\n    'pd_us.iso885915':                      'pd_US.ISO8859-15',\n    'pd_us@euro':                           'pd_US.ISO8859-15',\n    'ph':                                   'ph_PH.ISO8859-1',\n    'ph_ph':                                'ph_PH.ISO8859-1',\n    'ph_ph.iso88591':                       'ph_PH.ISO8859-1',\n    'pl':                                   'pl_PL.ISO8859-2',\n    'pl_pl':                                'pl_PL.ISO8859-2',\n    'pl_pl.iso88592':                       'pl_PL.ISO8859-2',\n    'polish':                               'pl_PL.ISO8859-2',\n    'portuguese':                           'pt_PT.ISO8859-1',\n    'portuguese.iso88591':                  'pt_PT.ISO8859-1',\n    'portuguese_brazil':                    'pt_BR.ISO8859-1',\n    'portuguese_brazil.8859':               'pt_BR.ISO8859-1',\n    'posix':                                'C',\n    'posix-utf2':                           'C',\n    'pp':                                   'pp_AN.ISO8859-1',\n    'pp_an':                                'pp_AN.ISO8859-1',\n    'pp_an.iso88591':                       'pp_AN.ISO8859-1',\n    'ps_af':                                'ps_AF.UTF-8',\n    'pt':                                   'pt_PT.ISO8859-1',\n    'pt.iso885915':                         'pt_PT.ISO8859-15',\n    'pt_br':                                'pt_BR.ISO8859-1',\n    'pt_br.88591':                          'pt_BR.ISO8859-1',\n    'pt_br.iso88591':                       'pt_BR.ISO8859-1',\n    'pt_br.iso885915':                      'pt_BR.ISO8859-15',\n    'pt_br@euro':                           'pt_BR.ISO8859-15',\n    'pt_pt':                                'pt_PT.ISO8859-1',\n    'pt_pt.88591':                          'pt_PT.ISO8859-1',\n    'pt_pt.iso88591':                       'pt_PT.ISO8859-1',\n    'pt_pt.iso885915':                      'pt_PT.ISO8859-15',\n    'pt_pt.iso885915@euro':                 'pt_PT.ISO8859-15',\n    'pt_pt.utf8@euro':                      'pt_PT.UTF-8',\n    'pt_pt@euro':                           'pt_PT.ISO8859-15',\n    'ro':                                   'ro_RO.ISO8859-2',\n    'ro_ro':                                'ro_RO.ISO8859-2',\n    'ro_ro.iso88592':                       'ro_RO.ISO8859-2',\n    'romanian':                             'ro_RO.ISO8859-2',\n    'ru':                                   'ru_RU.UTF-8',\n    'ru.koi8r':                             'ru_RU.KOI8-R',\n    'ru_ru':                                'ru_RU.UTF-8',\n    'ru_ru.cp1251':                         'ru_RU.CP1251',\n    'ru_ru.iso88595':                       'ru_RU.ISO8859-5',\n    'ru_ru.koi8r':                          'ru_RU.KOI8-R',\n    'ru_ru.microsoftcp1251':                'ru_RU.CP1251',\n    'ru_ua':                                'ru_UA.KOI8-U',\n    'ru_ua.cp1251':                         'ru_UA.CP1251',\n    'ru_ua.koi8u':                          'ru_UA.KOI8-U',\n    'ru_ua.microsoftcp1251':                'ru_UA.CP1251',\n    'rumanian':                             'ro_RO.ISO8859-2',\n    'russian':                              'ru_RU.ISO8859-5',\n    'rw':                                   'rw_RW.ISO8859-1',\n    'rw_rw':                                'rw_RW.ISO8859-1',\n    'rw_rw.iso88591':                       'rw_RW.ISO8859-1',\n    'sa_in':                                'sa_IN.UTF-8',\n    'sat_in':                               'sat_IN.UTF-8',\n    'sc_it':                                'sc_IT.UTF-8',\n    'sd':                                   'sd_IN.UTF-8',\n    'sd@devanagari':                        'sd_IN.UTF-8@devanagari',\n    'sd_in':                                'sd_IN.UTF-8',\n    'sd_in@devanagari':                     'sd_IN.UTF-8@devanagari',\n    'sd_in@devanagari.utf8':                'sd_IN.UTF-8@devanagari',\n    'sd_pk':                                'sd_PK.UTF-8',\n    'se_no':                                'se_NO.UTF-8',\n    'serbocroatian':                        'sr_RS.UTF-8@latin',\n    'sh':                                   'sr_RS.UTF-8@latin',\n    'sh_ba.iso88592@bosnia':                'sr_CS.ISO8859-2',\n    'sh_hr':                                'sh_HR.ISO8859-2',\n    'sh_hr.iso88592':                       'hr_HR.ISO8859-2',\n    'sh_sp':                                'sr_CS.ISO8859-2',\n    'sh_yu':                                'sr_RS.UTF-8@latin',\n    'shs_ca':                               'shs_CA.UTF-8',\n    'si':                                   'si_LK.UTF-8',\n    'si_lk':                                'si_LK.UTF-8',\n    'sid_et':                               'sid_ET.UTF-8',\n    'sinhala':                              'si_LK.UTF-8',\n    'sk':                                   'sk_SK.ISO8859-2',\n    'sk_sk':                                'sk_SK.ISO8859-2',\n    'sk_sk.iso88592':                       'sk_SK.ISO8859-2',\n    'sl':                                   'sl_SI.ISO8859-2',\n    'sl_cs':                                'sl_CS.ISO8859-2',\n    'sl_si':                                'sl_SI.ISO8859-2',\n    'sl_si.iso88592':                       'sl_SI.ISO8859-2',\n    'slovak':                               'sk_SK.ISO8859-2',\n    'slovene':                              'sl_SI.ISO8859-2',\n    'slovenian':                            'sl_SI.ISO8859-2',\n    'so_dj':                                'so_DJ.ISO8859-1',\n    'so_et':                                'so_ET.UTF-8',\n    'so_ke':                                'so_KE.ISO8859-1',\n    'so_so':                                'so_SO.ISO8859-1',\n    'sp':                                   'sr_CS.ISO8859-5',\n    'sp_yu':                                'sr_CS.ISO8859-5',\n    'spanish':                              'es_ES.ISO8859-1',\n    'spanish.iso88591':                     'es_ES.ISO8859-1',\n    'spanish_spain':                        'es_ES.ISO8859-1',\n    'spanish_spain.8859':                   'es_ES.ISO8859-1',\n    'sq':                                   'sq_AL.ISO8859-2',\n    'sq_al':                                'sq_AL.ISO8859-2',\n    'sq_al.iso88592':                       'sq_AL.ISO8859-2',\n    'sq_mk':                                'sq_MK.UTF-8',\n    'sr':                                   'sr_RS.UTF-8',\n    'sr@cyrillic':                          'sr_RS.UTF-8',\n    'sr@latin':                             'sr_RS.UTF-8@latin',\n    'sr@latn':                              'sr_CS.UTF-8@latin',\n    'sr_cs':                                'sr_CS.UTF-8',\n    'sr_cs.iso88592':                       'sr_CS.ISO8859-2',\n    'sr_cs.iso88592@latn':                  'sr_CS.ISO8859-2',\n    'sr_cs.iso88595':                       'sr_CS.ISO8859-5',\n    'sr_cs.utf8@latn':                      'sr_CS.UTF-8@latin',\n    'sr_cs@latn':                           'sr_CS.UTF-8@latin',\n    'sr_me':                                'sr_ME.UTF-8',\n    'sr_rs':                                'sr_RS.UTF-8',\n    'sr_rs@latin':                          'sr_RS.UTF-8@latin',\n    'sr_rs@latn':                           'sr_RS.UTF-8@latin',\n    'sr_sp':                                'sr_CS.ISO8859-2',\n    'sr_yu':                                'sr_RS.UTF-8@latin',\n    'sr_yu.cp1251@cyrillic':                'sr_CS.CP1251',\n    'sr_yu.iso88592':                       'sr_CS.ISO8859-2',\n    'sr_yu.iso88595':                       'sr_CS.ISO8859-5',\n    'sr_yu.iso88595@cyrillic':              'sr_CS.ISO8859-5',\n    'sr_yu.microsoftcp1251@cyrillic':       'sr_CS.CP1251',\n    'sr_yu.utf8':                           'sr_RS.UTF-8',\n    'sr_yu.utf8@cyrillic':                  'sr_RS.UTF-8',\n    'sr_yu@cyrillic':                       'sr_RS.UTF-8',\n    'ss':                                   'ss_ZA.ISO8859-1',\n    'ss_za':                                'ss_ZA.ISO8859-1',\n    'ss_za.iso88591':                       'ss_ZA.ISO8859-1',\n    'st':                                   'st_ZA.ISO8859-1',\n    'st_za':                                'st_ZA.ISO8859-1',\n    'st_za.iso88591':                       'st_ZA.ISO8859-1',\n    'sv':                                   'sv_SE.ISO8859-1',\n    'sv.iso885915':                         'sv_SE.ISO8859-15',\n    'sv_fi':                                'sv_FI.ISO8859-1',\n    'sv_fi.iso88591':                       'sv_FI.ISO8859-1',\n    'sv_fi.iso885915':                      'sv_FI.ISO8859-15',\n    'sv_fi.iso885915@euro':                 'sv_FI.ISO8859-15',\n    'sv_fi.utf8@euro':                      'sv_FI.UTF-8',\n    'sv_fi@euro':                           'sv_FI.ISO8859-15',\n    'sv_se':                                'sv_SE.ISO8859-1',\n    'sv_se.88591':                          'sv_SE.ISO8859-1',\n    'sv_se.iso88591':                       'sv_SE.ISO8859-1',\n    'sv_se.iso885915':                      'sv_SE.ISO8859-15',\n    'sv_se@euro':                           'sv_SE.ISO8859-15',\n    'sw_ke':                                'sw_KE.UTF-8',\n    'sw_tz':                                'sw_TZ.UTF-8',\n    'swedish':                              'sv_SE.ISO8859-1',\n    'swedish.iso88591':                     'sv_SE.ISO8859-1',\n    'szl_pl':                               'szl_PL.UTF-8',\n    'ta':                                   'ta_IN.TSCII-0',\n    'ta_in':                                'ta_IN.TSCII-0',\n    'ta_in.tscii':                          'ta_IN.TSCII-0',\n    'ta_in.tscii0':                         'ta_IN.TSCII-0',\n    'ta_lk':                                'ta_LK.UTF-8',\n    'te':                                   'te_IN.UTF-8',\n    'te_in':                                'te_IN.UTF-8',\n    'tg':                                   'tg_TJ.KOI8-C',\n    'tg_tj':                                'tg_TJ.KOI8-C',\n    'tg_tj.koi8c':                          'tg_TJ.KOI8-C',\n    'th':                                   'th_TH.ISO8859-11',\n    'th_th':                                'th_TH.ISO8859-11',\n    'th_th.iso885911':                      'th_TH.ISO8859-11',\n    'th_th.tactis':                         'th_TH.TIS620',\n    'th_th.tis620':                         'th_TH.TIS620',\n    'thai':                                 'th_TH.ISO8859-11',\n    'ti_er':                                'ti_ER.UTF-8',\n    'ti_et':                                'ti_ET.UTF-8',\n    'tig_er':                               'tig_ER.UTF-8',\n    'tk_tm':                                'tk_TM.UTF-8',\n    'tl':                                   'tl_PH.ISO8859-1',\n    'tl_ph':                                'tl_PH.ISO8859-1',\n    'tl_ph.iso88591':                       'tl_PH.ISO8859-1',\n    'tn':                                   'tn_ZA.ISO8859-15',\n    'tn_za':                                'tn_ZA.ISO8859-15',\n    'tn_za.iso885915':                      'tn_ZA.ISO8859-15',\n    'tr':                                   'tr_TR.ISO8859-9',\n    'tr_cy':                                'tr_CY.ISO8859-9',\n    'tr_tr':                                'tr_TR.ISO8859-9',\n    'tr_tr.iso88599':                       'tr_TR.ISO8859-9',\n    'ts':                                   'ts_ZA.ISO8859-1',\n    'ts_za':                                'ts_ZA.ISO8859-1',\n    'ts_za.iso88591':                       'ts_ZA.ISO8859-1',\n    'tt':                                   'tt_RU.TATAR-CYR',\n    'tt_ru':                                'tt_RU.TATAR-CYR',\n    'tt_ru.koi8c':                          'tt_RU.KOI8-C',\n    'tt_ru.tatarcyr':                       'tt_RU.TATAR-CYR',\n    'tt_ru@iqtelif':                        'tt_RU.UTF-8@iqtelif',\n    'turkish':                              'tr_TR.ISO8859-9',\n    'turkish.iso88599':                     'tr_TR.ISO8859-9',\n    'ug_cn':                                'ug_CN.UTF-8',\n    'uk':                                   'uk_UA.KOI8-U',\n    'uk_ua':                                'uk_UA.KOI8-U',\n    'uk_ua.cp1251':                         'uk_UA.CP1251',\n    'uk_ua.iso88595':                       'uk_UA.ISO8859-5',\n    'uk_ua.koi8u':                          'uk_UA.KOI8-U',\n    'uk_ua.microsoftcp1251':                'uk_UA.CP1251',\n    'univ':                                 'en_US.utf',\n    'universal':                            'en_US.utf',\n    'universal.utf8@ucs4':                  'en_US.UTF-8',\n    'unm_us':                               'unm_US.UTF-8',\n    'ur':                                   'ur_PK.CP1256',\n    'ur_in':                                'ur_IN.UTF-8',\n    'ur_pk':                                'ur_PK.CP1256',\n    'ur_pk.cp1256':                         'ur_PK.CP1256',\n    'ur_pk.microsoftcp1256':                'ur_PK.CP1256',\n    'uz':                                   'uz_UZ.UTF-8',\n    'uz_uz':                                'uz_UZ.UTF-8',\n    'uz_uz.iso88591':                       'uz_UZ.ISO8859-1',\n    'uz_uz.utf8@cyrillic':                  'uz_UZ.UTF-8',\n    'uz_uz@cyrillic':                       'uz_UZ.UTF-8',\n    've':                                   've_ZA.UTF-8',\n    've_za':                                've_ZA.UTF-8',\n    'vi':                                   'vi_VN.TCVN',\n    'vi_vn':                                'vi_VN.TCVN',\n    'vi_vn.tcvn':                           'vi_VN.TCVN',\n    'vi_vn.tcvn5712':                       'vi_VN.TCVN',\n    'vi_vn.viscii':                         'vi_VN.VISCII',\n    'vi_vn.viscii111':                      'vi_VN.VISCII',\n    'wa':                                   'wa_BE.ISO8859-1',\n    'wa_be':                                'wa_BE.ISO8859-1',\n    'wa_be.iso88591':                       'wa_BE.ISO8859-1',\n    'wa_be.iso885915':                      'wa_BE.ISO8859-15',\n    'wa_be.iso885915@euro':                 'wa_BE.ISO8859-15',\n    'wa_be@euro':                           'wa_BE.ISO8859-15',\n    'wae_ch':                               'wae_CH.UTF-8',\n    'wal_et':                               'wal_ET.UTF-8',\n    'wo_sn':                                'wo_SN.UTF-8',\n    'xh':                                   'xh_ZA.ISO8859-1',\n    'xh_za':                                'xh_ZA.ISO8859-1',\n    'xh_za.iso88591':                       'xh_ZA.ISO8859-1',\n    'yi':                                   'yi_US.CP1255',\n    'yi_us':                                'yi_US.CP1255',\n    'yi_us.cp1255':                         'yi_US.CP1255',\n    'yi_us.microsoftcp1255':                'yi_US.CP1255',\n    'yo_ng':                                'yo_NG.UTF-8',\n    'yue_hk':                               'yue_HK.UTF-8',\n    'zh':                                   'zh_CN.eucCN',\n    'zh_cn':                                'zh_CN.gb2312',\n    'zh_cn.big5':                           'zh_TW.big5',\n    'zh_cn.euc':                            'zh_CN.eucCN',\n    'zh_cn.gb18030':                        'zh_CN.gb18030',\n    'zh_cn.gb2312':                         'zh_CN.gb2312',\n    'zh_cn.gbk':                            'zh_CN.gbk',\n    'zh_hk':                                'zh_HK.big5hkscs',\n    'zh_hk.big5':                           'zh_HK.big5',\n    'zh_hk.big5hk':                         'zh_HK.big5hkscs',\n    'zh_hk.big5hkscs':                      'zh_HK.big5hkscs',\n    'zh_sg':                                'zh_SG.GB2312',\n    'zh_sg.gbk':                            'zh_SG.GBK',\n    'zh_tw':                                'zh_TW.big5',\n    'zh_tw.big5':                           'zh_TW.big5',\n    'zh_tw.euc':                            'zh_TW.eucTW',\n    'zh_tw.euctw':                          'zh_TW.eucTW',\n    'zu':                                   'zu_ZA.ISO8859-1',\n    'zu_za':                                'zu_ZA.ISO8859-1',\n    'zu_za.iso88591':                       'zu_ZA.ISO8859-1',\n}\n\n#\n# This maps Windows language identifiers to locale strings.\n#\n# This list has been updated from\n# http://msdn.microsoft.com/library/default.asp?url=/library/en-us/intl/nls_238z.asp\n# to include every locale up to Windows Vista.\n#\n# NOTE: this mapping is incomplete.  If your language is missing, please\n# submit a bug report to the Python bug tracker at http://bugs.python.org/\n# Make sure you include the missing language identifier and the suggested\n# locale code.\n#\n\nwindows_locale = {\n    0x0436: \"af_ZA\", # Afrikaans\n    0x041c: \"sq_AL\", # Albanian\n    0x0484: \"gsw_FR\",# Alsatian - France\n    0x045e: \"am_ET\", # Amharic - Ethiopia\n    0x0401: \"ar_SA\", # Arabic - Saudi Arabia\n    0x0801: \"ar_IQ\", # Arabic - Iraq\n    0x0c01: \"ar_EG\", # Arabic - Egypt\n    0x1001: \"ar_LY\", # Arabic - Libya\n    0x1401: \"ar_DZ\", # Arabic - Algeria\n    0x1801: \"ar_MA\", # Arabic - Morocco\n    0x1c01: \"ar_TN\", # Arabic - Tunisia\n    0x2001: \"ar_OM\", # Arabic - Oman\n    0x2401: \"ar_YE\", # Arabic - Yemen\n    0x2801: \"ar_SY\", # Arabic - Syria\n    0x2c01: \"ar_JO\", # Arabic - Jordan\n    0x3001: \"ar_LB\", # Arabic - Lebanon\n    0x3401: \"ar_KW\", # Arabic - Kuwait\n    0x3801: \"ar_AE\", # Arabic - United Arab Emirates\n    0x3c01: \"ar_BH\", # Arabic - Bahrain\n    0x4001: \"ar_QA\", # Arabic - Qatar\n    0x042b: \"hy_AM\", # Armenian\n    0x044d: \"as_IN\", # Assamese - India\n    0x042c: \"az_AZ\", # Azeri - Latin\n    0x082c: \"az_AZ\", # Azeri - Cyrillic\n    0x046d: \"ba_RU\", # Bashkir\n    0x042d: \"eu_ES\", # Basque - Russia\n    0x0423: \"be_BY\", # Belarusian\n    0x0445: \"bn_IN\", # Begali\n    0x201a: \"bs_BA\", # Bosnian - Cyrillic\n    0x141a: \"bs_BA\", # Bosnian - Latin\n    0x047e: \"br_FR\", # Breton - France\n    0x0402: \"bg_BG\", # Bulgarian\n#    0x0455: \"my_MM\", # Burmese - Not supported\n    0x0403: \"ca_ES\", # Catalan\n    0x0004: \"zh_CHS\",# Chinese - Simplified\n    0x0404: \"zh_TW\", # Chinese - Taiwan\n    0x0804: \"zh_CN\", # Chinese - PRC\n    0x0c04: \"zh_HK\", # Chinese - Hong Kong S.A.R.\n    0x1004: \"zh_SG\", # Chinese - Singapore\n    0x1404: \"zh_MO\", # Chinese - Macao S.A.R.\n    0x7c04: \"zh_CHT\",# Chinese - Traditional\n    0x0483: \"co_FR\", # Corsican - France\n    0x041a: \"hr_HR\", # Croatian\n    0x101a: \"hr_BA\", # Croatian - Bosnia\n    0x0405: \"cs_CZ\", # Czech\n    0x0406: \"da_DK\", # Danish\n    0x048c: \"gbz_AF\",# Dari - Afghanistan\n    0x0465: \"div_MV\",# Divehi - Maldives\n    0x0413: \"nl_NL\", # Dutch - The Netherlands\n    0x0813: \"nl_BE\", # Dutch - Belgium\n    0x0409: \"en_US\", # English - United States\n    0x0809: \"en_GB\", # English - United Kingdom\n    0x0c09: \"en_AU\", # English - Australia\n    0x1009: \"en_CA\", # English - Canada\n    0x1409: \"en_NZ\", # English - New Zealand\n    0x1809: \"en_IE\", # English - Ireland\n    0x1c09: \"en_ZA\", # English - South Africa\n    0x2009: \"en_JA\", # English - Jamaica\n    0x2409: \"en_CB\", # English - Carribbean\n    0x2809: \"en_BZ\", # English - Belize\n    0x2c09: \"en_TT\", # English - Trinidad\n    0x3009: \"en_ZW\", # English - Zimbabwe\n    0x3409: \"en_PH\", # English - Philippines\n    0x4009: \"en_IN\", # English - India\n    0x4409: \"en_MY\", # English - Malaysia\n    0x4809: \"en_IN\", # English - Singapore\n    0x0425: \"et_EE\", # Estonian\n    0x0438: \"fo_FO\", # Faroese\n    0x0464: \"fil_PH\",# Filipino\n    0x040b: \"fi_FI\", # Finnish\n    0x040c: \"fr_FR\", # French - France\n    0x080c: \"fr_BE\", # French - Belgium\n    0x0c0c: \"fr_CA\", # French - Canada\n    0x100c: \"fr_CH\", # French - Switzerland\n    0x140c: \"fr_LU\", # French - Luxembourg\n    0x180c: \"fr_MC\", # French - Monaco\n    0x0462: \"fy_NL\", # Frisian - Netherlands\n    0x0456: \"gl_ES\", # Galician\n    0x0437: \"ka_GE\", # Georgian\n    0x0407: \"de_DE\", # German - Germany\n    0x0807: \"de_CH\", # German - Switzerland\n    0x0c07: \"de_AT\", # German - Austria\n    0x1007: \"de_LU\", # German - Luxembourg\n    0x1407: \"de_LI\", # German - Liechtenstein\n    0x0408: \"el_GR\", # Greek\n    0x046f: \"kl_GL\", # Greenlandic - Greenland\n    0x0447: \"gu_IN\", # Gujarati\n    0x0468: \"ha_NG\", # Hausa - Latin\n    0x040d: \"he_IL\", # Hebrew\n    0x0439: \"hi_IN\", # Hindi\n    0x040e: \"hu_HU\", # Hungarian\n    0x040f: \"is_IS\", # Icelandic\n    0x0421: \"id_ID\", # Indonesian\n    0x045d: \"iu_CA\", # Inuktitut - Syllabics\n    0x085d: \"iu_CA\", # Inuktitut - Latin\n    0x083c: \"ga_IE\", # Irish - Ireland\n    0x0410: \"it_IT\", # Italian - Italy\n    0x0810: \"it_CH\", # Italian - Switzerland\n    0x0411: \"ja_JP\", # Japanese\n    0x044b: \"kn_IN\", # Kannada - India\n    0x043f: \"kk_KZ\", # Kazakh\n    0x0453: \"kh_KH\", # Khmer - Cambodia\n    0x0486: \"qut_GT\",# K'iche - Guatemala\n    0x0487: \"rw_RW\", # Kinyarwanda - Rwanda\n    0x0457: \"kok_IN\",# Konkani\n    0x0412: \"ko_KR\", # Korean\n    0x0440: \"ky_KG\", # Kyrgyz\n    0x0454: \"lo_LA\", # Lao - Lao PDR\n    0x0426: \"lv_LV\", # Latvian\n    0x0427: \"lt_LT\", # Lithuanian\n    0x082e: \"dsb_DE\",# Lower Sorbian - Germany\n    0x046e: \"lb_LU\", # Luxembourgish\n    0x042f: \"mk_MK\", # FYROM Macedonian\n    0x043e: \"ms_MY\", # Malay - Malaysia\n    0x083e: \"ms_BN\", # Malay - Brunei Darussalam\n    0x044c: \"ml_IN\", # Malayalam - India\n    0x043a: \"mt_MT\", # Maltese\n    0x0481: \"mi_NZ\", # Maori\n    0x047a: \"arn_CL\",# Mapudungun\n    0x044e: \"mr_IN\", # Marathi\n    0x047c: \"moh_CA\",# Mohawk - Canada\n    0x0450: \"mn_MN\", # Mongolian - Cyrillic\n    0x0850: \"mn_CN\", # Mongolian - PRC\n    0x0461: \"ne_NP\", # Nepali\n    0x0414: \"nb_NO\", # Norwegian - Bokmal\n    0x0814: \"nn_NO\", # Norwegian - Nynorsk\n    0x0482: \"oc_FR\", # Occitan - France\n    0x0448: \"or_IN\", # Oriya - India\n    0x0463: \"ps_AF\", # Pashto - Afghanistan\n    0x0429: \"fa_IR\", # Persian\n    0x0415: \"pl_PL\", # Polish\n    0x0416: \"pt_BR\", # Portuguese - Brazil\n    0x0816: \"pt_PT\", # Portuguese - Portugal\n    0x0446: \"pa_IN\", # Punjabi\n    0x046b: \"quz_BO\",# Quechua (Bolivia)\n    0x086b: \"quz_EC\",# Quechua (Ecuador)\n    0x0c6b: \"quz_PE\",# Quechua (Peru)\n    0x0418: \"ro_RO\", # Romanian - Romania\n    0x0417: \"rm_CH\", # Romansh\n    0x0419: \"ru_RU\", # Russian\n    0x243b: \"smn_FI\",# Sami Finland\n    0x103b: \"smj_NO\",# Sami Norway\n    0x143b: \"smj_SE\",# Sami Sweden\n    0x043b: \"se_NO\", # Sami Northern Norway\n    0x083b: \"se_SE\", # Sami Northern Sweden\n    0x0c3b: \"se_FI\", # Sami Northern Finland\n    0x203b: \"sms_FI\",# Sami Skolt\n    0x183b: \"sma_NO\",# Sami Southern Norway\n    0x1c3b: \"sma_SE\",# Sami Southern Sweden\n    0x044f: \"sa_IN\", # Sanskrit\n    0x0c1a: \"sr_SP\", # Serbian - Cyrillic\n    0x1c1a: \"sr_BA\", # Serbian - Bosnia Cyrillic\n    0x081a: \"sr_SP\", # Serbian - Latin\n    0x181a: \"sr_BA\", # Serbian - Bosnia Latin\n    0x045b: \"si_LK\", # Sinhala - Sri Lanka\n    0x046c: \"ns_ZA\", # Northern Sotho\n    0x0432: \"tn_ZA\", # Setswana - Southern Africa\n    0x041b: \"sk_SK\", # Slovak\n    0x0424: \"sl_SI\", # Slovenian\n    0x040a: \"es_ES\", # Spanish - Spain\n    0x080a: \"es_MX\", # Spanish - Mexico\n    0x0c0a: \"es_ES\", # Spanish - Spain (Modern)\n    0x100a: \"es_GT\", # Spanish - Guatemala\n    0x140a: \"es_CR\", # Spanish - Costa Rica\n    0x180a: \"es_PA\", # Spanish - Panama\n    0x1c0a: \"es_DO\", # Spanish - Dominican Republic\n    0x200a: \"es_VE\", # Spanish - Venezuela\n    0x240a: \"es_CO\", # Spanish - Colombia\n    0x280a: \"es_PE\", # Spanish - Peru\n    0x2c0a: \"es_AR\", # Spanish - Argentina\n    0x300a: \"es_EC\", # Spanish - Ecuador\n    0x340a: \"es_CL\", # Spanish - Chile\n    0x380a: \"es_UR\", # Spanish - Uruguay\n    0x3c0a: \"es_PY\", # Spanish - Paraguay\n    0x400a: \"es_BO\", # Spanish - Bolivia\n    0x440a: \"es_SV\", # Spanish - El Salvador\n    0x480a: \"es_HN\", # Spanish - Honduras\n    0x4c0a: \"es_NI\", # Spanish - Nicaragua\n    0x500a: \"es_PR\", # Spanish - Puerto Rico\n    0x540a: \"es_US\", # Spanish - United States\n#    0x0430: \"\", # Sutu - Not supported\n    0x0441: \"sw_KE\", # Swahili\n    0x041d: \"sv_SE\", # Swedish - Sweden\n    0x081d: \"sv_FI\", # Swedish - Finland\n    0x045a: \"syr_SY\",# Syriac\n    0x0428: \"tg_TJ\", # Tajik - Cyrillic\n    0x085f: \"tmz_DZ\",# Tamazight - Latin\n    0x0449: \"ta_IN\", # Tamil\n    0x0444: \"tt_RU\", # Tatar\n    0x044a: \"te_IN\", # Telugu\n    0x041e: \"th_TH\", # Thai\n    0x0851: \"bo_BT\", # Tibetan - Bhutan\n    0x0451: \"bo_CN\", # Tibetan - PRC\n    0x041f: \"tr_TR\", # Turkish\n    0x0442: \"tk_TM\", # Turkmen - Cyrillic\n    0x0480: \"ug_CN\", # Uighur - Arabic\n    0x0422: \"uk_UA\", # Ukrainian\n    0x042e: \"wen_DE\",# Upper Sorbian - Germany\n    0x0420: \"ur_PK\", # Urdu\n    0x0820: \"ur_IN\", # Urdu - India\n    0x0443: \"uz_UZ\", # Uzbek - Latin\n    0x0843: \"uz_UZ\", # Uzbek - Cyrillic\n    0x042a: \"vi_VN\", # Vietnamese\n    0x0452: \"cy_GB\", # Welsh\n    0x0488: \"wo_SN\", # Wolof - Senegal\n    0x0434: \"xh_ZA\", # Xhosa - South Africa\n    0x0485: \"sah_RU\",# Yakut - Cyrillic\n    0x0478: \"ii_CN\", # Yi - PRC\n    0x046a: \"yo_NG\", # Yoruba - Nigeria\n    0x0435: \"zu_ZA\", # Zulu\n}\n\ndef _print_locale():\n\n    \"\"\" Test function.\n    \"\"\"\n    categories = {}\n    def _init_categories(categories=categories):\n        for k,v in globals().items():\n            if k[:3] == 'LC_':\n                categories[k] = v\n    _init_categories()\n    del categories['LC_ALL']\n\n    print 'Locale defaults as determined by getdefaultlocale():'\n    print '-'*72\n    lang, enc = getdefaultlocale()\n    print 'Language: ', lang or '(undefined)'\n    print 'Encoding: ', enc or '(undefined)'\n    print\n\n    print 'Locale settings on startup:'\n    print '-'*72\n    for name,category in categories.items():\n        print name, '...'\n        lang, enc = getlocale(category)\n        print '   Language: ', lang or '(undefined)'\n        print '   Encoding: ', enc or '(undefined)'\n        print\n\n    print\n    print 'Locale settings after calling resetlocale():'\n    print '-'*72\n    resetlocale()\n    for name,category in categories.items():\n        print name, '...'\n        lang, enc = getlocale(category)\n        print '   Language: ', lang or '(undefined)'\n        print '   Encoding: ', enc or '(undefined)'\n        print\n\n    try:\n        setlocale(LC_ALL, \"\")\n    except:\n        print 'NOTE:'\n        print 'setlocale(LC_ALL, \"\") does not support the default locale'\n        print 'given in the OS environment variables.'\n    else:\n        print\n        print 'Locale settings after calling setlocale(LC_ALL, \"\"):'\n        print '-'*72\n        for name,category in categories.items():\n            print name, '...'\n            lang, enc = getlocale(category)\n            print '   Language: ', lang or '(undefined)'\n            print '   Encoding: ', enc or '(undefined)'\n            print\n\n###\n\ntry:\n    LC_MESSAGES\nexcept NameError:\n    pass\nelse:\n    __all__.append(\"LC_MESSAGES\")\n\nif __name__=='__main__':\n    print 'Locale aliasing:'\n    print\n    _print_locale()\n    print\n    print 'Number formatting:'\n    print\n    _test()\n", 
    "logging.__init__": "# Copyright 2001-2014 by Vinay Sajip. All Rights Reserved.\n#\n# Permission to use, copy, modify, and distribute this software and its\n# documentation for any purpose and without fee is hereby granted,\n# provided that the above copyright notice appear in all copies and that\n# both that copyright notice and this permission notice appear in\n# supporting documentation, and that the name of Vinay Sajip\n# not be used in advertising or publicity pertaining to distribution\n# of the software without specific, written prior permission.\n# VINAY SAJIP DISCLAIMS ALL WARRANTIES WITH REGARD TO THIS SOFTWARE, INCLUDING\n# ALL IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS. IN NO EVENT SHALL\n# VINAY SAJIP BE LIABLE FOR ANY SPECIAL, INDIRECT OR CONSEQUENTIAL DAMAGES OR\n# ANY DAMAGES WHATSOEVER RESULTING FROM LOSS OF USE, DATA OR PROFITS, WHETHER\n# IN AN ACTION OF CONTRACT, NEGLIGENCE OR OTHER TORTIOUS ACTION, ARISING OUT\n# OF OR IN CONNECTION WITH THE USE OR PERFORMANCE OF THIS SOFTWARE.\n\n\"\"\"\nLogging package for Python. Based on PEP 282 and comments thereto in\ncomp.lang.python.\n\nCopyright (C) 2001-2014 Vinay Sajip. All Rights Reserved.\n\nTo use, simply 'import logging' and log away!\n\"\"\"\n\nimport sys, os, time, cStringIO, traceback, warnings, weakref, collections\n\n__all__ = ['BASIC_FORMAT', 'BufferingFormatter', 'CRITICAL', 'DEBUG', 'ERROR',\n           'FATAL', 'FileHandler', 'Filter', 'Formatter', 'Handler', 'INFO',\n           'LogRecord', 'Logger', 'LoggerAdapter', 'NOTSET', 'NullHandler',\n           'StreamHandler', 'WARN', 'WARNING', 'addLevelName', 'basicConfig',\n           'captureWarnings', 'critical', 'debug', 'disable', 'error',\n           'exception', 'fatal', 'getLevelName', 'getLogger', 'getLoggerClass',\n           'info', 'log', 'makeLogRecord', 'setLoggerClass', 'warn', 'warning']\n\ntry:\n    import codecs\nexcept ImportError:\n    codecs = None\n\ntry:\n    import thread\n    import threading\nexcept ImportError:\n    thread = None\n\n__author__  = \"Vinay Sajip <vinay_sajip@red-dove.com>\"\n__status__  = \"production\"\n# Note: the attributes below are no longer maintained.\n__version__ = \"0.5.1.2\"\n__date__    = \"07 February 2010\"\n\n#---------------------------------------------------------------------------\n#   Miscellaneous module data\n#---------------------------------------------------------------------------\ntry:\n    unicode\n    _unicode = True\nexcept NameError:\n    _unicode = False\n\n#\n# _srcfile is used when walking the stack to check when we've got the first\n# caller stack frame.\n#\nif hasattr(sys, 'frozen'): #support for py2exe\n    _srcfile = \"logging%s__init__%s\" % (os.sep, __file__[-4:])\nelif __file__[-4:].lower() in ['.pyc', '.pyo']:\n    _srcfile = __file__[:-4] + '.py'\nelse:\n    _srcfile = __file__\n_srcfile = os.path.normcase(_srcfile)\n\n# next bit filched from 1.5.2's inspect.py\ndef currentframe():\n    \"\"\"Return the frame object for the caller's stack frame.\"\"\"\n    try:\n        raise Exception\n    except:\n        return sys.exc_info()[2].tb_frame.f_back\n\nif hasattr(sys, '_getframe'): currentframe = lambda: sys._getframe(3)\n# done filching\n\n# _srcfile is only used in conjunction with sys._getframe().\n# To provide compatibility with older versions of Python, set _srcfile\n# to None if _getframe() is not available; this value will prevent\n# findCaller() from being called.\n#if not hasattr(sys, \"_getframe\"):\n#    _srcfile = None\n\n#\n#_startTime is used as the base when calculating the relative time of events\n#\n_startTime = time.time()\n\n#\n#raiseExceptions is used to see if exceptions during handling should be\n#propagated\n#\nraiseExceptions = 1\n\n#\n# If you don't want threading information in the log, set this to zero\n#\nlogThreads = 1\n\n#\n# If you don't want multiprocessing information in the log, set this to zero\n#\nlogMultiprocessing = 1\n\n#\n# If you don't want process information in the log, set this to zero\n#\nlogProcesses = 1\n\n#---------------------------------------------------------------------------\n#   Level related stuff\n#---------------------------------------------------------------------------\n#\n# Default levels and level names, these can be replaced with any positive set\n# of values having corresponding names. There is a pseudo-level, NOTSET, which\n# is only really there as a lower limit for user-defined levels. Handlers and\n# loggers are initialized with NOTSET so that they will log all messages, even\n# at user-defined levels.\n#\n\nCRITICAL = 50\nFATAL = CRITICAL\nERROR = 40\nWARNING = 30\nWARN = WARNING\nINFO = 20\nDEBUG = 10\nNOTSET = 0\n\n# NOTE(flaper87): This is different from\n# python's stdlib module since pypy's\n# dicts are much faster when their\n# keys are all of the same type.\n# Introduced in commit 9de7b40c586f\n_levelToName = {\n    CRITICAL: 'CRITICAL',\n    ERROR: 'ERROR',\n    WARNING: 'WARNING',\n    INFO: 'INFO',\n    DEBUG: 'DEBUG',\n    NOTSET: 'NOTSET',\n}\n_nameToLevel = {\n    'CRITICAL': CRITICAL,\n    'ERROR': ERROR,\n    'WARN': WARNING,\n    'WARNING': WARNING,\n    'INFO': INFO,\n    'DEBUG': DEBUG,\n    'NOTSET': NOTSET,\n}\n_levelNames = dict(_levelToName)\n_levelNames.update(_nameToLevel)   # backward compatibility\n\ndef getLevelName(level):\n    \"\"\"\n    Return the textual representation of logging level 'level'.\n\n    If the level is one of the predefined levels (CRITICAL, ERROR, WARNING,\n    INFO, DEBUG) then you get the corresponding string. If you have\n    associated levels with names using addLevelName then the name you have\n    associated with 'level' is returned.\n\n    If a numeric value corresponding to one of the defined levels is passed\n    in, the corresponding string representation is returned.\n\n    Otherwise, the string \"Level %s\" % level is returned.\n    \"\"\"\n\n    # NOTE(flaper87): Check also in _nameToLevel\n    # if value is None.\n    return (_levelToName.get(level) or\n            _nameToLevel.get(level, (\"Level %s\" % level)))\n\ndef addLevelName(level, levelName):\n    \"\"\"\n    Associate 'levelName' with 'level'.\n\n    This is used when converting levels to text during message formatting.\n    \"\"\"\n    _acquireLock()\n    try:    #unlikely to cause an exception, but you never know...\n        _levelToName[level] = levelName\n        _nameToLevel[levelName] = level\n    finally:\n        _releaseLock()\n\ndef _checkLevel(level):\n    if isinstance(level, (int, long)):\n        rv = level\n    elif str(level) == level:\n        if level not in _nameToLevel:\n            raise ValueError(\"Unknown level: %r\" % level)\n        rv = _nameToLevel[level]\n    else:\n        raise TypeError(\"Level not an integer or a valid string: %r\" % level)\n    return rv\n\n#---------------------------------------------------------------------------\n#   Thread-related stuff\n#---------------------------------------------------------------------------\n\n#\n#_lock is used to serialize access to shared data structures in this module.\n#This needs to be an RLock because fileConfig() creates and configures\n#Handlers, and so might arbitrary user threads. Since Handler code updates the\n#shared dictionary _handlers, it needs to acquire the lock. But if configuring,\n#the lock would already have been acquired - so we need an RLock.\n#The same argument applies to Loggers and Manager.loggerDict.\n#\nif thread:\n    _lock = threading.RLock()\nelse:\n    _lock = None\n\ndef _acquireLock():\n    \"\"\"\n    Acquire the module-level lock for serializing access to shared data.\n\n    This should be released with _releaseLock().\n    \"\"\"\n    if _lock:\n        _lock.acquire()\n\ndef _releaseLock():\n    \"\"\"\n    Release the module-level lock acquired by calling _acquireLock().\n    \"\"\"\n    if _lock:\n        _lock.release()\n\n#---------------------------------------------------------------------------\n#   The logging record\n#---------------------------------------------------------------------------\n\nclass LogRecord(object):\n    \"\"\"\n    A LogRecord instance represents an event being logged.\n\n    LogRecord instances are created every time something is logged. They\n    contain all the information pertinent to the event being logged. The\n    main information passed in is in msg and args, which are combined\n    using str(msg) % args to create the message field of the record. The\n    record also includes information such as when the record was created,\n    the source line where the logging call was made, and any exception\n    information to be logged.\n    \"\"\"\n    def __init__(self, name, level, pathname, lineno,\n                 msg, args, exc_info, func=None):\n        \"\"\"\n        Initialize a logging record with interesting information.\n        \"\"\"\n        ct = time.time()\n        self.name = name\n        self.msg = msg\n        #\n        # The following statement allows passing of a dictionary as a sole\n        # argument, so that you can do something like\n        #  logging.debug(\"a %(a)d b %(b)s\", {'a':1, 'b':2})\n        # Suggested by Stefan Behnel.\n        # Note that without the test for args[0], we get a problem because\n        # during formatting, we test to see if the arg is present using\n        # 'if self.args:'. If the event being logged is e.g. 'Value is %d'\n        # and if the passed arg fails 'if self.args:' then no formatting\n        # is done. For example, logger.warn('Value is %d', 0) would log\n        # 'Value is %d' instead of 'Value is 0'.\n        # For the use case of passing a dictionary, this should not be a\n        # problem.\n        # Issue #21172: a request was made to relax the isinstance check\n        # to hasattr(args[0], '__getitem__'). However, the docs on string\n        # formatting still seem to suggest a mapping object is required.\n        # Thus, while not removing the isinstance check, it does now look\n        # for collections.Mapping rather than, as before, dict.\n        if (args and len(args) == 1 and isinstance(args[0], collections.Mapping)\n            and args[0]):\n            args = args[0]\n        self.args = args\n        self.levelname = getLevelName(level)\n        self.levelno = level\n        self.pathname = pathname\n        try:\n            self.filename = os.path.basename(pathname)\n            self.module = os.path.splitext(self.filename)[0]\n        except (TypeError, ValueError, AttributeError):\n            self.filename = pathname\n            self.module = \"Unknown module\"\n        self.exc_info = exc_info\n        self.exc_text = None      # used to cache the traceback text\n        self.lineno = lineno\n        self.funcName = func\n        self.created = ct\n        self.msecs = (ct - int(ct)) * 1000\n        self.relativeCreated = (self.created - _startTime) * 1000\n        if logThreads and thread:\n            self.thread = thread.get_ident()\n            self.threadName = threading.current_thread().name\n        else:\n            self.thread = None\n            self.threadName = None\n        if not logMultiprocessing:\n            self.processName = None\n        else:\n            self.processName = 'MainProcess'\n            mp = sys.modules.get('multiprocessing')\n            if mp is not None:\n                # Errors may occur if multiprocessing has not finished loading\n                # yet - e.g. if a custom import hook causes third-party code\n                # to run when multiprocessing calls import. See issue 8200\n                # for an example\n                try:\n                    self.processName = mp.current_process().name\n                except StandardError:\n                    pass\n        if logProcesses and hasattr(os, 'getpid'):\n            self.process = os.getpid()\n        else:\n            self.process = None\n\n    def __str__(self):\n        return '<LogRecord: %s, %s, %s, %s, \"%s\">'%(self.name, self.levelno,\n            self.pathname, self.lineno, self.msg)\n\n    def getMessage(self):\n        \"\"\"\n        Return the message for this LogRecord.\n\n        Return the message for this LogRecord after merging any user-supplied\n        arguments with the message.\n        \"\"\"\n        if not _unicode: #if no unicode support...\n            msg = str(self.msg)\n        else:\n            msg = self.msg\n            if not isinstance(msg, basestring):\n                try:\n                    msg = str(self.msg)\n                except UnicodeError:\n                    msg = self.msg      #Defer encoding till later\n        if self.args:\n            msg = msg % self.args\n        return msg\n\ndef makeLogRecord(dict):\n    \"\"\"\n    Make a LogRecord whose attributes are defined by the specified dictionary,\n    This function is useful for converting a logging event received over\n    a socket connection (which is sent as a dictionary) into a LogRecord\n    instance.\n    \"\"\"\n    rv = LogRecord(None, None, \"\", 0, \"\", (), None, None)\n    rv.__dict__.update(dict)\n    return rv\n\n#---------------------------------------------------------------------------\n#   Formatter classes and functions\n#---------------------------------------------------------------------------\n\nclass Formatter(object):\n    \"\"\"\n    Formatter instances are used to convert a LogRecord to text.\n\n    Formatters need to know how a LogRecord is constructed. They are\n    responsible for converting a LogRecord to (usually) a string which can\n    be interpreted by either a human or an external system. The base Formatter\n    allows a formatting string to be specified. If none is supplied, the\n    default value of \"%s(message)\\\\n\" is used.\n\n    The Formatter can be initialized with a format string which makes use of\n    knowledge of the LogRecord attributes - e.g. the default value mentioned\n    above makes use of the fact that the user's message and arguments are pre-\n    formatted into a LogRecord's message attribute. Currently, the useful\n    attributes in a LogRecord are described by:\n\n    %(name)s            Name of the logger (logging channel)\n    %(levelno)s         Numeric logging level for the message (DEBUG, INFO,\n                        WARNING, ERROR, CRITICAL)\n    %(levelname)s       Text logging level for the message (\"DEBUG\", \"INFO\",\n                        \"WARNING\", \"ERROR\", \"CRITICAL\")\n    %(pathname)s        Full pathname of the source file where the logging\n                        call was issued (if available)\n    %(filename)s        Filename portion of pathname\n    %(module)s          Module (name portion of filename)\n    %(lineno)d          Source line number where the logging call was issued\n                        (if available)\n    %(funcName)s        Function name\n    %(created)f         Time when the LogRecord was created (time.time()\n                        return value)\n    %(asctime)s         Textual time when the LogRecord was created\n    %(msecs)d           Millisecond portion of the creation time\n    %(relativeCreated)d Time in milliseconds when the LogRecord was created,\n                        relative to the time the logging module was loaded\n                        (typically at application startup time)\n    %(thread)d          Thread ID (if available)\n    %(threadName)s      Thread name (if available)\n    %(process)d         Process ID (if available)\n    %(message)s         The result of record.getMessage(), computed just as\n                        the record is emitted\n    \"\"\"\n\n    converter = time.localtime\n\n    def __init__(self, fmt=None, datefmt=None):\n        \"\"\"\n        Initialize the formatter with specified format strings.\n\n        Initialize the formatter either with the specified format string, or a\n        default as described above. Allow for specialized date formatting with\n        the optional datefmt argument (if omitted, you get the ISO8601 format).\n        \"\"\"\n        if fmt:\n            self._fmt = fmt\n        else:\n            self._fmt = \"%(message)s\"\n        self.datefmt = datefmt\n\n    def formatTime(self, record, datefmt=None):\n        \"\"\"\n        Return the creation time of the specified LogRecord as formatted text.\n\n        This method should be called from format() by a formatter which\n        wants to make use of a formatted time. This method can be overridden\n        in formatters to provide for any specific requirement, but the\n        basic behaviour is as follows: if datefmt (a string) is specified,\n        it is used with time.strftime() to format the creation time of the\n        record. Otherwise, the ISO8601 format is used. The resulting\n        string is returned. This function uses a user-configurable function\n        to convert the creation time to a tuple. By default, time.localtime()\n        is used; to change this for a particular formatter instance, set the\n        'converter' attribute to a function with the same signature as\n        time.localtime() or time.gmtime(). To change it for all formatters,\n        for example if you want all logging times to be shown in GMT,\n        set the 'converter' attribute in the Formatter class.\n        \"\"\"\n        ct = self.converter(record.created)\n        if datefmt:\n            s = time.strftime(datefmt, ct)\n        else:\n            t = time.strftime(\"%Y-%m-%d %H:%M:%S\", ct)\n            s = \"%s,%03d\" % (t, record.msecs)\n        return s\n\n    def formatException(self, ei):\n        \"\"\"\n        Format and return the specified exception information as a string.\n\n        This default implementation just uses\n        traceback.print_exception()\n        \"\"\"\n        sio = cStringIO.StringIO()\n        traceback.print_exception(ei[0], ei[1], ei[2], None, sio)\n        s = sio.getvalue()\n        sio.close()\n        if s[-1:] == \"\\n\":\n            s = s[:-1]\n        return s\n\n    def usesTime(self):\n        \"\"\"\n        Check if the format uses the creation time of the record.\n        \"\"\"\n        return self._fmt.find(\"%(asctime)\") >= 0\n\n    def format(self, record):\n        \"\"\"\n        Format the specified record as text.\n\n        The record's attribute dictionary is used as the operand to a\n        string formatting operation which yields the returned string.\n        Before formatting the dictionary, a couple of preparatory steps\n        are carried out. The message attribute of the record is computed\n        using LogRecord.getMessage(). If the formatting string uses the\n        time (as determined by a call to usesTime(), formatTime() is\n        called to format the event time. If there is exception information,\n        it is formatted using formatException() and appended to the message.\n        \"\"\"\n        record.message = record.getMessage()\n        if self.usesTime():\n            record.asctime = self.formatTime(record, self.datefmt)\n        s = self._fmt % record.__dict__\n        if record.exc_info:\n            # Cache the traceback text to avoid converting it multiple times\n            # (it's constant anyway)\n            if not record.exc_text:\n                record.exc_text = self.formatException(record.exc_info)\n        if record.exc_text:\n            if s[-1:] != \"\\n\":\n                s = s + \"\\n\"\n            try:\n                s = s + record.exc_text\n            except UnicodeError:\n                # Sometimes filenames have non-ASCII chars, which can lead\n                # to errors when s is Unicode and record.exc_text is str\n                # See issue 8924.\n                # We also use replace for when there are multiple\n                # encodings, e.g. UTF-8 for the filesystem and latin-1\n                # for a script. See issue 13232.\n                s = s + record.exc_text.decode(sys.getfilesystemencoding(),\n                                               'replace')\n        return s\n\n#\n#   The default formatter to use when no other is specified\n#\n_defaultFormatter = Formatter()\n\nclass BufferingFormatter(object):\n    \"\"\"\n    A formatter suitable for formatting a number of records.\n    \"\"\"\n    def __init__(self, linefmt=None):\n        \"\"\"\n        Optionally specify a formatter which will be used to format each\n        individual record.\n        \"\"\"\n        if linefmt:\n            self.linefmt = linefmt\n        else:\n            self.linefmt = _defaultFormatter\n\n    def formatHeader(self, records):\n        \"\"\"\n        Return the header string for the specified records.\n        \"\"\"\n        return \"\"\n\n    def formatFooter(self, records):\n        \"\"\"\n        Return the footer string for the specified records.\n        \"\"\"\n        return \"\"\n\n    def format(self, records):\n        \"\"\"\n        Format the specified records and return the result as a string.\n        \"\"\"\n        rv = \"\"\n        if len(records) > 0:\n            rv = rv + self.formatHeader(records)\n            for record in records:\n                rv = rv + self.linefmt.format(record)\n            rv = rv + self.formatFooter(records)\n        return rv\n\n#---------------------------------------------------------------------------\n#   Filter classes and functions\n#---------------------------------------------------------------------------\n\nclass Filter(object):\n    \"\"\"\n    Filter instances are used to perform arbitrary filtering of LogRecords.\n\n    Loggers and Handlers can optionally use Filter instances to filter\n    records as desired. The base filter class only allows events which are\n    below a certain point in the logger hierarchy. For example, a filter\n    initialized with \"A.B\" will allow events logged by loggers \"A.B\",\n    \"A.B.C\", \"A.B.C.D\", \"A.B.D\" etc. but not \"A.BB\", \"B.A.B\" etc. If\n    initialized with the empty string, all events are passed.\n    \"\"\"\n    def __init__(self, name=''):\n        \"\"\"\n        Initialize a filter.\n\n        Initialize with the name of the logger which, together with its\n        children, will have its events allowed through the filter. If no\n        name is specified, allow every event.\n        \"\"\"\n        self.name = name\n        self.nlen = len(name)\n\n    def filter(self, record):\n        \"\"\"\n        Determine if the specified record is to be logged.\n\n        Is the specified record to be logged? Returns 0 for no, nonzero for\n        yes. If deemed appropriate, the record may be modified in-place.\n        \"\"\"\n        if self.nlen == 0:\n            return 1\n        elif self.name == record.name:\n            return 1\n        elif record.name.find(self.name, 0, self.nlen) != 0:\n            return 0\n        return (record.name[self.nlen] == \".\")\n\nclass Filterer(object):\n    \"\"\"\n    A base class for loggers and handlers which allows them to share\n    common code.\n    \"\"\"\n    def __init__(self):\n        \"\"\"\n        Initialize the list of filters to be an empty list.\n        \"\"\"\n        self.filters = []\n\n    def addFilter(self, filter):\n        \"\"\"\n        Add the specified filter to this handler.\n        \"\"\"\n        if not (filter in self.filters):\n            self.filters.append(filter)\n\n    def removeFilter(self, filter):\n        \"\"\"\n        Remove the specified filter from this handler.\n        \"\"\"\n        if filter in self.filters:\n            self.filters.remove(filter)\n\n    def filter(self, record):\n        \"\"\"\n        Determine if a record is loggable by consulting all the filters.\n\n        The default is to allow the record to be logged; any filter can veto\n        this and the record is then dropped. Returns a zero value if a record\n        is to be dropped, else non-zero.\n        \"\"\"\n        rv = 1\n        for f in self.filters:\n            if not f.filter(record):\n                rv = 0\n                break\n        return rv\n\n#---------------------------------------------------------------------------\n#   Handler classes and functions\n#---------------------------------------------------------------------------\n\n_handlers = weakref.WeakValueDictionary()  #map of handler names to handlers\n_handlerList = [] # added to allow handlers to be removed in reverse of order initialized\n\ndef _removeHandlerRef(wr):\n    \"\"\"\n    Remove a handler reference from the internal cleanup list.\n    \"\"\"\n    # This function can be called during module teardown, when globals are\n    # set to None. It can also be called from another thread. So we need to\n    # pre-emptively grab the necessary globals and check if they're None,\n    # to prevent race conditions and failures during interpreter shutdown.\n    acquire, release, handlers = _acquireLock, _releaseLock, _handlerList\n    if acquire and release and handlers:\n        acquire()\n        try:\n            if wr in handlers:\n                handlers.remove(wr)\n        finally:\n            release()\n\ndef _addHandlerRef(handler):\n    \"\"\"\n    Add a handler to the internal cleanup list using a weak reference.\n    \"\"\"\n    _acquireLock()\n    try:\n        _handlerList.append(weakref.ref(handler, _removeHandlerRef))\n    finally:\n        _releaseLock()\n\nclass Handler(Filterer):\n    \"\"\"\n    Handler instances dispatch logging events to specific destinations.\n\n    The base handler class. Acts as a placeholder which defines the Handler\n    interface. Handlers can optionally use Formatter instances to format\n    records as desired. By default, no formatter is specified; in this case,\n    the 'raw' message as determined by record.message is logged.\n    \"\"\"\n    def __init__(self, level=NOTSET):\n        \"\"\"\n        Initializes the instance - basically setting the formatter to None\n        and the filter list to empty.\n        \"\"\"\n        Filterer.__init__(self)\n        self._name = None\n        self.level = _checkLevel(level)\n        self.formatter = None\n        # Add the handler to the global _handlerList (for cleanup on shutdown)\n        _addHandlerRef(self)\n        self.createLock()\n\n    def get_name(self):\n        return self._name\n\n    def set_name(self, name):\n        _acquireLock()\n        try:\n            if self._name in _handlers:\n                del _handlers[self._name]\n            self._name = name\n            if name:\n                _handlers[name] = self\n        finally:\n            _releaseLock()\n\n    name = property(get_name, set_name)\n\n    def createLock(self):\n        \"\"\"\n        Acquire a thread lock for serializing access to the underlying I/O.\n        \"\"\"\n        if thread:\n            self.lock = threading.RLock()\n        else:\n            self.lock = None\n\n    def acquire(self):\n        \"\"\"\n        Acquire the I/O thread lock.\n        \"\"\"\n        if self.lock:\n            self.lock.acquire()\n\n    def release(self):\n        \"\"\"\n        Release the I/O thread lock.\n        \"\"\"\n        if self.lock:\n            self.lock.release()\n\n    def setLevel(self, level):\n        \"\"\"\n        Set the logging level of this handler.\n        \"\"\"\n        self.level = _checkLevel(level)\n\n    def format(self, record):\n        \"\"\"\n        Format the specified record.\n\n        If a formatter is set, use it. Otherwise, use the default formatter\n        for the module.\n        \"\"\"\n        if self.formatter:\n            fmt = self.formatter\n        else:\n            fmt = _defaultFormatter\n        return fmt.format(record)\n\n    def emit(self, record):\n        \"\"\"\n        Do whatever it takes to actually log the specified logging record.\n\n        This version is intended to be implemented by subclasses and so\n        raises a NotImplementedError.\n        \"\"\"\n        raise NotImplementedError('emit must be implemented '\n                                  'by Handler subclasses')\n\n    def handle(self, record):\n        \"\"\"\n        Conditionally emit the specified logging record.\n\n        Emission depends on filters which may have been added to the handler.\n        Wrap the actual emission of the record with acquisition/release of\n        the I/O thread lock. Returns whether the filter passed the record for\n        emission.\n        \"\"\"\n        rv = self.filter(record)\n        if rv:\n            self.acquire()\n            try:\n                self.emit(record)\n            finally:\n                self.release()\n        return rv\n\n    def setFormatter(self, fmt):\n        \"\"\"\n        Set the formatter for this handler.\n        \"\"\"\n        self.formatter = fmt\n\n    def flush(self):\n        \"\"\"\n        Ensure all logging output has been flushed.\n\n        This version does nothing and is intended to be implemented by\n        subclasses.\n        \"\"\"\n        pass\n\n    def close(self):\n        \"\"\"\n        Tidy up any resources used by the handler.\n\n        This version removes the handler from an internal map of handlers,\n        _handlers, which is used for handler lookup by name. Subclasses\n        should ensure that this gets called from overridden close()\n        methods.\n        \"\"\"\n        #get the module data lock, as we're updating a shared structure.\n        _acquireLock()\n        try:    #unlikely to raise an exception, but you never know...\n            if self._name and self._name in _handlers:\n                del _handlers[self._name]\n        finally:\n            _releaseLock()\n\n    def handleError(self, record):\n        \"\"\"\n        Handle errors which occur during an emit() call.\n\n        This method should be called from handlers when an exception is\n        encountered during an emit() call. If raiseExceptions is false,\n        exceptions get silently ignored. This is what is mostly wanted\n        for a logging system - most users will not care about errors in\n        the logging system, they are more interested in application errors.\n        You could, however, replace this with a custom handler if you wish.\n        The record which was being processed is passed in to this method.\n        \"\"\"\n        if raiseExceptions and sys.stderr:  # see issue 13807\n            ei = sys.exc_info()\n            try:\n                traceback.print_exception(ei[0], ei[1], ei[2],\n                                          None, sys.stderr)\n                sys.stderr.write('Logged from file %s, line %s\\n' % (\n                                 record.filename, record.lineno))\n            except IOError:\n                pass    # see issue 5971\n            finally:\n                del ei\n\nclass StreamHandler(Handler):\n    \"\"\"\n    A handler class which writes logging records, appropriately formatted,\n    to a stream. Note that this class does not close the stream, as\n    sys.stdout or sys.stderr may be used.\n    \"\"\"\n\n    def __init__(self, stream=None):\n        \"\"\"\n        Initialize the handler.\n\n        If stream is not specified, sys.stderr is used.\n        \"\"\"\n        Handler.__init__(self)\n        if stream is None:\n            stream = sys.stderr\n        self.stream = stream\n\n    def flush(self):\n        \"\"\"\n        Flushes the stream.\n        \"\"\"\n        self.acquire()\n        try:\n            if self.stream and hasattr(self.stream, \"flush\"):\n                self.stream.flush()\n        finally:\n            self.release()\n\n    def emit(self, record):\n        \"\"\"\n        Emit a record.\n\n        If a formatter is specified, it is used to format the record.\n        The record is then written to the stream with a trailing newline.  If\n        exception information is present, it is formatted using\n        traceback.print_exception and appended to the stream.  If the stream\n        has an 'encoding' attribute, it is used to determine how to do the\n        output to the stream.\n        \"\"\"\n        try:\n            msg = self.format(record)\n            stream = self.stream\n            fs = \"%s\\n\"\n            if not _unicode: #if no unicode support...\n                stream.write(fs % msg)\n            else:\n                try:\n                    if (isinstance(msg, unicode) and\n                        getattr(stream, 'encoding', None)):\n                        ufs = u'%s\\n'\n                        try:\n                            stream.write(ufs % msg)\n                        except UnicodeEncodeError:\n                            #Printing to terminals sometimes fails. For example,\n                            #with an encoding of 'cp1251', the above write will\n                            #work if written to a stream opened or wrapped by\n                            #the codecs module, but fail when writing to a\n                            #terminal even when the codepage is set to cp1251.\n                            #An extra encoding step seems to be needed.\n                            stream.write((ufs % msg).encode(stream.encoding))\n                    else:\n                        stream.write(fs % msg)\n                except UnicodeError:\n                    stream.write(fs % msg.encode(\"UTF-8\"))\n            self.flush()\n        except (KeyboardInterrupt, SystemExit):\n            raise\n        except:\n            self.handleError(record)\n\nclass FileHandler(StreamHandler):\n    \"\"\"\n    A handler class which writes formatted logging records to disk files.\n    \"\"\"\n    def __init__(self, filename, mode='a', encoding=None, delay=0):\n        \"\"\"\n        Open the specified file and use it as the stream for logging.\n        \"\"\"\n        #keep the absolute path, otherwise derived classes which use this\n        #may come a cropper when the current directory changes\n        if codecs is None:\n            encoding = None\n        self.baseFilename = os.path.abspath(filename)\n        self.mode = mode\n        self.encoding = encoding\n        self.delay = delay\n        if delay:\n            #We don't open the stream, but we still need to call the\n            #Handler constructor to set level, formatter, lock etc.\n            Handler.__init__(self)\n            self.stream = None\n        else:\n            StreamHandler.__init__(self, self._open())\n\n    def close(self):\n        \"\"\"\n        Closes the stream.\n        \"\"\"\n        self.acquire()\n        try:\n            if self.stream:\n                self.flush()\n                if hasattr(self.stream, \"close\"):\n                    self.stream.close()\n                self.stream = None\n            # Issue #19523: call unconditionally to\n            # prevent a handler leak when delay is set\n            StreamHandler.close(self)\n        finally:\n            self.release()\n\n    def _open(self):\n        \"\"\"\n        Open the current base file with the (original) mode and encoding.\n        Return the resulting stream.\n        \"\"\"\n        if self.encoding is None:\n            stream = open(self.baseFilename, self.mode)\n        else:\n            stream = codecs.open(self.baseFilename, self.mode, self.encoding)\n        return stream\n\n    def emit(self, record):\n        \"\"\"\n        Emit a record.\n\n        If the stream was not opened because 'delay' was specified in the\n        constructor, open it before calling the superclass's emit.\n        \"\"\"\n        if self.stream is None:\n            self.stream = self._open()\n        StreamHandler.emit(self, record)\n\n#---------------------------------------------------------------------------\n#   Manager classes and functions\n#---------------------------------------------------------------------------\n\nclass PlaceHolder(object):\n    \"\"\"\n    PlaceHolder instances are used in the Manager logger hierarchy to take\n    the place of nodes for which no loggers have been defined. This class is\n    intended for internal use only and not as part of the public API.\n    \"\"\"\n    def __init__(self, alogger):\n        \"\"\"\n        Initialize with the specified logger being a child of this placeholder.\n        \"\"\"\n        #self.loggers = [alogger]\n        self.loggerMap = { alogger : None }\n\n    def append(self, alogger):\n        \"\"\"\n        Add the specified logger as a child of this placeholder.\n        \"\"\"\n        #if alogger not in self.loggers:\n        if alogger not in self.loggerMap:\n            #self.loggers.append(alogger)\n            self.loggerMap[alogger] = None\n\n#\n#   Determine which class to use when instantiating loggers.\n#\n_loggerClass = None\n\ndef setLoggerClass(klass):\n    \"\"\"\n    Set the class to be used when instantiating a logger. The class should\n    define __init__() such that only a name argument is required, and the\n    __init__() should call Logger.__init__()\n    \"\"\"\n    if klass != Logger:\n        if not issubclass(klass, Logger):\n            raise TypeError(\"logger not derived from logging.Logger: \"\n                            + klass.__name__)\n    global _loggerClass\n    _loggerClass = klass\n\ndef getLoggerClass():\n    \"\"\"\n    Return the class to be used when instantiating a logger.\n    \"\"\"\n\n    return _loggerClass\n\nclass Manager(object):\n    \"\"\"\n    There is [under normal circumstances] just one Manager instance, which\n    holds the hierarchy of loggers.\n    \"\"\"\n    def __init__(self, rootnode):\n        \"\"\"\n        Initialize the manager with the root node of the logger hierarchy.\n        \"\"\"\n        self.root = rootnode\n        self.disable = 0\n        self.emittedNoHandlerWarning = 0\n        self.loggerDict = {}\n        self.loggerClass = None\n\n    def getLogger(self, name):\n        \"\"\"\n        Get a logger with the specified name (channel name), creating it\n        if it doesn't yet exist. This name is a dot-separated hierarchical\n        name, such as \"a\", \"a.b\", \"a.b.c\" or similar.\n\n        If a PlaceHolder existed for the specified name [i.e. the logger\n        didn't exist but a child of it did], replace it with the created\n        logger and fix up the parent/child references which pointed to the\n        placeholder to now point to the logger.\n        \"\"\"\n        rv = None\n        if not isinstance(name, basestring):\n            raise TypeError('A logger name must be string or Unicode')\n        if isinstance(name, unicode):\n            name = name.encode('utf-8')\n        _acquireLock()\n        try:\n            if name in self.loggerDict:\n                rv = self.loggerDict[name]\n                if isinstance(rv, PlaceHolder):\n                    ph = rv\n                    rv = (self.loggerClass or _loggerClass)(name)\n                    rv.manager = self\n                    self.loggerDict[name] = rv\n                    self._fixupChildren(ph, rv)\n                    self._fixupParents(rv)\n            else:\n                rv = (self.loggerClass or _loggerClass)(name)\n                rv.manager = self\n                self.loggerDict[name] = rv\n                self._fixupParents(rv)\n        finally:\n            _releaseLock()\n        return rv\n\n    def setLoggerClass(self, klass):\n        \"\"\"\n        Set the class to be used when instantiating a logger with this Manager.\n        \"\"\"\n        if klass != Logger:\n            if not issubclass(klass, Logger):\n                raise TypeError(\"logger not derived from logging.Logger: \"\n                                + klass.__name__)\n        self.loggerClass = klass\n\n    def _fixupParents(self, alogger):\n        \"\"\"\n        Ensure that there are either loggers or placeholders all the way\n        from the specified logger to the root of the logger hierarchy.\n        \"\"\"\n        name = alogger.name\n        i = name.rfind(\".\")\n        rv = None\n        while (i > 0) and not rv:\n            substr = name[:i]\n            if substr not in self.loggerDict:\n                self.loggerDict[substr] = PlaceHolder(alogger)\n            else:\n                obj = self.loggerDict[substr]\n                if isinstance(obj, Logger):\n                    rv = obj\n                else:\n                    assert isinstance(obj, PlaceHolder)\n                    obj.append(alogger)\n            i = name.rfind(\".\", 0, i - 1)\n        if not rv:\n            rv = self.root\n        alogger.parent = rv\n\n    def _fixupChildren(self, ph, alogger):\n        \"\"\"\n        Ensure that children of the placeholder ph are connected to the\n        specified logger.\n        \"\"\"\n        name = alogger.name\n        namelen = len(name)\n        for c in ph.loggerMap.keys():\n            #The if means ... if not c.parent.name.startswith(nm)\n            if c.parent.name[:namelen] != name:\n                alogger.parent = c.parent\n                c.parent = alogger\n\n#---------------------------------------------------------------------------\n#   Logger classes and functions\n#---------------------------------------------------------------------------\n\nclass Logger(Filterer):\n    \"\"\"\n    Instances of the Logger class represent a single logging channel. A\n    \"logging channel\" indicates an area of an application. Exactly how an\n    \"area\" is defined is up to the application developer. Since an\n    application can have any number of areas, logging channels are identified\n    by a unique string. Application areas can be nested (e.g. an area\n    of \"input processing\" might include sub-areas \"read CSV files\", \"read\n    XLS files\" and \"read Gnumeric files\"). To cater for this natural nesting,\n    channel names are organized into a namespace hierarchy where levels are\n    separated by periods, much like the Java or Python package namespace. So\n    in the instance given above, channel names might be \"input\" for the upper\n    level, and \"input.csv\", \"input.xls\" and \"input.gnu\" for the sub-levels.\n    There is no arbitrary limit to the depth of nesting.\n    \"\"\"\n    def __init__(self, name, level=NOTSET):\n        \"\"\"\n        Initialize the logger with a name and an optional level.\n        \"\"\"\n        Filterer.__init__(self)\n        self.name = name\n        self.level = _checkLevel(level)\n        self.parent = None\n        self.propagate = 1\n        self.handlers = []\n        self.disabled = 0\n\n    def setLevel(self, level):\n        \"\"\"\n        Set the logging level of this logger.\n        \"\"\"\n        self.level = _checkLevel(level)\n\n    def debug(self, msg, *args, **kwargs):\n        \"\"\"\n        Log 'msg % args' with severity 'DEBUG'.\n\n        To pass exception information, use the keyword argument exc_info with\n        a true value, e.g.\n\n        logger.debug(\"Houston, we have a %s\", \"thorny problem\", exc_info=1)\n        \"\"\"\n        if self.isEnabledFor(DEBUG):\n            self._log(DEBUG, msg, args, **kwargs)\n\n    def info(self, msg, *args, **kwargs):\n        \"\"\"\n        Log 'msg % args' with severity 'INFO'.\n\n        To pass exception information, use the keyword argument exc_info with\n        a true value, e.g.\n\n        logger.info(\"Houston, we have a %s\", \"interesting problem\", exc_info=1)\n        \"\"\"\n        if self.isEnabledFor(INFO):\n            self._log(INFO, msg, args, **kwargs)\n\n    def warning(self, msg, *args, **kwargs):\n        \"\"\"\n        Log 'msg % args' with severity 'WARNING'.\n\n        To pass exception information, use the keyword argument exc_info with\n        a true value, e.g.\n\n        logger.warning(\"Houston, we have a %s\", \"bit of a problem\", exc_info=1)\n        \"\"\"\n        if self.isEnabledFor(WARNING):\n            self._log(WARNING, msg, args, **kwargs)\n\n    warn = warning\n\n    def error(self, msg, *args, **kwargs):\n        \"\"\"\n        Log 'msg % args' with severity 'ERROR'.\n\n        To pass exception information, use the keyword argument exc_info with\n        a true value, e.g.\n\n        logger.error(\"Houston, we have a %s\", \"major problem\", exc_info=1)\n        \"\"\"\n        if self.isEnabledFor(ERROR):\n            self._log(ERROR, msg, args, **kwargs)\n\n    def exception(self, msg, *args, **kwargs):\n        \"\"\"\n        Convenience method for logging an ERROR with exception information.\n        \"\"\"\n        kwargs['exc_info'] = 1\n        self.error(msg, *args, **kwargs)\n\n    def critical(self, msg, *args, **kwargs):\n        \"\"\"\n        Log 'msg % args' with severity 'CRITICAL'.\n\n        To pass exception information, use the keyword argument exc_info with\n        a true value, e.g.\n\n        logger.critical(\"Houston, we have a %s\", \"major disaster\", exc_info=1)\n        \"\"\"\n        if self.isEnabledFor(CRITICAL):\n            self._log(CRITICAL, msg, args, **kwargs)\n\n    fatal = critical\n\n    def log(self, level, msg, *args, **kwargs):\n        \"\"\"\n        Log 'msg % args' with the integer severity 'level'.\n\n        To pass exception information, use the keyword argument exc_info with\n        a true value, e.g.\n\n        logger.log(level, \"We have a %s\", \"mysterious problem\", exc_info=1)\n        \"\"\"\n        if not isinstance(level, int):\n            if raiseExceptions:\n                raise TypeError(\"level must be an integer\")\n            else:\n                return\n        if self.isEnabledFor(level):\n            self._log(level, msg, args, **kwargs)\n\n    def findCaller(self):\n        \"\"\"\n        Find the stack frame of the caller so that we can note the source\n        file name, line number and function name.\n        \"\"\"\n        f = currentframe()\n        #On some versions of IronPython, currentframe() returns None if\n        #IronPython isn't run with -X:Frames.\n        if f is not None:\n            f = f.f_back\n        rv = \"(unknown file)\", 0, \"(unknown function)\"\n        while hasattr(f, \"f_code\"):\n            co = f.f_code\n            filename = os.path.normcase(co.co_filename)\n            if filename == _srcfile:\n                f = f.f_back\n                continue\n            rv = (co.co_filename, f.f_lineno, co.co_name)\n            break\n        return rv\n\n    def makeRecord(self, name, level, fn, lno, msg, args, exc_info, func=None, extra=None):\n        \"\"\"\n        A factory method which can be overridden in subclasses to create\n        specialized LogRecords.\n        \"\"\"\n        rv = LogRecord(name, level, fn, lno, msg, args, exc_info, func)\n        if extra is not None:\n            for key in extra:\n                if (key in [\"message\", \"asctime\"]) or (key in rv.__dict__):\n                    raise KeyError(\"Attempt to overwrite %r in LogRecord\" % key)\n                rv.__dict__[key] = extra[key]\n        return rv\n\n    def _log(self, level, msg, args, exc_info=None, extra=None):\n        \"\"\"\n        Low-level logging routine which creates a LogRecord and then calls\n        all the handlers of this logger to handle the record.\n        \"\"\"\n        if _srcfile:\n            #IronPython doesn't track Python frames, so findCaller raises an\n            #exception on some versions of IronPython. We trap it here so that\n            #IronPython can use logging.\n            try:\n                fn, lno, func = self.findCaller()\n            except ValueError:\n                fn, lno, func = \"(unknown file)\", 0, \"(unknown function)\"\n        else:\n            fn, lno, func = \"(unknown file)\", 0, \"(unknown function)\"\n        if exc_info:\n            if not isinstance(exc_info, tuple):\n                exc_info = sys.exc_info()\n        record = self.makeRecord(self.name, level, fn, lno, msg, args, exc_info, func, extra)\n        self.handle(record)\n\n    def handle(self, record):\n        \"\"\"\n        Call the handlers for the specified record.\n\n        This method is used for unpickled records received from a socket, as\n        well as those created locally. Logger-level filtering is applied.\n        \"\"\"\n        if (not self.disabled) and self.filter(record):\n            self.callHandlers(record)\n\n    def addHandler(self, hdlr):\n        \"\"\"\n        Add the specified handler to this logger.\n        \"\"\"\n        _acquireLock()\n        try:\n            if not (hdlr in self.handlers):\n                self.handlers.append(hdlr)\n        finally:\n            _releaseLock()\n\n    def removeHandler(self, hdlr):\n        \"\"\"\n        Remove the specified handler from this logger.\n        \"\"\"\n        _acquireLock()\n        try:\n            if hdlr in self.handlers:\n                self.handlers.remove(hdlr)\n        finally:\n            _releaseLock()\n\n    def callHandlers(self, record):\n        \"\"\"\n        Pass a record to all relevant handlers.\n\n        Loop through all handlers for this logger and its parents in the\n        logger hierarchy. If no handler was found, output a one-off error\n        message to sys.stderr. Stop searching up the hierarchy whenever a\n        logger with the \"propagate\" attribute set to zero is found - that\n        will be the last logger whose handlers are called.\n        \"\"\"\n        c = self\n        found = 0\n        while c:\n            for hdlr in c.handlers:\n                found = found + 1\n                if record.levelno >= hdlr.level:\n                    hdlr.handle(record)\n            if not c.propagate:\n                c = None    #break out\n            else:\n                c = c.parent\n        if (found == 0) and raiseExceptions and not self.manager.emittedNoHandlerWarning:\n            sys.stderr.write(\"No handlers could be found for logger\"\n                             \" \\\"%s\\\"\\n\" % self.name)\n            self.manager.emittedNoHandlerWarning = 1\n\n    def getEffectiveLevel(self):\n        \"\"\"\n        Get the effective level for this logger.\n\n        Loop through this logger and its parents in the logger hierarchy,\n        looking for a non-zero logging level. Return the first one found.\n        \"\"\"\n        logger = self\n        while logger:\n            if logger.level:\n                return logger.level\n            logger = logger.parent\n        return NOTSET\n\n    def isEnabledFor(self, level):\n        \"\"\"\n        Is this logger enabled for level 'level'?\n        \"\"\"\n        if self.manager.disable >= level:\n            return 0\n        return level >= self.getEffectiveLevel()\n\n    def getChild(self, suffix):\n        \"\"\"\n        Get a logger which is a descendant to this one.\n\n        This is a convenience method, such that\n\n        logging.getLogger('abc').getChild('def.ghi')\n\n        is the same as\n\n        logging.getLogger('abc.def.ghi')\n\n        It's useful, for example, when the parent logger is named using\n        __name__ rather than a literal string.\n        \"\"\"\n        if self.root is not self:\n            suffix = '.'.join((self.name, suffix))\n        return self.manager.getLogger(suffix)\n\nclass RootLogger(Logger):\n    \"\"\"\n    A root logger is not that different to any other logger, except that\n    it must have a logging level and there is only one instance of it in\n    the hierarchy.\n    \"\"\"\n    def __init__(self, level):\n        \"\"\"\n        Initialize the logger with the name \"root\".\n        \"\"\"\n        Logger.__init__(self, \"root\", level)\n\n_loggerClass = Logger\n\nclass LoggerAdapter(object):\n    \"\"\"\n    An adapter for loggers which makes it easier to specify contextual\n    information in logging output.\n    \"\"\"\n\n    def __init__(self, logger, extra):\n        \"\"\"\n        Initialize the adapter with a logger and a dict-like object which\n        provides contextual information. This constructor signature allows\n        easy stacking of LoggerAdapters, if so desired.\n\n        You can effectively pass keyword arguments as shown in the\n        following example:\n\n        adapter = LoggerAdapter(someLogger, dict(p1=v1, p2=\"v2\"))\n        \"\"\"\n        self.logger = logger\n        self.extra = extra\n\n    def process(self, msg, kwargs):\n        \"\"\"\n        Process the logging message and keyword arguments passed in to\n        a logging call to insert contextual information. You can either\n        manipulate the message itself, the keyword args or both. Return\n        the message and kwargs modified (or not) to suit your needs.\n\n        Normally, you'll only need to override this one method in a\n        LoggerAdapter subclass for your specific needs.\n        \"\"\"\n        kwargs[\"extra\"] = self.extra\n        return msg, kwargs\n\n    def debug(self, msg, *args, **kwargs):\n        \"\"\"\n        Delegate a debug call to the underlying logger, after adding\n        contextual information from this adapter instance.\n        \"\"\"\n        msg, kwargs = self.process(msg, kwargs)\n        self.logger.debug(msg, *args, **kwargs)\n\n    def info(self, msg, *args, **kwargs):\n        \"\"\"\n        Delegate an info call to the underlying logger, after adding\n        contextual information from this adapter instance.\n        \"\"\"\n        msg, kwargs = self.process(msg, kwargs)\n        self.logger.info(msg, *args, **kwargs)\n\n    def warning(self, msg, *args, **kwargs):\n        \"\"\"\n        Delegate a warning call to the underlying logger, after adding\n        contextual information from this adapter instance.\n        \"\"\"\n        msg, kwargs = self.process(msg, kwargs)\n        self.logger.warning(msg, *args, **kwargs)\n\n    def error(self, msg, *args, **kwargs):\n        \"\"\"\n        Delegate an error call to the underlying logger, after adding\n        contextual information from this adapter instance.\n        \"\"\"\n        msg, kwargs = self.process(msg, kwargs)\n        self.logger.error(msg, *args, **kwargs)\n\n    def exception(self, msg, *args, **kwargs):\n        \"\"\"\n        Delegate an exception call to the underlying logger, after adding\n        contextual information from this adapter instance.\n        \"\"\"\n        msg, kwargs = self.process(msg, kwargs)\n        kwargs[\"exc_info\"] = 1\n        self.logger.error(msg, *args, **kwargs)\n\n    def critical(self, msg, *args, **kwargs):\n        \"\"\"\n        Delegate a critical call to the underlying logger, after adding\n        contextual information from this adapter instance.\n        \"\"\"\n        msg, kwargs = self.process(msg, kwargs)\n        self.logger.critical(msg, *args, **kwargs)\n\n    def log(self, level, msg, *args, **kwargs):\n        \"\"\"\n        Delegate a log call to the underlying logger, after adding\n        contextual information from this adapter instance.\n        \"\"\"\n        msg, kwargs = self.process(msg, kwargs)\n        self.logger.log(level, msg, *args, **kwargs)\n\n    def isEnabledFor(self, level):\n        \"\"\"\n        See if the underlying logger is enabled for the specified level.\n        \"\"\"\n        return self.logger.isEnabledFor(level)\n\nroot = RootLogger(WARNING)\nLogger.root = root\nLogger.manager = Manager(Logger.root)\n\n#---------------------------------------------------------------------------\n# Configuration classes and functions\n#---------------------------------------------------------------------------\n\nBASIC_FORMAT = \"%(levelname)s:%(name)s:%(message)s\"\n\ndef basicConfig(**kwargs):\n    \"\"\"\n    Do basic configuration for the logging system.\n\n    This function does nothing if the root logger already has handlers\n    configured. It is a convenience method intended for use by simple scripts\n    to do one-shot configuration of the logging package.\n\n    The default behaviour is to create a StreamHandler which writes to\n    sys.stderr, set a formatter using the BASIC_FORMAT format string, and\n    add the handler to the root logger.\n\n    A number of optional keyword arguments may be specified, which can alter\n    the default behaviour.\n\n    filename  Specifies that a FileHandler be created, using the specified\n              filename, rather than a StreamHandler.\n    filemode  Specifies the mode to open the file, if filename is specified\n              (if filemode is unspecified, it defaults to 'a').\n    format    Use the specified format string for the handler.\n    datefmt   Use the specified date/time format.\n    level     Set the root logger level to the specified level.\n    stream    Use the specified stream to initialize the StreamHandler. Note\n              that this argument is incompatible with 'filename' - if both\n              are present, 'stream' is ignored.\n\n    Note that you could specify a stream created using open(filename, mode)\n    rather than passing the filename and mode in. However, it should be\n    remembered that StreamHandler does not close its stream (since it may be\n    using sys.stdout or sys.stderr), whereas FileHandler closes its stream\n    when the handler is closed.\n    \"\"\"\n    # Add thread safety in case someone mistakenly calls\n    # basicConfig() from multiple threads\n    _acquireLock()\n    try:\n        if len(root.handlers) == 0:\n            filename = kwargs.get(\"filename\")\n            if filename:\n                mode = kwargs.get(\"filemode\", 'a')\n                hdlr = FileHandler(filename, mode)\n            else:\n                stream = kwargs.get(\"stream\")\n                hdlr = StreamHandler(stream)\n            fs = kwargs.get(\"format\", BASIC_FORMAT)\n            dfs = kwargs.get(\"datefmt\", None)\n            fmt = Formatter(fs, dfs)\n            hdlr.setFormatter(fmt)\n            root.addHandler(hdlr)\n            level = kwargs.get(\"level\")\n            if level is not None:\n                root.setLevel(level)\n    finally:\n        _releaseLock()\n\n#---------------------------------------------------------------------------\n# Utility functions at module level.\n# Basically delegate everything to the root logger.\n#---------------------------------------------------------------------------\n\ndef getLogger(name=None):\n    \"\"\"\n    Return a logger with the specified name, creating it if necessary.\n\n    If no name is specified, return the root logger.\n    \"\"\"\n    if name:\n        return Logger.manager.getLogger(name)\n    else:\n        return root\n\n#def getRootLogger():\n#    \"\"\"\n#    Return the root logger.\n#\n#    Note that getLogger('') now does the same thing, so this function is\n#    deprecated and may disappear in the future.\n#    \"\"\"\n#    return root\n\ndef critical(msg, *args, **kwargs):\n    \"\"\"\n    Log a message with severity 'CRITICAL' on the root logger.\n    \"\"\"\n    if len(root.handlers) == 0:\n        basicConfig()\n    root.critical(msg, *args, **kwargs)\n\nfatal = critical\n\ndef error(msg, *args, **kwargs):\n    \"\"\"\n    Log a message with severity 'ERROR' on the root logger.\n    \"\"\"\n    if len(root.handlers) == 0:\n        basicConfig()\n    root.error(msg, *args, **kwargs)\n\ndef exception(msg, *args, **kwargs):\n    \"\"\"\n    Log a message with severity 'ERROR' on the root logger,\n    with exception information.\n    \"\"\"\n    kwargs['exc_info'] = 1\n    error(msg, *args, **kwargs)\n\ndef warning(msg, *args, **kwargs):\n    \"\"\"\n    Log a message with severity 'WARNING' on the root logger.\n    \"\"\"\n    if len(root.handlers) == 0:\n        basicConfig()\n    root.warning(msg, *args, **kwargs)\n\nwarn = warning\n\ndef info(msg, *args, **kwargs):\n    \"\"\"\n    Log a message with severity 'INFO' on the root logger.\n    \"\"\"\n    if len(root.handlers) == 0:\n        basicConfig()\n    root.info(msg, *args, **kwargs)\n\ndef debug(msg, *args, **kwargs):\n    \"\"\"\n    Log a message with severity 'DEBUG' on the root logger.\n    \"\"\"\n    if len(root.handlers) == 0:\n        basicConfig()\n    root.debug(msg, *args, **kwargs)\n\ndef log(level, msg, *args, **kwargs):\n    \"\"\"\n    Log 'msg % args' with the integer severity 'level' on the root logger.\n    \"\"\"\n    if len(root.handlers) == 0:\n        basicConfig()\n    root.log(level, msg, *args, **kwargs)\n\ndef disable(level):\n    \"\"\"\n    Disable all logging calls of severity 'level' and below.\n    \"\"\"\n    root.manager.disable = level\n\ndef shutdown(handlerList=_handlerList):\n    \"\"\"\n    Perform any cleanup actions in the logging system (e.g. flushing\n    buffers).\n\n    Should be called at application exit.\n    \"\"\"\n    for wr in reversed(handlerList[:]):\n        #errors might occur, for example, if files are locked\n        #we just ignore them if raiseExceptions is not set\n        try:\n            h = wr()\n            if h:\n                try:\n                    h.acquire()\n                    h.flush()\n                    h.close()\n                except (IOError, ValueError):\n                    # Ignore errors which might be caused\n                    # because handlers have been closed but\n                    # references to them are still around at\n                    # application exit.\n                    pass\n                finally:\n                    h.release()\n        except:\n            if raiseExceptions:\n                raise\n            #else, swallow\n\n#Let's try and shutdown automatically on application exit...\nimport atexit\natexit.register(shutdown)\n\n# Null handler\n\nclass NullHandler(Handler):\n    \"\"\"\n    This handler does nothing. It's intended to be used to avoid the\n    \"No handlers could be found for logger XXX\" one-off warning. This is\n    important for library code, which may contain code to log events. If a user\n    of the library does not configure logging, the one-off warning might be\n    produced; to avoid this, the library developer simply needs to instantiate\n    a NullHandler and add it to the top-level logger of the library module or\n    package.\n    \"\"\"\n    def handle(self, record):\n        pass\n\n    def emit(self, record):\n        pass\n\n    def createLock(self):\n        self.lock = None\n\n# Warnings integration\n\n_warnings_showwarning = None\n\ndef _showwarning(message, category, filename, lineno, file=None, line=None):\n    \"\"\"\n    Implementation of showwarnings which redirects to logging, which will first\n    check to see if the file parameter is None. If a file is specified, it will\n    delegate to the original warnings implementation of showwarning. Otherwise,\n    it will call warnings.formatwarning and will log the resulting string to a\n    warnings logger named \"py.warnings\" with level logging.WARNING.\n    \"\"\"\n    if file is not None:\n        if _warnings_showwarning is not None:\n            _warnings_showwarning(message, category, filename, lineno, file, line)\n    else:\n        s = warnings.formatwarning(message, category, filename, lineno, line)\n        logger = getLogger(\"py.warnings\")\n        if not logger.handlers:\n            logger.addHandler(NullHandler())\n        logger.warning(\"%s\", s)\n\ndef captureWarnings(capture):\n    \"\"\"\n    If capture is true, redirect all warnings to the logging package.\n    If capture is False, ensure that warnings are not redirected to logging\n    but to their original destinations.\n    \"\"\"\n    global _warnings_showwarning\n    if capture:\n        if _warnings_showwarning is None:\n            _warnings_showwarning = warnings.showwarning\n            warnings.showwarning = _showwarning\n    else:\n        if _warnings_showwarning is not None:\n            warnings.showwarning = _warnings_showwarning\n            _warnings_showwarning = None\n", 
    "marshal": "# Note that PyPy contains also a built-in module 'marshal' which will\n# hide this one if compiled in.\n\nfrom _marshal import __doc__\nfrom _marshal import *\n", 
    "opcode": "\"\"\"\nopcode module - potentially shared between dis and other modules which\noperate on bytecodes (e.g. peephole optimizers).\n\"\"\"\n\n__all__ = [\"cmp_op\", \"hasconst\", \"hasname\", \"hasjrel\", \"hasjabs\",\n           \"haslocal\", \"hascompare\", \"hasfree\", \"opname\", \"opmap\",\n           \"HAVE_ARGUMENT\", \"EXTENDED_ARG\"]\n\ncmp_op = ('<', '<=', '==', '!=', '>', '>=', 'in', 'not in', 'is',\n        'is not', 'exception match', 'BAD')\n\nhasconst = []\nhasname = []\nhasjrel = []\nhasjabs = []\nhaslocal = []\nhascompare = []\nhasfree = []\n\nopmap = {}\nopname = [''] * 256\nfor op in range(256): opname[op] = '<%r>' % (op,)\ndel op\n\ndef def_op(name, op):\n    opname[op] = name\n    opmap[name] = op\n\ndef name_op(name, op):\n    def_op(name, op)\n    hasname.append(op)\n\ndef jrel_op(name, op):\n    def_op(name, op)\n    hasjrel.append(op)\n\ndef jabs_op(name, op):\n    def_op(name, op)\n    hasjabs.append(op)\n\n# Instruction opcodes for compiled code\n# Blank lines correspond to available opcodes\n\ndef_op('STOP_CODE', 0)\ndef_op('POP_TOP', 1)\ndef_op('ROT_TWO', 2)\ndef_op('ROT_THREE', 3)\ndef_op('DUP_TOP', 4)\ndef_op('ROT_FOUR', 5)\n\ndef_op('NOP', 9)\ndef_op('UNARY_POSITIVE', 10)\ndef_op('UNARY_NEGATIVE', 11)\ndef_op('UNARY_NOT', 12)\ndef_op('UNARY_CONVERT', 13)\n\ndef_op('UNARY_INVERT', 15)\n\ndef_op('BINARY_POWER', 19)\ndef_op('BINARY_MULTIPLY', 20)\ndef_op('BINARY_DIVIDE', 21)\ndef_op('BINARY_MODULO', 22)\ndef_op('BINARY_ADD', 23)\ndef_op('BINARY_SUBTRACT', 24)\ndef_op('BINARY_SUBSCR', 25)\ndef_op('BINARY_FLOOR_DIVIDE', 26)\ndef_op('BINARY_TRUE_DIVIDE', 27)\ndef_op('INPLACE_FLOOR_DIVIDE', 28)\ndef_op('INPLACE_TRUE_DIVIDE', 29)\ndef_op('SLICE+0', 30)\ndef_op('SLICE+1', 31)\ndef_op('SLICE+2', 32)\ndef_op('SLICE+3', 33)\n\ndef_op('STORE_SLICE+0', 40)\ndef_op('STORE_SLICE+1', 41)\ndef_op('STORE_SLICE+2', 42)\ndef_op('STORE_SLICE+3', 43)\n\ndef_op('DELETE_SLICE+0', 50)\ndef_op('DELETE_SLICE+1', 51)\ndef_op('DELETE_SLICE+2', 52)\ndef_op('DELETE_SLICE+3', 53)\n\ndef_op('STORE_MAP', 54)\ndef_op('INPLACE_ADD', 55)\ndef_op('INPLACE_SUBTRACT', 56)\ndef_op('INPLACE_MULTIPLY', 57)\ndef_op('INPLACE_DIVIDE', 58)\ndef_op('INPLACE_MODULO', 59)\ndef_op('STORE_SUBSCR', 60)\ndef_op('DELETE_SUBSCR', 61)\ndef_op('BINARY_LSHIFT', 62)\ndef_op('BINARY_RSHIFT', 63)\ndef_op('BINARY_AND', 64)\ndef_op('BINARY_XOR', 65)\ndef_op('BINARY_OR', 66)\ndef_op('INPLACE_POWER', 67)\ndef_op('GET_ITER', 68)\n\ndef_op('PRINT_EXPR', 70)\ndef_op('PRINT_ITEM', 71)\ndef_op('PRINT_NEWLINE', 72)\ndef_op('PRINT_ITEM_TO', 73)\ndef_op('PRINT_NEWLINE_TO', 74)\ndef_op('INPLACE_LSHIFT', 75)\ndef_op('INPLACE_RSHIFT', 76)\ndef_op('INPLACE_AND', 77)\ndef_op('INPLACE_XOR', 78)\ndef_op('INPLACE_OR', 79)\ndef_op('BREAK_LOOP', 80)\ndef_op('WITH_CLEANUP', 81)\ndef_op('LOAD_LOCALS', 82)\ndef_op('RETURN_VALUE', 83)\ndef_op('IMPORT_STAR', 84)\ndef_op('EXEC_STMT', 85)\ndef_op('YIELD_VALUE', 86)\ndef_op('POP_BLOCK', 87)\ndef_op('END_FINALLY', 88)\ndef_op('BUILD_CLASS', 89)\n\nHAVE_ARGUMENT = 90              # Opcodes from here have an argument:\n\nname_op('STORE_NAME', 90)       # Index in name list\nname_op('DELETE_NAME', 91)      # \"\"\ndef_op('UNPACK_SEQUENCE', 92)   # Number of tuple items\njrel_op('FOR_ITER', 93)\ndef_op('LIST_APPEND', 94)\nname_op('STORE_ATTR', 95)       # Index in name list\nname_op('DELETE_ATTR', 96)      # \"\"\nname_op('STORE_GLOBAL', 97)     # \"\"\nname_op('DELETE_GLOBAL', 98)    # \"\"\ndef_op('DUP_TOPX', 99)          # number of items to duplicate\ndef_op('LOAD_CONST', 100)       # Index in const list\nhasconst.append(100)\nname_op('LOAD_NAME', 101)       # Index in name list\ndef_op('BUILD_TUPLE', 102)      # Number of tuple items\ndef_op('BUILD_LIST', 103)       # Number of list items\ndef_op('BUILD_SET', 104)        # Number of set items\ndef_op('BUILD_MAP', 105)        # Number of dict entries (upto 255)\nname_op('LOAD_ATTR', 106)       # Index in name list\ndef_op('COMPARE_OP', 107)       # Comparison operator\nhascompare.append(107)\nname_op('IMPORT_NAME', 108)     # Index in name list\nname_op('IMPORT_FROM', 109)     # Index in name list\njrel_op('JUMP_FORWARD', 110)    # Number of bytes to skip\njabs_op('JUMP_IF_FALSE_OR_POP', 111) # Target byte offset from beginning of code\njabs_op('JUMP_IF_TRUE_OR_POP', 112)  # \"\"\njabs_op('JUMP_ABSOLUTE', 113)        # \"\"\njabs_op('POP_JUMP_IF_FALSE', 114)    # \"\"\njabs_op('POP_JUMP_IF_TRUE', 115)     # \"\"\n\nname_op('LOAD_GLOBAL', 116)     # Index in name list\n\njabs_op('CONTINUE_LOOP', 119)   # Target address\njrel_op('SETUP_LOOP', 120)      # Distance to target address\njrel_op('SETUP_EXCEPT', 121)    # \"\"\njrel_op('SETUP_FINALLY', 122)   # \"\"\n\ndef_op('LOAD_FAST', 124)        # Local variable number\nhaslocal.append(124)\ndef_op('STORE_FAST', 125)       # Local variable number\nhaslocal.append(125)\ndef_op('DELETE_FAST', 126)      # Local variable number\nhaslocal.append(126)\n\ndef_op('RAISE_VARARGS', 130)    # Number of raise arguments (1, 2, or 3)\ndef_op('CALL_FUNCTION', 131)    # #args + (#kwargs << 8)\ndef_op('MAKE_FUNCTION', 132)    # Number of args with default values\ndef_op('BUILD_SLICE', 133)      # Number of items\ndef_op('MAKE_CLOSURE', 134)\ndef_op('LOAD_CLOSURE', 135)\nhasfree.append(135)\ndef_op('LOAD_DEREF', 136)\nhasfree.append(136)\ndef_op('STORE_DEREF', 137)\nhasfree.append(137)\n\ndef_op('CALL_FUNCTION_VAR', 140)     # #args + (#kwargs << 8)\ndef_op('CALL_FUNCTION_KW', 141)      # #args + (#kwargs << 8)\ndef_op('CALL_FUNCTION_VAR_KW', 142)  # #args + (#kwargs << 8)\n\njrel_op('SETUP_WITH', 143)\n\ndef_op('EXTENDED_ARG', 145)\nEXTENDED_ARG = 145\ndef_op('SET_ADD', 146)\ndef_op('MAP_ADD', 147)\n\n# pypy modification, experimental bytecode\ndef_op('LOOKUP_METHOD', 201)          # Index in name list\nhasname.append(201)\ndef_op('CALL_METHOD', 202)            # #args not including 'self'\ndef_op('BUILD_LIST_FROM_ARG', 203)\njrel_op('JUMP_IF_NOT_DEBUG', 204)     # jump over assert statements\n\ndel def_op, name_op, jrel_op, jabs_op\n", 
    "optparse": "\"\"\"A powerful, extensible, and easy-to-use option parser.\n\nBy Greg Ward <gward@python.net>\n\nOriginally distributed as Optik.\n\nFor support, use the optik-users@lists.sourceforge.net mailing list\n(http://lists.sourceforge.net/lists/listinfo/optik-users).\n\nSimple usage example:\n\n   from optparse import OptionParser\n\n   parser = OptionParser()\n   parser.add_option(\"-f\", \"--file\", dest=\"filename\",\n                     help=\"write report to FILE\", metavar=\"FILE\")\n   parser.add_option(\"-q\", \"--quiet\",\n                     action=\"store_false\", dest=\"verbose\", default=True,\n                     help=\"don't print status messages to stdout\")\n\n   (options, args) = parser.parse_args()\n\"\"\"\n\n__version__ = \"1.5.3\"\n\n__all__ = ['Option',\n           'make_option',\n           'SUPPRESS_HELP',\n           'SUPPRESS_USAGE',\n           'Values',\n           'OptionContainer',\n           'OptionGroup',\n           'OptionParser',\n           'HelpFormatter',\n           'IndentedHelpFormatter',\n           'TitledHelpFormatter',\n           'OptParseError',\n           'OptionError',\n           'OptionConflictError',\n           'OptionValueError',\n           'BadOptionError']\n\n__copyright__ = \"\"\"\nCopyright (c) 2001-2006 Gregory P. Ward.  All rights reserved.\nCopyright (c) 2002-2006 Python Software Foundation.  All rights reserved.\n\nRedistribution and use in source and binary forms, with or without\nmodification, are permitted provided that the following conditions are\nmet:\n\n  * Redistributions of source code must retain the above copyright\n    notice, this list of conditions and the following disclaimer.\n\n  * Redistributions in binary form must reproduce the above copyright\n    notice, this list of conditions and the following disclaimer in the\n    documentation and/or other materials provided with the distribution.\n\n  * Neither the name of the author nor the names of its\n    contributors may be used to endorse or promote products derived from\n    this software without specific prior written permission.\n\nTHIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS\nIS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED\nTO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A\nPARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE AUTHOR OR\nCONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\nEXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,\nPROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR\nPROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF\nLIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING\nNEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS\nSOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n\"\"\"\n\nimport sys, os\nimport types\nimport textwrap\n\ndef _repr(self):\n    return \"<%s at 0x%x: %s>\" % (self.__class__.__name__, id(self), self)\n\n\n# This file was generated from:\n#   Id: option_parser.py 527 2006-07-23 15:21:30Z greg\n#   Id: option.py 522 2006-06-11 16:22:03Z gward\n#   Id: help.py 527 2006-07-23 15:21:30Z greg\n#   Id: errors.py 509 2006-04-20 00:58:24Z gward\n\ntry:\n    from gettext import gettext\nexcept ImportError:\n    def gettext(message):\n        return message\n_ = gettext\n\n\nclass OptParseError (Exception):\n    def __init__(self, msg):\n        self.msg = msg\n\n    def __str__(self):\n        return self.msg\n\n\nclass OptionError (OptParseError):\n    \"\"\"\n    Raised if an Option instance is created with invalid or\n    inconsistent arguments.\n    \"\"\"\n\n    def __init__(self, msg, option):\n        self.msg = msg\n        self.option_id = str(option)\n\n    def __str__(self):\n        if self.option_id:\n            return \"option %s: %s\" % (self.option_id, self.msg)\n        else:\n            return self.msg\n\nclass OptionConflictError (OptionError):\n    \"\"\"\n    Raised if conflicting options are added to an OptionParser.\n    \"\"\"\n\nclass OptionValueError (OptParseError):\n    \"\"\"\n    Raised if an invalid option value is encountered on the command\n    line.\n    \"\"\"\n\nclass BadOptionError (OptParseError):\n    \"\"\"\n    Raised if an invalid option is seen on the command line.\n    \"\"\"\n    def __init__(self, opt_str):\n        self.opt_str = opt_str\n\n    def __str__(self):\n        return _(\"no such option: %s\") % self.opt_str\n\nclass AmbiguousOptionError (BadOptionError):\n    \"\"\"\n    Raised if an ambiguous option is seen on the command line.\n    \"\"\"\n    def __init__(self, opt_str, possibilities):\n        BadOptionError.__init__(self, opt_str)\n        self.possibilities = possibilities\n\n    def __str__(self):\n        return (_(\"ambiguous option: %s (%s?)\")\n                % (self.opt_str, \", \".join(self.possibilities)))\n\n\nclass HelpFormatter:\n\n    \"\"\"\n    Abstract base class for formatting option help.  OptionParser\n    instances should use one of the HelpFormatter subclasses for\n    formatting help; by default IndentedHelpFormatter is used.\n\n    Instance attributes:\n      parser : OptionParser\n        the controlling OptionParser instance\n      indent_increment : int\n        the number of columns to indent per nesting level\n      max_help_position : int\n        the maximum starting column for option help text\n      help_position : int\n        the calculated starting column for option help text;\n        initially the same as the maximum\n      width : int\n        total number of columns for output (pass None to constructor for\n        this value to be taken from the $COLUMNS environment variable)\n      level : int\n        current indentation level\n      current_indent : int\n        current indentation level (in columns)\n      help_width : int\n        number of columns available for option help text (calculated)\n      default_tag : str\n        text to replace with each option's default value, \"%default\"\n        by default.  Set to false value to disable default value expansion.\n      option_strings : { Option : str }\n        maps Option instances to the snippet of help text explaining\n        the syntax of that option, e.g. \"-h, --help\" or\n        \"-fFILE, --file=FILE\"\n      _short_opt_fmt : str\n        format string controlling how short options with values are\n        printed in help text.  Must be either \"%s%s\" (\"-fFILE\") or\n        \"%s %s\" (\"-f FILE\"), because those are the two syntaxes that\n        Optik supports.\n      _long_opt_fmt : str\n        similar but for long options; must be either \"%s %s\" (\"--file FILE\")\n        or \"%s=%s\" (\"--file=FILE\").\n    \"\"\"\n\n    NO_DEFAULT_VALUE = \"none\"\n\n    def __init__(self,\n                 indent_increment,\n                 max_help_position,\n                 width,\n                 short_first):\n        self.parser = None\n        self.indent_increment = indent_increment\n        if width is None:\n            try:\n                width = int(os.environ['COLUMNS'])\n            except (KeyError, ValueError):\n                width = 80\n            width -= 2\n        self.width = width\n        self.help_position = self.max_help_position = \\\n                min(max_help_position, max(width - 20, indent_increment * 2))\n        self.current_indent = 0\n        self.level = 0\n        self.help_width = None          # computed later\n        self.short_first = short_first\n        self.default_tag = \"%default\"\n        self.option_strings = {}\n        self._short_opt_fmt = \"%s %s\"\n        self._long_opt_fmt = \"%s=%s\"\n\n    def set_parser(self, parser):\n        self.parser = parser\n\n    def set_short_opt_delimiter(self, delim):\n        if delim not in (\"\", \" \"):\n            raise ValueError(\n                \"invalid metavar delimiter for short options: %r\" % delim)\n        self._short_opt_fmt = \"%s\" + delim + \"%s\"\n\n    def set_long_opt_delimiter(self, delim):\n        if delim not in (\"=\", \" \"):\n            raise ValueError(\n                \"invalid metavar delimiter for long options: %r\" % delim)\n        self._long_opt_fmt = \"%s\" + delim + \"%s\"\n\n    def indent(self):\n        self.current_indent += self.indent_increment\n        self.level += 1\n\n    def dedent(self):\n        self.current_indent -= self.indent_increment\n        assert self.current_indent >= 0, \"Indent decreased below 0.\"\n        self.level -= 1\n\n    def format_usage(self, usage):\n        raise NotImplementedError, \"subclasses must implement\"\n\n    def format_heading(self, heading):\n        raise NotImplementedError, \"subclasses must implement\"\n\n    def _format_text(self, text):\n        \"\"\"\n        Format a paragraph of free-form text for inclusion in the\n        help output at the current indentation level.\n        \"\"\"\n        text_width = max(self.width - self.current_indent, 11)\n        indent = \" \"*self.current_indent\n        return textwrap.fill(text,\n                             text_width,\n                             initial_indent=indent,\n                             subsequent_indent=indent)\n\n    def format_description(self, description):\n        if description:\n            return self._format_text(description) + \"\\n\"\n        else:\n            return \"\"\n\n    def format_epilog(self, epilog):\n        if epilog:\n            return \"\\n\" + self._format_text(epilog) + \"\\n\"\n        else:\n            return \"\"\n\n\n    def expand_default(self, option):\n        if self.parser is None or not self.default_tag:\n            return option.help\n\n        default_value = self.parser.defaults.get(option.dest)\n        if default_value is NO_DEFAULT or default_value is None:\n            default_value = self.NO_DEFAULT_VALUE\n\n        return option.help.replace(self.default_tag, str(default_value))\n\n    def format_option(self, option):\n        # The help for each option consists of two parts:\n        #   * the opt strings and metavars\n        #     eg. (\"-x\", or \"-fFILENAME, --file=FILENAME\")\n        #   * the user-supplied help string\n        #     eg. (\"turn on expert mode\", \"read data from FILENAME\")\n        #\n        # If possible, we write both of these on the same line:\n        #   -x      turn on expert mode\n        #\n        # But if the opt string list is too long, we put the help\n        # string on a second line, indented to the same column it would\n        # start in if it fit on the first line.\n        #   -fFILENAME, --file=FILENAME\n        #           read data from FILENAME\n        result = []\n        opts = self.option_strings[option]\n        opt_width = self.help_position - self.current_indent - 2\n        if len(opts) > opt_width:\n            opts = \"%*s%s\\n\" % (self.current_indent, \"\", opts)\n            indent_first = self.help_position\n        else:                       # start help on same line as opts\n            opts = \"%*s%-*s  \" % (self.current_indent, \"\", opt_width, opts)\n            indent_first = 0\n        result.append(opts)\n        if option.help:\n            help_text = self.expand_default(option)\n            help_lines = textwrap.wrap(help_text, self.help_width)\n            result.append(\"%*s%s\\n\" % (indent_first, \"\", help_lines[0]))\n            result.extend([\"%*s%s\\n\" % (self.help_position, \"\", line)\n                           for line in help_lines[1:]])\n        elif opts[-1] != \"\\n\":\n            result.append(\"\\n\")\n        return \"\".join(result)\n\n    def store_option_strings(self, parser):\n        self.indent()\n        max_len = 0\n        for opt in parser.option_list:\n            strings = self.format_option_strings(opt)\n            self.option_strings[opt] = strings\n            max_len = max(max_len, len(strings) + self.current_indent)\n        self.indent()\n        for group in parser.option_groups:\n            for opt in group.option_list:\n                strings = self.format_option_strings(opt)\n                self.option_strings[opt] = strings\n                max_len = max(max_len, len(strings) + self.current_indent)\n        self.dedent()\n        self.dedent()\n        self.help_position = min(max_len + 2, self.max_help_position)\n        self.help_width = max(self.width - self.help_position, 11)\n\n    def format_option_strings(self, option):\n        \"\"\"Return a comma-separated list of option strings & metavariables.\"\"\"\n        if option.takes_value():\n            metavar = option.metavar or option.dest.upper()\n            short_opts = [self._short_opt_fmt % (sopt, metavar)\n                          for sopt in option._short_opts]\n            long_opts = [self._long_opt_fmt % (lopt, metavar)\n                         for lopt in option._long_opts]\n        else:\n            short_opts = option._short_opts\n            long_opts = option._long_opts\n\n        if self.short_first:\n            opts = short_opts + long_opts\n        else:\n            opts = long_opts + short_opts\n\n        return \", \".join(opts)\n\nclass IndentedHelpFormatter (HelpFormatter):\n    \"\"\"Format help with indented section bodies.\n    \"\"\"\n\n    def __init__(self,\n                 indent_increment=2,\n                 max_help_position=24,\n                 width=None,\n                 short_first=1):\n        HelpFormatter.__init__(\n            self, indent_increment, max_help_position, width, short_first)\n\n    def format_usage(self, usage):\n        return _(\"Usage: %s\\n\") % usage\n\n    def format_heading(self, heading):\n        return \"%*s%s:\\n\" % (self.current_indent, \"\", heading)\n\n\nclass TitledHelpFormatter (HelpFormatter):\n    \"\"\"Format help with underlined section headers.\n    \"\"\"\n\n    def __init__(self,\n                 indent_increment=0,\n                 max_help_position=24,\n                 width=None,\n                 short_first=0):\n        HelpFormatter.__init__ (\n            self, indent_increment, max_help_position, width, short_first)\n\n    def format_usage(self, usage):\n        return \"%s  %s\\n\" % (self.format_heading(_(\"Usage\")), usage)\n\n    def format_heading(self, heading):\n        return \"%s\\n%s\\n\" % (heading, \"=-\"[self.level] * len(heading))\n\n\ndef _parse_num(val, type):\n    if val[:2].lower() == \"0x\":         # hexadecimal\n        radix = 16\n    elif val[:2].lower() == \"0b\":       # binary\n        radix = 2\n        val = val[2:] or \"0\"            # have to remove \"0b\" prefix\n    elif val[:1] == \"0\":                # octal\n        radix = 8\n    else:                               # decimal\n        radix = 10\n\n    return type(val, radix)\n\ndef _parse_int(val):\n    return _parse_num(val, int)\n\ndef _parse_long(val):\n    return _parse_num(val, long)\n\n_builtin_cvt = { \"int\" : (_parse_int, _(\"integer\")),\n                 \"long\" : (_parse_long, _(\"long integer\")),\n                 \"float\" : (float, _(\"floating-point\")),\n                 \"complex\" : (complex, _(\"complex\")) }\n\ndef check_builtin(option, opt, value):\n    (cvt, what) = _builtin_cvt[option.type]\n    try:\n        return cvt(value)\n    except ValueError:\n        raise OptionValueError(\n            _(\"option %s: invalid %s value: %r\") % (opt, what, value))\n\ndef check_choice(option, opt, value):\n    if value in option.choices:\n        return value\n    else:\n        choices = \", \".join(map(repr, option.choices))\n        raise OptionValueError(\n            _(\"option %s: invalid choice: %r (choose from %s)\")\n            % (opt, value, choices))\n\n# Not supplying a default is different from a default of None,\n# so we need an explicit \"not supplied\" value.\nNO_DEFAULT = (\"NO\", \"DEFAULT\")\n\n\nclass Option:\n    \"\"\"\n    Instance attributes:\n      _short_opts : [string]\n      _long_opts : [string]\n\n      action : string\n      type : string\n      dest : string\n      default : any\n      nargs : int\n      const : any\n      choices : [string]\n      callback : function\n      callback_args : (any*)\n      callback_kwargs : { string : any }\n      help : string\n      metavar : string\n    \"\"\"\n\n    # The list of instance attributes that may be set through\n    # keyword args to the constructor.\n    ATTRS = ['action',\n             'type',\n             'dest',\n             'default',\n             'nargs',\n             'const',\n             'choices',\n             'callback',\n             'callback_args',\n             'callback_kwargs',\n             'help',\n             'metavar']\n\n    # The set of actions allowed by option parsers.  Explicitly listed\n    # here so the constructor can validate its arguments.\n    ACTIONS = (\"store\",\n               \"store_const\",\n               \"store_true\",\n               \"store_false\",\n               \"append\",\n               \"append_const\",\n               \"count\",\n               \"callback\",\n               \"help\",\n               \"version\")\n\n    # The set of actions that involve storing a value somewhere;\n    # also listed just for constructor argument validation.  (If\n    # the action is one of these, there must be a destination.)\n    STORE_ACTIONS = (\"store\",\n                     \"store_const\",\n                     \"store_true\",\n                     \"store_false\",\n                     \"append\",\n                     \"append_const\",\n                     \"count\")\n\n    # The set of actions for which it makes sense to supply a value\n    # type, ie. which may consume an argument from the command line.\n    TYPED_ACTIONS = (\"store\",\n                     \"append\",\n                     \"callback\")\n\n    # The set of actions which *require* a value type, ie. that\n    # always consume an argument from the command line.\n    ALWAYS_TYPED_ACTIONS = (\"store\",\n                            \"append\")\n\n    # The set of actions which take a 'const' attribute.\n    CONST_ACTIONS = (\"store_const\",\n                     \"append_const\")\n\n    # The set of known types for option parsers.  Again, listed here for\n    # constructor argument validation.\n    TYPES = (\"string\", \"int\", \"long\", \"float\", \"complex\", \"choice\")\n\n    # Dictionary of argument checking functions, which convert and\n    # validate option arguments according to the option type.\n    #\n    # Signature of checking functions is:\n    #   check(option : Option, opt : string, value : string) -> any\n    # where\n    #   option is the Option instance calling the checker\n    #   opt is the actual option seen on the command-line\n    #     (eg. \"-a\", \"--file\")\n    #   value is the option argument seen on the command-line\n    #\n    # The return value should be in the appropriate Python type\n    # for option.type -- eg. an integer if option.type == \"int\".\n    #\n    # If no checker is defined for a type, arguments will be\n    # unchecked and remain strings.\n    TYPE_CHECKER = { \"int\"    : check_builtin,\n                     \"long\"   : check_builtin,\n                     \"float\"  : check_builtin,\n                     \"complex\": check_builtin,\n                     \"choice\" : check_choice,\n                   }\n\n\n    # CHECK_METHODS is a list of unbound method objects; they are called\n    # by the constructor, in order, after all attributes are\n    # initialized.  The list is created and filled in later, after all\n    # the methods are actually defined.  (I just put it here because I\n    # like to define and document all class attributes in the same\n    # place.)  Subclasses that add another _check_*() method should\n    # define their own CHECK_METHODS list that adds their check method\n    # to those from this class.\n    CHECK_METHODS = None\n\n\n    # -- Constructor/initialization methods ----------------------------\n\n    def __init__(self, *opts, **attrs):\n        # Set _short_opts, _long_opts attrs from 'opts' tuple.\n        # Have to be set now, in case no option strings are supplied.\n        self._short_opts = []\n        self._long_opts = []\n        opts = self._check_opt_strings(opts)\n        self._set_opt_strings(opts)\n\n        # Set all other attrs (action, type, etc.) from 'attrs' dict\n        self._set_attrs(attrs)\n\n        # Check all the attributes we just set.  There are lots of\n        # complicated interdependencies, but luckily they can be farmed\n        # out to the _check_*() methods listed in CHECK_METHODS -- which\n        # could be handy for subclasses!  The one thing these all share\n        # is that they raise OptionError if they discover a problem.\n        for checker in self.CHECK_METHODS:\n            checker(self)\n\n    def _check_opt_strings(self, opts):\n        # Filter out None because early versions of Optik had exactly\n        # one short option and one long option, either of which\n        # could be None.\n        opts = filter(None, opts)\n        if not opts:\n            raise TypeError(\"at least one option string must be supplied\")\n        return opts\n\n    def _set_opt_strings(self, opts):\n        for opt in opts:\n            if len(opt) < 2:\n                raise OptionError(\n                    \"invalid option string %r: \"\n                    \"must be at least two characters long\" % opt, self)\n            elif len(opt) == 2:\n                if not (opt[0] == \"-\" and opt[1] != \"-\"):\n                    raise OptionError(\n                        \"invalid short option string %r: \"\n                        \"must be of the form -x, (x any non-dash char)\" % opt,\n                        self)\n                self._short_opts.append(opt)\n            else:\n                if not (opt[0:2] == \"--\" and opt[2] != \"-\"):\n                    raise OptionError(\n                        \"invalid long option string %r: \"\n                        \"must start with --, followed by non-dash\" % opt,\n                        self)\n                self._long_opts.append(opt)\n\n    def _set_attrs(self, attrs):\n        for attr in self.ATTRS:\n            if attr in attrs:\n                setattr(self, attr, attrs[attr])\n                del attrs[attr]\n            else:\n                if attr == 'default':\n                    setattr(self, attr, NO_DEFAULT)\n                else:\n                    setattr(self, attr, None)\n        if attrs:\n            attrs = attrs.keys()\n            attrs.sort()\n            raise OptionError(\n                \"invalid keyword arguments: %s\" % \", \".join(attrs),\n                self)\n\n\n    # -- Constructor validation methods --------------------------------\n\n    def _check_action(self):\n        if self.action is None:\n            self.action = \"store\"\n        elif self.action not in self.ACTIONS:\n            raise OptionError(\"invalid action: %r\" % self.action, self)\n\n    def _check_type(self):\n        if self.type is None:\n            if self.action in self.ALWAYS_TYPED_ACTIONS:\n                if self.choices is not None:\n                    # The \"choices\" attribute implies \"choice\" type.\n                    self.type = \"choice\"\n                else:\n                    # No type given?  \"string\" is the most sensible default.\n                    self.type = \"string\"\n        else:\n            # Allow type objects or builtin type conversion functions\n            # (int, str, etc.) as an alternative to their names.  (The\n            # complicated check of __builtin__ is only necessary for\n            # Python 2.1 and earlier, and is short-circuited by the\n            # first check on modern Pythons.)\n            import __builtin__\n            if ( type(self.type) is types.TypeType or\n                 (hasattr(self.type, \"__name__\") and\n                  getattr(__builtin__, self.type.__name__, None) is self.type) ):\n                self.type = self.type.__name__\n\n            if self.type == \"str\":\n                self.type = \"string\"\n\n            if self.type not in self.TYPES:\n                raise OptionError(\"invalid option type: %r\" % self.type, self)\n            if self.action not in self.TYPED_ACTIONS:\n                raise OptionError(\n                    \"must not supply a type for action %r\" % self.action, self)\n\n    def _check_choice(self):\n        if self.type == \"choice\":\n            if self.choices is None:\n                raise OptionError(\n                    \"must supply a list of choices for type 'choice'\", self)\n            elif type(self.choices) not in (types.TupleType, types.ListType):\n                raise OptionError(\n                    \"choices must be a list of strings ('%s' supplied)\"\n                    % str(type(self.choices)).split(\"'\")[1], self)\n        elif self.choices is not None:\n            raise OptionError(\n                \"must not supply choices for type %r\" % self.type, self)\n\n    def _check_dest(self):\n        # No destination given, and we need one for this action.  The\n        # self.type check is for callbacks that take a value.\n        takes_value = (self.action in self.STORE_ACTIONS or\n                       self.type is not None)\n        if self.dest is None and takes_value:\n\n            # Glean a destination from the first long option string,\n            # or from the first short option string if no long options.\n            if self._long_opts:\n                # eg. \"--foo-bar\" -> \"foo_bar\"\n                self.dest = self._long_opts[0][2:].replace('-', '_')\n            else:\n                self.dest = self._short_opts[0][1]\n\n    def _check_const(self):\n        if self.action not in self.CONST_ACTIONS and self.const is not None:\n            raise OptionError(\n                \"'const' must not be supplied for action %r\" % self.action,\n                self)\n\n    def _check_nargs(self):\n        if self.action in self.TYPED_ACTIONS:\n            if self.nargs is None:\n                self.nargs = 1\n        elif self.nargs is not None:\n            raise OptionError(\n                \"'nargs' must not be supplied for action %r\" % self.action,\n                self)\n\n    def _check_callback(self):\n        if self.action == \"callback\":\n            if not hasattr(self.callback, '__call__'):\n                raise OptionError(\n                    \"callback not callable: %r\" % self.callback, self)\n            if (self.callback_args is not None and\n                type(self.callback_args) is not types.TupleType):\n                raise OptionError(\n                    \"callback_args, if supplied, must be a tuple: not %r\"\n                    % self.callback_args, self)\n            if (self.callback_kwargs is not None and\n                type(self.callback_kwargs) is not types.DictType):\n                raise OptionError(\n                    \"callback_kwargs, if supplied, must be a dict: not %r\"\n                    % self.callback_kwargs, self)\n        else:\n            if self.callback is not None:\n                raise OptionError(\n                    \"callback supplied (%r) for non-callback option\"\n                    % self.callback, self)\n            if self.callback_args is not None:\n                raise OptionError(\n                    \"callback_args supplied for non-callback option\", self)\n            if self.callback_kwargs is not None:\n                raise OptionError(\n                    \"callback_kwargs supplied for non-callback option\", self)\n\n\n    CHECK_METHODS = [_check_action,\n                     _check_type,\n                     _check_choice,\n                     _check_dest,\n                     _check_const,\n                     _check_nargs,\n                     _check_callback]\n\n\n    # -- Miscellaneous methods -----------------------------------------\n\n    def __str__(self):\n        return \"/\".join(self._short_opts + self._long_opts)\n\n    __repr__ = _repr\n\n    def takes_value(self):\n        return self.type is not None\n\n    def get_opt_string(self):\n        if self._long_opts:\n            return self._long_opts[0]\n        else:\n            return self._short_opts[0]\n\n\n    # -- Processing methods --------------------------------------------\n\n    def check_value(self, opt, value):\n        checker = self.TYPE_CHECKER.get(self.type)\n        if checker is None:\n            return value\n        else:\n            return checker(self, opt, value)\n\n    def convert_value(self, opt, value):\n        if value is not None:\n            if self.nargs == 1:\n                return self.check_value(opt, value)\n            else:\n                return tuple([self.check_value(opt, v) for v in value])\n\n    def process(self, opt, value, values, parser):\n\n        # First, convert the value(s) to the right type.  Howl if any\n        # value(s) are bogus.\n        value = self.convert_value(opt, value)\n\n        # And then take whatever action is expected of us.\n        # This is a separate method to make life easier for\n        # subclasses to add new actions.\n        return self.take_action(\n            self.action, self.dest, opt, value, values, parser)\n\n    def take_action(self, action, dest, opt, value, values, parser):\n        if action == \"store\":\n            setattr(values, dest, value)\n        elif action == \"store_const\":\n            setattr(values, dest, self.const)\n        elif action == \"store_true\":\n            setattr(values, dest, True)\n        elif action == \"store_false\":\n            setattr(values, dest, False)\n        elif action == \"append\":\n            values.ensure_value(dest, []).append(value)\n        elif action == \"append_const\":\n            values.ensure_value(dest, []).append(self.const)\n        elif action == \"count\":\n            setattr(values, dest, values.ensure_value(dest, 0) + 1)\n        elif action == \"callback\":\n            args = self.callback_args or ()\n            kwargs = self.callback_kwargs or {}\n            self.callback(self, opt, value, parser, *args, **kwargs)\n        elif action == \"help\":\n            parser.print_help()\n            parser.exit()\n        elif action == \"version\":\n            parser.print_version()\n            parser.exit()\n        else:\n            raise ValueError(\"unknown action %r\" % self.action)\n\n        return 1\n\n# class Option\n\n\nSUPPRESS_HELP = \"SUPPRESS\"+\"HELP\"\nSUPPRESS_USAGE = \"SUPPRESS\"+\"USAGE\"\n\ntry:\n    basestring\nexcept NameError:\n    def isbasestring(x):\n        return isinstance(x, (types.StringType, types.UnicodeType))\nelse:\n    def isbasestring(x):\n        return isinstance(x, basestring)\n\nclass Values:\n\n    def __init__(self, defaults=None):\n        if defaults:\n            for (attr, val) in defaults.items():\n                setattr(self, attr, val)\n\n    def __str__(self):\n        return str(self.__dict__)\n\n    __repr__ = _repr\n\n    def __cmp__(self, other):\n        if isinstance(other, Values):\n            return cmp(self.__dict__, other.__dict__)\n        elif isinstance(other, types.DictType):\n            return cmp(self.__dict__, other)\n        else:\n            return -1\n\n    def _update_careful(self, dict):\n        \"\"\"\n        Update the option values from an arbitrary dictionary, but only\n        use keys from dict that already have a corresponding attribute\n        in self.  Any keys in dict without a corresponding attribute\n        are silently ignored.\n        \"\"\"\n        for attr in dir(self):\n            if attr in dict:\n                dval = dict[attr]\n                if dval is not None:\n                    setattr(self, attr, dval)\n\n    def _update_loose(self, dict):\n        \"\"\"\n        Update the option values from an arbitrary dictionary,\n        using all keys from the dictionary regardless of whether\n        they have a corresponding attribute in self or not.\n        \"\"\"\n        self.__dict__.update(dict)\n\n    def _update(self, dict, mode):\n        if mode == \"careful\":\n            self._update_careful(dict)\n        elif mode == \"loose\":\n            self._update_loose(dict)\n        else:\n            raise ValueError, \"invalid update mode: %r\" % mode\n\n    def read_module(self, modname, mode=\"careful\"):\n        __import__(modname)\n        mod = sys.modules[modname]\n        self._update(vars(mod), mode)\n\n    def read_file(self, filename, mode=\"careful\"):\n        vars = {}\n        execfile(filename, vars)\n        self._update(vars, mode)\n\n    def ensure_value(self, attr, value):\n        if not hasattr(self, attr) or getattr(self, attr) is None:\n            setattr(self, attr, value)\n        return getattr(self, attr)\n\n\nclass OptionContainer:\n\n    \"\"\"\n    Abstract base class.\n\n    Class attributes:\n      standard_option_list : [Option]\n        list of standard options that will be accepted by all instances\n        of this parser class (intended to be overridden by subclasses).\n\n    Instance attributes:\n      option_list : [Option]\n        the list of Option objects contained by this OptionContainer\n      _short_opt : { string : Option }\n        dictionary mapping short option strings, eg. \"-f\" or \"-X\",\n        to the Option instances that implement them.  If an Option\n        has multiple short option strings, it will appears in this\n        dictionary multiple times. [1]\n      _long_opt : { string : Option }\n        dictionary mapping long option strings, eg. \"--file\" or\n        \"--exclude\", to the Option instances that implement them.\n        Again, a given Option can occur multiple times in this\n        dictionary. [1]\n      defaults : { string : any }\n        dictionary mapping option destination names to default\n        values for each destination [1]\n\n    [1] These mappings are common to (shared by) all components of the\n        controlling OptionParser, where they are initially created.\n\n    \"\"\"\n\n    def __init__(self, option_class, conflict_handler, description):\n        # Initialize the option list and related data structures.\n        # This method must be provided by subclasses, and it must\n        # initialize at least the following instance attributes:\n        # option_list, _short_opt, _long_opt, defaults.\n        self._create_option_list()\n\n        self.option_class = option_class\n        self.set_conflict_handler(conflict_handler)\n        self.set_description(description)\n\n    def _create_option_mappings(self):\n        # For use by OptionParser constructor -- create the master\n        # option mappings used by this OptionParser and all\n        # OptionGroups that it owns.\n        self._short_opt = {}            # single letter -> Option instance\n        self._long_opt = {}             # long option -> Option instance\n        self.defaults = {}              # maps option dest -> default value\n\n\n    def _share_option_mappings(self, parser):\n        # For use by OptionGroup constructor -- use shared option\n        # mappings from the OptionParser that owns this OptionGroup.\n        self._short_opt = parser._short_opt\n        self._long_opt = parser._long_opt\n        self.defaults = parser.defaults\n\n    def set_conflict_handler(self, handler):\n        if handler not in (\"error\", \"resolve\"):\n            raise ValueError, \"invalid conflict_resolution value %r\" % handler\n        self.conflict_handler = handler\n\n    def set_description(self, description):\n        self.description = description\n\n    def get_description(self):\n        return self.description\n\n\n    def destroy(self):\n        \"\"\"see OptionParser.destroy().\"\"\"\n        del self._short_opt\n        del self._long_opt\n        del self.defaults\n\n\n    # -- Option-adding methods -----------------------------------------\n\n    def _check_conflict(self, option):\n        conflict_opts = []\n        for opt in option._short_opts:\n            if opt in self._short_opt:\n                conflict_opts.append((opt, self._short_opt[opt]))\n        for opt in option._long_opts:\n            if opt in self._long_opt:\n                conflict_opts.append((opt, self._long_opt[opt]))\n\n        if conflict_opts:\n            handler = self.conflict_handler\n            if handler == \"error\":\n                raise OptionConflictError(\n                    \"conflicting option string(s): %s\"\n                    % \", \".join([co[0] for co in conflict_opts]),\n                    option)\n            elif handler == \"resolve\":\n                for (opt, c_option) in conflict_opts:\n                    if opt.startswith(\"--\"):\n                        c_option._long_opts.remove(opt)\n                        del self._long_opt[opt]\n                    else:\n                        c_option._short_opts.remove(opt)\n                        del self._short_opt[opt]\n                    if not (c_option._short_opts or c_option._long_opts):\n                        c_option.container.option_list.remove(c_option)\n\n    def add_option(self, *args, **kwargs):\n        \"\"\"add_option(Option)\n           add_option(opt_str, ..., kwarg=val, ...)\n        \"\"\"\n        if type(args[0]) in types.StringTypes:\n            option = self.option_class(*args, **kwargs)\n        elif len(args) == 1 and not kwargs:\n            option = args[0]\n            if not isinstance(option, Option):\n                raise TypeError, \"not an Option instance: %r\" % option\n        else:\n            raise TypeError, \"invalid arguments\"\n\n        self._check_conflict(option)\n\n        self.option_list.append(option)\n        option.container = self\n        for opt in option._short_opts:\n            self._short_opt[opt] = option\n        for opt in option._long_opts:\n            self._long_opt[opt] = option\n\n        if option.dest is not None:     # option has a dest, we need a default\n            if option.default is not NO_DEFAULT:\n                self.defaults[option.dest] = option.default\n            elif option.dest not in self.defaults:\n                self.defaults[option.dest] = None\n\n        return option\n\n    def add_options(self, option_list):\n        for option in option_list:\n            self.add_option(option)\n\n    # -- Option query/removal methods ----------------------------------\n\n    def get_option(self, opt_str):\n        return (self._short_opt.get(opt_str) or\n                self._long_opt.get(opt_str))\n\n    def has_option(self, opt_str):\n        return (opt_str in self._short_opt or\n                opt_str in self._long_opt)\n\n    def remove_option(self, opt_str):\n        option = self._short_opt.get(opt_str)\n        if option is None:\n            option = self._long_opt.get(opt_str)\n        if option is None:\n            raise ValueError(\"no such option %r\" % opt_str)\n\n        for opt in option._short_opts:\n            del self._short_opt[opt]\n        for opt in option._long_opts:\n            del self._long_opt[opt]\n        option.container.option_list.remove(option)\n\n\n    # -- Help-formatting methods ---------------------------------------\n\n    def format_option_help(self, formatter):\n        if not self.option_list:\n            return \"\"\n        result = []\n        for option in self.option_list:\n            if not option.help is SUPPRESS_HELP:\n                result.append(formatter.format_option(option))\n        return \"\".join(result)\n\n    def format_description(self, formatter):\n        return formatter.format_description(self.get_description())\n\n    def format_help(self, formatter):\n        result = []\n        if self.description:\n            result.append(self.format_description(formatter))\n        if self.option_list:\n            result.append(self.format_option_help(formatter))\n        return \"\\n\".join(result)\n\n\nclass OptionGroup (OptionContainer):\n\n    def __init__(self, parser, title, description=None):\n        self.parser = parser\n        OptionContainer.__init__(\n            self, parser.option_class, parser.conflict_handler, description)\n        self.title = title\n\n    def _create_option_list(self):\n        self.option_list = []\n        self._share_option_mappings(self.parser)\n\n    def set_title(self, title):\n        self.title = title\n\n    def destroy(self):\n        \"\"\"see OptionParser.destroy().\"\"\"\n        OptionContainer.destroy(self)\n        del self.option_list\n\n    # -- Help-formatting methods ---------------------------------------\n\n    def format_help(self, formatter):\n        result = formatter.format_heading(self.title)\n        formatter.indent()\n        result += OptionContainer.format_help(self, formatter)\n        formatter.dedent()\n        return result\n\n\nclass OptionParser (OptionContainer):\n\n    \"\"\"\n    Class attributes:\n      standard_option_list : [Option]\n        list of standard options that will be accepted by all instances\n        of this parser class (intended to be overridden by subclasses).\n\n    Instance attributes:\n      usage : string\n        a usage string for your program.  Before it is displayed\n        to the user, \"%prog\" will be expanded to the name of\n        your program (self.prog or os.path.basename(sys.argv[0])).\n      prog : string\n        the name of the current program (to override\n        os.path.basename(sys.argv[0])).\n      description : string\n        A paragraph of text giving a brief overview of your program.\n        optparse reformats this paragraph to fit the current terminal\n        width and prints it when the user requests help (after usage,\n        but before the list of options).\n      epilog : string\n        paragraph of help text to print after option help\n\n      option_groups : [OptionGroup]\n        list of option groups in this parser (option groups are\n        irrelevant for parsing the command-line, but very useful\n        for generating help)\n\n      allow_interspersed_args : bool = true\n        if true, positional arguments may be interspersed with options.\n        Assuming -a and -b each take a single argument, the command-line\n          -ablah foo bar -bboo baz\n        will be interpreted the same as\n          -ablah -bboo -- foo bar baz\n        If this flag were false, that command line would be interpreted as\n          -ablah -- foo bar -bboo baz\n        -- ie. we stop processing options as soon as we see the first\n        non-option argument.  (This is the tradition followed by\n        Python's getopt module, Perl's Getopt::Std, and other argument-\n        parsing libraries, but it is generally annoying to users.)\n\n      process_default_values : bool = true\n        if true, option default values are processed similarly to option\n        values from the command line: that is, they are passed to the\n        type-checking function for the option's type (as long as the\n        default value is a string).  (This really only matters if you\n        have defined custom types; see SF bug #955889.)  Set it to false\n        to restore the behaviour of Optik 1.4.1 and earlier.\n\n      rargs : [string]\n        the argument list currently being parsed.  Only set when\n        parse_args() is active, and continually trimmed down as\n        we consume arguments.  Mainly there for the benefit of\n        callback options.\n      largs : [string]\n        the list of leftover arguments that we have skipped while\n        parsing options.  If allow_interspersed_args is false, this\n        list is always empty.\n      values : Values\n        the set of option values currently being accumulated.  Only\n        set when parse_args() is active.  Also mainly for callbacks.\n\n    Because of the 'rargs', 'largs', and 'values' attributes,\n    OptionParser is not thread-safe.  If, for some perverse reason, you\n    need to parse command-line arguments simultaneously in different\n    threads, use different OptionParser instances.\n\n    \"\"\"\n\n    standard_option_list = []\n\n    def __init__(self,\n                 usage=None,\n                 option_list=None,\n                 option_class=Option,\n                 version=None,\n                 conflict_handler=\"error\",\n                 description=None,\n                 formatter=None,\n                 add_help_option=True,\n                 prog=None,\n                 epilog=None):\n        OptionContainer.__init__(\n            self, option_class, conflict_handler, description)\n        self.set_usage(usage)\n        self.prog = prog\n        self.version = version\n        self.allow_interspersed_args = True\n        self.process_default_values = True\n        if formatter is None:\n            formatter = IndentedHelpFormatter()\n        self.formatter = formatter\n        self.formatter.set_parser(self)\n        self.epilog = epilog\n\n        # Populate the option list; initial sources are the\n        # standard_option_list class attribute, the 'option_list'\n        # argument, and (if applicable) the _add_version_option() and\n        # _add_help_option() methods.\n        self._populate_option_list(option_list,\n                                   add_help=add_help_option)\n\n        self._init_parsing_state()\n\n\n    def destroy(self):\n        \"\"\"\n        Declare that you are done with this OptionParser.  This cleans up\n        reference cycles so the OptionParser (and all objects referenced by\n        it) can be garbage-collected promptly.  After calling destroy(), the\n        OptionParser is unusable.\n        \"\"\"\n        OptionContainer.destroy(self)\n        for group in self.option_groups:\n            group.destroy()\n        del self.option_list\n        del self.option_groups\n        del self.formatter\n\n\n    # -- Private methods -----------------------------------------------\n    # (used by our or OptionContainer's constructor)\n\n    def _create_option_list(self):\n        self.option_list = []\n        self.option_groups = []\n        self._create_option_mappings()\n\n    def _add_help_option(self):\n        self.add_option(\"-h\", \"--help\",\n                        action=\"help\",\n                        help=_(\"show this help message and exit\"))\n\n    def _add_version_option(self):\n        self.add_option(\"--version\",\n                        action=\"version\",\n                        help=_(\"show program's version number and exit\"))\n\n    def _populate_option_list(self, option_list, add_help=True):\n        if self.standard_option_list:\n            self.add_options(self.standard_option_list)\n        if option_list:\n            self.add_options(option_list)\n        if self.version:\n            self._add_version_option()\n        if add_help:\n            self._add_help_option()\n\n    def _init_parsing_state(self):\n        # These are set in parse_args() for the convenience of callbacks.\n        self.rargs = None\n        self.largs = None\n        self.values = None\n\n\n    # -- Simple modifier methods ---------------------------------------\n\n    def set_usage(self, usage):\n        if usage is None:\n            self.usage = _(\"%prog [options]\")\n        elif usage is SUPPRESS_USAGE:\n            self.usage = None\n        # For backwards compatibility with Optik 1.3 and earlier.\n        elif usage.lower().startswith(\"usage: \"):\n            self.usage = usage[7:]\n        else:\n            self.usage = usage\n\n    def enable_interspersed_args(self):\n        \"\"\"Set parsing to not stop on the first non-option, allowing\n        interspersing switches with command arguments. This is the\n        default behavior. See also disable_interspersed_args() and the\n        class documentation description of the attribute\n        allow_interspersed_args.\"\"\"\n        self.allow_interspersed_args = True\n\n    def disable_interspersed_args(self):\n        \"\"\"Set parsing to stop on the first non-option. Use this if\n        you have a command processor which runs another command that\n        has options of its own and you want to make sure these options\n        don't get confused.\n        \"\"\"\n        self.allow_interspersed_args = False\n\n    def set_process_default_values(self, process):\n        self.process_default_values = process\n\n    def set_default(self, dest, value):\n        self.defaults[dest] = value\n\n    def set_defaults(self, **kwargs):\n        self.defaults.update(kwargs)\n\n    def _get_all_options(self):\n        options = self.option_list[:]\n        for group in self.option_groups:\n            options.extend(group.option_list)\n        return options\n\n    def get_default_values(self):\n        if not self.process_default_values:\n            # Old, pre-Optik 1.5 behaviour.\n            return Values(self.defaults)\n\n        defaults = self.defaults.copy()\n        for option in self._get_all_options():\n            default = defaults.get(option.dest)\n            if isbasestring(default):\n                opt_str = option.get_opt_string()\n                defaults[option.dest] = option.check_value(opt_str, default)\n\n        return Values(defaults)\n\n\n    # -- OptionGroup methods -------------------------------------------\n\n    def add_option_group(self, *args, **kwargs):\n        # XXX lots of overlap with OptionContainer.add_option()\n        if type(args[0]) is types.StringType:\n            group = OptionGroup(self, *args, **kwargs)\n        elif len(args) == 1 and not kwargs:\n            group = args[0]\n            if not isinstance(group, OptionGroup):\n                raise TypeError, \"not an OptionGroup instance: %r\" % group\n            if group.parser is not self:\n                raise ValueError, \"invalid OptionGroup (wrong parser)\"\n        else:\n            raise TypeError, \"invalid arguments\"\n\n        self.option_groups.append(group)\n        return group\n\n    def get_option_group(self, opt_str):\n        option = (self._short_opt.get(opt_str) or\n                  self._long_opt.get(opt_str))\n        if option and option.container is not self:\n            return option.container\n        return None\n\n\n    # -- Option-parsing methods ----------------------------------------\n\n    def _get_args(self, args):\n        if args is None:\n            return sys.argv[1:]\n        else:\n            return args[:]              # don't modify caller's list\n\n    def parse_args(self, args=None, values=None):\n        \"\"\"\n        parse_args(args : [string] = sys.argv[1:],\n                   values : Values = None)\n        -> (values : Values, args : [string])\n\n        Parse the command-line options found in 'args' (default:\n        sys.argv[1:]).  Any errors result in a call to 'error()', which\n        by default prints the usage message to stderr and calls\n        sys.exit() with an error message.  On success returns a pair\n        (values, args) where 'values' is an Values instance (with all\n        your option values) and 'args' is the list of arguments left\n        over after parsing options.\n        \"\"\"\n        rargs = self._get_args(args)\n        if values is None:\n            values = self.get_default_values()\n\n        # Store the halves of the argument list as attributes for the\n        # convenience of callbacks:\n        #   rargs\n        #     the rest of the command-line (the \"r\" stands for\n        #     \"remaining\" or \"right-hand\")\n        #   largs\n        #     the leftover arguments -- ie. what's left after removing\n        #     options and their arguments (the \"l\" stands for \"leftover\"\n        #     or \"left-hand\")\n        self.rargs = rargs\n        self.largs = largs = []\n        self.values = values\n\n        try:\n            stop = self._process_args(largs, rargs, values)\n        except (BadOptionError, OptionValueError), err:\n            self.error(str(err))\n\n        args = largs + rargs\n        return self.check_values(values, args)\n\n    def check_values(self, values, args):\n        \"\"\"\n        check_values(values : Values, args : [string])\n        -> (values : Values, args : [string])\n\n        Check that the supplied option values and leftover arguments are\n        valid.  Returns the option values and leftover arguments\n        (possibly adjusted, possibly completely new -- whatever you\n        like).  Default implementation just returns the passed-in\n        values; subclasses may override as desired.\n        \"\"\"\n        return (values, args)\n\n    def _process_args(self, largs, rargs, values):\n        \"\"\"_process_args(largs : [string],\n                         rargs : [string],\n                         values : Values)\n\n        Process command-line arguments and populate 'values', consuming\n        options and arguments from 'rargs'.  If 'allow_interspersed_args' is\n        false, stop at the first non-option argument.  If true, accumulate any\n        interspersed non-option arguments in 'largs'.\n        \"\"\"\n        while rargs:\n            arg = rargs[0]\n            # We handle bare \"--\" explicitly, and bare \"-\" is handled by the\n            # standard arg handler since the short arg case ensures that the\n            # len of the opt string is greater than 1.\n            if arg == \"--\":\n                del rargs[0]\n                return\n            elif arg[0:2] == \"--\":\n                # process a single long option (possibly with value(s))\n                self._process_long_opt(rargs, values)\n            elif arg[:1] == \"-\" and len(arg) > 1:\n                # process a cluster of short options (possibly with\n                # value(s) for the last one only)\n                self._process_short_opts(rargs, values)\n            elif self.allow_interspersed_args:\n                largs.append(arg)\n                del rargs[0]\n            else:\n                return                  # stop now, leave this arg in rargs\n\n        # Say this is the original argument list:\n        # [arg0, arg1, ..., arg(i-1), arg(i), arg(i+1), ..., arg(N-1)]\n        #                            ^\n        # (we are about to process arg(i)).\n        #\n        # Then rargs is [arg(i), ..., arg(N-1)] and largs is a *subset* of\n        # [arg0, ..., arg(i-1)] (any options and their arguments will have\n        # been removed from largs).\n        #\n        # The while loop will usually consume 1 or more arguments per pass.\n        # If it consumes 1 (eg. arg is an option that takes no arguments),\n        # then after _process_arg() is done the situation is:\n        #\n        #   largs = subset of [arg0, ..., arg(i)]\n        #   rargs = [arg(i+1), ..., arg(N-1)]\n        #\n        # If allow_interspersed_args is false, largs will always be\n        # *empty* -- still a subset of [arg0, ..., arg(i-1)], but\n        # not a very interesting subset!\n\n    def _match_long_opt(self, opt):\n        \"\"\"_match_long_opt(opt : string) -> string\n\n        Determine which long option string 'opt' matches, ie. which one\n        it is an unambiguous abbreviation for.  Raises BadOptionError if\n        'opt' doesn't unambiguously match any long option string.\n        \"\"\"\n        return _match_abbrev(opt, self._long_opt)\n\n    def _process_long_opt(self, rargs, values):\n        arg = rargs.pop(0)\n\n        # Value explicitly attached to arg?  Pretend it's the next\n        # argument.\n        if \"=\" in arg:\n            (opt, next_arg) = arg.split(\"=\", 1)\n            rargs.insert(0, next_arg)\n            had_explicit_value = True\n        else:\n            opt = arg\n            had_explicit_value = False\n\n        opt = self._match_long_opt(opt)\n        option = self._long_opt[opt]\n        if option.takes_value():\n            nargs = option.nargs\n            if len(rargs) < nargs:\n                if nargs == 1:\n                    self.error(_(\"%s option requires an argument\") % opt)\n                else:\n                    self.error(_(\"%s option requires %d arguments\")\n                               % (opt, nargs))\n            elif nargs == 1:\n                value = rargs.pop(0)\n            else:\n                value = tuple(rargs[0:nargs])\n                del rargs[0:nargs]\n\n        elif had_explicit_value:\n            self.error(_(\"%s option does not take a value\") % opt)\n\n        else:\n            value = None\n\n        option.process(opt, value, values, self)\n\n    def _process_short_opts(self, rargs, values):\n        arg = rargs.pop(0)\n        stop = False\n        i = 1\n        for ch in arg[1:]:\n            opt = \"-\" + ch\n            option = self._short_opt.get(opt)\n            i += 1                      # we have consumed a character\n\n            if not option:\n                raise BadOptionError(opt)\n            if option.takes_value():\n                # Any characters left in arg?  Pretend they're the\n                # next arg, and stop consuming characters of arg.\n                if i < len(arg):\n                    rargs.insert(0, arg[i:])\n                    stop = True\n\n                nargs = option.nargs\n                if len(rargs) < nargs:\n                    if nargs == 1:\n                        self.error(_(\"%s option requires an argument\") % opt)\n                    else:\n                        self.error(_(\"%s option requires %d arguments\")\n                                   % (opt, nargs))\n                elif nargs == 1:\n                    value = rargs.pop(0)\n                else:\n                    value = tuple(rargs[0:nargs])\n                    del rargs[0:nargs]\n\n            else:                       # option doesn't take a value\n                value = None\n\n            option.process(opt, value, values, self)\n\n            if stop:\n                break\n\n\n    # -- Feedback methods ----------------------------------------------\n\n    def get_prog_name(self):\n        if self.prog is None:\n            return os.path.basename(sys.argv[0])\n        else:\n            return self.prog\n\n    def expand_prog_name(self, s):\n        return s.replace(\"%prog\", self.get_prog_name())\n\n    def get_description(self):\n        return self.expand_prog_name(self.description)\n\n    def exit(self, status=0, msg=None):\n        if msg:\n            sys.stderr.write(msg)\n        sys.exit(status)\n\n    def error(self, msg):\n        \"\"\"error(msg : string)\n\n        Print a usage message incorporating 'msg' to stderr and exit.\n        If you override this in a subclass, it should not return -- it\n        should either exit or raise an exception.\n        \"\"\"\n        self.print_usage(sys.stderr)\n        self.exit(2, \"%s: error: %s\\n\" % (self.get_prog_name(), msg))\n\n    def get_usage(self):\n        if self.usage:\n            return self.formatter.format_usage(\n                self.expand_prog_name(self.usage))\n        else:\n            return \"\"\n\n    def print_usage(self, file=None):\n        \"\"\"print_usage(file : file = stdout)\n\n        Print the usage message for the current program (self.usage) to\n        'file' (default stdout).  Any occurrence of the string \"%prog\" in\n        self.usage is replaced with the name of the current program\n        (basename of sys.argv[0]).  Does nothing if self.usage is empty\n        or not defined.\n        \"\"\"\n        if self.usage:\n            print >>file, self.get_usage()\n\n    def get_version(self):\n        if self.version:\n            return self.expand_prog_name(self.version)\n        else:\n            return \"\"\n\n    def print_version(self, file=None):\n        \"\"\"print_version(file : file = stdout)\n\n        Print the version message for this program (self.version) to\n        'file' (default stdout).  As with print_usage(), any occurrence\n        of \"%prog\" in self.version is replaced by the current program's\n        name.  Does nothing if self.version is empty or undefined.\n        \"\"\"\n        if self.version:\n            print >>file, self.get_version()\n\n    def format_option_help(self, formatter=None):\n        if formatter is None:\n            formatter = self.formatter\n        formatter.store_option_strings(self)\n        result = []\n        result.append(formatter.format_heading(_(\"Options\")))\n        formatter.indent()\n        if self.option_list:\n            result.append(OptionContainer.format_option_help(self, formatter))\n            result.append(\"\\n\")\n        for group in self.option_groups:\n            result.append(group.format_help(formatter))\n            result.append(\"\\n\")\n        formatter.dedent()\n        # Drop the last \"\\n\", or the header if no options or option groups:\n        return \"\".join(result[:-1])\n\n    def format_epilog(self, formatter):\n        return formatter.format_epilog(self.epilog)\n\n    def format_help(self, formatter=None):\n        if formatter is None:\n            formatter = self.formatter\n        result = []\n        if self.usage:\n            result.append(self.get_usage() + \"\\n\")\n        if self.description:\n            result.append(self.format_description(formatter) + \"\\n\")\n        result.append(self.format_option_help(formatter))\n        result.append(self.format_epilog(formatter))\n        return \"\".join(result)\n\n    # used by test suite\n    def _get_encoding(self, file):\n        encoding = getattr(file, \"encoding\", None)\n        if not encoding:\n            encoding = sys.getdefaultencoding()\n        return encoding\n\n    def print_help(self, file=None):\n        \"\"\"print_help(file : file = stdout)\n\n        Print an extended help message, listing all options and any\n        help text provided with them, to 'file' (default stdout).\n        \"\"\"\n        if file is None:\n            file = sys.stdout\n        encoding = self._get_encoding(file)\n        file.write(self.format_help().encode(encoding, \"replace\"))\n\n# class OptionParser\n\n\ndef _match_abbrev(s, wordmap):\n    \"\"\"_match_abbrev(s : string, wordmap : {string : Option}) -> string\n\n    Return the string key in 'wordmap' for which 's' is an unambiguous\n    abbreviation.  If 's' is found to be ambiguous or doesn't match any of\n    'words', raise BadOptionError.\n    \"\"\"\n    # Is there an exact match?\n    if s in wordmap:\n        return s\n    else:\n        # Isolate all words with s as a prefix.\n        possibilities = [word for word in wordmap.keys()\n                         if word.startswith(s)]\n        # No exact match, so there had better be just one possibility.\n        if len(possibilities) == 1:\n            return possibilities[0]\n        elif not possibilities:\n            raise BadOptionError(s)\n        else:\n            # More than one possible completion: ambiguous prefix.\n            possibilities.sort()\n            raise AmbiguousOptionError(s, possibilities)\n\n\n# Some day, there might be many Option classes.  As of Optik 1.3, the\n# preferred way to instantiate Options is indirectly, via make_option(),\n# which will become a factory function when there are many Option\n# classes.\nmake_option = Option\n", 
    "os": "r\"\"\"OS routines for NT or Posix depending on what system we're on.\n\nThis exports:\n  - all functions from posix, nt, os2, or ce, e.g. unlink, stat, etc.\n  - os.path is one of the modules posixpath, or ntpath\n  - os.name is 'posix', 'nt', 'os2', 'ce' or 'riscos'\n  - os.curdir is a string representing the current directory ('.' or ':')\n  - os.pardir is a string representing the parent directory ('..' or '::')\n  - os.sep is the (or a most common) pathname separator ('/' or ':' or '\\\\')\n  - os.extsep is the extension separator ('.' or '/')\n  - os.altsep is the alternate pathname separator (None or '/')\n  - os.pathsep is the component separator used in $PATH etc\n  - os.linesep is the line separator in text files ('\\r' or '\\n' or '\\r\\n')\n  - os.defpath is the default search path for executables\n  - os.devnull is the file path of the null device ('/dev/null', etc.)\n\nPrograms that import and use 'os' stand a better chance of being\nportable between different platforms.  Of course, they must then\nonly use functions that are defined by all platforms (e.g., unlink\nand opendir), and leave all pathname manipulation to os.path\n(e.g., split and join).\n\"\"\"\n\n#'\n\nimport sys, errno\n\n_names = sys.builtin_module_names\n\n# Note:  more names are added to __all__ later.\n__all__ = [\"altsep\", \"curdir\", \"pardir\", \"sep\", \"extsep\", \"pathsep\", \"linesep\",\n           \"defpath\", \"name\", \"path\", \"devnull\",\n           \"SEEK_SET\", \"SEEK_CUR\", \"SEEK_END\"]\n\ndef _get_exports_list(module):\n    try:\n        return list(module.__all__)\n    except AttributeError:\n        return [n for n in dir(module) if n[0] != '_']\n\nif 'posix' in _names:\n    name = 'posix'\n    linesep = '\\n'\n    from posix import *\n    try:\n        from posix import _exit\n    except ImportError:\n        pass\n    import posixpath as path\n\n    import posix\n    __all__.extend(_get_exports_list(posix))\n    del posix\n\nelif 'nt' in _names:\n    name = 'nt'\n    linesep = '\\r\\n'\n    from nt import *\n    try:\n        from nt import _exit\n    except ImportError:\n        pass\n    import ntpath as path\n\n    import nt\n    __all__.extend(_get_exports_list(nt))\n    del nt\n\nelif 'os2' in _names:\n    name = 'os2'\n    linesep = '\\r\\n'\n    from os2 import *\n    try:\n        from os2 import _exit\n    except ImportError:\n        pass\n    if sys.version.find('EMX GCC') == -1:\n        import ntpath as path\n    else:\n        import os2emxpath as path\n        from _emx_link import link\n\n    import os2\n    __all__.extend(_get_exports_list(os2))\n    del os2\n\nelif 'ce' in _names:\n    name = 'ce'\n    linesep = '\\r\\n'\n    from ce import *\n    try:\n        from ce import _exit\n    except ImportError:\n        pass\n    # We can use the standard Windows path.\n    import ntpath as path\n\n    import ce\n    __all__.extend(_get_exports_list(ce))\n    del ce\n\nelif 'riscos' in _names:\n    name = 'riscos'\n    linesep = '\\n'\n    from riscos import *\n    try:\n        from riscos import _exit\n    except ImportError:\n        pass\n    import riscospath as path\n\n    import riscos\n    __all__.extend(_get_exports_list(riscos))\n    del riscos\n\nelse:\n    raise ImportError, 'no os specific module found'\n\nsys.modules['os.path'] = path\nfrom os.path import (curdir, pardir, sep, pathsep, defpath, extsep, altsep,\n    devnull)\n\ndel _names\n\n# Python uses fixed values for the SEEK_ constants; they are mapped\n# to native constants if necessary in posixmodule.c\nSEEK_SET = 0\nSEEK_CUR = 1\nSEEK_END = 2\n\n#'\n\n# Super directory utilities.\n# (Inspired by Eric Raymond; the doc strings are mostly his)\n\ndef makedirs(name, mode=0777):\n    \"\"\"makedirs(path [, mode=0777])\n\n    Super-mkdir; create a leaf directory and all intermediate ones.\n    Works like mkdir, except that any intermediate path segment (not\n    just the rightmost) will be created if it does not exist.  This is\n    recursive.\n\n    \"\"\"\n    head, tail = path.split(name)\n    if not tail:\n        head, tail = path.split(head)\n    if head and tail and not path.exists(head):\n        try:\n            makedirs(head, mode)\n        except OSError, e:\n            # be happy if someone already created the path\n            if e.errno != errno.EEXIST:\n                raise\n        if tail == curdir:           # xxx/newdir/. exists if xxx/newdir exists\n            return\n    mkdir(name, mode)\n\ndef removedirs(name):\n    \"\"\"removedirs(path)\n\n    Super-rmdir; remove a leaf directory and all empty intermediate\n    ones.  Works like rmdir except that, if the leaf directory is\n    successfully removed, directories corresponding to rightmost path\n    segments will be pruned away until either the whole path is\n    consumed or an error occurs.  Errors during this latter phase are\n    ignored -- they generally mean that a directory was not empty.\n\n    \"\"\"\n    rmdir(name)\n    head, tail = path.split(name)\n    if not tail:\n        head, tail = path.split(head)\n    while head and tail:\n        try:\n            rmdir(head)\n        except error:\n            break\n        head, tail = path.split(head)\n\ndef renames(old, new):\n    \"\"\"renames(old, new)\n\n    Super-rename; create directories as necessary and delete any left\n    empty.  Works like rename, except creation of any intermediate\n    directories needed to make the new pathname good is attempted\n    first.  After the rename, directories corresponding to rightmost\n    path segments of the old name will be pruned way until either the\n    whole path is consumed or a nonempty directory is found.\n\n    Note: this function can fail with the new directory structure made\n    if you lack permissions needed to unlink the leaf directory or\n    file.\n\n    \"\"\"\n    head, tail = path.split(new)\n    if head and tail and not path.exists(head):\n        makedirs(head)\n    rename(old, new)\n    head, tail = path.split(old)\n    if head and tail:\n        try:\n            removedirs(head)\n        except error:\n            pass\n\n__all__.extend([\"makedirs\", \"removedirs\", \"renames\"])\n\ndef walk(top, topdown=True, onerror=None, followlinks=False):\n    \"\"\"Directory tree generator.\n\n    For each directory in the directory tree rooted at top (including top\n    itself, but excluding '.' and '..'), yields a 3-tuple\n\n        dirpath, dirnames, filenames\n\n    dirpath is a string, the path to the directory.  dirnames is a list of\n    the names of the subdirectories in dirpath (excluding '.' and '..').\n    filenames is a list of the names of the non-directory files in dirpath.\n    Note that the names in the lists are just names, with no path components.\n    To get a full path (which begins with top) to a file or directory in\n    dirpath, do os.path.join(dirpath, name).\n\n    If optional arg 'topdown' is true or not specified, the triple for a\n    directory is generated before the triples for any of its subdirectories\n    (directories are generated top down).  If topdown is false, the triple\n    for a directory is generated after the triples for all of its\n    subdirectories (directories are generated bottom up).\n\n    When topdown is true, the caller can modify the dirnames list in-place\n    (e.g., via del or slice assignment), and walk will only recurse into the\n    subdirectories whose names remain in dirnames; this can be used to prune the\n    search, or to impose a specific order of visiting.  Modifying dirnames when\n    topdown is false is ineffective, since the directories in dirnames have\n    already been generated by the time dirnames itself is generated. No matter\n    the value of topdown, the list of subdirectories is retrieved before the\n    tuples for the directory and its subdirectories are generated.\n\n    By default errors from the os.listdir() call are ignored.  If\n    optional arg 'onerror' is specified, it should be a function; it\n    will be called with one argument, an os.error instance.  It can\n    report the error to continue with the walk, or raise the exception\n    to abort the walk.  Note that the filename is available as the\n    filename attribute of the exception object.\n\n    By default, os.walk does not follow symbolic links to subdirectories on\n    systems that support them.  In order to get this functionality, set the\n    optional argument 'followlinks' to true.\n\n    Caution:  if you pass a relative pathname for top, don't change the\n    current working directory between resumptions of walk.  walk never\n    changes the current directory, and assumes that the client doesn't\n    either.\n\n    Example:\n\n    import os\n    from os.path import join, getsize\n    for root, dirs, files in os.walk('python/Lib/email'):\n        print root, \"consumes\",\n        print sum([getsize(join(root, name)) for name in files]),\n        print \"bytes in\", len(files), \"non-directory files\"\n        if 'CVS' in dirs:\n            dirs.remove('CVS')  # don't visit CVS directories\n\n    \"\"\"\n\n    islink, join, isdir = path.islink, path.join, path.isdir\n\n    # We may not have read permission for top, in which case we can't\n    # get a list of the files the directory contains.  os.path.walk\n    # always suppressed the exception then, rather than blow up for a\n    # minor reason when (say) a thousand readable directories are still\n    # left to visit.  That logic is copied here.\n    try:\n        # Note that listdir and error are globals in this module due\n        # to earlier import-*.\n        names = listdir(top)\n    except error, err:\n        if onerror is not None:\n            onerror(err)\n        return\n\n    dirs, nondirs = [], []\n    for name in names:\n        if isdir(join(top, name)):\n            dirs.append(name)\n        else:\n            nondirs.append(name)\n\n    if topdown:\n        yield top, dirs, nondirs\n    for name in dirs:\n        new_path = join(top, name)\n        if followlinks or not islink(new_path):\n            for x in walk(new_path, topdown, onerror, followlinks):\n                yield x\n    if not topdown:\n        yield top, dirs, nondirs\n\n__all__.append(\"walk\")\n\n# Make sure os.environ exists, at least\ntry:\n    environ\nexcept NameError:\n    environ = {}\n\ndef execl(file, *args):\n    \"\"\"execl(file, *args)\n\n    Execute the executable file with argument list args, replacing the\n    current process. \"\"\"\n    execv(file, args)\n\ndef execle(file, *args):\n    \"\"\"execle(file, *args, env)\n\n    Execute the executable file with argument list args and\n    environment env, replacing the current process. \"\"\"\n    env = args[-1]\n    execve(file, args[:-1], env)\n\ndef execlp(file, *args):\n    \"\"\"execlp(file, *args)\n\n    Execute the executable file (which is searched for along $PATH)\n    with argument list args, replacing the current process. \"\"\"\n    execvp(file, args)\n\ndef execlpe(file, *args):\n    \"\"\"execlpe(file, *args, env)\n\n    Execute the executable file (which is searched for along $PATH)\n    with argument list args and environment env, replacing the current\n    process. \"\"\"\n    env = args[-1]\n    execvpe(file, args[:-1], env)\n\ndef execvp(file, args):\n    \"\"\"execvp(file, args)\n\n    Execute the executable file (which is searched for along $PATH)\n    with argument list args, replacing the current process.\n    args may be a list or tuple of strings. \"\"\"\n    _execvpe(file, args)\n\ndef execvpe(file, args, env):\n    \"\"\"execvpe(file, args, env)\n\n    Execute the executable file (which is searched for along $PATH)\n    with argument list args and environment env , replacing the\n    current process.\n    args may be a list or tuple of strings. \"\"\"\n    _execvpe(file, args, env)\n\n__all__.extend([\"execl\",\"execle\",\"execlp\",\"execlpe\",\"execvp\",\"execvpe\"])\n\ndef _execvpe(file, args, env=None):\n    if env is not None:\n        func = execve\n        argrest = (args, env)\n    else:\n        func = execv\n        argrest = (args,)\n        env = environ\n\n    head, tail = path.split(file)\n    if head:\n        func(file, *argrest)\n        return\n    if 'PATH' in env:\n        envpath = env['PATH']\n    else:\n        envpath = defpath\n    PATH = envpath.split(pathsep)\n    saved_exc = None\n    saved_tb = None\n    for dir in PATH:\n        fullname = path.join(dir, file)\n        try:\n            func(fullname, *argrest)\n        except error, e:\n            tb = sys.exc_info()[2]\n            if (e.errno != errno.ENOENT and e.errno != errno.ENOTDIR\n                and saved_exc is None):\n                saved_exc = e\n                saved_tb = tb\n    if saved_exc:\n        raise error, saved_exc, saved_tb\n    raise error, e, tb\n\n# Change environ to automatically call putenv() if it exists\ntry:\n    # This will fail if there's no putenv\n    putenv\nexcept NameError:\n    pass\nelse:\n    import UserDict\n\n    # Fake unsetenv() for Windows\n    # not sure about os2 here but\n    # I'm guessing they are the same.\n\n    if name in ('os2', 'nt'):\n        def unsetenv(key):\n            putenv(key, \"\")\n\n    if name == \"riscos\":\n        # On RISC OS, all env access goes through getenv and putenv\n        from riscosenviron import _Environ\n    elif name in ('os2', 'nt'):  # Where Env Var Names Must Be UPPERCASE\n        # But we store them as upper case\n        class _Environ(UserDict.IterableUserDict):\n            def __init__(self, environ):\n                UserDict.UserDict.__init__(self)\n                data = self.data\n                for k, v in environ.items():\n                    data[k.upper()] = v\n            def __setitem__(self, key, item):\n                putenv(key, item)\n                self.data[key.upper()] = item\n            def __getitem__(self, key):\n                return self.data[key.upper()]\n            try:\n                unsetenv\n            except NameError:\n                def __delitem__(self, key):\n                    del self.data[key.upper()]\n            else:\n                def __delitem__(self, key):\n                    unsetenv(key)\n                    del self.data[key.upper()]\n                def clear(self):\n                    for key in self.data.keys():\n                        unsetenv(key)\n                        del self.data[key]\n                def pop(self, key, *args):\n                    unsetenv(key)\n                    return self.data.pop(key.upper(), *args)\n            def has_key(self, key):\n                return key.upper() in self.data\n            def __contains__(self, key):\n                return key.upper() in self.data\n            def get(self, key, failobj=None):\n                return self.data.get(key.upper(), failobj)\n            def update(self, dict=None, **kwargs):\n                if dict:\n                    try:\n                        keys = dict.keys()\n                    except AttributeError:\n                        # List of (key, value)\n                        for k, v in dict:\n                            self[k] = v\n                    else:\n                        # got keys\n                        # cannot use items(), since mappings\n                        # may not have them.\n                        for k in keys:\n                            self[k] = dict[k]\n                if kwargs:\n                    self.update(kwargs)\n            def copy(self):\n                return dict(self)\n\n    else:  # Where Env Var Names Can Be Mixed Case\n        class _Environ(UserDict.IterableUserDict):\n            def __init__(self, environ):\n                UserDict.UserDict.__init__(self)\n                self.data = environ\n            def __setitem__(self, key, item):\n                putenv(key, item)\n                self.data[key] = item\n            def update(self,  dict=None, **kwargs):\n                if dict:\n                    try:\n                        keys = dict.keys()\n                    except AttributeError:\n                        # List of (key, value)\n                        for k, v in dict:\n                            self[k] = v\n                    else:\n                        # got keys\n                        # cannot use items(), since mappings\n                        # may not have them.\n                        for k in keys:\n                            self[k] = dict[k]\n                if kwargs:\n                    self.update(kwargs)\n            try:\n                unsetenv\n            except NameError:\n                pass\n            else:\n                def __delitem__(self, key):\n                    unsetenv(key)\n                    del self.data[key]\n                def clear(self):\n                    for key in self.data.keys():\n                        unsetenv(key)\n                        del self.data[key]\n                def pop(self, key, *args):\n                    unsetenv(key)\n                    return self.data.pop(key, *args)\n            def copy(self):\n                return dict(self)\n\n\n    environ = _Environ(environ)\n\ndef getenv(key, default=None):\n    \"\"\"Get an environment variable, return None if it doesn't exist.\n    The optional second argument can specify an alternate default.\"\"\"\n    return environ.get(key, default)\n__all__.append(\"getenv\")\n\ndef _exists(name):\n    return name in globals()\n\n# Supply spawn*() (probably only for Unix)\nif _exists(\"fork\") and not _exists(\"spawnv\") and _exists(\"execv\"):\n\n    P_WAIT = 0\n    P_NOWAIT = P_NOWAITO = 1\n\n    # XXX Should we support P_DETACH?  I suppose it could fork()**2\n    # and close the std I/O streams.  Also, P_OVERLAY is the same\n    # as execv*()?\n\n    def _spawnvef(mode, file, args, env, func):\n        # Internal helper; func is the exec*() function to use\n        pid = fork()\n        if not pid:\n            # Child\n            try:\n                if env is None:\n                    func(file, args)\n                else:\n                    func(file, args, env)\n            except:\n                _exit(127)\n        else:\n            # Parent\n            if mode == P_NOWAIT:\n                return pid # Caller is responsible for waiting!\n            while 1:\n                wpid, sts = waitpid(pid, 0)\n                if WIFSTOPPED(sts):\n                    continue\n                elif WIFSIGNALED(sts):\n                    return -WTERMSIG(sts)\n                elif WIFEXITED(sts):\n                    return WEXITSTATUS(sts)\n                else:\n                    raise error, \"Not stopped, signaled or exited???\"\n\n    def spawnv(mode, file, args):\n        \"\"\"spawnv(mode, file, args) -> integer\n\nExecute file with arguments from args in a subprocess.\nIf mode == P_NOWAIT return the pid of the process.\nIf mode == P_WAIT return the process's exit code if it exits normally;\notherwise return -SIG, where SIG is the signal that killed it. \"\"\"\n        return _spawnvef(mode, file, args, None, execv)\n\n    def spawnve(mode, file, args, env):\n        \"\"\"spawnve(mode, file, args, env) -> integer\n\nExecute file with arguments from args in a subprocess with the\nspecified environment.\nIf mode == P_NOWAIT return the pid of the process.\nIf mode == P_WAIT return the process's exit code if it exits normally;\notherwise return -SIG, where SIG is the signal that killed it. \"\"\"\n        return _spawnvef(mode, file, args, env, execve)\n\n    # Note: spawnvp[e] is't currently supported on Windows\n\n    def spawnvp(mode, file, args):\n        \"\"\"spawnvp(mode, file, args) -> integer\n\nExecute file (which is looked for along $PATH) with arguments from\nargs in a subprocess.\nIf mode == P_NOWAIT return the pid of the process.\nIf mode == P_WAIT return the process's exit code if it exits normally;\notherwise return -SIG, where SIG is the signal that killed it. \"\"\"\n        return _spawnvef(mode, file, args, None, execvp)\n\n    def spawnvpe(mode, file, args, env):\n        \"\"\"spawnvpe(mode, file, args, env) -> integer\n\nExecute file (which is looked for along $PATH) with arguments from\nargs in a subprocess with the supplied environment.\nIf mode == P_NOWAIT return the pid of the process.\nIf mode == P_WAIT return the process's exit code if it exits normally;\notherwise return -SIG, where SIG is the signal that killed it. \"\"\"\n        return _spawnvef(mode, file, args, env, execvpe)\n\nif _exists(\"spawnv\"):\n    # These aren't supplied by the basic Windows code\n    # but can be easily implemented in Python\n\n    def spawnl(mode, file, *args):\n        \"\"\"spawnl(mode, file, *args) -> integer\n\nExecute file with arguments from args in a subprocess.\nIf mode == P_NOWAIT return the pid of the process.\nIf mode == P_WAIT return the process's exit code if it exits normally;\notherwise return -SIG, where SIG is the signal that killed it. \"\"\"\n        return spawnv(mode, file, args)\n\n    def spawnle(mode, file, *args):\n        \"\"\"spawnle(mode, file, *args, env) -> integer\n\nExecute file with arguments from args in a subprocess with the\nsupplied environment.\nIf mode == P_NOWAIT return the pid of the process.\nIf mode == P_WAIT return the process's exit code if it exits normally;\notherwise return -SIG, where SIG is the signal that killed it. \"\"\"\n        env = args[-1]\n        return spawnve(mode, file, args[:-1], env)\n\n\n    __all__.extend([\"spawnv\", \"spawnve\", \"spawnl\", \"spawnle\",])\n\n\nif _exists(\"spawnvp\"):\n    # At the moment, Windows doesn't implement spawnvp[e],\n    # so it won't have spawnlp[e] either.\n    def spawnlp(mode, file, *args):\n        \"\"\"spawnlp(mode, file, *args) -> integer\n\nExecute file (which is looked for along $PATH) with arguments from\nargs in a subprocess with the supplied environment.\nIf mode == P_NOWAIT return the pid of the process.\nIf mode == P_WAIT return the process's exit code if it exits normally;\notherwise return -SIG, where SIG is the signal that killed it. \"\"\"\n        return spawnvp(mode, file, args)\n\n    def spawnlpe(mode, file, *args):\n        \"\"\"spawnlpe(mode, file, *args, env) -> integer\n\nExecute file (which is looked for along $PATH) with arguments from\nargs in a subprocess with the supplied environment.\nIf mode == P_NOWAIT return the pid of the process.\nIf mode == P_WAIT return the process's exit code if it exits normally;\notherwise return -SIG, where SIG is the signal that killed it. \"\"\"\n        env = args[-1]\n        return spawnvpe(mode, file, args[:-1], env)\n\n\n    __all__.extend([\"spawnvp\", \"spawnvpe\", \"spawnlp\", \"spawnlpe\",])\n\n\n# Supply popen2 etc. (for Unix)\nif _exists(\"fork\"):\n    if not _exists(\"popen2\"):\n        def popen2(cmd, mode=\"t\", bufsize=-1):\n            \"\"\"Execute the shell command 'cmd' in a sub-process.  On UNIX, 'cmd'\n            may be a sequence, in which case arguments will be passed directly to\n            the program without shell intervention (as with os.spawnv()).  If 'cmd'\n            is a string it will be passed to the shell (as with os.system()). If\n            'bufsize' is specified, it sets the buffer size for the I/O pipes.  The\n            file objects (child_stdin, child_stdout) are returned.\"\"\"\n            import warnings\n            msg = \"os.popen2 is deprecated.  Use the subprocess module.\"\n            warnings.warn(msg, DeprecationWarning, stacklevel=2)\n\n            import subprocess\n            PIPE = subprocess.PIPE\n            p = subprocess.Popen(cmd, shell=isinstance(cmd, basestring),\n                                 bufsize=bufsize, stdin=PIPE, stdout=PIPE,\n                                 close_fds=True)\n            return p.stdin, p.stdout\n        __all__.append(\"popen2\")\n\n    if not _exists(\"popen3\"):\n        def popen3(cmd, mode=\"t\", bufsize=-1):\n            \"\"\"Execute the shell command 'cmd' in a sub-process.  On UNIX, 'cmd'\n            may be a sequence, in which case arguments will be passed directly to\n            the program without shell intervention (as with os.spawnv()).  If 'cmd'\n            is a string it will be passed to the shell (as with os.system()). If\n            'bufsize' is specified, it sets the buffer size for the I/O pipes.  The\n            file objects (child_stdin, child_stdout, child_stderr) are returned.\"\"\"\n            import warnings\n            msg = \"os.popen3 is deprecated.  Use the subprocess module.\"\n            warnings.warn(msg, DeprecationWarning, stacklevel=2)\n\n            import subprocess\n            PIPE = subprocess.PIPE\n            p = subprocess.Popen(cmd, shell=isinstance(cmd, basestring),\n                                 bufsize=bufsize, stdin=PIPE, stdout=PIPE,\n                                 stderr=PIPE, close_fds=True)\n            return p.stdin, p.stdout, p.stderr\n        __all__.append(\"popen3\")\n\n    if not _exists(\"popen4\"):\n        def popen4(cmd, mode=\"t\", bufsize=-1):\n            \"\"\"Execute the shell command 'cmd' in a sub-process.  On UNIX, 'cmd'\n            may be a sequence, in which case arguments will be passed directly to\n            the program without shell intervention (as with os.spawnv()).  If 'cmd'\n            is a string it will be passed to the shell (as with os.system()). If\n            'bufsize' is specified, it sets the buffer size for the I/O pipes.  The\n            file objects (child_stdin, child_stdout_stderr) are returned.\"\"\"\n            import warnings\n            msg = \"os.popen4 is deprecated.  Use the subprocess module.\"\n            warnings.warn(msg, DeprecationWarning, stacklevel=2)\n\n            import subprocess\n            PIPE = subprocess.PIPE\n            p = subprocess.Popen(cmd, shell=isinstance(cmd, basestring),\n                                 bufsize=bufsize, stdin=PIPE, stdout=PIPE,\n                                 stderr=subprocess.STDOUT, close_fds=True)\n            return p.stdin, p.stdout\n        __all__.append(\"popen4\")\n\nimport copy_reg as _copy_reg\n\ndef _make_stat_result(tup, dict):\n    return stat_result(tup, dict)\n\ndef _pickle_stat_result(sr):\n    (type, args) = sr.__reduce__()\n    return (_make_stat_result, args)\n\ntry:\n    _copy_reg.pickle(stat_result, _pickle_stat_result, _make_stat_result)\nexcept NameError: # stat_result may not exist\n    pass\n\ndef _make_statvfs_result(tup, dict):\n    return statvfs_result(tup, dict)\n\ndef _pickle_statvfs_result(sr):\n    (type, args) = sr.__reduce__()\n    return (_make_statvfs_result, args)\n\ntry:\n    _copy_reg.pickle(statvfs_result, _pickle_statvfs_result,\n                     _make_statvfs_result)\nexcept NameError: # statvfs_result may not exist\n    pass\n", 
    "pdb": "#! /usr/bin/env python\n\n\"\"\"A Python debugger.\"\"\"\n\n# (See pdb.doc for documentation.)\n\nimport sys\nimport linecache\nimport cmd\nimport bdb\nfrom repr import Repr\nimport os\nimport re\nimport pprint\nimport traceback\n\n\nclass Restart(Exception):\n    \"\"\"Causes a debugger to be restarted for the debugged python program.\"\"\"\n    pass\n\n# Create a custom safe Repr instance and increase its maxstring.\n# The default of 30 truncates error messages too easily.\n_repr = Repr()\n_repr.maxstring = 200\n_saferepr = _repr.repr\n\n__all__ = [\"run\", \"pm\", \"Pdb\", \"runeval\", \"runctx\", \"runcall\", \"set_trace\",\n           \"post_mortem\", \"help\"]\n\ndef find_function(funcname, filename):\n    cre = re.compile(r'def\\s+%s\\s*[(]' % re.escape(funcname))\n    try:\n        fp = open(filename)\n    except IOError:\n        return None\n    # consumer of this info expects the first line to be 1\n    lineno = 1\n    answer = None\n    while 1:\n        line = fp.readline()\n        if line == '':\n            break\n        if cre.match(line):\n            answer = funcname, filename, lineno\n            break\n        lineno = lineno + 1\n    fp.close()\n    return answer\n\n\n# Interaction prompt line will separate file and call info from code\n# text using value of line_prefix string.  A newline and arrow may\n# be to your liking.  You can set it once pdb is imported using the\n# command \"pdb.line_prefix = '\\n% '\".\n# line_prefix = ': '    # Use this to get the old situation back\nline_prefix = '\\n-> '   # Probably a better default\n\nclass Pdb(bdb.Bdb, cmd.Cmd):\n\n    def __init__(self, completekey='tab', stdin=None, stdout=None, skip=None):\n        bdb.Bdb.__init__(self, skip=skip)\n        cmd.Cmd.__init__(self, completekey, stdin, stdout)\n        if stdout:\n            self.use_rawinput = 0\n        self.prompt = '(Pdb) '\n        self.aliases = {}\n        self.mainpyfile = ''\n        self._wait_for_mainpyfile = 0\n        # Try to load readline if it exists\n        try:\n            import readline\n        except ImportError:\n            pass\n\n        # Read $HOME/.pdbrc and ./.pdbrc\n        self.rcLines = []\n        if 'HOME' in os.environ:\n            envHome = os.environ['HOME']\n            try:\n                rcFile = open(os.path.join(envHome, \".pdbrc\"))\n            except IOError:\n                pass\n            else:\n                for line in rcFile.readlines():\n                    self.rcLines.append(line)\n                rcFile.close()\n        try:\n            rcFile = open(\".pdbrc\")\n        except IOError:\n            pass\n        else:\n            for line in rcFile.readlines():\n                self.rcLines.append(line)\n            rcFile.close()\n\n        self.commands = {} # associates a command list to breakpoint numbers\n        self.commands_doprompt = {} # for each bp num, tells if the prompt\n                                    # must be disp. after execing the cmd list\n        self.commands_silent = {} # for each bp num, tells if the stack trace\n                                  # must be disp. after execing the cmd list\n        self.commands_defining = False # True while in the process of defining\n                                       # a command list\n        self.commands_bnum = None # The breakpoint number for which we are\n                                  # defining a list\n\n    def reset(self):\n        bdb.Bdb.reset(self)\n        self.forget()\n\n    def forget(self):\n        self.lineno = None\n        self.stack = []\n        self.curindex = 0\n        self.curframe = None\n\n    def setup(self, f, t):\n        self.forget()\n        self.stack, self.curindex = self.get_stack(f, t)\n        self.curframe = self.stack[self.curindex][0]\n        # The f_locals dictionary is updated from the actual frame\n        # locals whenever the .f_locals accessor is called, so we\n        # cache it here to ensure that modifications are not overwritten.\n        self.curframe_locals = self.curframe.f_locals\n        self.execRcLines()\n\n    # Can be executed earlier than 'setup' if desired\n    def execRcLines(self):\n        if self.rcLines:\n            # Make local copy because of recursion\n            rcLines = self.rcLines\n            # executed only once\n            self.rcLines = []\n            for line in rcLines:\n                line = line[:-1]\n                if len(line) > 0 and line[0] != '#':\n                    self.onecmd(line)\n\n    # Override Bdb methods\n\n    def user_call(self, frame, argument_list):\n        \"\"\"This method is called when there is the remote possibility\n        that we ever need to stop in this function.\"\"\"\n        if self._wait_for_mainpyfile:\n            return\n        if self.stop_here(frame):\n            print >>self.stdout, '--Call--'\n            self.interaction(frame, None)\n\n    def user_line(self, frame):\n        \"\"\"This function is called when we stop or break at this line.\"\"\"\n        if self._wait_for_mainpyfile:\n            if (self.mainpyfile != self.canonic(frame.f_code.co_filename)\n                or frame.f_lineno<= 0):\n                return\n            self._wait_for_mainpyfile = 0\n        if self.bp_commands(frame):\n            self.interaction(frame, None)\n\n    def bp_commands(self,frame):\n        \"\"\"Call every command that was set for the current active breakpoint\n        (if there is one).\n\n        Returns True if the normal interaction function must be called,\n        False otherwise.\"\"\"\n        # self.currentbp is set in bdb in Bdb.break_here if a breakpoint was hit\n        if getattr(self, \"currentbp\", False) and \\\n               self.currentbp in self.commands:\n            currentbp = self.currentbp\n            self.currentbp = 0\n            lastcmd_back = self.lastcmd\n            self.setup(frame, None)\n            for line in self.commands[currentbp]:\n                self.onecmd(line)\n            self.lastcmd = lastcmd_back\n            if not self.commands_silent[currentbp]:\n                self.print_stack_entry(self.stack[self.curindex])\n            if self.commands_doprompt[currentbp]:\n                self.cmdloop()\n            self.forget()\n            return\n        return 1\n\n    def user_return(self, frame, return_value):\n        \"\"\"This function is called when a return trap is set here.\"\"\"\n        if self._wait_for_mainpyfile:\n            return\n        frame.f_locals['__return__'] = return_value\n        print >>self.stdout, '--Return--'\n        self.interaction(frame, None)\n\n    def user_exception(self, frame, exc_info):\n        \"\"\"This function is called if an exception occurs,\n        but only if we are to stop at or just below this level.\"\"\"\n        if self._wait_for_mainpyfile:\n            return\n        exc_type, exc_value, exc_traceback = exc_info\n        frame.f_locals['__exception__'] = exc_type, exc_value\n        if type(exc_type) == type(''):\n            exc_type_name = exc_type\n        else: exc_type_name = exc_type.__name__\n        print >>self.stdout, exc_type_name + ':', _saferepr(exc_value)\n        self.interaction(frame, exc_traceback)\n\n    # General interaction function\n\n    def interaction(self, frame, traceback):\n        self.setup(frame, traceback)\n        self.print_stack_entry(self.stack[self.curindex])\n        self.cmdloop()\n        self.forget()\n\n    def displayhook(self, obj):\n        \"\"\"Custom displayhook for the exec in default(), which prevents\n        assignment of the _ variable in the builtins.\n        \"\"\"\n        # reproduce the behavior of the standard displayhook, not printing None\n        if obj is not None:\n            print repr(obj)\n\n    def default(self, line):\n        if line[:1] == '!': line = line[1:]\n        locals = self.curframe_locals\n        globals = self.curframe.f_globals\n        try:\n            code = compile(line + '\\n', '<stdin>', 'single')\n            save_stdout = sys.stdout\n            save_stdin = sys.stdin\n            save_displayhook = sys.displayhook\n            try:\n                sys.stdin = self.stdin\n                sys.stdout = self.stdout\n                sys.displayhook = self.displayhook\n                exec code in globals, locals\n            finally:\n                sys.stdout = save_stdout\n                sys.stdin = save_stdin\n                sys.displayhook = save_displayhook\n        except:\n            t, v = sys.exc_info()[:2]\n            if type(t) == type(''):\n                exc_type_name = t\n            else: exc_type_name = t.__name__\n            print >>self.stdout, '***', exc_type_name + ':', v\n\n    def precmd(self, line):\n        \"\"\"Handle alias expansion and ';;' separator.\"\"\"\n        if not line.strip():\n            return line\n        args = line.split()\n        while args[0] in self.aliases:\n            line = self.aliases[args[0]]\n            ii = 1\n            for tmpArg in args[1:]:\n                line = line.replace(\"%\" + str(ii),\n                                      tmpArg)\n                ii = ii + 1\n            line = line.replace(\"%*\", ' '.join(args[1:]))\n            args = line.split()\n        # split into ';;' separated commands\n        # unless it's an alias command\n        if args[0] != 'alias':\n            marker = line.find(';;')\n            if marker >= 0:\n                # queue up everything after marker\n                next = line[marker+2:].lstrip()\n                self.cmdqueue.append(next)\n                line = line[:marker].rstrip()\n        return line\n\n    def onecmd(self, line):\n        \"\"\"Interpret the argument as though it had been typed in response\n        to the prompt.\n\n        Checks whether this line is typed at the normal prompt or in\n        a breakpoint command list definition.\n        \"\"\"\n        if not self.commands_defining:\n            return cmd.Cmd.onecmd(self, line)\n        else:\n            return self.handle_command_def(line)\n\n    def handle_command_def(self,line):\n        \"\"\"Handles one command line during command list definition.\"\"\"\n        cmd, arg, line = self.parseline(line)\n        if not cmd:\n            return\n        if cmd == 'silent':\n            self.commands_silent[self.commands_bnum] = True\n            return # continue to handle other cmd def in the cmd list\n        elif cmd == 'end':\n            self.cmdqueue = []\n            return 1 # end of cmd list\n        cmdlist = self.commands[self.commands_bnum]\n        if arg:\n            cmdlist.append(cmd+' '+arg)\n        else:\n            cmdlist.append(cmd)\n        # Determine if we must stop\n        try:\n            func = getattr(self, 'do_' + cmd)\n        except AttributeError:\n            func = self.default\n        # one of the resuming commands\n        if func.func_name in self.commands_resuming:\n            self.commands_doprompt[self.commands_bnum] = False\n            self.cmdqueue = []\n            return 1\n        return\n\n    # Command definitions, called by cmdloop()\n    # The argument is the remaining string on the command line\n    # Return true to exit from the command loop\n\n    do_h = cmd.Cmd.do_help\n\n    def do_commands(self, arg):\n        \"\"\"Defines a list of commands associated to a breakpoint.\n\n        Those commands will be executed whenever the breakpoint causes\n        the program to stop execution.\"\"\"\n        if not arg:\n            bnum = len(bdb.Breakpoint.bpbynumber)-1\n        else:\n            try:\n                bnum = int(arg)\n            except:\n                print >>self.stdout, \"Usage : commands [bnum]\\n        ...\" \\\n                                     \"\\n        end\"\n                return\n        self.commands_bnum = bnum\n        self.commands[bnum] = []\n        self.commands_doprompt[bnum] = True\n        self.commands_silent[bnum] = False\n        prompt_back = self.prompt\n        self.prompt = '(com) '\n        self.commands_defining = True\n        try:\n            self.cmdloop()\n        finally:\n            self.commands_defining = False\n            self.prompt = prompt_back\n\n    def do_break(self, arg, temporary = 0):\n        # break [ ([filename:]lineno | function) [, \"condition\"] ]\n        if not arg:\n            if self.breaks:  # There's at least one\n                print >>self.stdout, \"Num Type         Disp Enb   Where\"\n                for bp in bdb.Breakpoint.bpbynumber:\n                    if bp:\n                        bp.bpprint(self.stdout)\n            return\n        # parse arguments; comma has lowest precedence\n        # and cannot occur in filename\n        filename = None\n        lineno = None\n        cond = None\n        comma = arg.find(',')\n        if comma > 0:\n            # parse stuff after comma: \"condition\"\n            cond = arg[comma+1:].lstrip()\n            arg = arg[:comma].rstrip()\n        # parse stuff before comma: [filename:]lineno | function\n        colon = arg.rfind(':')\n        funcname = None\n        if colon >= 0:\n            filename = arg[:colon].rstrip()\n            f = self.lookupmodule(filename)\n            if not f:\n                print >>self.stdout, '*** ', repr(filename),\n                print >>self.stdout, 'not found from sys.path'\n                return\n            else:\n                filename = f\n            arg = arg[colon+1:].lstrip()\n            try:\n                lineno = int(arg)\n            except ValueError, msg:\n                print >>self.stdout, '*** Bad lineno:', arg\n                return\n        else:\n            # no colon; can be lineno or function\n            try:\n                lineno = int(arg)\n            except ValueError:\n                try:\n                    func = eval(arg,\n                                self.curframe.f_globals,\n                                self.curframe_locals)\n                except:\n                    func = arg\n                try:\n                    if hasattr(func, 'im_func'):\n                        func = func.im_func\n                    code = func.func_code\n                    #use co_name to identify the bkpt (function names\n                    #could be aliased, but co_name is invariant)\n                    funcname = code.co_name\n                    lineno = code.co_firstlineno\n                    filename = code.co_filename\n                except:\n                    # last thing to try\n                    (ok, filename, ln) = self.lineinfo(arg)\n                    if not ok:\n                        print >>self.stdout, '*** The specified object',\n                        print >>self.stdout, repr(arg),\n                        print >>self.stdout, 'is not a function'\n                        print >>self.stdout, 'or was not found along sys.path.'\n                        return\n                    funcname = ok # ok contains a function name\n                    lineno = int(ln)\n        if not filename:\n            filename = self.defaultFile()\n        # Check for reasonable breakpoint\n        line = self.checkline(filename, lineno)\n        if line:\n            # now set the break point\n            err = self.set_break(filename, line, temporary, cond, funcname)\n            if err: print >>self.stdout, '***', err\n            else:\n                bp = self.get_breaks(filename, line)[-1]\n                print >>self.stdout, \"Breakpoint %d at %s:%d\" % (bp.number,\n                                                                 bp.file,\n                                                                 bp.line)\n\n    # To be overridden in derived debuggers\n    def defaultFile(self):\n        \"\"\"Produce a reasonable default.\"\"\"\n        filename = self.curframe.f_code.co_filename\n        if filename == '<string>' and self.mainpyfile:\n            filename = self.mainpyfile\n        return filename\n\n    do_b = do_break\n\n    def do_tbreak(self, arg):\n        self.do_break(arg, 1)\n\n    def lineinfo(self, identifier):\n        failed = (None, None, None)\n        # Input is identifier, may be in single quotes\n        idstring = identifier.split(\"'\")\n        if len(idstring) == 1:\n            # not in single quotes\n            id = idstring[0].strip()\n        elif len(idstring) == 3:\n            # quoted\n            id = idstring[1].strip()\n        else:\n            return failed\n        if id == '': return failed\n        parts = id.split('.')\n        # Protection for derived debuggers\n        if parts[0] == 'self':\n            del parts[0]\n            if len(parts) == 0:\n                return failed\n        # Best first guess at file to look at\n        fname = self.defaultFile()\n        if len(parts) == 1:\n            item = parts[0]\n        else:\n            # More than one part.\n            # First is module, second is method/class\n            f = self.lookupmodule(parts[0])\n            if f:\n                fname = f\n            item = parts[1]\n        answer = find_function(item, fname)\n        return answer or failed\n\n    def checkline(self, filename, lineno):\n        \"\"\"Check whether specified line seems to be executable.\n\n        Return `lineno` if it is, 0 if not (e.g. a docstring, comment, blank\n        line or EOF). Warning: testing is not comprehensive.\n        \"\"\"\n        # this method should be callable before starting debugging, so default\n        # to \"no globals\" if there is no current frame\n        globs = self.curframe.f_globals if hasattr(self, 'curframe') else None\n        line = linecache.getline(filename, lineno, globs)\n        if not line:\n            print >>self.stdout, 'End of file'\n            return 0\n        line = line.strip()\n        # Don't allow setting breakpoint at a blank line\n        if (not line or (line[0] == '#') or\n             (line[:3] == '\"\"\"') or line[:3] == \"'''\"):\n            print >>self.stdout, '*** Blank or comment'\n            return 0\n        return lineno\n\n    def do_enable(self, arg):\n        args = arg.split()\n        for i in args:\n            try:\n                i = int(i)\n            except ValueError:\n                print >>self.stdout, 'Breakpoint index %r is not a number' % i\n                continue\n\n            if not (0 <= i < len(bdb.Breakpoint.bpbynumber)):\n                print >>self.stdout, 'No breakpoint numbered', i\n                continue\n\n            bp = bdb.Breakpoint.bpbynumber[i]\n            if bp:\n                bp.enable()\n\n    def do_disable(self, arg):\n        args = arg.split()\n        for i in args:\n            try:\n                i = int(i)\n            except ValueError:\n                print >>self.stdout, 'Breakpoint index %r is not a number' % i\n                continue\n\n            if not (0 <= i < len(bdb.Breakpoint.bpbynumber)):\n                print >>self.stdout, 'No breakpoint numbered', i\n                continue\n\n            bp = bdb.Breakpoint.bpbynumber[i]\n            if bp:\n                bp.disable()\n\n    def do_condition(self, arg):\n        # arg is breakpoint number and condition\n        args = arg.split(' ', 1)\n        try:\n            bpnum = int(args[0].strip())\n        except ValueError:\n            # something went wrong\n            print >>self.stdout, \\\n                'Breakpoint index %r is not a number' % args[0]\n            return\n        try:\n            cond = args[1]\n        except:\n            cond = None\n        try:\n            bp = bdb.Breakpoint.bpbynumber[bpnum]\n        except IndexError:\n            print >>self.stdout, 'Breakpoint index %r is not valid' % args[0]\n            return\n        if bp:\n            bp.cond = cond\n            if not cond:\n                print >>self.stdout, 'Breakpoint', bpnum,\n                print >>self.stdout, 'is now unconditional.'\n\n    def do_ignore(self,arg):\n        \"\"\"arg is bp number followed by ignore count.\"\"\"\n        args = arg.split()\n        try:\n            bpnum = int(args[0].strip())\n        except ValueError:\n            # something went wrong\n            print >>self.stdout, \\\n                'Breakpoint index %r is not a number' % args[0]\n            return\n        try:\n            count = int(args[1].strip())\n        except:\n            count = 0\n        try:\n            bp = bdb.Breakpoint.bpbynumber[bpnum]\n        except IndexError:\n            print >>self.stdout, 'Breakpoint index %r is not valid' % args[0]\n            return\n        if bp:\n            bp.ignore = count\n            if count > 0:\n                reply = 'Will ignore next '\n                if count > 1:\n                    reply = reply + '%d crossings' % count\n                else:\n                    reply = reply + '1 crossing'\n                print >>self.stdout, reply + ' of breakpoint %d.' % bpnum\n            else:\n                print >>self.stdout, 'Will stop next time breakpoint',\n                print >>self.stdout, bpnum, 'is reached.'\n\n    def do_clear(self, arg):\n        \"\"\"Three possibilities, tried in this order:\n        clear -> clear all breaks, ask for confirmation\n        clear file:lineno -> clear all breaks at file:lineno\n        clear bpno bpno ... -> clear breakpoints by number\"\"\"\n        if not arg:\n            try:\n                reply = raw_input('Clear all breaks? ')\n            except EOFError:\n                reply = 'no'\n            reply = reply.strip().lower()\n            if reply in ('y', 'yes'):\n                self.clear_all_breaks()\n            return\n        if ':' in arg:\n            # Make sure it works for \"clear C:\\foo\\bar.py:12\"\n            i = arg.rfind(':')\n            filename = arg[:i]\n            arg = arg[i+1:]\n            try:\n                lineno = int(arg)\n            except ValueError:\n                err = \"Invalid line number (%s)\" % arg\n            else:\n                err = self.clear_break(filename, lineno)\n            if err: print >>self.stdout, '***', err\n            return\n        numberlist = arg.split()\n        for i in numberlist:\n            try:\n                i = int(i)\n            except ValueError:\n                print >>self.stdout, 'Breakpoint index %r is not a number' % i\n                continue\n\n            if not (0 <= i < len(bdb.Breakpoint.bpbynumber)):\n                print >>self.stdout, 'No breakpoint numbered', i\n                continue\n            err = self.clear_bpbynumber(i)\n            if err:\n                print >>self.stdout, '***', err\n            else:\n                print >>self.stdout, 'Deleted breakpoint', i\n    do_cl = do_clear # 'c' is already an abbreviation for 'continue'\n\n    def do_where(self, arg):\n        self.print_stack_trace()\n    do_w = do_where\n    do_bt = do_where\n\n    def do_up(self, arg):\n        if self.curindex == 0:\n            print >>self.stdout, '*** Oldest frame'\n        else:\n            self.curindex = self.curindex - 1\n            self.curframe = self.stack[self.curindex][0]\n            self.curframe_locals = self.curframe.f_locals\n            self.print_stack_entry(self.stack[self.curindex])\n            self.lineno = None\n    do_u = do_up\n\n    def do_down(self, arg):\n        if self.curindex + 1 == len(self.stack):\n            print >>self.stdout, '*** Newest frame'\n        else:\n            self.curindex = self.curindex + 1\n            self.curframe = self.stack[self.curindex][0]\n            self.curframe_locals = self.curframe.f_locals\n            self.print_stack_entry(self.stack[self.curindex])\n            self.lineno = None\n    do_d = do_down\n\n    def do_until(self, arg):\n        self.set_until(self.curframe)\n        return 1\n    do_unt = do_until\n\n    def do_step(self, arg):\n        self.set_step()\n        return 1\n    do_s = do_step\n\n    def do_next(self, arg):\n        self.set_next(self.curframe)\n        return 1\n    do_n = do_next\n\n    def do_run(self, arg):\n        \"\"\"Restart program by raising an exception to be caught in the main\n        debugger loop.  If arguments were given, set them in sys.argv.\"\"\"\n        if arg:\n            import shlex\n            argv0 = sys.argv[0:1]\n            sys.argv = shlex.split(arg)\n            sys.argv[:0] = argv0\n        raise Restart\n\n    do_restart = do_run\n\n    def do_return(self, arg):\n        self.set_return(self.curframe)\n        return 1\n    do_r = do_return\n\n    def do_continue(self, arg):\n        self.set_continue()\n        return 1\n    do_c = do_cont = do_continue\n\n    def do_jump(self, arg):\n        if self.curindex + 1 != len(self.stack):\n            print >>self.stdout, \"*** You can only jump within the bottom frame\"\n            return\n        try:\n            arg = int(arg)\n        except ValueError:\n            print >>self.stdout, \"*** The 'jump' command requires a line number.\"\n        else:\n            try:\n                # Do the jump, fix up our copy of the stack, and display the\n                # new position\n                self.curframe.f_lineno = arg\n                self.stack[self.curindex] = self.stack[self.curindex][0], arg\n                self.print_stack_entry(self.stack[self.curindex])\n            except ValueError, e:\n                print >>self.stdout, '*** Jump failed:', e\n    do_j = do_jump\n\n    def do_debug(self, arg):\n        sys.settrace(None)\n        globals = self.curframe.f_globals\n        locals = self.curframe_locals\n        p = Pdb(self.completekey, self.stdin, self.stdout)\n        p.prompt = \"(%s) \" % self.prompt.strip()\n        print >>self.stdout, \"ENTERING RECURSIVE DEBUGGER\"\n        sys.call_tracing(p.run, (arg, globals, locals))\n        print >>self.stdout, \"LEAVING RECURSIVE DEBUGGER\"\n        sys.settrace(self.trace_dispatch)\n        self.lastcmd = p.lastcmd\n\n    def do_quit(self, arg):\n        self._user_requested_quit = 1\n        self.set_quit()\n        return 1\n\n    do_q = do_quit\n    do_exit = do_quit\n\n    def do_EOF(self, arg):\n        print >>self.stdout\n        self._user_requested_quit = 1\n        self.set_quit()\n        return 1\n\n    def do_args(self, arg):\n        co = self.curframe.f_code\n        dict = self.curframe_locals\n        n = co.co_argcount\n        if co.co_flags & 4: n = n+1\n        if co.co_flags & 8: n = n+1\n        for i in range(n):\n            name = co.co_varnames[i]\n            print >>self.stdout, name, '=',\n            if name in dict: print >>self.stdout, dict[name]\n            else: print >>self.stdout, \"*** undefined ***\"\n    do_a = do_args\n\n    def do_retval(self, arg):\n        if '__return__' in self.curframe_locals:\n            print >>self.stdout, self.curframe_locals['__return__']\n        else:\n            print >>self.stdout, '*** Not yet returned!'\n    do_rv = do_retval\n\n    def _getval(self, arg):\n        try:\n            return eval(arg, self.curframe.f_globals,\n                        self.curframe_locals)\n        except:\n            t, v = sys.exc_info()[:2]\n            if isinstance(t, str):\n                exc_type_name = t\n            else: exc_type_name = t.__name__\n            print >>self.stdout, '***', exc_type_name + ':', repr(v)\n            raise\n\n    def do_p(self, arg):\n        try:\n            print >>self.stdout, repr(self._getval(arg))\n        except:\n            pass\n\n    def do_pp(self, arg):\n        try:\n            pprint.pprint(self._getval(arg), self.stdout)\n        except:\n            pass\n\n    def do_list(self, arg):\n        self.lastcmd = 'list'\n        last = None\n        if arg:\n            try:\n                x = eval(arg, {}, {})\n                if type(x) == type(()):\n                    first, last = x\n                    first = int(first)\n                    last = int(last)\n                    if last < first:\n                        # Assume it's a count\n                        last = first + last\n                else:\n                    first = max(1, int(x) - 5)\n            except:\n                print >>self.stdout, '*** Error in argument:', repr(arg)\n                return\n        elif self.lineno is None:\n            first = max(1, self.curframe.f_lineno - 5)\n        else:\n            first = self.lineno + 1\n        if last is None:\n            last = first + 10\n        filename = self.curframe.f_code.co_filename\n        breaklist = self.get_file_breaks(filename)\n        try:\n            for lineno in range(first, last+1):\n                line = linecache.getline(filename, lineno,\n                                         self.curframe.f_globals)\n                if not line:\n                    print >>self.stdout, '[EOF]'\n                    break\n                else:\n                    s = repr(lineno).rjust(3)\n                    if len(s) < 4: s = s + ' '\n                    if lineno in breaklist: s = s + 'B'\n                    else: s = s + ' '\n                    if lineno == self.curframe.f_lineno:\n                        s = s + '->'\n                    print >>self.stdout, s + '\\t' + line,\n                    self.lineno = lineno\n        except KeyboardInterrupt:\n            pass\n    do_l = do_list\n\n    def do_whatis(self, arg):\n        try:\n            value = eval(arg, self.curframe.f_globals,\n                            self.curframe_locals)\n        except:\n            t, v = sys.exc_info()[:2]\n            if type(t) == type(''):\n                exc_type_name = t\n            else: exc_type_name = t.__name__\n            print >>self.stdout, '***', exc_type_name + ':', repr(v)\n            return\n        code = None\n        # Is it a function?\n        try: code = value.func_code\n        except: pass\n        if code:\n            print >>self.stdout, 'Function', code.co_name\n            return\n        # Is it an instance method?\n        try: code = value.im_func.func_code\n        except: pass\n        if code:\n            print >>self.stdout, 'Method', code.co_name\n            return\n        # None of the above...\n        print >>self.stdout, type(value)\n\n    def do_alias(self, arg):\n        args = arg.split()\n        if len(args) == 0:\n            keys = self.aliases.keys()\n            keys.sort()\n            for alias in keys:\n                print >>self.stdout, \"%s = %s\" % (alias, self.aliases[alias])\n            return\n        if args[0] in self.aliases and len(args) == 1:\n            print >>self.stdout, \"%s = %s\" % (args[0], self.aliases[args[0]])\n        else:\n            self.aliases[args[0]] = ' '.join(args[1:])\n\n    def do_unalias(self, arg):\n        args = arg.split()\n        if len(args) == 0: return\n        if args[0] in self.aliases:\n            del self.aliases[args[0]]\n\n    #list of all the commands making the program resume execution.\n    commands_resuming = ['do_continue', 'do_step', 'do_next', 'do_return',\n                         'do_quit', 'do_jump']\n\n    # Print a traceback starting at the top stack frame.\n    # The most recently entered frame is printed last;\n    # this is different from dbx and gdb, but consistent with\n    # the Python interpreter's stack trace.\n    # It is also consistent with the up/down commands (which are\n    # compatible with dbx and gdb: up moves towards 'main()'\n    # and down moves towards the most recent stack frame).\n\n    def print_stack_trace(self):\n        try:\n            for frame_lineno in self.stack:\n                self.print_stack_entry(frame_lineno)\n        except KeyboardInterrupt:\n            pass\n\n    def print_stack_entry(self, frame_lineno, prompt_prefix=line_prefix):\n        frame, lineno = frame_lineno\n        if frame is self.curframe:\n            print >>self.stdout, '>',\n        else:\n            print >>self.stdout, ' ',\n        print >>self.stdout, self.format_stack_entry(frame_lineno,\n                                                     prompt_prefix)\n\n\n    # Help methods (derived from pdb.doc)\n\n    def help_help(self):\n        self.help_h()\n\n    def help_h(self):\n        print >>self.stdout, \"\"\"h(elp)\nWithout argument, print the list of available commands.\nWith a command name as argument, print help about that command\n\"help pdb\" pipes the full documentation file to the $PAGER\n\"help exec\" gives help on the ! command\"\"\"\n\n    def help_where(self):\n        self.help_w()\n\n    def help_w(self):\n        print >>self.stdout, \"\"\"w(here)\nPrint a stack trace, with the most recent frame at the bottom.\nAn arrow indicates the \"current frame\", which determines the\ncontext of most commands.  'bt' is an alias for this command.\"\"\"\n\n    help_bt = help_w\n\n    def help_down(self):\n        self.help_d()\n\n    def help_d(self):\n        print >>self.stdout, \"\"\"d(own)\nMove the current frame one level down in the stack trace\n(to a newer frame).\"\"\"\n\n    def help_up(self):\n        self.help_u()\n\n    def help_u(self):\n        print >>self.stdout, \"\"\"u(p)\nMove the current frame one level up in the stack trace\n(to an older frame).\"\"\"\n\n    def help_break(self):\n        self.help_b()\n\n    def help_b(self):\n        print >>self.stdout, \"\"\"b(reak) ([file:]lineno | function) [, condition]\nWith a line number argument, set a break there in the current\nfile.  With a function name, set a break at first executable line\nof that function.  Without argument, list all breaks.  If a second\nargument is present, it is a string specifying an expression\nwhich must evaluate to true before the breakpoint is honored.\n\nThe line number may be prefixed with a filename and a colon,\nto specify a breakpoint in another file (probably one that\nhasn't been loaded yet).  The file is searched for on sys.path;\nthe .py suffix may be omitted.\"\"\"\n\n    def help_clear(self):\n        self.help_cl()\n\n    def help_cl(self):\n        print >>self.stdout, \"cl(ear) filename:lineno\"\n        print >>self.stdout, \"\"\"cl(ear) [bpnumber [bpnumber...]]\nWith a space separated list of breakpoint numbers, clear\nthose breakpoints.  Without argument, clear all breaks (but\nfirst ask confirmation).  With a filename:lineno argument,\nclear all breaks at that line in that file.\n\nNote that the argument is different from previous versions of\nthe debugger (in python distributions 1.5.1 and before) where\na linenumber was used instead of either filename:lineno or\nbreakpoint numbers.\"\"\"\n\n    def help_tbreak(self):\n        print >>self.stdout, \"\"\"tbreak  same arguments as break, but breakpoint\nis removed when first hit.\"\"\"\n\n    def help_enable(self):\n        print >>self.stdout, \"\"\"enable bpnumber [bpnumber ...]\nEnables the breakpoints given as a space separated list of\nbp numbers.\"\"\"\n\n    def help_disable(self):\n        print >>self.stdout, \"\"\"disable bpnumber [bpnumber ...]\nDisables the breakpoints given as a space separated list of\nbp numbers.\"\"\"\n\n    def help_ignore(self):\n        print >>self.stdout, \"\"\"ignore bpnumber count\nSets the ignore count for the given breakpoint number.  A breakpoint\nbecomes active when the ignore count is zero.  When non-zero, the\ncount is decremented each time the breakpoint is reached and the\nbreakpoint is not disabled and any associated condition evaluates\nto true.\"\"\"\n\n    def help_condition(self):\n        print >>self.stdout, \"\"\"condition bpnumber str_condition\nstr_condition is a string specifying an expression which\nmust evaluate to true before the breakpoint is honored.\nIf str_condition is absent, any existing condition is removed;\ni.e., the breakpoint is made unconditional.\"\"\"\n\n    def help_step(self):\n        self.help_s()\n\n    def help_s(self):\n        print >>self.stdout, \"\"\"s(tep)\nExecute the current line, stop at the first possible occasion\n(either in a function that is called or in the current function).\"\"\"\n\n    def help_until(self):\n        self.help_unt()\n\n    def help_unt(self):\n        print \"\"\"unt(il)\nContinue execution until the line with a number greater than the current\none is reached or until the current frame returns\"\"\"\n\n    def help_next(self):\n        self.help_n()\n\n    def help_n(self):\n        print >>self.stdout, \"\"\"n(ext)\nContinue execution until the next line in the current function\nis reached or it returns.\"\"\"\n\n    def help_return(self):\n        self.help_r()\n\n    def help_r(self):\n        print >>self.stdout, \"\"\"r(eturn)\nContinue execution until the current function returns.\"\"\"\n\n    def help_continue(self):\n        self.help_c()\n\n    def help_cont(self):\n        self.help_c()\n\n    def help_c(self):\n        print >>self.stdout, \"\"\"c(ont(inue))\nContinue execution, only stop when a breakpoint is encountered.\"\"\"\n\n    def help_jump(self):\n        self.help_j()\n\n    def help_j(self):\n        print >>self.stdout, \"\"\"j(ump) lineno\nSet the next line that will be executed.\"\"\"\n\n    def help_debug(self):\n        print >>self.stdout, \"\"\"debug code\nEnter a recursive debugger that steps through the code argument\n(which is an arbitrary expression or statement to be executed\nin the current environment).\"\"\"\n\n    def help_list(self):\n        self.help_l()\n\n    def help_l(self):\n        print >>self.stdout, \"\"\"l(ist) [first [,last]]\nList source code for the current file.\nWithout arguments, list 11 lines around the current line\nor continue the previous listing.\nWith one argument, list 11 lines starting at that line.\nWith two arguments, list the given range;\nif the second argument is less than the first, it is a count.\"\"\"\n\n    def help_args(self):\n        self.help_a()\n\n    def help_a(self):\n        print >>self.stdout, \"\"\"a(rgs)\nPrint the arguments of the current function.\"\"\"\n\n    def help_p(self):\n        print >>self.stdout, \"\"\"p expression\nPrint the value of the expression.\"\"\"\n\n    def help_pp(self):\n        print >>self.stdout, \"\"\"pp expression\nPretty-print the value of the expression.\"\"\"\n\n    def help_exec(self):\n        print >>self.stdout, \"\"\"(!) statement\nExecute the (one-line) statement in the context of\nthe current stack frame.\nThe exclamation point can be omitted unless the first word\nof the statement resembles a debugger command.\nTo assign to a global variable you must always prefix the\ncommand with a 'global' command, e.g.:\n(Pdb) global list_options; list_options = ['-l']\n(Pdb)\"\"\"\n\n    def help_run(self):\n        print \"\"\"run [args...]\nRestart the debugged python program. If a string is supplied, it is\nsplit with \"shlex\" and the result is used as the new sys.argv.\nHistory, breakpoints, actions and debugger options are preserved.\n\"restart\" is an alias for \"run\".\"\"\"\n\n    help_restart = help_run\n\n    def help_quit(self):\n        self.help_q()\n\n    def help_q(self):\n        print >>self.stdout, \"\"\"q(uit) or exit - Quit from the debugger.\nThe program being executed is aborted.\"\"\"\n\n    help_exit = help_q\n\n    def help_whatis(self):\n        print >>self.stdout, \"\"\"whatis arg\nPrints the type of the argument.\"\"\"\n\n    def help_EOF(self):\n        print >>self.stdout, \"\"\"EOF\nHandles the receipt of EOF as a command.\"\"\"\n\n    def help_alias(self):\n        print >>self.stdout, \"\"\"alias [name [command [parameter parameter ...]]]\nCreates an alias called 'name' the executes 'command'.  The command\nmust *not* be enclosed in quotes.  Replaceable parameters are\nindicated by %1, %2, and so on, while %* is replaced by all the\nparameters.  If no command is given, the current alias for name\nis shown. If no name is given, all aliases are listed.\n\nAliases may be nested and can contain anything that can be\nlegally typed at the pdb prompt.  Note!  You *can* override\ninternal pdb commands with aliases!  Those internal commands\nare then hidden until the alias is removed.  Aliasing is recursively\napplied to the first word of the command line; all other words\nin the line are left alone.\n\nSome useful aliases (especially when placed in the .pdbrc file) are:\n\n#Print instance variables (usage \"pi classInst\")\nalias pi for k in %1.__dict__.keys(): print \"%1.\",k,\"=\",%1.__dict__[k]\n\n#Print instance variables in self\nalias ps pi self\n\"\"\"\n\n    def help_unalias(self):\n        print >>self.stdout, \"\"\"unalias name\nDeletes the specified alias.\"\"\"\n\n    def help_commands(self):\n        print >>self.stdout, \"\"\"commands [bpnumber]\n(com) ...\n(com) end\n(Pdb)\n\nSpecify a list of commands for breakpoint number bpnumber.  The\ncommands themselves appear on the following lines.  Type a line\ncontaining just 'end' to terminate the commands.\n\nTo remove all commands from a breakpoint, type commands and\nfollow it immediately with  end; that is, give no commands.\n\nWith no bpnumber argument, commands refers to the last\nbreakpoint set.\n\nYou can use breakpoint commands to start your program up again.\nSimply use the continue command, or step, or any other\ncommand that resumes execution.\n\nSpecifying any command resuming execution (currently continue,\nstep, next, return, jump, quit and their abbreviations) terminates\nthe command list (as if that command was immediately followed by end).\nThis is because any time you resume execution\n(even with a simple next or step), you may encounter\nanother breakpoint--which could have its own command list, leading to\nambiguities about which list to execute.\n\n   If you use the 'silent' command in the command list, the\nusual message about stopping at a breakpoint is not printed.  This may\nbe desirable for breakpoints that are to print a specific message and\nthen continue.  If none of the other commands print anything, you\nsee no sign that the breakpoint was reached.\n\"\"\"\n\n    def help_pdb(self):\n        help()\n\n    def lookupmodule(self, filename):\n        \"\"\"Helper function for break/clear parsing -- may be overridden.\n\n        lookupmodule() translates (possibly incomplete) file or module name\n        into an absolute file name.\n        \"\"\"\n        if os.path.isabs(filename) and  os.path.exists(filename):\n            return filename\n        f = os.path.join(sys.path[0], filename)\n        if  os.path.exists(f) and self.canonic(f) == self.mainpyfile:\n            return f\n        root, ext = os.path.splitext(filename)\n        if ext == '':\n            filename = filename + '.py'\n        if os.path.isabs(filename):\n            return filename\n        for dirname in sys.path:\n            while os.path.islink(dirname):\n                dirname = os.readlink(dirname)\n            fullname = os.path.join(dirname, filename)\n            if os.path.exists(fullname):\n                return fullname\n        return None\n\n    def _runscript(self, filename):\n        # The script has to run in __main__ namespace (or imports from\n        # __main__ will break).\n        #\n        # So we clear up the __main__ and set several special variables\n        # (this gets rid of pdb's globals and cleans old variables on restarts).\n        import __main__\n        __main__.__dict__.clear()\n        __main__.__dict__.update({\"__name__\"    : \"__main__\",\n                                  \"__file__\"    : filename,\n                                  \"__builtins__\": __builtins__,\n                                 })\n\n        # When bdb sets tracing, a number of call and line events happens\n        # BEFORE debugger even reaches user's code (and the exact sequence of\n        # events depends on python version). So we take special measures to\n        # avoid stopping before we reach the main script (see user_line and\n        # user_call for details).\n        self._wait_for_mainpyfile = 1\n        self.mainpyfile = self.canonic(filename)\n        self._user_requested_quit = 0\n        statement = 'execfile(%r)' % filename\n        self.run(statement)\n\n# Simplified interface\n\ndef run(statement, globals=None, locals=None):\n    Pdb().run(statement, globals, locals)\n\ndef runeval(expression, globals=None, locals=None):\n    return Pdb().runeval(expression, globals, locals)\n\ndef runctx(statement, globals, locals):\n    # B/W compatibility\n    run(statement, globals, locals)\n\ndef runcall(*args, **kwds):\n    return Pdb().runcall(*args, **kwds)\n\ndef set_trace():\n    Pdb().set_trace(sys._getframe().f_back)\n\n# Post-Mortem interface\n\ndef post_mortem(t=None):\n    # handling the default\n    if t is None:\n        # sys.exc_info() returns (type, value, traceback) if an exception is\n        # being handled, otherwise it returns None\n        t = sys.exc_info()[2]\n        if t is None:\n            raise ValueError(\"A valid traceback must be passed if no \"\n                                               \"exception is being handled\")\n\n    p = Pdb()\n    p.reset()\n    p.interaction(None, t)\n\ndef pm():\n    post_mortem(sys.last_traceback)\n\n\n# Main program for testing\n\nTESTCMD = 'import x; x.main()'\n\ndef test():\n    run(TESTCMD)\n\n# print help\ndef help():\n    for dirname in sys.path:\n        fullname = os.path.join(dirname, 'pdb.doc')\n        if os.path.exists(fullname):\n            sts = os.system('${PAGER-more} '+fullname)\n            if sts: print '*** Pager exit status:', sts\n            break\n    else:\n        print 'Sorry, can\\'t find the help file \"pdb.doc\"',\n        print 'along the Python search path'\n\ndef main():\n    if not sys.argv[1:] or sys.argv[1] in (\"--help\", \"-h\"):\n        print \"usage: pdb.py scriptfile [arg] ...\"\n        sys.exit(2)\n\n    mainpyfile =  sys.argv[1]     # Get script filename\n    if not os.path.exists(mainpyfile):\n        print 'Error:', mainpyfile, 'does not exist'\n        sys.exit(1)\n\n    del sys.argv[0]         # Hide \"pdb.py\" from argument list\n\n    # Replace pdb's dir with script's dir in front of module search path.\n    sys.path[0] = os.path.dirname(mainpyfile)\n\n    # Note on saving/restoring sys.argv: it's a good idea when sys.argv was\n    # modified by the script being debugged. It's a bad idea when it was\n    # changed by the user from the command line. There is a \"restart\" command\n    # which allows explicit specification of command line arguments.\n    pdb = Pdb()\n    while True:\n        try:\n            pdb._runscript(mainpyfile)\n            if pdb._user_requested_quit:\n                break\n            print \"The program finished and will be restarted\"\n        except Restart:\n            print \"Restarting\", mainpyfile, \"with arguments:\"\n            print \"\\t\" + \" \".join(sys.argv[1:])\n        except SystemExit:\n            # In most cases SystemExit does not warrant a post-mortem session.\n            print \"The program exited via sys.exit(). Exit status: \",\n            print sys.exc_info()[1]\n        except:\n            traceback.print_exc()\n            print \"Uncaught exception. Entering post mortem debugging\"\n            print \"Running 'cont' or 'step' will restart the program\"\n            t = sys.exc_info()[2]\n            pdb.interaction(None, t)\n            print \"Post mortem debugger finished. The \" + mainpyfile + \\\n                  \" will be restarted\"\n\n\n# When invoked as main program, invoke the debugger on a script\nif __name__ == '__main__':\n    import pdb\n    pdb.main()\n", 
    "pickle": "\"\"\"Create portable serialized representations of Python objects.\n\nSee module cPickle for a (much) faster implementation.\nSee module copy_reg for a mechanism for registering custom picklers.\nSee module pickletools source for extensive comments.\n\nClasses:\n\n    Pickler\n    Unpickler\n\nFunctions:\n\n    dump(object, file)\n    dumps(object) -> string\n    load(file) -> object\n    loads(string) -> object\n\nMisc variables:\n\n    __version__\n    format_version\n    compatible_formats\n\n\"\"\"\n\n__version__ = \"$Revision: 72223 $\"       # Code version\n\nfrom types import *\nfrom copy_reg import dispatch_table\nfrom copy_reg import _extension_registry, _inverted_registry, _extension_cache\nimport marshal\nimport sys\nimport struct\nimport re\n\n__all__ = [\"PickleError\", \"PicklingError\", \"UnpicklingError\", \"Pickler\",\n           \"Unpickler\", \"dump\", \"dumps\", \"load\", \"loads\"]\n\n# These are purely informational; no code uses these.\nformat_version = \"2.0\"                  # File format version we write\ncompatible_formats = [\"1.0\",            # Original protocol 0\n                      \"1.1\",            # Protocol 0 with INST added\n                      \"1.2\",            # Original protocol 1\n                      \"1.3\",            # Protocol 1 with BINFLOAT added\n                      \"2.0\",            # Protocol 2\n                      ]                 # Old format versions we can read\n\n# Keep in synch with cPickle.  This is the highest protocol number we\n# know how to read.\nHIGHEST_PROTOCOL = 2\n\n# Why use struct.pack() for pickling but marshal.loads() for\n# unpickling?  struct.pack() is 40% faster than marshal.dumps(), but\n# marshal.loads() is twice as fast as struct.unpack()!\nmloads = marshal.loads\n\nclass PickleError(Exception):\n    \"\"\"A common base class for the other pickling exceptions.\"\"\"\n    pass\n\nclass PicklingError(PickleError):\n    \"\"\"This exception is raised when an unpicklable object is passed to the\n    dump() method.\n\n    \"\"\"\n    pass\n\nclass UnpicklingError(PickleError):\n    \"\"\"This exception is raised when there is a problem unpickling an object,\n    such as a security violation.\n\n    Note that other exceptions may also be raised during unpickling, including\n    (but not necessarily limited to) AttributeError, EOFError, ImportError,\n    and IndexError.\n\n    \"\"\"\n    pass\n\n# An instance of _Stop is raised by Unpickler.load_stop() in response to\n# the STOP opcode, passing the object that is the result of unpickling.\nclass _Stop(Exception):\n    def __init__(self, value):\n        self.value = value\n\n# Jython has PyStringMap; it's a dict subclass with string keys\ntry:\n    from org.python.core import PyStringMap\nexcept ImportError:\n    PyStringMap = None\n\n# UnicodeType may or may not be exported (normally imported from types)\ntry:\n    UnicodeType\nexcept NameError:\n    UnicodeType = None\n\n# Pickle opcodes.  See pickletools.py for extensive docs.  The listing\n# here is in kind-of alphabetical order of 1-character pickle code.\n# pickletools groups them by purpose.\n\nMARK            = '('   # push special markobject on stack\nSTOP            = '.'   # every pickle ends with STOP\nPOP             = '0'   # discard topmost stack item\nPOP_MARK        = '1'   # discard stack top through topmost markobject\nDUP             = '2'   # duplicate top stack item\nFLOAT           = 'F'   # push float object; decimal string argument\nINT             = 'I'   # push integer or bool; decimal string argument\nBININT          = 'J'   # push four-byte signed int\nBININT1         = 'K'   # push 1-byte unsigned int\nLONG            = 'L'   # push long; decimal string argument\nBININT2         = 'M'   # push 2-byte unsigned int\nNONE            = 'N'   # push None\nPERSID          = 'P'   # push persistent object; id is taken from string arg\nBINPERSID       = 'Q'   #  \"       \"         \"  ;  \"  \"   \"     \"  stack\nREDUCE          = 'R'   # apply callable to argtuple, both on stack\nSTRING          = 'S'   # push string; NL-terminated string argument\nBINSTRING       = 'T'   # push string; counted binary string argument\nSHORT_BINSTRING = 'U'   #  \"     \"   ;    \"      \"       \"      \" < 256 bytes\nUNICODE         = 'V'   # push Unicode string; raw-unicode-escaped'd argument\nBINUNICODE      = 'X'   #   \"     \"       \"  ; counted UTF-8 string argument\nAPPEND          = 'a'   # append stack top to list below it\nBUILD           = 'b'   # call __setstate__ or __dict__.update()\nGLOBAL          = 'c'   # push self.find_class(modname, name); 2 string args\nDICT            = 'd'   # build a dict from stack items\nEMPTY_DICT      = '}'   # push empty dict\nAPPENDS         = 'e'   # extend list on stack by topmost stack slice\nGET             = 'g'   # push item from memo on stack; index is string arg\nBINGET          = 'h'   #   \"    \"    \"    \"   \"   \"  ;   \"    \" 1-byte arg\nINST            = 'i'   # build & push class instance\nLONG_BINGET     = 'j'   # push item from memo on stack; index is 4-byte arg\nLIST            = 'l'   # build list from topmost stack items\nEMPTY_LIST      = ']'   # push empty list\nOBJ             = 'o'   # build & push class instance\nPUT             = 'p'   # store stack top in memo; index is string arg\nBINPUT          = 'q'   #   \"     \"    \"   \"   \" ;   \"    \" 1-byte arg\nLONG_BINPUT     = 'r'   #   \"     \"    \"   \"   \" ;   \"    \" 4-byte arg\nSETITEM         = 's'   # add key+value pair to dict\nTUPLE           = 't'   # build tuple from topmost stack items\nEMPTY_TUPLE     = ')'   # push empty tuple\nSETITEMS        = 'u'   # modify dict by adding topmost key+value pairs\nBINFLOAT        = 'G'   # push float; arg is 8-byte float encoding\n\nTRUE            = 'I01\\n'  # not an opcode; see INT docs in pickletools.py\nFALSE           = 'I00\\n'  # not an opcode; see INT docs in pickletools.py\n\n# Protocol 2\n\nPROTO           = '\\x80'  # identify pickle protocol\nNEWOBJ          = '\\x81'  # build object by applying cls.__new__ to argtuple\nEXT1            = '\\x82'  # push object from extension registry; 1-byte index\nEXT2            = '\\x83'  # ditto, but 2-byte index\nEXT4            = '\\x84'  # ditto, but 4-byte index\nTUPLE1          = '\\x85'  # build 1-tuple from stack top\nTUPLE2          = '\\x86'  # build 2-tuple from two topmost stack items\nTUPLE3          = '\\x87'  # build 3-tuple from three topmost stack items\nNEWTRUE         = '\\x88'  # push True\nNEWFALSE        = '\\x89'  # push False\nLONG1           = '\\x8a'  # push long from < 256 bytes\nLONG4           = '\\x8b'  # push really big long\n\n_tuplesize2code = [EMPTY_TUPLE, TUPLE1, TUPLE2, TUPLE3]\n\n\n__all__.extend([x for x in dir() if re.match(\"[A-Z][A-Z0-9_]+$\",x)])\ndel x\n\n\n# Pickling machinery\n\nclass Pickler(object):\n\n    def __init__(self, file, protocol=None):\n        \"\"\"This takes a file-like object for writing a pickle data stream.\n\n        The optional protocol argument tells the pickler to use the\n        given protocol; supported protocols are 0, 1, 2.  The default\n        protocol is 0, to be backwards compatible.  (Protocol 0 is the\n        only protocol that can be written to a file opened in text\n        mode and read back successfully.  When using a protocol higher\n        than 0, make sure the file is opened in binary mode, both when\n        pickling and unpickling.)\n\n        Protocol 1 is more efficient than protocol 0; protocol 2 is\n        more efficient than protocol 1.\n\n        Specifying a negative protocol version selects the highest\n        protocol version supported.  The higher the protocol used, the\n        more recent the version of Python needed to read the pickle\n        produced.\n\n        The file parameter must have a write() method that accepts a single\n        string argument.  It can thus be an open file object, a StringIO\n        object, or any other custom object that meets this interface.\n\n        \"\"\"\n        if protocol is None:\n            protocol = 0\n        if protocol < 0:\n            protocol = HIGHEST_PROTOCOL\n        elif not 0 <= protocol <= HIGHEST_PROTOCOL:\n            raise ValueError(\"pickle protocol must be <= %d\" % HIGHEST_PROTOCOL)\n        self.write = file.write\n        self.memo = {}\n        self.proto = int(protocol)\n        self.bin = protocol >= 1\n        self.fast = 0\n\n    def clear_memo(self):\n        \"\"\"Clears the pickler's \"memo\".\n\n        The memo is the data structure that remembers which objects the\n        pickler has already seen, so that shared or recursive objects are\n        pickled by reference and not by value.  This method is useful when\n        re-using picklers.\n\n        \"\"\"\n        self.memo.clear()\n\n    def dump(self, obj):\n        \"\"\"Write a pickled representation of obj to the open file.\"\"\"\n        if self.proto >= 2:\n            self.write(PROTO + chr(self.proto))\n        self.save(obj)\n        self.write(STOP)\n\n    def memoize(self, obj):\n        \"\"\"Store an object in the memo.\"\"\"\n\n        # The Pickler memo is a dictionary mapping object ids to 2-tuples\n        # that contain the Unpickler memo key and the object being memoized.\n        # The memo key is written to the pickle and will become\n        # the key in the Unpickler's memo.  The object is stored in the\n        # Pickler memo so that transient objects are kept alive during\n        # pickling.\n\n        # The use of the Unpickler memo length as the memo key is just a\n        # convention.  The only requirement is that the memo values be unique.\n        # But there appears no advantage to any other scheme, and this\n        # scheme allows the Unpickler memo to be implemented as a plain (but\n        # growable) array, indexed by memo key.\n        if self.fast:\n            return\n        assert id(obj) not in self.memo\n        memo_len = len(self.memo)\n        self.write(self.put(memo_len))\n        self.memo[id(obj)] = memo_len, obj\n\n    # Return a PUT (BINPUT, LONG_BINPUT) opcode string, with argument i.\n    def put(self, i, pack=struct.pack):\n        if self.bin:\n            if i < 256:\n                return BINPUT + chr(i)\n            else:\n                return LONG_BINPUT + pack(\"<i\", i)\n\n        return PUT + repr(i) + '\\n'\n\n    # Return a GET (BINGET, LONG_BINGET) opcode string, with argument i.\n    def get(self, i, pack=struct.pack):\n        if self.bin:\n            if i < 256:\n                return BINGET + chr(i)\n            else:\n                return LONG_BINGET + pack(\"<i\", i)\n\n        return GET + repr(i) + '\\n'\n\n    def save(self, obj):\n        # Check for persistent id (defined by a subclass)\n        pid = self.persistent_id(obj)\n        if pid is not None:\n            self.save_pers(pid)\n            return\n\n        # Check the memo\n        x = self.memo.get(id(obj))\n        if x:\n            self.write(self.get(x[0]))\n            return\n\n        # Check the type dispatch table\n        t = type(obj)\n        f = self.dispatch.get(t)\n        if f:\n            f(self, obj) # Call unbound method with explicit self\n            return\n\n        # Check copy_reg.dispatch_table\n        reduce = dispatch_table.get(t)\n        if reduce:\n            rv = reduce(obj)\n        else:\n            # Check for a class with a custom metaclass; treat as regular class\n            try:\n                issc = issubclass(t, TypeType)\n            except TypeError: # t is not a class (old Boost; see SF #502085)\n                issc = 0\n            if issc:\n                self.save_global(obj)\n                return\n\n            # Check for a __reduce_ex__ method, fall back to __reduce__\n            reduce = getattr(obj, \"__reduce_ex__\", None)\n            if reduce:\n                rv = reduce(self.proto)\n            else:\n                reduce = getattr(obj, \"__reduce__\", None)\n                if reduce:\n                    rv = reduce()\n                else:\n                    raise PicklingError(\"Can't pickle %r object: %r\" %\n                                        (t.__name__, obj))\n\n        # Check for string returned by reduce(), meaning \"save as global\"\n        if type(rv) is StringType:\n            self.save_global(obj, rv)\n            return\n\n        # Assert that reduce() returned a tuple\n        if type(rv) is not TupleType:\n            raise PicklingError(\"%s must return string or tuple\" % reduce)\n\n        # Assert that it returned an appropriately sized tuple\n        l = len(rv)\n        if not (2 <= l <= 5):\n            raise PicklingError(\"Tuple returned by %s must have \"\n                                \"two to five elements\" % reduce)\n\n        # Save the reduce() output and finally memoize the object\n        self.save_reduce(obj=obj, *rv)\n\n    def persistent_id(self, obj):\n        # This exists so a subclass can override it\n        return None\n\n    def save_pers(self, pid):\n        # Save a persistent id reference\n        if self.bin:\n            self.save(pid)\n            self.write(BINPERSID)\n        else:\n            self.write(PERSID + str(pid) + '\\n')\n\n    def save_reduce(self, func, args, state=None,\n                    listitems=None, dictitems=None, obj=None):\n        # This API is called by some subclasses\n\n        # Assert that args is a tuple or None\n        if not isinstance(args, TupleType):\n            raise PicklingError(\"args from reduce() should be a tuple\")\n\n        # Assert that func is callable\n        if not hasattr(func, '__call__'):\n            raise PicklingError(\"func from reduce should be callable\")\n\n        save = self.save\n        write = self.write\n\n        # Protocol 2 special case: if func's name is __newobj__, use NEWOBJ\n        if self.proto >= 2 and getattr(func, \"__name__\", \"\") == \"__newobj__\":\n            # A __reduce__ implementation can direct protocol 2 to\n            # use the more efficient NEWOBJ opcode, while still\n            # allowing protocol 0 and 1 to work normally.  For this to\n            # work, the function returned by __reduce__ should be\n            # called __newobj__, and its first argument should be a\n            # new-style class.  The implementation for __newobj__\n            # should be as follows, although pickle has no way to\n            # verify this:\n            #\n            # def __newobj__(cls, *args):\n            #     return cls.__new__(cls, *args)\n            #\n            # Protocols 0 and 1 will pickle a reference to __newobj__,\n            # while protocol 2 (and above) will pickle a reference to\n            # cls, the remaining args tuple, and the NEWOBJ code,\n            # which calls cls.__new__(cls, *args) at unpickling time\n            # (see load_newobj below).  If __reduce__ returns a\n            # three-tuple, the state from the third tuple item will be\n            # pickled regardless of the protocol, calling __setstate__\n            # at unpickling time (see load_build below).\n            #\n            # Note that no standard __newobj__ implementation exists;\n            # you have to provide your own.  This is to enforce\n            # compatibility with Python 2.2 (pickles written using\n            # protocol 0 or 1 in Python 2.3 should be unpicklable by\n            # Python 2.2).\n            cls = args[0]\n            if not hasattr(cls, \"__new__\"):\n                raise PicklingError(\n                    \"args[0] from __newobj__ args has no __new__\")\n            if obj is not None and cls is not obj.__class__:\n                raise PicklingError(\n                    \"args[0] from __newobj__ args has the wrong class\")\n            args = args[1:]\n            save(cls)\n            save(args)\n            write(NEWOBJ)\n        else:\n            save(func)\n            save(args)\n            write(REDUCE)\n\n        if obj is not None:\n            self.memoize(obj)\n\n        # More new special cases (that work with older protocols as\n        # well): when __reduce__ returns a tuple with 4 or 5 items,\n        # the 4th and 5th item should be iterators that provide list\n        # items and dict items (as (key, value) tuples), or None.\n\n        if listitems is not None:\n            self._batch_appends(listitems)\n\n        if dictitems is not None:\n            self._batch_setitems(dictitems)\n\n        if state is not None:\n            save(state)\n            write(BUILD)\n\n    # Methods below this point are dispatched through the dispatch table\n\n    dispatch = {}\n\n    def save_none(self, obj):\n        self.write(NONE)\n    dispatch[NoneType] = save_none\n\n    def save_bool(self, obj):\n        if self.proto >= 2:\n            self.write(obj and NEWTRUE or NEWFALSE)\n        else:\n            self.write(obj and TRUE or FALSE)\n    dispatch[bool] = save_bool\n\n    def save_int(self, obj, pack=struct.pack):\n        if self.bin:\n            # If the int is small enough to fit in a signed 4-byte 2's-comp\n            # format, we can store it more efficiently than the general\n            # case.\n            # First one- and two-byte unsigned ints:\n            if obj >= 0:\n                if obj <= 0xff:\n                    self.write(BININT1 + chr(obj))\n                    return\n                if obj <= 0xffff:\n                    self.write(\"%c%c%c\" % (BININT2, obj&0xff, obj>>8))\n                    return\n            # Next check for 4-byte signed ints:\n            high_bits = obj >> 31  # note that Python shift sign-extends\n            if high_bits == 0 or high_bits == -1:\n                # All high bits are copies of bit 2**31, so the value\n                # fits in a 4-byte signed int.\n                self.write(BININT + pack(\"<i\", obj))\n                return\n        # Text pickle, or int too big to fit in signed 4-byte format.\n        self.write(INT + repr(obj) + '\\n')\n    dispatch[IntType] = save_int\n\n    def save_long(self, obj, pack=struct.pack):\n        if self.proto >= 2:\n            bytes = encode_long(obj)\n            n = len(bytes)\n            if n < 256:\n                self.write(LONG1 + chr(n) + bytes)\n            else:\n                self.write(LONG4 + pack(\"<i\", n) + bytes)\n            return\n        self.write(LONG + repr(obj) + '\\n')\n    dispatch[LongType] = save_long\n\n    def save_float(self, obj, pack=struct.pack):\n        if self.bin:\n            self.write(BINFLOAT + pack('>d', obj))\n        else:\n            self.write(FLOAT + repr(obj) + '\\n')\n    dispatch[FloatType] = save_float\n\n    def save_string(self, obj, pack=struct.pack):\n        if self.bin:\n            n = len(obj)\n            if n < 256:\n                self.write(SHORT_BINSTRING + chr(n) + obj)\n            else:\n                self.write(BINSTRING + pack(\"<i\", n) + obj)\n        else:\n            self.write(STRING + repr(obj) + '\\n')\n        self.memoize(obj)\n    dispatch[StringType] = save_string\n\n    def save_unicode(self, obj, pack=struct.pack):\n        if self.bin:\n            encoding = obj.encode('utf-8')\n            n = len(encoding)\n            self.write(BINUNICODE + pack(\"<i\", n) + encoding)\n        else:\n            obj = obj.replace(\"\\\\\", \"\\\\u005c\")\n            obj = obj.replace(\"\\n\", \"\\\\u000a\")\n            self.write(UNICODE + obj.encode('raw-unicode-escape') + '\\n')\n        self.memoize(obj)\n    dispatch[UnicodeType] = save_unicode\n\n    if StringType is UnicodeType:\n        # This is true for Jython\n        def save_string(self, obj, pack=struct.pack):\n            unicode = obj.isunicode()\n\n            if self.bin:\n                if unicode:\n                    obj = obj.encode(\"utf-8\")\n                l = len(obj)\n                if l < 256 and not unicode:\n                    self.write(SHORT_BINSTRING + chr(l) + obj)\n                else:\n                    s = pack(\"<i\", l)\n                    if unicode:\n                        self.write(BINUNICODE + s + obj)\n                    else:\n                        self.write(BINSTRING + s + obj)\n            else:\n                if unicode:\n                    obj = obj.replace(\"\\\\\", \"\\\\u005c\")\n                    obj = obj.replace(\"\\n\", \"\\\\u000a\")\n                    obj = obj.encode('raw-unicode-escape')\n                    self.write(UNICODE + obj + '\\n')\n                else:\n                    self.write(STRING + repr(obj) + '\\n')\n            self.memoize(obj)\n        dispatch[StringType] = save_string\n\n    def save_tuple(self, obj):\n        write = self.write\n        proto = self.proto\n\n        n = len(obj)\n        if n == 0:\n            if proto:\n                write(EMPTY_TUPLE)\n            else:\n                write(MARK + TUPLE)\n            return\n\n        save = self.save\n        memo = self.memo\n        if n <= 3 and proto >= 2:\n            for element in obj:\n                save(element)\n            # Subtle.  Same as in the big comment below.\n            if id(obj) in memo:\n                get = self.get(memo[id(obj)][0])\n                write(POP * n + get)\n            else:\n                write(_tuplesize2code[n])\n                self.memoize(obj)\n            return\n\n        # proto 0 or proto 1 and tuple isn't empty, or proto > 1 and tuple\n        # has more than 3 elements.\n        write(MARK)\n        for element in obj:\n            save(element)\n\n        if id(obj) in memo:\n            # Subtle.  d was not in memo when we entered save_tuple(), so\n            # the process of saving the tuple's elements must have saved\n            # the tuple itself:  the tuple is recursive.  The proper action\n            # now is to throw away everything we put on the stack, and\n            # simply GET the tuple (it's already constructed).  This check\n            # could have been done in the \"for element\" loop instead, but\n            # recursive tuples are a rare thing.\n            get = self.get(memo[id(obj)][0])\n            if proto:\n                write(POP_MARK + get)\n            else:   # proto 0 -- POP_MARK not available\n                write(POP * (n+1) + get)\n            return\n\n        # No recursion.\n        self.write(TUPLE)\n        self.memoize(obj)\n\n    dispatch[TupleType] = save_tuple\n\n    # save_empty_tuple() isn't used by anything in Python 2.3.  However, I\n    # found a Pickler subclass in Zope3 that calls it, so it's not harmless\n    # to remove it.\n    def save_empty_tuple(self, obj):\n        self.write(EMPTY_TUPLE)\n\n    def save_list(self, obj):\n        write = self.write\n\n        if self.bin:\n            write(EMPTY_LIST)\n        else:   # proto 0 -- can't use EMPTY_LIST\n            write(MARK + LIST)\n\n        self.memoize(obj)\n        self._batch_appends(iter(obj))\n\n    dispatch[ListType] = save_list\n\n    # Keep in synch with cPickle's BATCHSIZE.  Nothing will break if it gets\n    # out of synch, though.\n    _BATCHSIZE = 1000\n\n    def _batch_appends(self, items):\n        # Helper to batch up APPENDS sequences\n        save = self.save\n        write = self.write\n\n        if not self.bin:\n            for x in items:\n                save(x)\n                write(APPEND)\n            return\n\n        r = xrange(self._BATCHSIZE)\n        while items is not None:\n            tmp = []\n            for i in r:\n                try:\n                    x = items.next()\n                    tmp.append(x)\n                except StopIteration:\n                    items = None\n                    break\n            n = len(tmp)\n            if n > 1:\n                write(MARK)\n                for x in tmp:\n                    save(x)\n                write(APPENDS)\n            elif n:\n                save(tmp[0])\n                write(APPEND)\n            # else tmp is empty, and we're done\n\n    def save_dict(self, obj):\n        modict_saver = self._pickle_maybe_moduledict(obj)\n        if modict_saver is not None:\n            return self.save_reduce(*modict_saver)\n\n        write = self.write\n\n        if self.bin:\n            write(EMPTY_DICT)\n        else:   # proto 0 -- can't use EMPTY_DICT\n            write(MARK + DICT)\n\n        self.memoize(obj)\n        self._batch_setitems(obj.iteritems())\n\n    dispatch[DictionaryType] = save_dict\n    if not PyStringMap is None:\n        dispatch[PyStringMap] = save_dict\n\n    def _batch_setitems(self, items):\n        # Helper to batch up SETITEMS sequences; proto >= 1 only\n        save = self.save\n        write = self.write\n\n        if not self.bin:\n            for k, v in items:\n                save(k)\n                save(v)\n                write(SETITEM)\n            return\n\n        r = xrange(self._BATCHSIZE)\n        while items is not None:\n            tmp = []\n            for i in r:\n                try:\n                    tmp.append(items.next())\n                except StopIteration:\n                    items = None\n                    break\n            n = len(tmp)\n            if n > 1:\n                write(MARK)\n                for k, v in tmp:\n                    save(k)\n                    save(v)\n                write(SETITEMS)\n            elif n:\n                k, v = tmp[0]\n                save(k)\n                save(v)\n                write(SETITEM)\n            # else tmp is empty, and we're done\n\n    def _pickle_maybe_moduledict(self, obj):\n        # save module dictionary as \"getattr(module, '__dict__')\"\n        try:\n            name = obj['__name__']\n            if type(name) is not str:\n                return None\n            themodule = sys.modules[name]\n            if type(themodule) is not ModuleType:\n                return None\n            if themodule.__dict__ is not obj:\n                return None\n        except (AttributeError, KeyError, TypeError):\n            return None\n\n        return getattr, (themodule, '__dict__')\n\n\n    def save_inst(self, obj):\n        cls = obj.__class__\n\n        memo  = self.memo\n        write = self.write\n        save  = self.save\n\n        if hasattr(obj, '__getinitargs__'):\n            args = obj.__getinitargs__()\n            len(args) # XXX Assert it's a sequence\n            _keep_alive(args, memo)\n        else:\n            args = ()\n\n        write(MARK)\n\n        if self.bin:\n            save(cls)\n            for arg in args:\n                save(arg)\n            write(OBJ)\n        else:\n            for arg in args:\n                save(arg)\n            write(INST + cls.__module__ + '\\n' + cls.__name__ + '\\n')\n\n        self.memoize(obj)\n\n        try:\n            getstate = obj.__getstate__\n        except AttributeError:\n            stuff = obj.__dict__\n        else:\n            stuff = getstate()\n            _keep_alive(stuff, memo)\n        save(stuff)\n        write(BUILD)\n\n    dispatch[InstanceType] = save_inst\n\n    def save_function(self, obj):\n        try:\n            return self.save_global(obj)\n        except PicklingError, e:\n            pass\n        # Check copy_reg.dispatch_table\n        reduce = dispatch_table.get(type(obj))\n        if reduce:\n            rv = reduce(obj)\n        else:\n            # Check for a __reduce_ex__ method, fall back to __reduce__\n            reduce = getattr(obj, \"__reduce_ex__\", None)\n            if reduce:\n                rv = reduce(self.proto)\n            else:\n                reduce = getattr(obj, \"__reduce__\", None)\n                if reduce:\n                    rv = reduce()\n                else:\n                    raise e\n        return self.save_reduce(obj=obj, *rv)\n    dispatch[FunctionType] = save_function\n\n    def save_global(self, obj, name=None, pack=struct.pack):\n        write = self.write\n        memo = self.memo\n\n        if name is None:\n            name = obj.__name__\n\n        module = getattr(obj, \"__module__\", None)\n        if module is None:\n            module = whichmodule(obj, name)\n\n        try:\n            __import__(module)\n            mod = sys.modules[module]\n            klass = getattr(mod, name)\n        except (ImportError, KeyError, AttributeError):\n            raise PicklingError(\n                \"Can't pickle %r: it's not found as %s.%s\" %\n                (obj, module, name))\n        else:\n            if klass is not obj:\n                raise PicklingError(\n                    \"Can't pickle %r: it's not the same object as %s.%s\" %\n                    (obj, module, name))\n\n        if self.proto >= 2:\n            code = _extension_registry.get((module, name))\n            if code:\n                assert code > 0\n                if code <= 0xff:\n                    write(EXT1 + chr(code))\n                elif code <= 0xffff:\n                    write(\"%c%c%c\" % (EXT2, code&0xff, code>>8))\n                else:\n                    write(EXT4 + pack(\"<i\", code))\n                return\n\n        write(GLOBAL + module + '\\n' + name + '\\n')\n        self.memoize(obj)\n\n    dispatch[ClassType] = save_global\n    dispatch[BuiltinFunctionType] = save_global\n    dispatch[TypeType] = save_global\n\n# Pickling helpers\n\ndef _keep_alive(x, memo):\n    \"\"\"Keeps a reference to the object x in the memo.\n\n    Because we remember objects by their id, we have\n    to assure that possibly temporary objects are kept\n    alive by referencing them.\n    We store a reference at the id of the memo, which should\n    normally not be used unless someone tries to deepcopy\n    the memo itself...\n    \"\"\"\n    try:\n        memo[id(memo)].append(x)\n    except KeyError:\n        # aha, this is the first one :-)\n        memo[id(memo)]=[x]\n\n\n# A cache for whichmodule(), mapping a function object to the name of\n# the module in which the function was found.\n\nclassmap = {} # called classmap for backwards compatibility\n\ndef whichmodule(func, funcname):\n    \"\"\"Figure out the module in which a function occurs.\n\n    Search sys.modules for the module.\n    Cache in classmap.\n    Return a module name.\n    If the function cannot be found, return \"__main__\".\n    \"\"\"\n    # Python functions should always get an __module__ from their globals.\n    mod = getattr(func, \"__module__\", None)\n    if mod is not None:\n        return mod\n    if func in classmap:\n        return classmap[func]\n\n    for name, module in sys.modules.items():\n        if module is None:\n            continue # skip dummy package entries\n        if name != '__main__' and getattr(module, funcname, None) is func:\n            break\n    else:\n        name = '__main__'\n    classmap[func] = name\n    return name\n\n\n# Unpickling machinery\n\nclass Unpickler(object):\n\n    def __init__(self, file):\n        \"\"\"This takes a file-like object for reading a pickle data stream.\n\n        The protocol version of the pickle is detected automatically, so no\n        proto argument is needed.\n\n        The file-like object must have two methods, a read() method that\n        takes an integer argument, and a readline() method that requires no\n        arguments.  Both methods should return a string.  Thus file-like\n        object can be a file object opened for reading, a StringIO object,\n        or any other custom object that meets this interface.\n        \"\"\"\n        self.readline = file.readline\n        self.read = file.read\n        self.memo = {}\n\n    def load(self):\n        \"\"\"Read a pickled object representation from the open file.\n\n        Return the reconstituted object hierarchy specified in the file.\n        \"\"\"\n        self.mark = object() # any new unique object\n        self.stack = []\n        self.append = self.stack.append\n        read = self.read\n        dispatch = self.dispatch\n        try:\n            while 1:\n                key = read(1)\n                dispatch[key](self)\n        except _Stop, stopinst:\n            return stopinst.value\n\n    # Return largest index k such that self.stack[k] is self.mark.\n    # If the stack doesn't contain a mark, eventually raises IndexError.\n    # This could be sped by maintaining another stack, of indices at which\n    # the mark appears.  For that matter, the latter stack would suffice,\n    # and we wouldn't need to push mark objects on self.stack at all.\n    # Doing so is probably a good thing, though, since if the pickle is\n    # corrupt (or hostile) we may get a clue from finding self.mark embedded\n    # in unpickled objects.\n    def marker(self):\n        stack = self.stack\n        mark = self.mark\n        k = len(stack)-1\n        while stack[k] is not mark: k = k-1\n        return k\n\n    dispatch = {}\n\n    def load_eof(self):\n        raise EOFError\n    dispatch[''] = load_eof\n\n    def load_proto(self):\n        proto = ord(self.read(1))\n        if not 0 <= proto <= 2:\n            raise ValueError, \"unsupported pickle protocol: %d\" % proto\n    dispatch[PROTO] = load_proto\n\n    def load_persid(self):\n        pid = self.readline()[:-1]\n        self.append(self.persistent_load(pid))\n    dispatch[PERSID] = load_persid\n\n    def load_binpersid(self):\n        pid = self.stack.pop()\n        self.append(self.persistent_load(pid))\n    dispatch[BINPERSID] = load_binpersid\n\n    def load_none(self):\n        self.append(None)\n    dispatch[NONE] = load_none\n\n    def load_false(self):\n        self.append(False)\n    dispatch[NEWFALSE] = load_false\n\n    def load_true(self):\n        self.append(True)\n    dispatch[NEWTRUE] = load_true\n\n    def load_int(self):\n        data = self.readline()\n        if data == FALSE[1:]:\n            val = False\n        elif data == TRUE[1:]:\n            val = True\n        else:\n            try:\n                val = int(data)\n            except ValueError:\n                val = long(data)\n        self.append(val)\n    dispatch[INT] = load_int\n\n    def load_binint(self):\n        self.append(mloads('i' + self.read(4)))\n    dispatch[BININT] = load_binint\n\n    def load_binint1(self):\n        self.append(ord(self.read(1)))\n    dispatch[BININT1] = load_binint1\n\n    def load_binint2(self):\n        self.append(mloads('i' + self.read(2) + '\\000\\000'))\n    dispatch[BININT2] = load_binint2\n\n    def load_long(self):\n        self.append(long(self.readline()[:-1], 0))\n    dispatch[LONG] = load_long\n\n    def load_long1(self):\n        n = ord(self.read(1))\n        bytes = self.read(n)\n        self.append(decode_long(bytes))\n    dispatch[LONG1] = load_long1\n\n    def load_long4(self):\n        n = mloads('i' + self.read(4))\n        bytes = self.read(n)\n        self.append(decode_long(bytes))\n    dispatch[LONG4] = load_long4\n\n    def load_float(self):\n        self.append(float(self.readline()[:-1]))\n    dispatch[FLOAT] = load_float\n\n    def load_binfloat(self, unpack=struct.unpack):\n        self.append(unpack('>d', self.read(8))[0])\n    dispatch[BINFLOAT] = load_binfloat\n\n    def load_string(self):\n        rep = self.readline()[:-1]\n        for q in \"\\\"'\": # double or single quote\n            if rep.startswith(q):\n                if len(rep) < 2 or not rep.endswith(q):\n                    raise ValueError, \"insecure string pickle\"\n                rep = rep[len(q):-len(q)]\n                break\n        else:\n            raise ValueError, \"insecure string pickle\"\n        self.append(rep.decode(\"string-escape\"))\n    dispatch[STRING] = load_string\n\n    def load_binstring(self):\n        len = mloads('i' + self.read(4))\n        self.append(self.read(len))\n    dispatch[BINSTRING] = load_binstring\n\n    def load_unicode(self):\n        self.append(unicode(self.readline()[:-1],'raw-unicode-escape'))\n    dispatch[UNICODE] = load_unicode\n\n    def load_binunicode(self):\n        len = mloads('i' + self.read(4))\n        self.append(unicode(self.read(len),'utf-8'))\n    dispatch[BINUNICODE] = load_binunicode\n\n    def load_short_binstring(self):\n        len = ord(self.read(1))\n        self.append(self.read(len))\n    dispatch[SHORT_BINSTRING] = load_short_binstring\n\n    def load_tuple(self):\n        k = self.marker()\n        self.stack[k:] = [tuple(self.stack[k+1:])]\n    dispatch[TUPLE] = load_tuple\n\n    def load_empty_tuple(self):\n        self.stack.append(())\n    dispatch[EMPTY_TUPLE] = load_empty_tuple\n\n    def load_tuple1(self):\n        self.stack[-1] = (self.stack[-1],)\n    dispatch[TUPLE1] = load_tuple1\n\n    def load_tuple2(self):\n        self.stack[-2:] = [(self.stack[-2], self.stack[-1])]\n    dispatch[TUPLE2] = load_tuple2\n\n    def load_tuple3(self):\n        self.stack[-3:] = [(self.stack[-3], self.stack[-2], self.stack[-1])]\n    dispatch[TUPLE3] = load_tuple3\n\n    def load_empty_list(self):\n        self.stack.append([])\n    dispatch[EMPTY_LIST] = load_empty_list\n\n    def load_empty_dictionary(self):\n        self.stack.append({})\n    dispatch[EMPTY_DICT] = load_empty_dictionary\n\n    def load_list(self):\n        k = self.marker()\n        self.stack[k:] = [self.stack[k+1:]]\n    dispatch[LIST] = load_list\n\n    def load_dict(self):\n        k = self.marker()\n        d = {}\n        items = self.stack[k+1:]\n        for i in range(0, len(items), 2):\n            key = items[i]\n            value = items[i+1]\n            d[key] = value\n        self.stack[k:] = [d]\n    dispatch[DICT] = load_dict\n\n    # INST and OBJ differ only in how they get a class object.  It's not\n    # only sensible to do the rest in a common routine, the two routines\n    # previously diverged and grew different bugs.\n    # klass is the class to instantiate, and k points to the topmost mark\n    # object, following which are the arguments for klass.__init__.\n    def _instantiate(self, klass, k):\n        args = tuple(self.stack[k+1:])\n        del self.stack[k:]\n        instantiated = 0\n        if (not args and\n                type(klass) is ClassType and\n                not hasattr(klass, \"__getinitargs__\")):\n            try:\n                value = _EmptyClass()\n                value.__class__ = klass\n                instantiated = 1\n            except RuntimeError:\n                # In restricted execution, assignment to inst.__class__ is\n                # prohibited\n                pass\n        if not instantiated:\n            try:\n                value = klass(*args)\n            except TypeError, err:\n                raise TypeError, \"in constructor for %s: %s\" % (\n                    klass.__name__, str(err)), sys.exc_info()[2]\n        self.append(value)\n\n    def load_inst(self):\n        module = self.readline()[:-1]\n        name = self.readline()[:-1]\n        klass = self.find_class(module, name)\n        self._instantiate(klass, self.marker())\n    dispatch[INST] = load_inst\n\n    def load_obj(self):\n        # Stack is ... markobject classobject arg1 arg2 ...\n        k = self.marker()\n        klass = self.stack.pop(k+1)\n        self._instantiate(klass, k)\n    dispatch[OBJ] = load_obj\n\n    def load_newobj(self):\n        args = self.stack.pop()\n        cls = self.stack[-1]\n        obj = cls.__new__(cls, *args)\n        self.stack[-1] = obj\n    dispatch[NEWOBJ] = load_newobj\n\n    def load_global(self):\n        module = self.readline()[:-1]\n        name = self.readline()[:-1]\n        klass = self.find_class(module, name)\n        self.append(klass)\n    dispatch[GLOBAL] = load_global\n\n    def load_ext1(self):\n        code = ord(self.read(1))\n        self.get_extension(code)\n    dispatch[EXT1] = load_ext1\n\n    def load_ext2(self):\n        code = mloads('i' + self.read(2) + '\\000\\000')\n        self.get_extension(code)\n    dispatch[EXT2] = load_ext2\n\n    def load_ext4(self):\n        code = mloads('i' + self.read(4))\n        self.get_extension(code)\n    dispatch[EXT4] = load_ext4\n\n    def get_extension(self, code):\n        nil = []\n        obj = _extension_cache.get(code, nil)\n        if obj is not nil:\n            self.append(obj)\n            return\n        key = _inverted_registry.get(code)\n        if not key:\n            raise ValueError(\"unregistered extension code %d\" % code)\n        obj = self.find_class(*key)\n        _extension_cache[code] = obj\n        self.append(obj)\n\n    def find_class(self, module, name):\n        # Subclasses may override this\n        __import__(module)\n        mod = sys.modules[module]\n        klass = getattr(mod, name)\n        return klass\n\n    def load_reduce(self):\n        stack = self.stack\n        args = stack.pop()\n        func = stack[-1]\n        value = func(*args)\n        stack[-1] = value\n    dispatch[REDUCE] = load_reduce\n\n    def load_pop(self):\n        del self.stack[-1]\n    dispatch[POP] = load_pop\n\n    def load_pop_mark(self):\n        k = self.marker()\n        del self.stack[k:]\n    dispatch[POP_MARK] = load_pop_mark\n\n    def load_dup(self):\n        self.append(self.stack[-1])\n    dispatch[DUP] = load_dup\n\n    def load_get(self):\n        self.append(self.memo[self.readline()[:-1]])\n    dispatch[GET] = load_get\n\n    def load_binget(self):\n        i = ord(self.read(1))\n        self.append(self.memo[repr(i)])\n    dispatch[BINGET] = load_binget\n\n    def load_long_binget(self):\n        i = mloads('i' + self.read(4))\n        self.append(self.memo[repr(i)])\n    dispatch[LONG_BINGET] = load_long_binget\n\n    def load_put(self):\n        self.memo[self.readline()[:-1]] = self.stack[-1]\n    dispatch[PUT] = load_put\n\n    def load_binput(self):\n        i = ord(self.read(1))\n        self.memo[repr(i)] = self.stack[-1]\n    dispatch[BINPUT] = load_binput\n\n    def load_long_binput(self):\n        i = mloads('i' + self.read(4))\n        self.memo[repr(i)] = self.stack[-1]\n    dispatch[LONG_BINPUT] = load_long_binput\n\n    def load_append(self):\n        stack = self.stack\n        value = stack.pop()\n        list = stack[-1]\n        list.append(value)\n    dispatch[APPEND] = load_append\n\n    def load_appends(self):\n        stack = self.stack\n        mark = self.marker()\n        list = stack[mark - 1]\n        list.extend(stack[mark + 1:])\n        del stack[mark:]\n    dispatch[APPENDS] = load_appends\n\n    def load_setitem(self):\n        stack = self.stack\n        value = stack.pop()\n        key = stack.pop()\n        dict = stack[-1]\n        dict[key] = value\n    dispatch[SETITEM] = load_setitem\n\n    def load_setitems(self):\n        stack = self.stack\n        mark = self.marker()\n        dict = stack[mark - 1]\n        for i in range(mark + 1, len(stack), 2):\n            dict[stack[i]] = stack[i + 1]\n\n        del stack[mark:]\n    dispatch[SETITEMS] = load_setitems\n\n    def load_build(self):\n        stack = self.stack\n        state = stack.pop()\n        inst = stack[-1]\n        setstate = getattr(inst, \"__setstate__\", None)\n        if setstate:\n            setstate(state)\n            return\n        slotstate = None\n        if isinstance(state, tuple) and len(state) == 2:\n            state, slotstate = state\n        if state:\n            try:\n                d = inst.__dict__\n                try:\n                    for k, v in state.iteritems():\n                        d[intern(k)] = v\n                # keys in state don't have to be strings\n                # don't blow up, but don't go out of our way\n                except TypeError:\n                    d.update(state)\n\n            except RuntimeError:\n                # XXX In restricted execution, the instance's __dict__\n                # is not accessible.  Use the old way of unpickling\n                # the instance variables.  This is a semantic\n                # difference when unpickling in restricted\n                # vs. unrestricted modes.\n                # Note, however, that cPickle has never tried to do the\n                # .update() business, and always uses\n                #     PyObject_SetItem(inst.__dict__, key, value) in a\n                # loop over state.items().\n                for k, v in state.items():\n                    setattr(inst, k, v)\n        if slotstate:\n            for k, v in slotstate.items():\n                setattr(inst, k, v)\n    dispatch[BUILD] = load_build\n\n    def load_mark(self):\n        self.append(self.mark)\n    dispatch[MARK] = load_mark\n\n    def load_stop(self):\n        value = self.stack.pop()\n        raise _Stop(value)\n    dispatch[STOP] = load_stop\n\n# Helper class for load_inst/load_obj\n\nclass _EmptyClass:\n    pass\n\n# Encode/decode longs in linear time.\n\nimport binascii as _binascii\n\ndef encode_long(x):\n    r\"\"\"Encode a long to a two's complement little-endian binary string.\n    Note that 0L is a special case, returning an empty string, to save a\n    byte in the LONG1 pickling context.\n\n    >>> encode_long(0L)\n    ''\n    >>> encode_long(255L)\n    '\\xff\\x00'\n    >>> encode_long(32767L)\n    '\\xff\\x7f'\n    >>> encode_long(-256L)\n    '\\x00\\xff'\n    >>> encode_long(-32768L)\n    '\\x00\\x80'\n    >>> encode_long(-128L)\n    '\\x80'\n    >>> encode_long(127L)\n    '\\x7f'\n    >>>\n    \"\"\"\n\n    if x == 0:\n        return ''\n    if x > 0:\n        ashex = hex(x)\n        assert ashex.startswith(\"0x\")\n        njunkchars = 2 + ashex.endswith('L')\n        nibbles = len(ashex) - njunkchars\n        if nibbles & 1:\n            # need an even # of nibbles for unhexlify\n            ashex = \"0x0\" + ashex[2:]\n        elif int(ashex[2], 16) >= 8:\n            # \"looks negative\", so need a byte of sign bits\n            ashex = \"0x00\" + ashex[2:]\n    else:\n        # Build the 256's-complement:  (1L << nbytes) + x.  The trick is\n        # to find the number of bytes in linear time (although that should\n        # really be a constant-time task).\n        ashex = hex(-x)\n        assert ashex.startswith(\"0x\")\n        njunkchars = 2 + ashex.endswith('L')\n        nibbles = len(ashex) - njunkchars\n        if nibbles & 1:\n            # Extend to a full byte.\n            nibbles += 1\n        nbits = nibbles * 4\n        x += 1L << nbits\n        assert x > 0\n        ashex = hex(x)\n        njunkchars = 2 + ashex.endswith('L')\n        newnibbles = len(ashex) - njunkchars\n        if newnibbles < nibbles:\n            ashex = \"0x\" + \"0\" * (nibbles - newnibbles) + ashex[2:]\n        if int(ashex[2], 16) < 8:\n            # \"looks positive\", so need a byte of sign bits\n            ashex = \"0xff\" + ashex[2:]\n\n    if ashex.endswith('L'):\n        ashex = ashex[2:-1]\n    else:\n        ashex = ashex[2:]\n    assert len(ashex) & 1 == 0, (x, ashex)\n    binary = _binascii.unhexlify(ashex)\n    return binary[::-1]\n\ndef decode_long(data):\n    r\"\"\"Decode a long from a two's complement little-endian binary string.\n\n    >>> decode_long('')\n    0L\n    >>> decode_long(\"\\xff\\x00\")\n    255L\n    >>> decode_long(\"\\xff\\x7f\")\n    32767L\n    >>> decode_long(\"\\x00\\xff\")\n    -256L\n    >>> decode_long(\"\\x00\\x80\")\n    -32768L\n    >>> decode_long(\"\\x80\")\n    -128L\n    >>> decode_long(\"\\x7f\")\n    127L\n    \"\"\"\n\n    nbytes = len(data)\n    if nbytes == 0:\n        return 0L\n    ashex = _binascii.hexlify(data[::-1])\n    n = long(ashex, 16) # quadratic time before Python 2.3; linear now\n    if data[-1] >= '\\x80':\n        n -= 1L << (nbytes * 8)\n    return n\n\n# Shorthands\n\ntry:\n    from cStringIO import StringIO\nexcept ImportError:\n    from StringIO import StringIO\n\ndef dump(obj, file, protocol=None):\n    Pickler(file, protocol).dump(obj)\n\ndef dumps(obj, protocol=None):\n    file = StringIO()\n    Pickler(file, protocol).dump(obj)\n    return file.getvalue()\n\ndef load(file):\n    return Unpickler(file).load()\n\ndef loads(str):\n    file = StringIO(str)\n    return Unpickler(file).load()\n\n# Doctest\n\ndef _test():\n    import doctest\n    return doctest.testmod()\n\nif __name__ == \"__main__\":\n    _test()\n", 
    "posixpath": "\"\"\"Common operations on Posix pathnames.\n\nInstead of importing this module directly, import os and refer to\nthis module as os.path.  The \"os.path\" name is an alias for this\nmodule on Posix systems; on other systems (e.g. Mac, Windows),\nos.path provides the same operations in a manner specific to that\nplatform, and is an alias to another module (e.g. macpath, ntpath).\n\nSome of this can actually be useful on non-Posix systems too, e.g.\nfor manipulation of the pathname component of URLs.\n\"\"\"\n\nimport os\nimport sys\nimport stat\nimport genericpath\nimport warnings\nfrom genericpath import *\n\ntry:\n    _unicode = unicode\nexcept NameError:\n    # If Python is built without Unicode support, the unicode type\n    # will not exist. Fake one.\n    class _unicode(object):\n        pass\n\n__all__ = [\"normcase\",\"isabs\",\"join\",\"splitdrive\",\"split\",\"splitext\",\n           \"basename\",\"dirname\",\"commonprefix\",\"getsize\",\"getmtime\",\n           \"getatime\",\"getctime\",\"islink\",\"exists\",\"lexists\",\"isdir\",\"isfile\",\n           \"ismount\",\"walk\",\"expanduser\",\"expandvars\",\"normpath\",\"abspath\",\n           \"samefile\",\"sameopenfile\",\"samestat\",\n           \"curdir\",\"pardir\",\"sep\",\"pathsep\",\"defpath\",\"altsep\",\"extsep\",\n           \"devnull\",\"realpath\",\"supports_unicode_filenames\",\"relpath\"]\n\n# strings representing various path-related bits and pieces\ncurdir = '.'\npardir = '..'\nextsep = '.'\nsep = '/'\npathsep = ':'\ndefpath = ':/bin:/usr/bin'\naltsep = None\ndevnull = '/dev/null'\n\n# Normalize the case of a pathname.  Trivial in Posix, string.lower on Mac.\n# On MS-DOS this may also turn slashes into backslashes; however, other\n# normalizations (such as optimizing '../' away) are not allowed\n# (another function should be defined to do that).\n\ndef normcase(s):\n    \"\"\"Normalize case of pathname.  Has no effect under Posix\"\"\"\n    return s\n\n\n# Return whether a path is absolute.\n# Trivial in Posix, harder on the Mac or MS-DOS.\n\ndef isabs(s):\n    \"\"\"Test whether a path is absolute\"\"\"\n    return s.startswith('/')\n\n\n# Join pathnames.\n# Ignore the previous parts if a part is absolute.\n# Insert a '/' unless the first part is empty or already ends in '/'.\n\ndef join(a, *p):\n    \"\"\"Join two or more pathname components, inserting '/' as needed.\n    If any component is an absolute path, all previous path components\n    will be discarded.  An empty last part will result in a path that\n    ends with a separator.\"\"\"\n    path = a\n    for b in p:\n        if b.startswith('/'):\n            path = b\n        elif path == '' or path.endswith('/'):\n            path +=  b\n        else:\n            path += '/' + b\n    return path\n\n\n# Split a path in head (everything up to the last '/') and tail (the\n# rest).  If the path ends in '/', tail will be empty.  If there is no\n# '/' in the path, head  will be empty.\n# Trailing '/'es are stripped from head unless it is the root.\n\ndef split(p):\n    \"\"\"Split a pathname.  Returns tuple \"(head, tail)\" where \"tail\" is\n    everything after the final slash.  Either part may be empty.\"\"\"\n    i = p.rfind('/') + 1\n    head, tail = p[:i], p[i:]\n    if head and head != '/'*len(head):\n        head = head.rstrip('/')\n    return head, tail\n\n\n# Split a path in root and extension.\n# The extension is everything starting at the last dot in the last\n# pathname component; the root is everything before that.\n# It is always true that root + ext == p.\n\ndef splitext(p):\n    return genericpath._splitext(p, sep, altsep, extsep)\nsplitext.__doc__ = genericpath._splitext.__doc__\n\n# Split a pathname into a drive specification and the rest of the\n# path.  Useful on DOS/Windows/NT; on Unix, the drive is always empty.\n\ndef splitdrive(p):\n    \"\"\"Split a pathname into drive and path. On Posix, drive is always\n    empty.\"\"\"\n    return '', p\n\n\n# Return the tail (basename) part of a path, same as split(path)[1].\n\ndef basename(p):\n    \"\"\"Returns the final component of a pathname\"\"\"\n    i = p.rfind('/') + 1\n    return p[i:]\n\n\n# Return the head (dirname) part of a path, same as split(path)[0].\n\ndef dirname(p):\n    \"\"\"Returns the directory component of a pathname\"\"\"\n    i = p.rfind('/') + 1\n    head = p[:i]\n    if head and head != '/'*len(head):\n        head = head.rstrip('/')\n    return head\n\n\n# Is a path a symbolic link?\n# This will always return false on systems where os.lstat doesn't exist.\n\ndef islink(path):\n    \"\"\"Test whether a path is a symbolic link\"\"\"\n    try:\n        st = os.lstat(path)\n    except (os.error, AttributeError):\n        return False\n    return stat.S_ISLNK(st.st_mode)\n\n# Being true for dangling symbolic links is also useful.\n\ndef lexists(path):\n    \"\"\"Test whether a path exists.  Returns True for broken symbolic links\"\"\"\n    try:\n        os.lstat(path)\n    except os.error:\n        return False\n    return True\n\n\n# Are two filenames really pointing to the same file?\n\ndef samefile(f1, f2):\n    \"\"\"Test whether two pathnames reference the same actual file\"\"\"\n    s1 = os.stat(f1)\n    s2 = os.stat(f2)\n    return samestat(s1, s2)\n\n\n# Are two open files really referencing the same file?\n# (Not necessarily the same file descriptor!)\n\ndef sameopenfile(fp1, fp2):\n    \"\"\"Test whether two open file objects reference the same file\"\"\"\n    s1 = os.fstat(fp1)\n    s2 = os.fstat(fp2)\n    return samestat(s1, s2)\n\n\n# Are two stat buffers (obtained from stat, fstat or lstat)\n# describing the same file?\n\ndef samestat(s1, s2):\n    \"\"\"Test whether two stat buffers reference the same file\"\"\"\n    return s1.st_ino == s2.st_ino and \\\n           s1.st_dev == s2.st_dev\n\n\n# Is a path a mount point?\n# (Does this work for all UNIXes?  Is it even guaranteed to work by Posix?)\n\ndef ismount(path):\n    \"\"\"Test whether a path is a mount point\"\"\"\n    if islink(path):\n        # A symlink can never be a mount point\n        return False\n    try:\n        s1 = os.lstat(path)\n        s2 = os.lstat(join(path, '..'))\n    except os.error:\n        return False # It doesn't exist -- so not a mount point :-)\n    dev1 = s1.st_dev\n    dev2 = s2.st_dev\n    if dev1 != dev2:\n        return True     # path/.. on a different device as path\n    ino1 = s1.st_ino\n    ino2 = s2.st_ino\n    if ino1 == ino2:\n        return True     # path/.. is the same i-node as path\n    return False\n\n\n# Directory tree walk.\n# For each directory under top (including top itself, but excluding\n# '.' and '..'), func(arg, dirname, filenames) is called, where\n# dirname is the name of the directory and filenames is the list\n# of files (and subdirectories etc.) in the directory.\n# The func may modify the filenames list, to implement a filter,\n# or to impose a different order of visiting.\n\ndef walk(top, func, arg):\n    \"\"\"Directory tree walk with callback function.\n\n    For each directory in the directory tree rooted at top (including top\n    itself, but excluding '.' and '..'), call func(arg, dirname, fnames).\n    dirname is the name of the directory, and fnames a list of the names of\n    the files and subdirectories in dirname (excluding '.' and '..').  func\n    may modify the fnames list in-place (e.g. via del or slice assignment),\n    and walk will only recurse into the subdirectories whose names remain in\n    fnames; this can be used to implement a filter, or to impose a specific\n    order of visiting.  No semantics are defined for, or required of, arg,\n    beyond that arg is always passed to func.  It can be used, e.g., to pass\n    a filename pattern, or a mutable object designed to accumulate\n    statistics.  Passing None for arg is common.\"\"\"\n    warnings.warnpy3k(\"In 3.x, os.path.walk is removed in favor of os.walk.\",\n                      stacklevel=2)\n    try:\n        names = os.listdir(top)\n    except os.error:\n        return\n    func(arg, top, names)\n    for name in names:\n        name = join(top, name)\n        try:\n            st = os.lstat(name)\n        except os.error:\n            continue\n        if stat.S_ISDIR(st.st_mode):\n            walk(name, func, arg)\n\n\n# Expand paths beginning with '~' or '~user'.\n# '~' means $HOME; '~user' means that user's home directory.\n# If the path doesn't begin with '~', or if the user or $HOME is unknown,\n# the path is returned unchanged (leaving error reporting to whatever\n# function is called with the expanded path as argument).\n# See also module 'glob' for expansion of *, ? and [...] in pathnames.\n# (A function should also be defined to do full *sh-style environment\n# variable expansion.)\n\ndef expanduser(path):\n    \"\"\"Expand ~ and ~user constructions.  If user or $HOME is unknown,\n    do nothing.\"\"\"\n    if not path.startswith('~'):\n        return path\n    i = path.find('/', 1)\n    if i < 0:\n        i = len(path)\n    if i == 1:\n        if 'HOME' not in os.environ:\n            import pwd\n            userhome = pwd.getpwuid(os.getuid()).pw_dir\n        else:\n            userhome = os.environ['HOME']\n    else:\n        import pwd\n        try:\n            pwent = pwd.getpwnam(path[1:i])\n        except KeyError:\n            return path\n        userhome = pwent.pw_dir\n    userhome = userhome.rstrip('/')\n    return (userhome + path[i:]) or '/'\n\n\n# Expand paths containing shell variable substitutions.\n# This expands the forms $variable and ${variable} only.\n# Non-existent variables are left unchanged.\n\n_varprog = None\n_uvarprog = None\n\ndef expandvars(path):\n    \"\"\"Expand shell variables of form $var and ${var}.  Unknown variables\n    are left unchanged.\"\"\"\n    global _varprog, _uvarprog\n    if '$' not in path:\n        return path\n    if isinstance(path, _unicode):\n        if not _varprog:\n            import re\n            _varprog = re.compile(r'\\$(\\w+|\\{[^}]*\\})')\n        varprog = _varprog\n        encoding = sys.getfilesystemencoding()\n    else:\n        if not _uvarprog:\n            import re\n            _uvarprog = re.compile(_unicode(r'\\$(\\w+|\\{[^}]*\\})'), re.UNICODE)\n        varprog = _uvarprog\n        encoding = None\n    i = 0\n    while True:\n        m = varprog.search(path, i)\n        if not m:\n            break\n        i, j = m.span(0)\n        name = m.group(1)\n        if name.startswith('{') and name.endswith('}'):\n            name = name[1:-1]\n        if encoding:\n            name = name.encode(encoding)\n        if name in os.environ:\n            tail = path[j:]\n            value = os.environ[name]\n            if encoding:\n                value = value.decode(encoding)\n            path = path[:i] + value\n            i = len(path)\n            path += tail\n        else:\n            i = j\n    return path\n\n\n# Normalize a path, e.g. A//B, A/./B and A/foo/../B all become A/B.\n# It should be understood that this may change the meaning of the path\n# if it contains symbolic links!\n\ndef normpath(path):\n    \"\"\"Normalize path, eliminating double slashes, etc.\"\"\"\n    # Preserve unicode (if path is unicode)\n    slash, dot = (u'/', u'.') if isinstance(path, _unicode) else ('/', '.')\n    if path == '':\n        return dot\n    initial_slashes = path.startswith('/')\n    # POSIX allows one or two initial slashes, but treats three or more\n    # as single slash.\n    if (initial_slashes and\n        path.startswith('//') and not path.startswith('///')):\n        initial_slashes = 2\n    comps = path.split('/')\n    new_comps = []\n    for comp in comps:\n        if comp in ('', '.'):\n            continue\n        if (comp != '..' or (not initial_slashes and not new_comps) or\n             (new_comps and new_comps[-1] == '..')):\n            new_comps.append(comp)\n        elif new_comps:\n            new_comps.pop()\n    comps = new_comps\n    path = slash.join(comps)\n    if initial_slashes:\n        path = slash*initial_slashes + path\n    return path or dot\n\n\ndef abspath(path):\n    \"\"\"Return an absolute path.\"\"\"\n    if not isabs(path):\n        if isinstance(path, _unicode):\n            cwd = os.getcwdu()\n        else:\n            cwd = os.getcwd()\n        path = join(cwd, path)\n    return normpath(path)\n\n\n# Return a canonical path (i.e. the absolute location of a file on the\n# filesystem).\n\ndef realpath(filename):\n    \"\"\"Return the canonical path of the specified filename, eliminating any\nsymbolic links encountered in the path.\"\"\"\n    path, ok = _joinrealpath('', filename, {})\n    return abspath(path)\n\n# Join two paths, normalizing ang eliminating any symbolic links\n# encountered in the second path.\ndef _joinrealpath(path, rest, seen):\n    if isabs(rest):\n        rest = rest[1:]\n        path = sep\n\n    while rest:\n        name, _, rest = rest.partition(sep)\n        if not name or name == curdir:\n            # current dir\n            continue\n        if name == pardir:\n            # parent dir\n            if path:\n                path, name = split(path)\n                if name == pardir:\n                    path = join(path, pardir, pardir)\n            else:\n                path = pardir\n            continue\n        newpath = join(path, name)\n        if not islink(newpath):\n            path = newpath\n            continue\n        # Resolve the symbolic link\n        if newpath in seen:\n            # Already seen this path\n            path = seen[newpath]\n            if path is not None:\n                # use cached value\n                continue\n            # The symlink is not resolved, so we must have a symlink loop.\n            # Return already resolved part + rest of the path unchanged.\n            return join(newpath, rest), False\n        seen[newpath] = None # not resolved symlink\n        path, ok = _joinrealpath(path, os.readlink(newpath), seen)\n        if not ok:\n            return join(path, rest), False\n        seen[newpath] = path # resolved symlink\n\n    return path, True\n\n\nsupports_unicode_filenames = (sys.platform == 'darwin')\n\ndef relpath(path, start=curdir):\n    \"\"\"Return a relative version of a path\"\"\"\n\n    if not path:\n        raise ValueError(\"no path specified\")\n\n    start_list = [x for x in abspath(start).split(sep) if x]\n    path_list = [x for x in abspath(path).split(sep) if x]\n\n    # Work out how much of the filepath is shared by start and path.\n    i = len(commonprefix([start_list, path_list]))\n\n    rel_list = [pardir] * (len(start_list)-i) + path_list[i:]\n    if not rel_list:\n        return curdir\n    return join(*rel_list)\n", 
    "pprint": "#  Author:      Fred L. Drake, Jr.\n#               fdrake@acm.org\n#\n#  This is a simple little module I wrote to make life easier.  I didn't\n#  see anything quite like it in the library, though I may have overlooked\n#  something.  I wrote this when I was trying to read some heavily nested\n#  tuples with fairly non-descriptive content.  This is modeled very much\n#  after Lisp/Scheme - style pretty-printing of lists.  If you find it\n#  useful, thank small children who sleep at night.\n\n\"\"\"Support to pretty-print lists, tuples, & dictionaries recursively.\n\nVery simple, but useful, especially in debugging data structures.\n\nClasses\n-------\n\nPrettyPrinter()\n    Handle pretty-printing operations onto a stream using a configured\n    set of formatting parameters.\n\nFunctions\n---------\n\npformat()\n    Format a Python object into a pretty-printed representation.\n\npprint()\n    Pretty-print a Python object to a stream [default is sys.stdout].\n\nsaferepr()\n    Generate a 'standard' repr()-like value, but protect against recursive\n    data structures.\n\n\"\"\"\n\nimport sys as _sys\nimport warnings\n\ntry:\n    from cStringIO import StringIO as _StringIO\nexcept ImportError:\n    from StringIO import StringIO as _StringIO\n\n__all__ = [\"pprint\",\"pformat\",\"isreadable\",\"isrecursive\",\"saferepr\",\n           \"PrettyPrinter\"]\n\n# cache these for faster access:\n_commajoin = \", \".join\n_id = id\n_len = len\n_type = type\n\n\ndef pprint(object, stream=None, indent=1, width=80, depth=None):\n    \"\"\"Pretty-print a Python object to a stream [default is sys.stdout].\"\"\"\n    printer = PrettyPrinter(\n        stream=stream, indent=indent, width=width, depth=depth)\n    printer.pprint(object)\n\ndef pformat(object, indent=1, width=80, depth=None):\n    \"\"\"Format a Python object into a pretty-printed representation.\"\"\"\n    return PrettyPrinter(indent=indent, width=width, depth=depth).pformat(object)\n\ndef saferepr(object):\n    \"\"\"Version of repr() which can handle recursive data structures.\"\"\"\n    return _safe_repr(object, {}, None, 0)[0]\n\ndef isreadable(object):\n    \"\"\"Determine if saferepr(object) is readable by eval().\"\"\"\n    return _safe_repr(object, {}, None, 0)[1]\n\ndef isrecursive(object):\n    \"\"\"Determine if object requires a recursive representation.\"\"\"\n    return _safe_repr(object, {}, None, 0)[2]\n\ndef _sorted(iterable):\n    with warnings.catch_warnings():\n        if _sys.py3kwarning:\n            warnings.filterwarnings(\"ignore\", \"comparing unequal types \"\n                                    \"not supported\", DeprecationWarning)\n        return sorted(iterable)\n\nclass PrettyPrinter:\n    def __init__(self, indent=1, width=80, depth=None, stream=None):\n        \"\"\"Handle pretty printing operations onto a stream using a set of\n        configured parameters.\n\n        indent\n            Number of spaces to indent for each level of nesting.\n\n        width\n            Attempted maximum number of columns in the output.\n\n        depth\n            The maximum depth to print out nested structures.\n\n        stream\n            The desired output stream.  If omitted (or false), the standard\n            output stream available at construction will be used.\n\n        \"\"\"\n        indent = int(indent)\n        width = int(width)\n        assert indent >= 0, \"indent must be >= 0\"\n        assert depth is None or depth > 0, \"depth must be > 0\"\n        assert width, \"width must be != 0\"\n        self._depth = depth\n        self._indent_per_level = indent\n        self._width = width\n        if stream is not None:\n            self._stream = stream\n        else:\n            self._stream = _sys.stdout\n\n    def pprint(self, object):\n        self._format(object, self._stream, 0, 0, {}, 0)\n        self._stream.write(\"\\n\")\n\n    def pformat(self, object):\n        sio = _StringIO()\n        self._format(object, sio, 0, 0, {}, 0)\n        return sio.getvalue()\n\n    def isrecursive(self, object):\n        return self.format(object, {}, 0, 0)[2]\n\n    def isreadable(self, object):\n        s, readable, recursive = self.format(object, {}, 0, 0)\n        return readable and not recursive\n\n    def _format(self, object, stream, indent, allowance, context, level):\n        level = level + 1\n        objid = _id(object)\n        if objid in context:\n            stream.write(_recursion(object))\n            self._recursive = True\n            self._readable = False\n            return\n        rep = self._repr(object, context, level - 1)\n        typ = _type(object)\n        sepLines = _len(rep) > (self._width - 1 - indent - allowance)\n        write = stream.write\n\n        if self._depth and level > self._depth:\n            write(rep)\n            return\n\n        r = getattr(typ, \"__repr__\", None)\n        if issubclass(typ, dict) and r == dict.__repr__:\n            write('{')\n            if self._indent_per_level > 1:\n                write((self._indent_per_level - 1) * ' ')\n            length = _len(object)\n            if length:\n                context[objid] = 1\n                indent = indent + self._indent_per_level\n                items = _sorted(object.items())\n                key, ent = items[0]\n                rep = self._repr(key, context, level)\n                write(rep)\n                write(': ')\n                self._format(ent, stream, indent + _len(rep) + 2,\n                              allowance + 1, context, level)\n                if length > 1:\n                    for key, ent in items[1:]:\n                        rep = self._repr(key, context, level)\n                        if sepLines:\n                            write(',\\n%s%s: ' % (' '*indent, rep))\n                        else:\n                            write(', %s: ' % rep)\n                        self._format(ent, stream, indent + _len(rep) + 2,\n                                      allowance + 1, context, level)\n                indent = indent - self._indent_per_level\n                del context[objid]\n            write('}')\n            return\n\n        if ((issubclass(typ, list) and r == list.__repr__) or\n            (issubclass(typ, tuple) and r == tuple.__repr__) or\n            (issubclass(typ, set) and r == set.__repr__) or\n            (issubclass(typ, frozenset) and r == frozenset.__repr__)\n           ):\n            length = _len(object)\n            if issubclass(typ, list):\n                write('[')\n                endchar = ']'\n            elif issubclass(typ, tuple):\n                write('(')\n                endchar = ')'\n            else:\n                if not length:\n                    write(rep)\n                    return\n                write(typ.__name__)\n                write('([')\n                endchar = '])'\n                indent += len(typ.__name__) + 1\n                object = _sorted(object)\n            if self._indent_per_level > 1 and sepLines:\n                write((self._indent_per_level - 1) * ' ')\n            if length:\n                context[objid] = 1\n                indent = indent + self._indent_per_level\n                self._format(object[0], stream, indent, allowance + 1,\n                             context, level)\n                if length > 1:\n                    for ent in object[1:]:\n                        if sepLines:\n                            write(',\\n' + ' '*indent)\n                        else:\n                            write(', ')\n                        self._format(ent, stream, indent,\n                                      allowance + 1, context, level)\n                indent = indent - self._indent_per_level\n                del context[objid]\n            if issubclass(typ, tuple) and length == 1:\n                write(',')\n            write(endchar)\n            return\n\n        write(rep)\n\n    def _repr(self, object, context, level):\n        repr, readable, recursive = self.format(object, context.copy(),\n                                                self._depth, level)\n        if not readable:\n            self._readable = False\n        if recursive:\n            self._recursive = True\n        return repr\n\n    def format(self, object, context, maxlevels, level):\n        \"\"\"Format object for a specific context, returning a string\n        and flags indicating whether the representation is 'readable'\n        and whether the object represents a recursive construct.\n        \"\"\"\n        return _safe_repr(object, context, maxlevels, level)\n\n\n# Return triple (repr_string, isreadable, isrecursive).\n\ndef _safe_repr(object, context, maxlevels, level):\n    typ = _type(object)\n    if typ is str:\n        if 'locale' not in _sys.modules:\n            return repr(object), True, False\n        if \"'\" in object and '\"' not in object:\n            closure = '\"'\n            quotes = {'\"': '\\\\\"'}\n        else:\n            closure = \"'\"\n            quotes = {\"'\": \"\\\\'\"}\n        qget = quotes.get\n        sio = _StringIO()\n        write = sio.write\n        for char in object:\n            if char.isalpha():\n                write(char)\n            else:\n                write(qget(char, repr(char)[1:-1]))\n        return (\"%s%s%s\" % (closure, sio.getvalue(), closure)), True, False\n\n    r = getattr(typ, \"__repr__\", None)\n    if issubclass(typ, dict) and r == dict.__repr__:\n        if not object:\n            return \"{}\", True, False\n        objid = _id(object)\n        if maxlevels and level >= maxlevels:\n            return \"{...}\", False, objid in context\n        if objid in context:\n            return _recursion(object), False, True\n        context[objid] = 1\n        readable = True\n        recursive = False\n        components = []\n        append = components.append\n        level += 1\n        saferepr = _safe_repr\n        for k, v in _sorted(object.items()):\n            krepr, kreadable, krecur = saferepr(k, context, maxlevels, level)\n            vrepr, vreadable, vrecur = saferepr(v, context, maxlevels, level)\n            append(\"%s: %s\" % (krepr, vrepr))\n            readable = readable and kreadable and vreadable\n            if krecur or vrecur:\n                recursive = True\n        del context[objid]\n        return \"{%s}\" % _commajoin(components), readable, recursive\n\n    if (issubclass(typ, list) and r == list.__repr__) or \\\n       (issubclass(typ, tuple) and r == tuple.__repr__):\n        if issubclass(typ, list):\n            if not object:\n                return \"[]\", True, False\n            format = \"[%s]\"\n        elif _len(object) == 1:\n            format = \"(%s,)\"\n        else:\n            if not object:\n                return \"()\", True, False\n            format = \"(%s)\"\n        objid = _id(object)\n        if maxlevels and level >= maxlevels:\n            return format % \"...\", False, objid in context\n        if objid in context:\n            return _recursion(object), False, True\n        context[objid] = 1\n        readable = True\n        recursive = False\n        components = []\n        append = components.append\n        level += 1\n        for o in object:\n            orepr, oreadable, orecur = _safe_repr(o, context, maxlevels, level)\n            append(orepr)\n            if not oreadable:\n                readable = False\n            if orecur:\n                recursive = True\n        del context[objid]\n        return format % _commajoin(components), readable, recursive\n\n    rep = repr(object)\n    return rep, (rep and not rep.startswith('<')), False\n\n\ndef _recursion(object):\n    return (\"<Recursion on %s with id=%s>\"\n            % (_type(object).__name__, _id(object)))\n\n\ndef _perfcheck(object=None):\n    import time\n    if object is None:\n        object = [(\"string\", (1, 2), [3, 4], {5: 6, 7: 8})] * 100000\n    p = PrettyPrinter()\n    t1 = time.time()\n    _safe_repr(object, {}, None, 0)\n    t2 = time.time()\n    p.pformat(object)\n    t3 = time.time()\n    print \"_safe_repr:\", t2 - t1\n    print \"pformat:\", t3 - t2\n\nif __name__ == \"__main__\":\n    _perfcheck()\n", 
    "pwd": "# ctypes implementation: Victor Stinner, 2008-05-08\n\"\"\"\nThis module provides access to the Unix password database.\nIt is available on all Unix versions.\n\nPassword database entries are reported as 7-tuples containing the following\nitems from the password database (see `<pwd.h>'), in order:\npw_name, pw_passwd, pw_uid, pw_gid, pw_gecos, pw_dir, pw_shell.\nThe uid and gid items are integers, all others are strings. An\nexception is raised if the entry asked for cannot be found.\n\"\"\"\n\nfrom _pwdgrp_cffi import ffi, lib\nimport _structseq\n\ntry: from __pypy__ import builtinify\nexcept ImportError: builtinify = lambda f: f\n\n\nclass struct_passwd:\n    \"\"\"\n    pwd.struct_passwd: Results from getpw*() routines.\n\n    This object may be accessed either as a tuple of\n      (pw_name,pw_passwd,pw_uid,pw_gid,pw_gecos,pw_dir,pw_shell)\n    or via the object attributes as named in the above tuple.\n    \"\"\"\n    __metaclass__ = _structseq.structseqtype\n    name = \"pwd.struct_passwd\"\n\n    pw_name = _structseq.structseqfield(0)\n    pw_passwd = _structseq.structseqfield(1)\n    pw_uid = _structseq.structseqfield(2)\n    pw_gid = _structseq.structseqfield(3)\n    pw_gecos = _structseq.structseqfield(4)\n    pw_dir = _structseq.structseqfield(5)\n    pw_shell = _structseq.structseqfield(6)\n\n\ndef _mkpwent(pw):\n    return struct_passwd([\n        ffi.string(pw.pw_name),\n        ffi.string(pw.pw_passwd),\n        pw.pw_uid,\n        pw.pw_gid,\n        ffi.string(pw.pw_gecos),\n        ffi.string(pw.pw_dir),\n        ffi.string(pw.pw_shell)])\n\n@builtinify\ndef getpwuid(uid):\n    \"\"\"\n    getpwuid(uid) -> (pw_name,pw_passwd,pw_uid,\n                      pw_gid,pw_gecos,pw_dir,pw_shell)\n    Return the password database entry for the given numeric user ID.\n    See pwd.__doc__ for more on password database entries.\n    \"\"\"\n    pw = lib.getpwuid(uid)\n    if not pw:\n        raise KeyError(\"getpwuid(): uid not found: %s\" % uid)\n    return _mkpwent(pw)\n\n@builtinify\ndef getpwnam(name):\n    \"\"\"\n    getpwnam(name) -> (pw_name,pw_passwd,pw_uid,\n                        pw_gid,pw_gecos,pw_dir,pw_shell)\n    Return the password database entry for the given user name.\n    See pwd.__doc__ for more on password database entries.\n    \"\"\"\n    if not isinstance(name, basestring):\n        raise TypeError(\"expected string\")\n    name = str(name)\n    pw = lib.getpwnam(name)\n    if not pw:\n        raise KeyError(\"getpwname(): name not found: %s\" % name)\n    return _mkpwent(pw)\n\n@builtinify\ndef getpwall():\n    \"\"\"\n    getpwall() -> list_of_entries\n    Return a list of all available password database entries, in arbitrary order.\n    See pwd.__doc__ for more on password database entries.\n    \"\"\"\n    users = []\n    lib.setpwent()\n    while True:\n        pw = lib.getpwent()\n        if not pw:\n            break\n        users.append(_mkpwent(pw))\n    lib.endpwent()\n    return users\n\n__all__ = ('struct_passwd', 'getpwuid', 'getpwnam', 'getpwall')\n\nif __name__ == \"__main__\":\n# Uncomment next line to test CPython implementation\n#    from pwd import getpwuid, getpwnam, getpwall\n    from os import getuid\n    uid = getuid()\n    pw = getpwuid(uid)\n    print(\"uid %s: %s\" % (pw.pw_uid, pw))\n    name = pw.pw_name\n    print(\"name %r: %s\" % (name, getpwnam(name)))\n    print(\"All:\")\n    for pw in getpwall():\n        print(pw)\n", 
    "random": "\"\"\"Random variable generators.\n\n    integers\n    --------\n           uniform within range\n\n    sequences\n    ---------\n           pick random element\n           pick random sample\n           generate random permutation\n\n    distributions on the real line:\n    ------------------------------\n           uniform\n           triangular\n           normal (Gaussian)\n           lognormal\n           negative exponential\n           gamma\n           beta\n           pareto\n           Weibull\n\n    distributions on the circle (angles 0 to 2pi)\n    ---------------------------------------------\n           circular uniform\n           von Mises\n\nGeneral notes on the underlying Mersenne Twister core generator:\n\n* The period is 2**19937-1.\n* It is one of the most extensively tested generators in existence.\n* Without a direct way to compute N steps forward, the semantics of\n  jumpahead(n) are weakened to simply jump to another distant state and rely\n  on the large period to avoid overlapping sequences.\n* The random() method is implemented in C, executes in a single Python step,\n  and is, therefore, threadsafe.\n\n\"\"\"\n\nfrom __future__ import division\nfrom warnings import warn as _warn\nfrom math import log as _log, exp as _exp, pi as _pi, e as _e, ceil as _ceil\nfrom math import sqrt as _sqrt, acos as _acos, cos as _cos, sin as _sin\nfrom os import urandom as _urandom\nfrom binascii import hexlify as _hexlify\nimport hashlib as _hashlib\n\n__all__ = [\"Random\",\"seed\",\"random\",\"uniform\",\"randint\",\"choice\",\"sample\",\n           \"randrange\",\"shuffle\",\"normalvariate\",\"lognormvariate\",\n           \"expovariate\",\"vonmisesvariate\",\"gammavariate\",\"triangular\",\n           \"gauss\",\"betavariate\",\"paretovariate\",\"weibullvariate\",\n           \"getstate\",\"setstate\",\"jumpahead\", \"WichmannHill\", \"getrandbits\",\n           \"SystemRandom\"]\n\nNV_MAGICCONST = 4 * _exp(-0.5)/_sqrt(2.0)\nTWOPI = 2.0*_pi\nLOG4 = _log(4.0)\nSG_MAGICCONST = 1.0 + _log(4.5)\nBPF = 53        # Number of bits in a float\nRECIP_BPF = 2**-BPF\n\n\n# Translated by Guido van Rossum from C source provided by\n# Adrian Baddeley.  Adapted by Raymond Hettinger for use with\n# the Mersenne Twister  and os.urandom() core generators.\n\nimport _random\n\nclass Random(_random.Random):\n    \"\"\"Random number generator base class used by bound module functions.\n\n    Used to instantiate instances of Random to get generators that don't\n    share state.  Especially useful for multi-threaded programs, creating\n    a different instance of Random for each thread, and using the jumpahead()\n    method to ensure that the generated sequences seen by each thread don't\n    overlap.\n\n    Class Random can also be subclassed if you want to use a different basic\n    generator of your own devising: in that case, override the following\n    methods: random(), seed(), getstate(), setstate() and jumpahead().\n    Optionally, implement a getrandbits() method so that randrange() can cover\n    arbitrarily large ranges.\n\n    \"\"\"\n\n    VERSION = 3     # used by getstate/setstate\n\n    def __init__(self, x=None):\n        \"\"\"Initialize an instance.\n\n        Optional argument x controls seeding, as for Random.seed().\n        \"\"\"\n\n        self.seed(x)\n        self.gauss_next = None\n\n    def seed(self, a=None):\n        \"\"\"Initialize internal state from hashable object.\n\n        None or no argument seeds from current time or from an operating\n        system specific randomness source if available.\n\n        If a is not None or an int or long, hash(a) is used instead.\n        \"\"\"\n\n        if a is None:\n            try:\n                # Seed with enough bytes to span the 19937 bit\n                # state space for the Mersenne Twister\n                a = long(_hexlify(_urandom(2500)), 16)\n            except NotImplementedError:\n                import time\n                a = long(time.time() * 256) # use fractional seconds\n\n        super(Random, self).seed(a)\n        self.gauss_next = None\n\n    def getstate(self):\n        \"\"\"Return internal state; can be passed to setstate() later.\"\"\"\n        return self.VERSION, super(Random, self).getstate(), self.gauss_next\n\n    def setstate(self, state):\n        \"\"\"Restore internal state from object returned by getstate().\"\"\"\n        version = state[0]\n        if version == 3:\n            version, internalstate, self.gauss_next = state\n            super(Random, self).setstate(internalstate)\n        elif version == 2:\n            version, internalstate, self.gauss_next = state\n            # In version 2, the state was saved as signed ints, which causes\n            #   inconsistencies between 32/64-bit systems. The state is\n            #   really unsigned 32-bit ints, so we convert negative ints from\n            #   version 2 to positive longs for version 3.\n            try:\n                internalstate = tuple( long(x) % (2**32) for x in internalstate )\n            except ValueError, e:\n                raise TypeError, e\n            super(Random, self).setstate(internalstate)\n        else:\n            raise ValueError(\"state with version %s passed to \"\n                             \"Random.setstate() of version %s\" %\n                             (version, self.VERSION))\n\n    def jumpahead(self, n):\n        \"\"\"Change the internal state to one that is likely far away\n        from the current state.  This method will not be in Py3.x,\n        so it is better to simply reseed.\n        \"\"\"\n        # The super.jumpahead() method uses shuffling to change state,\n        # so it needs a large and \"interesting\" n to work with.  Here,\n        # we use hashing to create a large n for the shuffle.\n        s = repr(n) + repr(self.getstate())\n        n = int(_hashlib.new('sha512', s).hexdigest(), 16)\n        super(Random, self).jumpahead(n)\n\n## ---- Methods below this point do not need to be overridden when\n## ---- subclassing for the purpose of using a different core generator.\n\n## -------------------- pickle support  -------------------\n\n    def __getstate__(self): # for pickle\n        return self.getstate()\n\n    def __setstate__(self, state):  # for pickle\n        self.setstate(state)\n\n    def __reduce__(self):\n        return self.__class__, (), self.getstate()\n\n## -------------------- integer methods  -------------------\n\n    def randrange(self, start, stop=None, step=1, _int=int, _maxwidth=1L<<BPF):\n        \"\"\"Choose a random item from range(start, stop[, step]).\n\n        This fixes the problem with randint() which includes the\n        endpoint; in Python this is usually not what you want.\n\n        \"\"\"\n\n        # This code is a bit messy to make it fast for the\n        # common case while still doing adequate error checking.\n        istart = _int(start)\n        if istart != start:\n            raise ValueError, \"non-integer arg 1 for randrange()\"\n        if stop is None:\n            if istart > 0:\n                if istart >= _maxwidth:\n                    return self._randbelow(istart)\n                return _int(self.random() * istart)\n            raise ValueError, \"empty range for randrange()\"\n\n        # stop argument supplied.\n        istop = _int(stop)\n        if istop != stop:\n            raise ValueError, \"non-integer stop for randrange()\"\n        width = istop - istart\n        if step == 1 and width > 0:\n            # Note that\n            #     int(istart + self.random()*width)\n            # instead would be incorrect.  For example, consider istart\n            # = -2 and istop = 0.  Then the guts would be in\n            # -2.0 to 0.0 exclusive on both ends (ignoring that random()\n            # might return 0.0), and because int() truncates toward 0, the\n            # final result would be -1 or 0 (instead of -2 or -1).\n            #     istart + int(self.random()*width)\n            # would also be incorrect, for a subtler reason:  the RHS\n            # can return a long, and then randrange() would also return\n            # a long, but we're supposed to return an int (for backward\n            # compatibility).\n\n            if width >= _maxwidth:\n                return _int(istart + self._randbelow(width))\n            return _int(istart + _int(self.random()*width))\n        if step == 1:\n            raise ValueError, \"empty range for randrange() (%d,%d, %d)\" % (istart, istop, width)\n\n        # Non-unit step argument supplied.\n        istep = _int(step)\n        if istep != step:\n            raise ValueError, \"non-integer step for randrange()\"\n        if istep > 0:\n            n = (width + istep - 1) // istep\n        elif istep < 0:\n            n = (width + istep + 1) // istep\n        else:\n            raise ValueError, \"zero step for randrange()\"\n\n        if n <= 0:\n            raise ValueError, \"empty range for randrange()\"\n\n        if n >= _maxwidth:\n            return istart + istep*self._randbelow(n)\n        return istart + istep*_int(self.random() * n)\n\n    def randint(self, a, b):\n        \"\"\"Return random integer in range [a, b], including both end points.\n        \"\"\"\n\n        return self.randrange(a, b+1)\n\n    def _randbelow(self, n, _log=_log, _int=int, _maxwidth=1L<<BPF):\n        \"\"\"Return a random int in the range [0,n)\n\n        Handles the case where n has more bits than returned\n        by a single call to the underlying generator.\n        \"\"\"\n\n        try:\n            getrandbits = self.getrandbits\n        except AttributeError:\n            pass\n        else:\n            # Only call self.getrandbits if the original random() builtin method\n            # has not been overridden or if a new getrandbits() was supplied.\n            # This assures that the two methods correspond.\n            if (self.random == super(Random, self).random or\n                    getrandbits != super(Random, self).getrandbits):\n                k = _int(1.00001 + _log(n-1, 2.0))   # 2**k > n-1 > 2**(k-2)\n                r = getrandbits(k)\n                while r >= n:\n                    r = getrandbits(k)\n                return r\n        if n >= _maxwidth:\n            _warn(\"Underlying random() generator does not supply \\n\"\n                \"enough bits to choose from a population range this large\")\n        return _int(self.random() * n)\n\n## -------------------- sequence methods  -------------------\n\n    def choice(self, seq):\n        \"\"\"Choose a random element from a non-empty sequence.\"\"\"\n        return seq[int(self.random() * len(seq))]  # raises IndexError if seq is empty\n\n    def shuffle(self, x, random=None):\n        \"\"\"x, random=random.random -> shuffle list x in place; return None.\n\n        Optional arg random is a 0-argument function returning a random\n        float in [0.0, 1.0); by default, the standard random.random.\n\n        \"\"\"\n\n        if random is None:\n            random = self.random\n        _int = int\n        for i in reversed(xrange(1, len(x))):\n            # pick an element in x[:i+1] with which to exchange x[i]\n            j = _int(random() * (i+1))\n            x[i], x[j] = x[j], x[i]\n\n    def sample(self, population, k):\n        \"\"\"Chooses k unique random elements from a population sequence.\n\n        Returns a new list containing elements from the population while\n        leaving the original population unchanged.  The resulting list is\n        in selection order so that all sub-slices will also be valid random\n        samples.  This allows raffle winners (the sample) to be partitioned\n        into grand prize and second place winners (the subslices).\n\n        Members of the population need not be hashable or unique.  If the\n        population contains repeats, then each occurrence is a possible\n        selection in the sample.\n\n        To choose a sample in a range of integers, use xrange as an argument.\n        This is especially fast and space efficient for sampling from a\n        large population:   sample(xrange(10000000), 60)\n        \"\"\"\n\n        # Sampling without replacement entails tracking either potential\n        # selections (the pool) in a list or previous selections in a set.\n\n        # When the number of selections is small compared to the\n        # population, then tracking selections is efficient, requiring\n        # only a small set and an occasional reselection.  For\n        # a larger number of selections, the pool tracking method is\n        # preferred since the list takes less space than the\n        # set and it doesn't suffer from frequent reselections.\n\n        n = len(population)\n        if not 0 <= k <= n:\n            raise ValueError(\"sample larger than population\")\n        random = self.random\n        _int = int\n        result = [None] * k\n        setsize = 21        # size of a small set minus size of an empty list\n        if k > 5:\n            setsize += 4 ** _ceil(_log(k * 3, 4)) # table size for big sets\n        if n <= setsize or hasattr(population, \"keys\"):\n            # An n-length list is smaller than a k-length set, or this is a\n            # mapping type so the other algorithm wouldn't work.\n            pool = list(population)\n            for i in xrange(k):         # invariant:  non-selected at [0,n-i)\n                j = _int(random() * (n-i))\n                result[i] = pool[j]\n                pool[j] = pool[n-i-1]   # move non-selected item into vacancy\n        else:\n            try:\n                selected = set()\n                selected_add = selected.add\n                for i in xrange(k):\n                    j = _int(random() * n)\n                    while j in selected:\n                        j = _int(random() * n)\n                    selected_add(j)\n                    result[i] = population[j]\n            except (TypeError, KeyError):   # handle (at least) sets\n                if isinstance(population, list):\n                    raise\n                return self.sample(tuple(population), k)\n        return result\n\n## -------------------- real-valued distributions  -------------------\n\n## -------------------- uniform distribution -------------------\n\n    def uniform(self, a, b):\n        \"Get a random number in the range [a, b) or [a, b] depending on rounding.\"\n        return a + (b-a) * self.random()\n\n## -------------------- triangular --------------------\n\n    def triangular(self, low=0.0, high=1.0, mode=None):\n        \"\"\"Triangular distribution.\n\n        Continuous distribution bounded by given lower and upper limits,\n        and having a given mode value in-between.\n\n        http://en.wikipedia.org/wiki/Triangular_distribution\n\n        \"\"\"\n        u = self.random()\n        try:\n            c = 0.5 if mode is None else (mode - low) / (high - low)\n        except ZeroDivisionError:\n            return low\n        if u > c:\n            u = 1.0 - u\n            c = 1.0 - c\n            low, high = high, low\n        return low + (high - low) * (u * c) ** 0.5\n\n## -------------------- normal distribution --------------------\n\n    def normalvariate(self, mu, sigma):\n        \"\"\"Normal distribution.\n\n        mu is the mean, and sigma is the standard deviation.\n\n        \"\"\"\n        # mu = mean, sigma = standard deviation\n\n        # Uses Kinderman and Monahan method. Reference: Kinderman,\n        # A.J. and Monahan, J.F., \"Computer generation of random\n        # variables using the ratio of uniform deviates\", ACM Trans\n        # Math Software, 3, (1977), pp257-260.\n\n        random = self.random\n        while 1:\n            u1 = random()\n            u2 = 1.0 - random()\n            z = NV_MAGICCONST*(u1-0.5)/u2\n            zz = z*z/4.0\n            if zz <= -_log(u2):\n                break\n        return mu + z*sigma\n\n## -------------------- lognormal distribution --------------------\n\n    def lognormvariate(self, mu, sigma):\n        \"\"\"Log normal distribution.\n\n        If you take the natural logarithm of this distribution, you'll get a\n        normal distribution with mean mu and standard deviation sigma.\n        mu can have any value, and sigma must be greater than zero.\n\n        \"\"\"\n        return _exp(self.normalvariate(mu, sigma))\n\n## -------------------- exponential distribution --------------------\n\n    def expovariate(self, lambd):\n        \"\"\"Exponential distribution.\n\n        lambd is 1.0 divided by the desired mean.  It should be\n        nonzero.  (The parameter would be called \"lambda\", but that is\n        a reserved word in Python.)  Returned values range from 0 to\n        positive infinity if lambd is positive, and from negative\n        infinity to 0 if lambd is negative.\n\n        \"\"\"\n        # lambd: rate lambd = 1/mean\n        # ('lambda' is a Python reserved word)\n\n        # we use 1-random() instead of random() to preclude the\n        # possibility of taking the log of zero.\n        return -_log(1.0 - self.random())/lambd\n\n## -------------------- von Mises distribution --------------------\n\n    def vonmisesvariate(self, mu, kappa):\n        \"\"\"Circular data distribution.\n\n        mu is the mean angle, expressed in radians between 0 and 2*pi, and\n        kappa is the concentration parameter, which must be greater than or\n        equal to zero.  If kappa is equal to zero, this distribution reduces\n        to a uniform random angle over the range 0 to 2*pi.\n\n        \"\"\"\n        # mu:    mean angle (in radians between 0 and 2*pi)\n        # kappa: concentration parameter kappa (>= 0)\n        # if kappa = 0 generate uniform random angle\n\n        # Based upon an algorithm published in: Fisher, N.I.,\n        # \"Statistical Analysis of Circular Data\", Cambridge\n        # University Press, 1993.\n\n        # Thanks to Magnus Kessler for a correction to the\n        # implementation of step 4.\n\n        random = self.random\n        if kappa <= 1e-6:\n            return TWOPI * random()\n\n        s = 0.5 / kappa\n        r = s + _sqrt(1.0 + s * s)\n\n        while 1:\n            u1 = random()\n            z = _cos(_pi * u1)\n\n            d = z / (r + z)\n            u2 = random()\n            if u2 < 1.0 - d * d or u2 <= (1.0 - d) * _exp(d):\n                break\n\n        q = 1.0 / r\n        f = (q + z) / (1.0 + q * z)\n        u3 = random()\n        if u3 > 0.5:\n            theta = (mu + _acos(f)) % TWOPI\n        else:\n            theta = (mu - _acos(f)) % TWOPI\n\n        return theta\n\n## -------------------- gamma distribution --------------------\n\n    def gammavariate(self, alpha, beta):\n        \"\"\"Gamma distribution.  Not the gamma function!\n\n        Conditions on the parameters are alpha > 0 and beta > 0.\n\n        The probability distribution function is:\n\n                    x ** (alpha - 1) * math.exp(-x / beta)\n          pdf(x) =  --------------------------------------\n                      math.gamma(alpha) * beta ** alpha\n\n        \"\"\"\n\n        # alpha > 0, beta > 0, mean is alpha*beta, variance is alpha*beta**2\n\n        # Warning: a few older sources define the gamma distribution in terms\n        # of alpha > -1.0\n        if alpha <= 0.0 or beta <= 0.0:\n            raise ValueError, 'gammavariate: alpha and beta must be > 0.0'\n\n        random = self.random\n        if alpha > 1.0:\n\n            # Uses R.C.H. Cheng, \"The generation of Gamma\n            # variables with non-integral shape parameters\",\n            # Applied Statistics, (1977), 26, No. 1, p71-74\n\n            ainv = _sqrt(2.0 * alpha - 1.0)\n            bbb = alpha - LOG4\n            ccc = alpha + ainv\n\n            while 1:\n                u1 = random()\n                if not 1e-7 < u1 < .9999999:\n                    continue\n                u2 = 1.0 - random()\n                v = _log(u1/(1.0-u1))/ainv\n                x = alpha*_exp(v)\n                z = u1*u1*u2\n                r = bbb+ccc*v-x\n                if r + SG_MAGICCONST - 4.5*z >= 0.0 or r >= _log(z):\n                    return x * beta\n\n        elif alpha == 1.0:\n            # expovariate(1)\n            u = random()\n            while u <= 1e-7:\n                u = random()\n            return -_log(u) * beta\n\n        else:   # alpha is between 0 and 1 (exclusive)\n\n            # Uses ALGORITHM GS of Statistical Computing - Kennedy & Gentle\n\n            while 1:\n                u = random()\n                b = (_e + alpha)/_e\n                p = b*u\n                if p <= 1.0:\n                    x = p ** (1.0/alpha)\n                else:\n                    x = -_log((b-p)/alpha)\n                u1 = random()\n                if p > 1.0:\n                    if u1 <= x ** (alpha - 1.0):\n                        break\n                elif u1 <= _exp(-x):\n                    break\n            return x * beta\n\n## -------------------- Gauss (faster alternative) --------------------\n\n    def gauss(self, mu, sigma):\n        \"\"\"Gaussian distribution.\n\n        mu is the mean, and sigma is the standard deviation.  This is\n        slightly faster than the normalvariate() function.\n\n        Not thread-safe without a lock around calls.\n\n        \"\"\"\n\n        # When x and y are two variables from [0, 1), uniformly\n        # distributed, then\n        #\n        #    cos(2*pi*x)*sqrt(-2*log(1-y))\n        #    sin(2*pi*x)*sqrt(-2*log(1-y))\n        #\n        # are two *independent* variables with normal distribution\n        # (mu = 0, sigma = 1).\n        # (Lambert Meertens)\n        # (corrected version; bug discovered by Mike Miller, fixed by LM)\n\n        # Multithreading note: When two threads call this function\n        # simultaneously, it is possible that they will receive the\n        # same return value.  The window is very small though.  To\n        # avoid this, you have to use a lock around all calls.  (I\n        # didn't want to slow this down in the serial case by using a\n        # lock here.)\n\n        random = self.random\n        z = self.gauss_next\n        self.gauss_next = None\n        if z is None:\n            x2pi = random() * TWOPI\n            g2rad = _sqrt(-2.0 * _log(1.0 - random()))\n            z = _cos(x2pi) * g2rad\n            self.gauss_next = _sin(x2pi) * g2rad\n\n        return mu + z*sigma\n\n## -------------------- beta --------------------\n## See\n## http://mail.python.org/pipermail/python-bugs-list/2001-January/003752.html\n## for Ivan Frohne's insightful analysis of why the original implementation:\n##\n##    def betavariate(self, alpha, beta):\n##        # Discrete Event Simulation in C, pp 87-88.\n##\n##        y = self.expovariate(alpha)\n##        z = self.expovariate(1.0/beta)\n##        return z/(y+z)\n##\n## was dead wrong, and how it probably got that way.\n\n    def betavariate(self, alpha, beta):\n        \"\"\"Beta distribution.\n\n        Conditions on the parameters are alpha > 0 and beta > 0.\n        Returned values range between 0 and 1.\n\n        \"\"\"\n\n        # This version due to Janne Sinkkonen, and matches all the std\n        # texts (e.g., Knuth Vol 2 Ed 3 pg 134 \"the beta distribution\").\n        y = self.gammavariate(alpha, 1.)\n        if y == 0:\n            return 0.0\n        else:\n            return y / (y + self.gammavariate(beta, 1.))\n\n## -------------------- Pareto --------------------\n\n    def paretovariate(self, alpha):\n        \"\"\"Pareto distribution.  alpha is the shape parameter.\"\"\"\n        # Jain, pg. 495\n\n        u = 1.0 - self.random()\n        return 1.0 / pow(u, 1.0/alpha)\n\n## -------------------- Weibull --------------------\n\n    def weibullvariate(self, alpha, beta):\n        \"\"\"Weibull distribution.\n\n        alpha is the scale parameter and beta is the shape parameter.\n\n        \"\"\"\n        # Jain, pg. 499; bug fix courtesy Bill Arms\n\n        u = 1.0 - self.random()\n        return alpha * pow(-_log(u), 1.0/beta)\n\n## -------------------- Wichmann-Hill -------------------\n\nclass WichmannHill(Random):\n\n    VERSION = 1     # used by getstate/setstate\n\n    def seed(self, a=None):\n        \"\"\"Initialize internal state from hashable object.\n\n        None or no argument seeds from current time or from an operating\n        system specific randomness source if available.\n\n        If a is not None or an int or long, hash(a) is used instead.\n\n        If a is an int or long, a is used directly.  Distinct values between\n        0 and 27814431486575L inclusive are guaranteed to yield distinct\n        internal states (this guarantee is specific to the default\n        Wichmann-Hill generator).\n        \"\"\"\n\n        if a is None:\n            try:\n                a = long(_hexlify(_urandom(16)), 16)\n            except NotImplementedError:\n                import time\n                a = long(time.time() * 256) # use fractional seconds\n\n        if not isinstance(a, (int, long)):\n            a = hash(a)\n\n        a, x = divmod(a, 30268)\n        a, y = divmod(a, 30306)\n        a, z = divmod(a, 30322)\n        self._seed = int(x)+1, int(y)+1, int(z)+1\n\n        self.gauss_next = None\n\n    def random(self):\n        \"\"\"Get the next random number in the range [0.0, 1.0).\"\"\"\n\n        # Wichman-Hill random number generator.\n        #\n        # Wichmann, B. A. & Hill, I. D. (1982)\n        # Algorithm AS 183:\n        # An efficient and portable pseudo-random number generator\n        # Applied Statistics 31 (1982) 188-190\n        #\n        # see also:\n        #        Correction to Algorithm AS 183\n        #        Applied Statistics 33 (1984) 123\n        #\n        #        McLeod, A. I. (1985)\n        #        A remark on Algorithm AS 183\n        #        Applied Statistics 34 (1985),198-200\n\n        # This part is thread-unsafe:\n        # BEGIN CRITICAL SECTION\n        x, y, z = self._seed\n        x = (171 * x) % 30269\n        y = (172 * y) % 30307\n        z = (170 * z) % 30323\n        self._seed = x, y, z\n        # END CRITICAL SECTION\n\n        # Note:  on a platform using IEEE-754 double arithmetic, this can\n        # never return 0.0 (asserted by Tim; proof too long for a comment).\n        return (x/30269.0 + y/30307.0 + z/30323.0) % 1.0\n\n    def getstate(self):\n        \"\"\"Return internal state; can be passed to setstate() later.\"\"\"\n        return self.VERSION, self._seed, self.gauss_next\n\n    def setstate(self, state):\n        \"\"\"Restore internal state from object returned by getstate().\"\"\"\n        version = state[0]\n        if version == 1:\n            version, self._seed, self.gauss_next = state\n        else:\n            raise ValueError(\"state with version %s passed to \"\n                             \"Random.setstate() of version %s\" %\n                             (version, self.VERSION))\n\n    def jumpahead(self, n):\n        \"\"\"Act as if n calls to random() were made, but quickly.\n\n        n is an int, greater than or equal to 0.\n\n        Example use:  If you have 2 threads and know that each will\n        consume no more than a million random numbers, create two Random\n        objects r1 and r2, then do\n            r2.setstate(r1.getstate())\n            r2.jumpahead(1000000)\n        Then r1 and r2 will use guaranteed-disjoint segments of the full\n        period.\n        \"\"\"\n\n        if not n >= 0:\n            raise ValueError(\"n must be >= 0\")\n        x, y, z = self._seed\n        x = int(x * pow(171, n, 30269)) % 30269\n        y = int(y * pow(172, n, 30307)) % 30307\n        z = int(z * pow(170, n, 30323)) % 30323\n        self._seed = x, y, z\n\n    def __whseed(self, x=0, y=0, z=0):\n        \"\"\"Set the Wichmann-Hill seed from (x, y, z).\n\n        These must be integers in the range [0, 256).\n        \"\"\"\n\n        if not type(x) == type(y) == type(z) == int:\n            raise TypeError('seeds must be integers')\n        if not (0 <= x < 256 and 0 <= y < 256 and 0 <= z < 256):\n            raise ValueError('seeds must be in range(0, 256)')\n        if 0 == x == y == z:\n            # Initialize from current time\n            import time\n            t = long(time.time() * 256)\n            t = int((t&0xffffff) ^ (t>>24))\n            t, x = divmod(t, 256)\n            t, y = divmod(t, 256)\n            t, z = divmod(t, 256)\n        # Zero is a poor seed, so substitute 1\n        self._seed = (x or 1, y or 1, z or 1)\n\n        self.gauss_next = None\n\n    def whseed(self, a=None):\n        \"\"\"Seed from hashable object's hash code.\n\n        None or no argument seeds from current time.  It is not guaranteed\n        that objects with distinct hash codes lead to distinct internal\n        states.\n\n        This is obsolete, provided for compatibility with the seed routine\n        used prior to Python 2.1.  Use the .seed() method instead.\n        \"\"\"\n\n        if a is None:\n            self.__whseed()\n            return\n        a = hash(a)\n        a, x = divmod(a, 256)\n        a, y = divmod(a, 256)\n        a, z = divmod(a, 256)\n        x = (x + a) % 256 or 1\n        y = (y + a) % 256 or 1\n        z = (z + a) % 256 or 1\n        self.__whseed(x, y, z)\n\n## --------------- Operating System Random Source  ------------------\n\nclass SystemRandom(Random):\n    \"\"\"Alternate random number generator using sources provided\n    by the operating system (such as /dev/urandom on Unix or\n    CryptGenRandom on Windows).\n\n     Not available on all systems (see os.urandom() for details).\n    \"\"\"\n\n    def random(self):\n        \"\"\"Get the next random number in the range [0.0, 1.0).\"\"\"\n        return (long(_hexlify(_urandom(7)), 16) >> 3) * RECIP_BPF\n\n    def getrandbits(self, k):\n        \"\"\"getrandbits(k) -> x.  Generates a long int with k random bits.\"\"\"\n        if k <= 0:\n            raise ValueError('number of bits must be greater than zero')\n        if k != int(k):\n            raise TypeError('number of bits should be an integer')\n        bytes = (k + 7) // 8                    # bits / 8 and rounded up\n        x = long(_hexlify(_urandom(bytes)), 16)\n        return x >> (bytes * 8 - k)             # trim excess bits\n\n    def _stub(self, *args, **kwds):\n        \"Stub method.  Not used for a system random number generator.\"\n        return None\n    seed = jumpahead = _stub\n\n    def _notimplemented(self, *args, **kwds):\n        \"Method should not be called for a system random number generator.\"\n        raise NotImplementedError('System entropy source does not have state.')\n    getstate = setstate = _notimplemented\n\n## -------------------- test program --------------------\n\ndef _test_generator(n, func, args):\n    import time\n    print n, 'times', func.__name__\n    total = 0.0\n    sqsum = 0.0\n    smallest = 1e10\n    largest = -1e10\n    t0 = time.time()\n    for i in range(n):\n        x = func(*args)\n        total += x\n        sqsum = sqsum + x*x\n        smallest = min(x, smallest)\n        largest = max(x, largest)\n    t1 = time.time()\n    print round(t1-t0, 3), 'sec,',\n    avg = total/n\n    stddev = _sqrt(sqsum/n - avg*avg)\n    print 'avg %g, stddev %g, min %g, max %g' % \\\n              (avg, stddev, smallest, largest)\n\n\ndef _test(N=2000):\n    _test_generator(N, random, ())\n    _test_generator(N, normalvariate, (0.0, 1.0))\n    _test_generator(N, lognormvariate, (0.0, 1.0))\n    _test_generator(N, vonmisesvariate, (0.0, 1.0))\n    _test_generator(N, gammavariate, (0.01, 1.0))\n    _test_generator(N, gammavariate, (0.1, 1.0))\n    _test_generator(N, gammavariate, (0.1, 2.0))\n    _test_generator(N, gammavariate, (0.5, 1.0))\n    _test_generator(N, gammavariate, (0.9, 1.0))\n    _test_generator(N, gammavariate, (1.0, 1.0))\n    _test_generator(N, gammavariate, (2.0, 1.0))\n    _test_generator(N, gammavariate, (20.0, 1.0))\n    _test_generator(N, gammavariate, (200.0, 1.0))\n    _test_generator(N, gauss, (0.0, 1.0))\n    _test_generator(N, betavariate, (3.0, 3.0))\n    _test_generator(N, triangular, (0.0, 1.0, 1.0/3.0))\n\n# Create one instance, seeded from current time, and export its methods\n# as module-level functions.  The functions share state across all uses\n#(both in the user's code and in the Python libraries), but that's fine\n# for most programs and is easier for the casual user than making them\n# instantiate their own Random() instance.\n\n_inst = Random()\nseed = _inst.seed\nrandom = _inst.random\nuniform = _inst.uniform\ntriangular = _inst.triangular\nrandint = _inst.randint\nchoice = _inst.choice\nrandrange = _inst.randrange\nsample = _inst.sample\nshuffle = _inst.shuffle\nnormalvariate = _inst.normalvariate\nlognormvariate = _inst.lognormvariate\nexpovariate = _inst.expovariate\nvonmisesvariate = _inst.vonmisesvariate\ngammavariate = _inst.gammavariate\ngauss = _inst.gauss\nbetavariate = _inst.betavariate\nparetovariate = _inst.paretovariate\nweibullvariate = _inst.weibullvariate\ngetstate = _inst.getstate\nsetstate = _inst.setstate\njumpahead = _inst.jumpahead\ngetrandbits = _inst.getrandbits\n\nif __name__ == '__main__':\n    _test()\n", 
    "re": "#\n# Secret Labs' Regular Expression Engine\n#\n# re-compatible interface for the sre matching engine\n#\n# Copyright (c) 1998-2001 by Secret Labs AB.  All rights reserved.\n#\n# This version of the SRE library can be redistributed under CNRI's\n# Python 1.6 license.  For any other use, please contact Secret Labs\n# AB (info@pythonware.com).\n#\n# Portions of this engine have been developed in cooperation with\n# CNRI.  Hewlett-Packard provided funding for 1.6 integration and\n# other compatibility work.\n#\n\nr\"\"\"Support for regular expressions (RE).\n\nThis module provides regular expression matching operations similar to\nthose found in Perl.  It supports both 8-bit and Unicode strings; both\nthe pattern and the strings being processed can contain null bytes and\ncharacters outside the US ASCII range.\n\nRegular expressions can contain both special and ordinary characters.\nMost ordinary characters, like \"A\", \"a\", or \"0\", are the simplest\nregular expressions; they simply match themselves.  You can\nconcatenate ordinary characters, so last matches the string 'last'.\n\nThe special characters are:\n    \".\"      Matches any character except a newline.\n    \"^\"      Matches the start of the string.\n    \"$\"      Matches the end of the string or just before the newline at\n             the end of the string.\n    \"*\"      Matches 0 or more (greedy) repetitions of the preceding RE.\n             Greedy means that it will match as many repetitions as possible.\n    \"+\"      Matches 1 or more (greedy) repetitions of the preceding RE.\n    \"?\"      Matches 0 or 1 (greedy) of the preceding RE.\n    *?,+?,?? Non-greedy versions of the previous three special characters.\n    {m,n}    Matches from m to n repetitions of the preceding RE.\n    {m,n}?   Non-greedy version of the above.\n    \"\\\\\"     Either escapes special characters or signals a special sequence.\n    []       Indicates a set of characters.\n             A \"^\" as the first character indicates a complementing set.\n    \"|\"      A|B, creates an RE that will match either A or B.\n    (...)    Matches the RE inside the parentheses.\n             The contents can be retrieved or matched later in the string.\n    (?iLmsux) Set the I, L, M, S, U, or X flag for the RE (see below).\n    (?:...)  Non-grouping version of regular parentheses.\n    (?P<name>...) The substring matched by the group is accessible by name.\n    (?P=name)     Matches the text matched earlier by the group named name.\n    (?#...)  A comment; ignored.\n    (?=...)  Matches if ... matches next, but doesn't consume the string.\n    (?!...)  Matches if ... doesn't match next.\n    (?<=...) Matches if preceded by ... (must be fixed length).\n    (?<!...) Matches if not preceded by ... (must be fixed length).\n    (?(id/name)yes|no) Matches yes pattern if the group with id/name matched,\n                       the (optional) no pattern otherwise.\n\nThe special sequences consist of \"\\\\\" and a character from the list\nbelow.  If the ordinary character is not on the list, then the\nresulting RE will match the second character.\n    \\number  Matches the contents of the group of the same number.\n    \\A       Matches only at the start of the string.\n    \\Z       Matches only at the end of the string.\n    \\b       Matches the empty string, but only at the start or end of a word.\n    \\B       Matches the empty string, but not at the start or end of a word.\n    \\d       Matches any decimal digit; equivalent to the set [0-9].\n    \\D       Matches any non-digit character; equivalent to the set [^0-9].\n    \\s       Matches any whitespace character; equivalent to [ \\t\\n\\r\\f\\v].\n    \\S       Matches any non-whitespace character; equiv. to [^ \\t\\n\\r\\f\\v].\n    \\w       Matches any alphanumeric character; equivalent to [a-zA-Z0-9_].\n             With LOCALE, it will match the set [0-9_] plus characters defined\n             as letters for the current locale.\n    \\W       Matches the complement of \\w.\n    \\\\       Matches a literal backslash.\n\nThis module exports the following functions:\n    match    Match a regular expression pattern to the beginning of a string.\n    search   Search a string for the presence of a pattern.\n    sub      Substitute occurrences of a pattern found in a string.\n    subn     Same as sub, but also return the number of substitutions made.\n    split    Split a string by the occurrences of a pattern.\n    findall  Find all occurrences of a pattern in a string.\n    finditer Return an iterator yielding a match object for each match.\n    compile  Compile a pattern into a RegexObject.\n    purge    Clear the regular expression cache.\n    escape   Backslash all non-alphanumerics in a string.\n\nSome of the functions in this module takes flags as optional parameters:\n    I  IGNORECASE  Perform case-insensitive matching.\n    L  LOCALE      Make \\w, \\W, \\b, \\B, dependent on the current locale.\n    M  MULTILINE   \"^\" matches the beginning of lines (after a newline)\n                   as well as the string.\n                   \"$\" matches the end of lines (before a newline) as well\n                   as the end of the string.\n    S  DOTALL      \".\" matches any character at all, including the newline.\n    X  VERBOSE     Ignore whitespace and comments for nicer looking RE's.\n    U  UNICODE     Make \\w, \\W, \\b, \\B, dependent on the Unicode locale.\n\nThis module also defines an exception 'error'.\n\n\"\"\"\n\nimport sys\nimport sre_compile\nimport sre_parse\ntry:\n    import _locale\nexcept ImportError:\n    _locale = None\n\n# public symbols\n__all__ = [ \"match\", \"search\", \"sub\", \"subn\", \"split\", \"findall\",\n    \"compile\", \"purge\", \"template\", \"escape\", \"I\", \"L\", \"M\", \"S\", \"X\",\n    \"U\", \"IGNORECASE\", \"LOCALE\", \"MULTILINE\", \"DOTALL\", \"VERBOSE\",\n    \"UNICODE\", \"error\" ]\n\n__version__ = \"2.2.1\"\n\n# flags\nI = IGNORECASE = sre_compile.SRE_FLAG_IGNORECASE # ignore case\nL = LOCALE = sre_compile.SRE_FLAG_LOCALE # assume current 8-bit locale\nU = UNICODE = sre_compile.SRE_FLAG_UNICODE # assume unicode locale\nM = MULTILINE = sre_compile.SRE_FLAG_MULTILINE # make anchors look for newline\nS = DOTALL = sre_compile.SRE_FLAG_DOTALL # make dot match newline\nX = VERBOSE = sre_compile.SRE_FLAG_VERBOSE # ignore whitespace and comments\n\n# sre extensions (experimental, don't rely on these)\nT = TEMPLATE = sre_compile.SRE_FLAG_TEMPLATE # disable backtracking\nDEBUG = sre_compile.SRE_FLAG_DEBUG # dump pattern after compilation\n\n# sre exception\nerror = sre_compile.error\n\n# --------------------------------------------------------------------\n# public interface\n\ndef match(pattern, string, flags=0):\n    \"\"\"Try to apply the pattern at the start of the string, returning\n    a match object, or None if no match was found.\"\"\"\n    return _compile(pattern, flags).match(string)\n\ndef search(pattern, string, flags=0):\n    \"\"\"Scan through string looking for a match to the pattern, returning\n    a match object, or None if no match was found.\"\"\"\n    return _compile(pattern, flags).search(string)\n\ndef sub(pattern, repl, string, count=0, flags=0):\n    \"\"\"Return the string obtained by replacing the leftmost\n    non-overlapping occurrences of the pattern in string by the\n    replacement repl.  repl can be either a string or a callable;\n    if a string, backslash escapes in it are processed.  If it is\n    a callable, it's passed the match object and must return\n    a replacement string to be used.\"\"\"\n    return _compile(pattern, flags).sub(repl, string, count)\n\ndef subn(pattern, repl, string, count=0, flags=0):\n    \"\"\"Return a 2-tuple containing (new_string, number).\n    new_string is the string obtained by replacing the leftmost\n    non-overlapping occurrences of the pattern in the source\n    string by the replacement repl.  number is the number of\n    substitutions that were made. repl can be either a string or a\n    callable; if a string, backslash escapes in it are processed.\n    If it is a callable, it's passed the match object and must\n    return a replacement string to be used.\"\"\"\n    return _compile(pattern, flags).subn(repl, string, count)\n\ndef split(pattern, string, maxsplit=0, flags=0):\n    \"\"\"Split the source string by the occurrences of the pattern,\n    returning a list containing the resulting substrings.\"\"\"\n    return _compile(pattern, flags).split(string, maxsplit)\n\ndef findall(pattern, string, flags=0):\n    \"\"\"Return a list of all non-overlapping matches in the string.\n\n    If one or more groups are present in the pattern, return a\n    list of groups; this will be a list of tuples if the pattern\n    has more than one group.\n\n    Empty matches are included in the result.\"\"\"\n    return _compile(pattern, flags).findall(string)\n\nif sys.hexversion >= 0x02020000:\n    __all__.append(\"finditer\")\n    def finditer(pattern, string, flags=0):\n        \"\"\"Return an iterator over all non-overlapping matches in the\n        string.  For each match, the iterator returns a match object.\n\n        Empty matches are included in the result.\"\"\"\n        return _compile(pattern, flags).finditer(string)\n\ndef compile(pattern, flags=0):\n    \"Compile a regular expression pattern, returning a pattern object.\"\n    return _compile(pattern, flags)\n\ndef purge():\n    \"Clear the regular expression cache\"\n    _cache.clear()\n    _cache_repl.clear()\n\ndef template(pattern, flags=0):\n    \"Compile a template pattern, returning a pattern object\"\n    return _compile(pattern, flags|T)\n\n_alphanum = frozenset(\n    \"abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789\")\n\ndef escape(pattern):\n    \"Escape all non-alphanumeric characters in pattern.\"\n    s = list(pattern)\n    alphanum = _alphanum\n    for i, c in enumerate(pattern):\n        if c not in alphanum:\n            if c == \"\\000\":\n                s[i] = \"\\\\000\"\n            else:\n                s[i] = \"\\\\\" + c\n    return pattern[:0].join(s)\n\n# --------------------------------------------------------------------\n# internals\n\n_cache = {}\n_cache_repl = {}\n\n_pattern_type = type(sre_compile.compile(\"\", 0))\n\n_MAXCACHE = 100\n\ndef _compile(*key):\n    # internal: compile pattern\n    pattern, flags = key\n    bypass_cache = flags & DEBUG\n    if not bypass_cache:\n        cachekey = (type(key[0]),) + key\n        try:\n            p, loc = _cache[cachekey]\n            if loc is None or loc == _locale.setlocale(_locale.LC_CTYPE):\n                return p\n        except KeyError:\n            pass\n    if isinstance(pattern, _pattern_type):\n        if flags:\n            raise ValueError('Cannot process flags argument with a compiled pattern')\n        return pattern\n    if not sre_compile.isstring(pattern):\n        raise TypeError, \"first argument must be string or compiled pattern\"\n    try:\n        p = sre_compile.compile(pattern, flags)\n    except error, v:\n        raise error, v # invalid expression\n    if not bypass_cache:\n        if len(_cache) >= _MAXCACHE:\n            _cache.clear()\n        if p.flags & LOCALE:\n            if not _locale:\n                return p\n            loc = _locale.setlocale(_locale.LC_CTYPE)\n        else:\n            loc = None\n        _cache[cachekey] = p, loc\n    return p\n\ndef _compile_repl(*key):\n    # internal: compile replacement pattern\n    p = _cache_repl.get(key)\n    if p is not None:\n        return p\n    repl, pattern = key\n    try:\n        p = sre_parse.parse_template(repl, pattern)\n    except error, v:\n        raise error, v # invalid expression\n    if len(_cache_repl) >= _MAXCACHE:\n        _cache_repl.clear()\n    _cache_repl[key] = p\n    return p\n\ndef _expand(pattern, match, template):\n    # internal: match.expand implementation hook\n    template = sre_parse.parse_template(template, pattern)\n    return sre_parse.expand_template(template, match)\n\ndef _subx(pattern, template):\n    # internal: pattern.sub/subn implementation helper\n    template = _compile_repl(template, pattern)\n    if not template[0] and len(template[1]) == 1:\n        # literal replacement\n        return template[1][0]\n    def filter(match, template=template):\n        return sre_parse.expand_template(template, match)\n    return filter\n\n# register myself for pickling\n\nimport copy_reg\n\ndef _pickle(p):\n    return _compile, (p.pattern, p.flags)\n\ncopy_reg.pickle(_pattern_type, _pickle, _compile)\n\n# --------------------------------------------------------------------\n# experimental stuff (see python-dev discussions for details)\n\nclass Scanner:\n    def __init__(self, lexicon, flags=0):\n        from sre_constants import BRANCH, SUBPATTERN\n        self.lexicon = lexicon\n        # combine phrases into a compound pattern\n        p = []\n        s = sre_parse.Pattern()\n        s.flags = flags\n        for phrase, action in lexicon:\n            p.append(sre_parse.SubPattern(s, [\n                (SUBPATTERN, (len(p)+1, sre_parse.parse(phrase, flags))),\n                ]))\n        s.groups = len(p)+1\n        p = sre_parse.SubPattern(s, [(BRANCH, (None, p))])\n        self.scanner = sre_compile.compile(p)\n    def scan(self, string):\n        result = []\n        append = result.append\n        match = self.scanner.scanner(string).match\n        i = 0\n        while 1:\n            m = match()\n            if not m:\n                break\n            j = m.end()\n            if i == j:\n                break\n            action = self.lexicon[m.lastindex-1][1]\n            if hasattr(action, '__call__'):\n                self.match = m\n                action = action(self, m.group())\n            if action is not None:\n                append(action)\n            i = j\n        return result, string[i:]\n", 
    "repr": "\"\"\"Redo the builtin repr() (representation) but with limits on most sizes.\"\"\"\n\n__all__ = [\"Repr\",\"repr\"]\n\nimport __builtin__\nfrom itertools import islice\n\nclass Repr:\n\n    def __init__(self):\n        self.maxlevel = 6\n        self.maxtuple = 6\n        self.maxlist = 6\n        self.maxarray = 5\n        self.maxdict = 4\n        self.maxset = 6\n        self.maxfrozenset = 6\n        self.maxdeque = 6\n        self.maxstring = 30\n        self.maxlong = 40\n        self.maxother = 20\n\n    def repr(self, x):\n        return self.repr1(x, self.maxlevel)\n\n    def repr1(self, x, level):\n        typename = type(x).__name__\n        if ' ' in typename:\n            parts = typename.split()\n            typename = '_'.join(parts)\n        if hasattr(self, 'repr_' + typename):\n            return getattr(self, 'repr_' + typename)(x, level)\n        else:\n            s = __builtin__.repr(x)\n            if len(s) > self.maxother:\n                i = max(0, (self.maxother-3)//2)\n                j = max(0, self.maxother-3-i)\n                s = s[:i] + '...' + s[len(s)-j:]\n            return s\n\n    def _repr_iterable(self, x, level, left, right, maxiter, trail=''):\n        n = len(x)\n        if level <= 0 and n:\n            s = '...'\n        else:\n            newlevel = level - 1\n            repr1 = self.repr1\n            pieces = [repr1(elem, newlevel) for elem in islice(x, maxiter)]\n            if n > maxiter:  pieces.append('...')\n            s = ', '.join(pieces)\n            if n == 1 and trail:  right = trail + right\n        return '%s%s%s' % (left, s, right)\n\n    def repr_tuple(self, x, level):\n        return self._repr_iterable(x, level, '(', ')', self.maxtuple, ',')\n\n    def repr_list(self, x, level):\n        return self._repr_iterable(x, level, '[', ']', self.maxlist)\n\n    def repr_array(self, x, level):\n        header = \"array('%s', [\" % x.typecode\n        return self._repr_iterable(x, level, header, '])', self.maxarray)\n\n    def repr_set(self, x, level):\n        x = _possibly_sorted(x)\n        return self._repr_iterable(x, level, 'set([', '])', self.maxset)\n\n    def repr_frozenset(self, x, level):\n        x = _possibly_sorted(x)\n        return self._repr_iterable(x, level, 'frozenset([', '])',\n                                   self.maxfrozenset)\n\n    def repr_deque(self, x, level):\n        return self._repr_iterable(x, level, 'deque([', '])', self.maxdeque)\n\n    def repr_dict(self, x, level):\n        n = len(x)\n        if n == 0: return '{}'\n        if level <= 0: return '{...}'\n        newlevel = level - 1\n        repr1 = self.repr1\n        pieces = []\n        for key in islice(_possibly_sorted(x), self.maxdict):\n            keyrepr = repr1(key, newlevel)\n            valrepr = repr1(x[key], newlevel)\n            pieces.append('%s: %s' % (keyrepr, valrepr))\n        if n > self.maxdict: pieces.append('...')\n        s = ', '.join(pieces)\n        return '{%s}' % (s,)\n\n    def repr_str(self, x, level):\n        s = __builtin__.repr(x[:self.maxstring])\n        if len(s) > self.maxstring:\n            i = max(0, (self.maxstring-3)//2)\n            j = max(0, self.maxstring-3-i)\n            s = __builtin__.repr(x[:i] + x[len(x)-j:])\n            s = s[:i] + '...' + s[len(s)-j:]\n        return s\n\n    def repr_long(self, x, level):\n        s = __builtin__.repr(x) # XXX Hope this isn't too slow...\n        if len(s) > self.maxlong:\n            i = max(0, (self.maxlong-3)//2)\n            j = max(0, self.maxlong-3-i)\n            s = s[:i] + '...' + s[len(s)-j:]\n        return s\n\n    def repr_instance(self, x, level):\n        try:\n            s = __builtin__.repr(x)\n            # Bugs in x.__repr__() can cause arbitrary\n            # exceptions -- then make up something\n        except Exception:\n            return '<%s instance at %x>' % (x.__class__.__name__, id(x))\n        if len(s) > self.maxstring:\n            i = max(0, (self.maxstring-3)//2)\n            j = max(0, self.maxstring-3-i)\n            s = s[:i] + '...' + s[len(s)-j:]\n        return s\n\n\ndef _possibly_sorted(x):\n    # Since not all sequences of items can be sorted and comparison\n    # functions may raise arbitrary exceptions, return an unsorted\n    # sequence in that case.\n    try:\n        return sorted(x)\n    except Exception:\n        return list(x)\n\naRepr = Repr()\nrepr = aRepr.repr\n", 
    "shlex": "# -*- coding: utf-8 -*-\n\"\"\"A lexical analyzer class for simple shell-like syntaxes.\"\"\"\n\n# Module and documentation by Eric S. Raymond, 21 Dec 1998\n# Input stacking and error message cleanup added by ESR, March 2000\n# push_source() and pop_source() made explicit by ESR, January 2001.\n# Posix compliance, split(), string arguments, and\n# iterator interface by Gustavo Niemeyer, April 2003.\n\nimport os.path\nimport sys\nfrom collections import deque\n\ntry:\n    from cStringIO import StringIO\nexcept ImportError:\n    from StringIO import StringIO\n\n__all__ = [\"shlex\", \"split\"]\n\nclass shlex:\n    \"A lexical analyzer class for simple shell-like syntaxes.\"\n    def __init__(self, instream=None, infile=None, posix=False):\n        if isinstance(instream, basestring):\n            instream = StringIO(instream)\n        if instream is not None:\n            self.instream = instream\n            self.infile = infile\n        else:\n            self.instream = sys.stdin\n            self.infile = None\n        self.posix = posix\n        if posix:\n            self.eof = None\n        else:\n            self.eof = ''\n        self.commenters = '#'\n        self.wordchars = ('abcdfeghijklmnopqrstuvwxyz'\n                          'ABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789_')\n        if self.posix:\n            self.wordchars += ('\u00df\u00e0\u00e1\u00e2\u00e3\u00e4\u00e5\u00e6\u00e7\u00e8\u00e9\u00ea\u00eb\u00ec\u00ed\u00ee\u00ef\u00f0\u00f1\u00f2\u00f3\u00f4\u00f5\u00f6\u00f8\u00f9\u00fa\u00fb\u00fc\u00fd\u00fe\u00ff'\n                               '\u00c0\u00c1\u00c2\u00c3\u00c4\u00c5\u00c6\u00c7\u00c8\u00c9\u00ca\u00cb\u00cc\u00cd\u00ce\u00cf\u00d0\u00d1\u00d2\u00d3\u00d4\u00d5\u00d6\u00d8\u00d9\u00da\u00db\u00dc\u00dd\u00de')\n        self.whitespace = ' \\t\\r\\n'\n        self.whitespace_split = False\n        self.quotes = '\\'\"'\n        self.escape = '\\\\'\n        self.escapedquotes = '\"'\n        self.state = ' '\n        self.pushback = deque()\n        self.lineno = 1\n        self.debug = 0\n        self.token = ''\n        self.filestack = deque()\n        self.source = None\n        if self.debug:\n            print 'shlex: reading from %s, line %d' \\\n                  % (self.instream, self.lineno)\n\n    def push_token(self, tok):\n        \"Push a token onto the stack popped by the get_token method\"\n        if self.debug >= 1:\n            print \"shlex: pushing token \" + repr(tok)\n        self.pushback.appendleft(tok)\n\n    def push_source(self, newstream, newfile=None):\n        \"Push an input source onto the lexer's input source stack.\"\n        if isinstance(newstream, basestring):\n            newstream = StringIO(newstream)\n        self.filestack.appendleft((self.infile, self.instream, self.lineno))\n        self.infile = newfile\n        self.instream = newstream\n        self.lineno = 1\n        if self.debug:\n            if newfile is not None:\n                print 'shlex: pushing to file %s' % (self.infile,)\n            else:\n                print 'shlex: pushing to stream %s' % (self.instream,)\n\n    def pop_source(self):\n        \"Pop the input source stack.\"\n        self.instream.close()\n        (self.infile, self.instream, self.lineno) = self.filestack.popleft()\n        if self.debug:\n            print 'shlex: popping to %s, line %d' \\\n                  % (self.instream, self.lineno)\n        self.state = ' '\n\n    def get_token(self):\n        \"Get a token from the input stream (or from stack if it's nonempty)\"\n        if self.pushback:\n            tok = self.pushback.popleft()\n            if self.debug >= 1:\n                print \"shlex: popping token \" + repr(tok)\n            return tok\n        # No pushback.  Get a token.\n        raw = self.read_token()\n        # Handle inclusions\n        if self.source is not None:\n            while raw == self.source:\n                spec = self.sourcehook(self.read_token())\n                if spec:\n                    (newfile, newstream) = spec\n                    self.push_source(newstream, newfile)\n                raw = self.get_token()\n        # Maybe we got EOF instead?\n        while raw == self.eof:\n            if not self.filestack:\n                return self.eof\n            else:\n                self.pop_source()\n                raw = self.get_token()\n        # Neither inclusion nor EOF\n        if self.debug >= 1:\n            if raw != self.eof:\n                print \"shlex: token=\" + repr(raw)\n            else:\n                print \"shlex: token=EOF\"\n        return raw\n\n    def read_token(self):\n        quoted = False\n        escapedstate = ' '\n        while True:\n            nextchar = self.instream.read(1)\n            if nextchar == '\\n':\n                self.lineno = self.lineno + 1\n            if self.debug >= 3:\n                print \"shlex: in state\", repr(self.state), \\\n                      \"I see character:\", repr(nextchar)\n            if self.state is None:\n                self.token = ''        # past end of file\n                break\n            elif self.state == ' ':\n                if not nextchar:\n                    self.state = None  # end of file\n                    break\n                elif nextchar in self.whitespace:\n                    if self.debug >= 2:\n                        print \"shlex: I see whitespace in whitespace state\"\n                    if self.token or (self.posix and quoted):\n                        break   # emit current token\n                    else:\n                        continue\n                elif nextchar in self.commenters:\n                    self.instream.readline()\n                    self.lineno = self.lineno + 1\n                elif self.posix and nextchar in self.escape:\n                    escapedstate = 'a'\n                    self.state = nextchar\n                elif nextchar in self.wordchars:\n                    self.token = nextchar\n                    self.state = 'a'\n                elif nextchar in self.quotes:\n                    if not self.posix:\n                        self.token = nextchar\n                    self.state = nextchar\n                elif self.whitespace_split:\n                    self.token = nextchar\n                    self.state = 'a'\n                else:\n                    self.token = nextchar\n                    if self.token or (self.posix and quoted):\n                        break   # emit current token\n                    else:\n                        continue\n            elif self.state in self.quotes:\n                quoted = True\n                if not nextchar:      # end of file\n                    if self.debug >= 2:\n                        print \"shlex: I see EOF in quotes state\"\n                    # XXX what error should be raised here?\n                    raise ValueError, \"No closing quotation\"\n                if nextchar == self.state:\n                    if not self.posix:\n                        self.token = self.token + nextchar\n                        self.state = ' '\n                        break\n                    else:\n                        self.state = 'a'\n                elif self.posix and nextchar in self.escape and \\\n                     self.state in self.escapedquotes:\n                    escapedstate = self.state\n                    self.state = nextchar\n                else:\n                    self.token = self.token + nextchar\n            elif self.state in self.escape:\n                if not nextchar:      # end of file\n                    if self.debug >= 2:\n                        print \"shlex: I see EOF in escape state\"\n                    # XXX what error should be raised here?\n                    raise ValueError, \"No escaped character\"\n                # In posix shells, only the quote itself or the escape\n                # character may be escaped within quotes.\n                if escapedstate in self.quotes and \\\n                   nextchar != self.state and nextchar != escapedstate:\n                    self.token = self.token + self.state\n                self.token = self.token + nextchar\n                self.state = escapedstate\n            elif self.state == 'a':\n                if not nextchar:\n                    self.state = None   # end of file\n                    break\n                elif nextchar in self.whitespace:\n                    if self.debug >= 2:\n                        print \"shlex: I see whitespace in word state\"\n                    self.state = ' '\n                    if self.token or (self.posix and quoted):\n                        break   # emit current token\n                    else:\n                        continue\n                elif nextchar in self.commenters:\n                    self.instream.readline()\n                    self.lineno = self.lineno + 1\n                    if self.posix:\n                        self.state = ' '\n                        if self.token or (self.posix and quoted):\n                            break   # emit current token\n                        else:\n                            continue\n                elif self.posix and nextchar in self.quotes:\n                    self.state = nextchar\n                elif self.posix and nextchar in self.escape:\n                    escapedstate = 'a'\n                    self.state = nextchar\n                elif nextchar in self.wordchars or nextchar in self.quotes \\\n                    or self.whitespace_split:\n                    self.token = self.token + nextchar\n                else:\n                    self.pushback.appendleft(nextchar)\n                    if self.debug >= 2:\n                        print \"shlex: I see punctuation in word state\"\n                    self.state = ' '\n                    if self.token:\n                        break   # emit current token\n                    else:\n                        continue\n        result = self.token\n        self.token = ''\n        if self.posix and not quoted and result == '':\n            result = None\n        if self.debug > 1:\n            if result:\n                print \"shlex: raw token=\" + repr(result)\n            else:\n                print \"shlex: raw token=EOF\"\n        return result\n\n    def sourcehook(self, newfile):\n        \"Hook called on a filename to be sourced.\"\n        if newfile[0] == '\"':\n            newfile = newfile[1:-1]\n        # This implements cpp-like semantics for relative-path inclusion.\n        if isinstance(self.infile, basestring) and not os.path.isabs(newfile):\n            newfile = os.path.join(os.path.dirname(self.infile), newfile)\n        return (newfile, open(newfile, \"r\"))\n\n    def error_leader(self, infile=None, lineno=None):\n        \"Emit a C-compiler-like, Emacs-friendly error-message leader.\"\n        if infile is None:\n            infile = self.infile\n        if lineno is None:\n            lineno = self.lineno\n        return \"\\\"%s\\\", line %d: \" % (infile, lineno)\n\n    def __iter__(self):\n        return self\n\n    def next(self):\n        token = self.get_token()\n        if token == self.eof:\n            raise StopIteration\n        return token\n\ndef split(s, comments=False, posix=True):\n    lex = shlex(s, posix=posix)\n    lex.whitespace_split = True\n    if not comments:\n        lex.commenters = ''\n    return list(lex)\n\nif __name__ == '__main__':\n    if len(sys.argv) == 1:\n        lexer = shlex()\n    else:\n        file = sys.argv[1]\n        lexer = shlex(open(file), file)\n    while 1:\n        tt = lexer.get_token()\n        if tt:\n            print \"Token: \" + repr(tt)\n        else:\n            break\n", 
    "six": "\"\"\"Utilities for writing code that runs on Python 2 and 3\"\"\"\n\n# Copyright (c) 2010-2015 Benjamin Peterson\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy\n# of this software and associated documentation files (the \"Software\"), to deal\n# in the Software without restriction, including without limitation the rights\n# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n# copies of the Software, and to permit persons to whom the Software is\n# furnished to do so, subject to the following conditions:\n#\n# The above copyright notice and this permission notice shall be included in all\n# copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n# SOFTWARE.\n\nfrom __future__ import absolute_import\n\nimport functools\nimport itertools\nimport operator\nimport sys\nimport types\n\n__author__ = \"Benjamin Peterson <benjamin@python.org>\"\n__version__ = \"1.10.0\"\n\n\n# Useful for very coarse version differentiation.\nPY2 = sys.version_info[0] == 2\nPY3 = sys.version_info[0] == 3\nPY34 = sys.version_info[0:2] >= (3, 4)\n\nif PY3:\n    string_types = str,\n    integer_types = int,\n    class_types = type,\n    text_type = str\n    binary_type = bytes\n\n    MAXSIZE = sys.maxsize\nelse:\n    string_types = basestring,\n    integer_types = (int, long)\n    class_types = (type, types.ClassType)\n    text_type = unicode\n    binary_type = str\n\n    if sys.platform.startswith(\"java\"):\n        # Jython always uses 32 bits.\n        MAXSIZE = int((1 << 31) - 1)\n    else:\n        # It's possible to have sizeof(long) != sizeof(Py_ssize_t).\n        class X(object):\n\n            def __len__(self):\n                return 1 << 31\n        try:\n            len(X())\n        except OverflowError:\n            # 32-bit\n            MAXSIZE = int((1 << 31) - 1)\n        else:\n            # 64-bit\n            MAXSIZE = int((1 << 63) - 1)\n        del X\n\n\ndef _add_doc(func, doc):\n    \"\"\"Add documentation to a function.\"\"\"\n    func.__doc__ = doc\n\n\ndef _import_module(name):\n    \"\"\"Import module, returning the module after the last dot.\"\"\"\n    __import__(name)\n    return sys.modules[name]\n\n\nclass _LazyDescr(object):\n\n    def __init__(self, name):\n        self.name = name\n\n    def __get__(self, obj, tp):\n        result = self._resolve()\n        setattr(obj, self.name, result)  # Invokes __set__.\n        try:\n            # This is a bit ugly, but it avoids running this again by\n            # removing this descriptor.\n            delattr(obj.__class__, self.name)\n        except AttributeError:\n            pass\n        return result\n\n\nclass MovedModule(_LazyDescr):\n\n    def __init__(self, name, old, new=None):\n        super(MovedModule, self).__init__(name)\n        if PY3:\n            if new is None:\n                new = name\n            self.mod = new\n        else:\n            self.mod = old\n\n    def _resolve(self):\n        return _import_module(self.mod)\n\n    def __getattr__(self, attr):\n        _module = self._resolve()\n        value = getattr(_module, attr)\n        setattr(self, attr, value)\n        return value\n\n\nclass _LazyModule(types.ModuleType):\n\n    def __init__(self, name):\n        super(_LazyModule, self).__init__(name)\n        self.__doc__ = self.__class__.__doc__\n\n    def __dir__(self):\n        attrs = [\"__doc__\", \"__name__\"]\n        attrs += [attr.name for attr in self._moved_attributes]\n        return attrs\n\n    # Subclasses should override this\n    _moved_attributes = []\n\n\nclass MovedAttribute(_LazyDescr):\n\n    def __init__(self, name, old_mod, new_mod, old_attr=None, new_attr=None):\n        super(MovedAttribute, self).__init__(name)\n        if PY3:\n            if new_mod is None:\n                new_mod = name\n            self.mod = new_mod\n            if new_attr is None:\n                if old_attr is None:\n                    new_attr = name\n                else:\n                    new_attr = old_attr\n            self.attr = new_attr\n        else:\n            self.mod = old_mod\n            if old_attr is None:\n                old_attr = name\n            self.attr = old_attr\n\n    def _resolve(self):\n        module = _import_module(self.mod)\n        return getattr(module, self.attr)\n\n\nclass _SixMetaPathImporter(object):\n\n    \"\"\"\n    A meta path importer to import six.moves and its submodules.\n\n    This class implements a PEP302 finder and loader. It should be compatible\n    with Python 2.5 and all existing versions of Python3\n    \"\"\"\n\n    def __init__(self, six_module_name):\n        self.name = six_module_name\n        self.known_modules = {}\n\n    def _add_module(self, mod, *fullnames):\n        for fullname in fullnames:\n            self.known_modules[self.name + \".\" + fullname] = mod\n\n    def _get_module(self, fullname):\n        return self.known_modules[self.name + \".\" + fullname]\n\n    def find_module(self, fullname, path=None):\n        if fullname in self.known_modules:\n            return self\n        return None\n\n    def __get_module(self, fullname):\n        try:\n            return self.known_modules[fullname]\n        except KeyError:\n            raise ImportError(\"This loader does not know module \" + fullname)\n\n    def load_module(self, fullname):\n        try:\n            # in case of a reload\n            return sys.modules[fullname]\n        except KeyError:\n            pass\n        mod = self.__get_module(fullname)\n        if isinstance(mod, MovedModule):\n            mod = mod._resolve()\n        else:\n            mod.__loader__ = self\n        sys.modules[fullname] = mod\n        return mod\n\n    def is_package(self, fullname):\n        \"\"\"\n        Return true, if the named module is a package.\n\n        We need this method to get correct spec objects with\n        Python 3.4 (see PEP451)\n        \"\"\"\n        return hasattr(self.__get_module(fullname), \"__path__\")\n\n    def get_code(self, fullname):\n        \"\"\"Return None\n\n        Required, if is_package is implemented\"\"\"\n        self.__get_module(fullname)  # eventually raises ImportError\n        return None\n    get_source = get_code  # same as get_code\n\n_importer = _SixMetaPathImporter(__name__)\n\n\nclass _MovedItems(_LazyModule):\n\n    \"\"\"Lazy loading of moved objects\"\"\"\n    __path__ = []  # mark as package\n\n\n_moved_attributes = [\n    MovedAttribute(\"cStringIO\", \"cStringIO\", \"io\", \"StringIO\"),\n    MovedAttribute(\"filter\", \"itertools\", \"builtins\", \"ifilter\", \"filter\"),\n    MovedAttribute(\"filterfalse\", \"itertools\", \"itertools\", \"ifilterfalse\", \"filterfalse\"),\n    MovedAttribute(\"input\", \"__builtin__\", \"builtins\", \"raw_input\", \"input\"),\n    MovedAttribute(\"intern\", \"__builtin__\", \"sys\"),\n    MovedAttribute(\"map\", \"itertools\", \"builtins\", \"imap\", \"map\"),\n    MovedAttribute(\"getcwd\", \"os\", \"os\", \"getcwdu\", \"getcwd\"),\n    MovedAttribute(\"getcwdb\", \"os\", \"os\", \"getcwd\", \"getcwdb\"),\n    MovedAttribute(\"range\", \"__builtin__\", \"builtins\", \"xrange\", \"range\"),\n    MovedAttribute(\"reload_module\", \"__builtin__\", \"importlib\" if PY34 else \"imp\", \"reload\"),\n    MovedAttribute(\"reduce\", \"__builtin__\", \"functools\"),\n    MovedAttribute(\"shlex_quote\", \"pipes\", \"shlex\", \"quote\"),\n    MovedAttribute(\"StringIO\", \"StringIO\", \"io\"),\n    MovedAttribute(\"UserDict\", \"UserDict\", \"collections\"),\n    MovedAttribute(\"UserList\", \"UserList\", \"collections\"),\n    MovedAttribute(\"UserString\", \"UserString\", \"collections\"),\n    MovedAttribute(\"xrange\", \"__builtin__\", \"builtins\", \"xrange\", \"range\"),\n    MovedAttribute(\"zip\", \"itertools\", \"builtins\", \"izip\", \"zip\"),\n    MovedAttribute(\"zip_longest\", \"itertools\", \"itertools\", \"izip_longest\", \"zip_longest\"),\n    MovedModule(\"builtins\", \"__builtin__\"),\n    MovedModule(\"configparser\", \"ConfigParser\"),\n    MovedModule(\"copyreg\", \"copy_reg\"),\n    MovedModule(\"dbm_gnu\", \"gdbm\", \"dbm.gnu\"),\n    MovedModule(\"_dummy_thread\", \"dummy_thread\", \"_dummy_thread\"),\n    MovedModule(\"http_cookiejar\", \"cookielib\", \"http.cookiejar\"),\n    MovedModule(\"http_cookies\", \"Cookie\", \"http.cookies\"),\n    MovedModule(\"html_entities\", \"htmlentitydefs\", \"html.entities\"),\n    MovedModule(\"html_parser\", \"HTMLParser\", \"html.parser\"),\n    MovedModule(\"http_client\", \"httplib\", \"http.client\"),\n    MovedModule(\"email_mime_multipart\", \"email.MIMEMultipart\", \"email.mime.multipart\"),\n    MovedModule(\"email_mime_nonmultipart\", \"email.MIMENonMultipart\", \"email.mime.nonmultipart\"),\n    MovedModule(\"email_mime_text\", \"email.MIMEText\", \"email.mime.text\"),\n    MovedModule(\"email_mime_base\", \"email.MIMEBase\", \"email.mime.base\"),\n    MovedModule(\"BaseHTTPServer\", \"BaseHTTPServer\", \"http.server\"),\n    MovedModule(\"CGIHTTPServer\", \"CGIHTTPServer\", \"http.server\"),\n    MovedModule(\"SimpleHTTPServer\", \"SimpleHTTPServer\", \"http.server\"),\n    MovedModule(\"cPickle\", \"cPickle\", \"pickle\"),\n    MovedModule(\"queue\", \"Queue\"),\n    MovedModule(\"reprlib\", \"repr\"),\n    MovedModule(\"socketserver\", \"SocketServer\"),\n    MovedModule(\"_thread\", \"thread\", \"_thread\"),\n    MovedModule(\"tkinter\", \"Tkinter\"),\n    MovedModule(\"tkinter_dialog\", \"Dialog\", \"tkinter.dialog\"),\n    MovedModule(\"tkinter_filedialog\", \"FileDialog\", \"tkinter.filedialog\"),\n    MovedModule(\"tkinter_scrolledtext\", \"ScrolledText\", \"tkinter.scrolledtext\"),\n    MovedModule(\"tkinter_simpledialog\", \"SimpleDialog\", \"tkinter.simpledialog\"),\n    MovedModule(\"tkinter_tix\", \"Tix\", \"tkinter.tix\"),\n    MovedModule(\"tkinter_ttk\", \"ttk\", \"tkinter.ttk\"),\n    MovedModule(\"tkinter_constants\", \"Tkconstants\", \"tkinter.constants\"),\n    MovedModule(\"tkinter_dnd\", \"Tkdnd\", \"tkinter.dnd\"),\n    MovedModule(\"tkinter_colorchooser\", \"tkColorChooser\",\n                \"tkinter.colorchooser\"),\n    MovedModule(\"tkinter_commondialog\", \"tkCommonDialog\",\n                \"tkinter.commondialog\"),\n    MovedModule(\"tkinter_tkfiledialog\", \"tkFileDialog\", \"tkinter.filedialog\"),\n    MovedModule(\"tkinter_font\", \"tkFont\", \"tkinter.font\"),\n    MovedModule(\"tkinter_messagebox\", \"tkMessageBox\", \"tkinter.messagebox\"),\n    MovedModule(\"tkinter_tksimpledialog\", \"tkSimpleDialog\",\n                \"tkinter.simpledialog\"),\n    MovedModule(\"urllib_parse\", __name__ + \".moves.urllib_parse\", \"urllib.parse\"),\n    MovedModule(\"urllib_error\", __name__ + \".moves.urllib_error\", \"urllib.error\"),\n    MovedModule(\"urllib\", __name__ + \".moves.urllib\", __name__ + \".moves.urllib\"),\n    MovedModule(\"urllib_robotparser\", \"robotparser\", \"urllib.robotparser\"),\n    MovedModule(\"xmlrpc_client\", \"xmlrpclib\", \"xmlrpc.client\"),\n    MovedModule(\"xmlrpc_server\", \"SimpleXMLRPCServer\", \"xmlrpc.server\"),\n]\n# Add windows specific modules.\nif sys.platform == \"win32\":\n    _moved_attributes += [\n        MovedModule(\"winreg\", \"_winreg\"),\n    ]\n\nfor attr in _moved_attributes:\n    setattr(_MovedItems, attr.name, attr)\n    if isinstance(attr, MovedModule):\n        _importer._add_module(attr, \"moves.\" + attr.name)\ndel attr\n\n_MovedItems._moved_attributes = _moved_attributes\n\nmoves = _MovedItems(__name__ + \".moves\")\n_importer._add_module(moves, \"moves\")\n\n\nclass Module_six_moves_urllib_parse(_LazyModule):\n\n    \"\"\"Lazy loading of moved objects in six.moves.urllib_parse\"\"\"\n\n\n_urllib_parse_moved_attributes = [\n    MovedAttribute(\"ParseResult\", \"urlparse\", \"urllib.parse\"),\n    MovedAttribute(\"SplitResult\", \"urlparse\", \"urllib.parse\"),\n    MovedAttribute(\"parse_qs\", \"urlparse\", \"urllib.parse\"),\n    MovedAttribute(\"parse_qsl\", \"urlparse\", \"urllib.parse\"),\n    MovedAttribute(\"urldefrag\", \"urlparse\", \"urllib.parse\"),\n    MovedAttribute(\"urljoin\", \"urlparse\", \"urllib.parse\"),\n    MovedAttribute(\"urlparse\", \"urlparse\", \"urllib.parse\"),\n    MovedAttribute(\"urlsplit\", \"urlparse\", \"urllib.parse\"),\n    MovedAttribute(\"urlunparse\", \"urlparse\", \"urllib.parse\"),\n    MovedAttribute(\"urlunsplit\", \"urlparse\", \"urllib.parse\"),\n    MovedAttribute(\"quote\", \"urllib\", \"urllib.parse\"),\n    MovedAttribute(\"quote_plus\", \"urllib\", \"urllib.parse\"),\n    MovedAttribute(\"unquote\", \"urllib\", \"urllib.parse\"),\n    MovedAttribute(\"unquote_plus\", \"urllib\", \"urllib.parse\"),\n    MovedAttribute(\"urlencode\", \"urllib\", \"urllib.parse\"),\n    MovedAttribute(\"splitquery\", \"urllib\", \"urllib.parse\"),\n    MovedAttribute(\"splittag\", \"urllib\", \"urllib.parse\"),\n    MovedAttribute(\"splituser\", \"urllib\", \"urllib.parse\"),\n    MovedAttribute(\"uses_fragment\", \"urlparse\", \"urllib.parse\"),\n    MovedAttribute(\"uses_netloc\", \"urlparse\", \"urllib.parse\"),\n    MovedAttribute(\"uses_params\", \"urlparse\", \"urllib.parse\"),\n    MovedAttribute(\"uses_query\", \"urlparse\", \"urllib.parse\"),\n    MovedAttribute(\"uses_relative\", \"urlparse\", \"urllib.parse\"),\n]\nfor attr in _urllib_parse_moved_attributes:\n    setattr(Module_six_moves_urllib_parse, attr.name, attr)\ndel attr\n\nModule_six_moves_urllib_parse._moved_attributes = _urllib_parse_moved_attributes\n\n_importer._add_module(Module_six_moves_urllib_parse(__name__ + \".moves.urllib_parse\"),\n                      \"moves.urllib_parse\", \"moves.urllib.parse\")\n\n\nclass Module_six_moves_urllib_error(_LazyModule):\n\n    \"\"\"Lazy loading of moved objects in six.moves.urllib_error\"\"\"\n\n\n_urllib_error_moved_attributes = [\n    MovedAttribute(\"URLError\", \"urllib2\", \"urllib.error\"),\n    MovedAttribute(\"HTTPError\", \"urllib2\", \"urllib.error\"),\n    MovedAttribute(\"ContentTooShortError\", \"urllib\", \"urllib.error\"),\n]\nfor attr in _urllib_error_moved_attributes:\n    setattr(Module_six_moves_urllib_error, attr.name, attr)\ndel attr\n\nModule_six_moves_urllib_error._moved_attributes = _urllib_error_moved_attributes\n\n_importer._add_module(Module_six_moves_urllib_error(__name__ + \".moves.urllib.error\"),\n                      \"moves.urllib_error\", \"moves.urllib.error\")\n\n\nclass Module_six_moves_urllib_request(_LazyModule):\n\n    \"\"\"Lazy loading of moved objects in six.moves.urllib_request\"\"\"\n\n\n_urllib_request_moved_attributes = [\n    MovedAttribute(\"urlopen\", \"urllib2\", \"urllib.request\"),\n    MovedAttribute(\"install_opener\", \"urllib2\", \"urllib.request\"),\n    MovedAttribute(\"build_opener\", \"urllib2\", \"urllib.request\"),\n    MovedAttribute(\"pathname2url\", \"urllib\", \"urllib.request\"),\n    MovedAttribute(\"url2pathname\", \"urllib\", \"urllib.request\"),\n    MovedAttribute(\"getproxies\", \"urllib\", \"urllib.request\"),\n    MovedAttribute(\"Request\", \"urllib2\", \"urllib.request\"),\n    MovedAttribute(\"OpenerDirector\", \"urllib2\", \"urllib.request\"),\n    MovedAttribute(\"HTTPDefaultErrorHandler\", \"urllib2\", \"urllib.request\"),\n    MovedAttribute(\"HTTPRedirectHandler\", \"urllib2\", \"urllib.request\"),\n    MovedAttribute(\"HTTPCookieProcessor\", \"urllib2\", \"urllib.request\"),\n    MovedAttribute(\"ProxyHandler\", \"urllib2\", \"urllib.request\"),\n    MovedAttribute(\"BaseHandler\", \"urllib2\", \"urllib.request\"),\n    MovedAttribute(\"HTTPPasswordMgr\", \"urllib2\", \"urllib.request\"),\n    MovedAttribute(\"HTTPPasswordMgrWithDefaultRealm\", \"urllib2\", \"urllib.request\"),\n    MovedAttribute(\"AbstractBasicAuthHandler\", \"urllib2\", \"urllib.request\"),\n    MovedAttribute(\"HTTPBasicAuthHandler\", \"urllib2\", \"urllib.request\"),\n    MovedAttribute(\"ProxyBasicAuthHandler\", \"urllib2\", \"urllib.request\"),\n    MovedAttribute(\"AbstractDigestAuthHandler\", \"urllib2\", \"urllib.request\"),\n    MovedAttribute(\"HTTPDigestAuthHandler\", \"urllib2\", \"urllib.request\"),\n    MovedAttribute(\"ProxyDigestAuthHandler\", \"urllib2\", \"urllib.request\"),\n    MovedAttribute(\"HTTPHandler\", \"urllib2\", \"urllib.request\"),\n    MovedAttribute(\"HTTPSHandler\", \"urllib2\", \"urllib.request\"),\n    MovedAttribute(\"FileHandler\", \"urllib2\", \"urllib.request\"),\n    MovedAttribute(\"FTPHandler\", \"urllib2\", \"urllib.request\"),\n    MovedAttribute(\"CacheFTPHandler\", \"urllib2\", \"urllib.request\"),\n    MovedAttribute(\"UnknownHandler\", \"urllib2\", \"urllib.request\"),\n    MovedAttribute(\"HTTPErrorProcessor\", \"urllib2\", \"urllib.request\"),\n    MovedAttribute(\"urlretrieve\", \"urllib\", \"urllib.request\"),\n    MovedAttribute(\"urlcleanup\", \"urllib\", \"urllib.request\"),\n    MovedAttribute(\"URLopener\", \"urllib\", \"urllib.request\"),\n    MovedAttribute(\"FancyURLopener\", \"urllib\", \"urllib.request\"),\n    MovedAttribute(\"proxy_bypass\", \"urllib\", \"urllib.request\"),\n]\nfor attr in _urllib_request_moved_attributes:\n    setattr(Module_six_moves_urllib_request, attr.name, attr)\ndel attr\n\nModule_six_moves_urllib_request._moved_attributes = _urllib_request_moved_attributes\n\n_importer._add_module(Module_six_moves_urllib_request(__name__ + \".moves.urllib.request\"),\n                      \"moves.urllib_request\", \"moves.urllib.request\")\n\n\nclass Module_six_moves_urllib_response(_LazyModule):\n\n    \"\"\"Lazy loading of moved objects in six.moves.urllib_response\"\"\"\n\n\n_urllib_response_moved_attributes = [\n    MovedAttribute(\"addbase\", \"urllib\", \"urllib.response\"),\n    MovedAttribute(\"addclosehook\", \"urllib\", \"urllib.response\"),\n    MovedAttribute(\"addinfo\", \"urllib\", \"urllib.response\"),\n    MovedAttribute(\"addinfourl\", \"urllib\", \"urllib.response\"),\n]\nfor attr in _urllib_response_moved_attributes:\n    setattr(Module_six_moves_urllib_response, attr.name, attr)\ndel attr\n\nModule_six_moves_urllib_response._moved_attributes = _urllib_response_moved_attributes\n\n_importer._add_module(Module_six_moves_urllib_response(__name__ + \".moves.urllib.response\"),\n                      \"moves.urllib_response\", \"moves.urllib.response\")\n\n\nclass Module_six_moves_urllib_robotparser(_LazyModule):\n\n    \"\"\"Lazy loading of moved objects in six.moves.urllib_robotparser\"\"\"\n\n\n_urllib_robotparser_moved_attributes = [\n    MovedAttribute(\"RobotFileParser\", \"robotparser\", \"urllib.robotparser\"),\n]\nfor attr in _urllib_robotparser_moved_attributes:\n    setattr(Module_six_moves_urllib_robotparser, attr.name, attr)\ndel attr\n\nModule_six_moves_urllib_robotparser._moved_attributes = _urllib_robotparser_moved_attributes\n\n_importer._add_module(Module_six_moves_urllib_robotparser(__name__ + \".moves.urllib.robotparser\"),\n                      \"moves.urllib_robotparser\", \"moves.urllib.robotparser\")\n\n\nclass Module_six_moves_urllib(types.ModuleType):\n\n    \"\"\"Create a six.moves.urllib namespace that resembles the Python 3 namespace\"\"\"\n    __path__ = []  # mark as package\n    parse = _importer._get_module(\"moves.urllib_parse\")\n    error = _importer._get_module(\"moves.urllib_error\")\n    request = _importer._get_module(\"moves.urllib_request\")\n    response = _importer._get_module(\"moves.urllib_response\")\n    robotparser = _importer._get_module(\"moves.urllib_robotparser\")\n\n    def __dir__(self):\n        return ['parse', 'error', 'request', 'response', 'robotparser']\n\n_importer._add_module(Module_six_moves_urllib(__name__ + \".moves.urllib\"),\n                      \"moves.urllib\")\n\n\ndef add_move(move):\n    \"\"\"Add an item to six.moves.\"\"\"\n    setattr(_MovedItems, move.name, move)\n\n\ndef remove_move(name):\n    \"\"\"Remove item from six.moves.\"\"\"\n    try:\n        delattr(_MovedItems, name)\n    except AttributeError:\n        try:\n            del moves.__dict__[name]\n        except KeyError:\n            raise AttributeError(\"no such move, %r\" % (name,))\n\n\nif PY3:\n    _meth_func = \"__func__\"\n    _meth_self = \"__self__\"\n\n    _func_closure = \"__closure__\"\n    _func_code = \"__code__\"\n    _func_defaults = \"__defaults__\"\n    _func_globals = \"__globals__\"\nelse:\n    _meth_func = \"im_func\"\n    _meth_self = \"im_self\"\n\n    _func_closure = \"func_closure\"\n    _func_code = \"func_code\"\n    _func_defaults = \"func_defaults\"\n    _func_globals = \"func_globals\"\n\n\ntry:\n    advance_iterator = next\nexcept NameError:\n    def advance_iterator(it):\n        return it.next()\nnext = advance_iterator\n\n\ntry:\n    callable = callable\nexcept NameError:\n    def callable(obj):\n        return any(\"__call__\" in klass.__dict__ for klass in type(obj).__mro__)\n\n\nif PY3:\n    def get_unbound_function(unbound):\n        return unbound\n\n    create_bound_method = types.MethodType\n\n    def create_unbound_method(func, cls):\n        return func\n\n    Iterator = object\nelse:\n    def get_unbound_function(unbound):\n        return unbound.im_func\n\n    def create_bound_method(func, obj):\n        return types.MethodType(func, obj, obj.__class__)\n\n    def create_unbound_method(func, cls):\n        return types.MethodType(func, None, cls)\n\n    class Iterator(object):\n\n        def next(self):\n            return type(self).__next__(self)\n\n    callable = callable\n_add_doc(get_unbound_function,\n         \"\"\"Get the function out of a possibly unbound function\"\"\")\n\n\nget_method_function = operator.attrgetter(_meth_func)\nget_method_self = operator.attrgetter(_meth_self)\nget_function_closure = operator.attrgetter(_func_closure)\nget_function_code = operator.attrgetter(_func_code)\nget_function_defaults = operator.attrgetter(_func_defaults)\nget_function_globals = operator.attrgetter(_func_globals)\n\n\nif PY3:\n    def iterkeys(d, **kw):\n        return iter(d.keys(**kw))\n\n    def itervalues(d, **kw):\n        return iter(d.values(**kw))\n\n    def iteritems(d, **kw):\n        return iter(d.items(**kw))\n\n    def iterlists(d, **kw):\n        return iter(d.lists(**kw))\n\n    viewkeys = operator.methodcaller(\"keys\")\n\n    viewvalues = operator.methodcaller(\"values\")\n\n    viewitems = operator.methodcaller(\"items\")\nelse:\n    def iterkeys(d, **kw):\n        return d.iterkeys(**kw)\n\n    def itervalues(d, **kw):\n        return d.itervalues(**kw)\n\n    def iteritems(d, **kw):\n        return d.iteritems(**kw)\n\n    def iterlists(d, **kw):\n        return d.iterlists(**kw)\n\n    viewkeys = operator.methodcaller(\"viewkeys\")\n\n    viewvalues = operator.methodcaller(\"viewvalues\")\n\n    viewitems = operator.methodcaller(\"viewitems\")\n\n_add_doc(iterkeys, \"Return an iterator over the keys of a dictionary.\")\n_add_doc(itervalues, \"Return an iterator over the values of a dictionary.\")\n_add_doc(iteritems,\n         \"Return an iterator over the (key, value) pairs of a dictionary.\")\n_add_doc(iterlists,\n         \"Return an iterator over the (key, [values]) pairs of a dictionary.\")\n\n\nif PY3:\n    def b(s):\n        return s.encode(\"latin-1\")\n\n    def u(s):\n        return s\n    unichr = chr\n    import struct\n    int2byte = struct.Struct(\">B\").pack\n    del struct\n    byte2int = operator.itemgetter(0)\n    indexbytes = operator.getitem\n    iterbytes = iter\n    import io\n    StringIO = io.StringIO\n    BytesIO = io.BytesIO\n    _assertCountEqual = \"assertCountEqual\"\n    if sys.version_info[1] <= 1:\n        _assertRaisesRegex = \"assertRaisesRegexp\"\n        _assertRegex = \"assertRegexpMatches\"\n    else:\n        _assertRaisesRegex = \"assertRaisesRegex\"\n        _assertRegex = \"assertRegex\"\nelse:\n    def b(s):\n        return s\n    # Workaround for standalone backslash\n\n    def u(s):\n        return unicode(s.replace(r'\\\\', r'\\\\\\\\'), \"unicode_escape\")\n    unichr = unichr\n    int2byte = chr\n\n    def byte2int(bs):\n        return ord(bs[0])\n\n    def indexbytes(buf, i):\n        return ord(buf[i])\n    iterbytes = functools.partial(itertools.imap, ord)\n    import StringIO\n    StringIO = BytesIO = StringIO.StringIO\n    _assertCountEqual = \"assertItemsEqual\"\n    _assertRaisesRegex = \"assertRaisesRegexp\"\n    _assertRegex = \"assertRegexpMatches\"\n_add_doc(b, \"\"\"Byte literal\"\"\")\n_add_doc(u, \"\"\"Text literal\"\"\")\n\n\ndef assertCountEqual(self, *args, **kwargs):\n    return getattr(self, _assertCountEqual)(*args, **kwargs)\n\n\ndef assertRaisesRegex(self, *args, **kwargs):\n    return getattr(self, _assertRaisesRegex)(*args, **kwargs)\n\n\ndef assertRegex(self, *args, **kwargs):\n    return getattr(self, _assertRegex)(*args, **kwargs)\n\n\nif PY3:\n    exec_ = getattr(moves.builtins, \"exec\")\n\n    def reraise(tp, value, tb=None):\n        if value is None:\n            value = tp()\n        if value.__traceback__ is not tb:\n            raise value.with_traceback(tb)\n        raise value\n\nelse:\n    def exec_(_code_, _globs_=None, _locs_=None):\n        \"\"\"Execute code in a namespace.\"\"\"\n        if _globs_ is None:\n            frame = sys._getframe(1)\n            _globs_ = frame.f_globals\n            if _locs_ is None:\n                _locs_ = frame.f_locals\n            del frame\n        elif _locs_ is None:\n            _locs_ = _globs_\n        exec(\"\"\"exec _code_ in _globs_, _locs_\"\"\")\n\n    exec_(\"\"\"def reraise(tp, value, tb=None):\n    raise tp, value, tb\n\"\"\")\n\n\nif sys.version_info[:2] == (3, 2):\n    exec_(\"\"\"def raise_from(value, from_value):\n    if from_value is None:\n        raise value\n    raise value from from_value\n\"\"\")\nelif sys.version_info[:2] > (3, 2):\n    exec_(\"\"\"def raise_from(value, from_value):\n    raise value from from_value\n\"\"\")\nelse:\n    def raise_from(value, from_value):\n        raise value\n\n\nprint_ = getattr(moves.builtins, \"print\", None)\nif print_ is None:\n    def print_(*args, **kwargs):\n        \"\"\"The new-style print function for Python 2.4 and 2.5.\"\"\"\n        fp = kwargs.pop(\"file\", sys.stdout)\n        if fp is None:\n            return\n\n        def write(data):\n            if not isinstance(data, basestring):\n                data = str(data)\n            # If the file has an encoding, encode unicode with it.\n            if (isinstance(fp, file) and\n                    isinstance(data, unicode) and\n                    fp.encoding is not None):\n                errors = getattr(fp, \"errors\", None)\n                if errors is None:\n                    errors = \"strict\"\n                data = data.encode(fp.encoding, errors)\n            fp.write(data)\n        want_unicode = False\n        sep = kwargs.pop(\"sep\", None)\n        if sep is not None:\n            if isinstance(sep, unicode):\n                want_unicode = True\n            elif not isinstance(sep, str):\n                raise TypeError(\"sep must be None or a string\")\n        end = kwargs.pop(\"end\", None)\n        if end is not None:\n            if isinstance(end, unicode):\n                want_unicode = True\n            elif not isinstance(end, str):\n                raise TypeError(\"end must be None or a string\")\n        if kwargs:\n            raise TypeError(\"invalid keyword arguments to print()\")\n        if not want_unicode:\n            for arg in args:\n                if isinstance(arg, unicode):\n                    want_unicode = True\n                    break\n        if want_unicode:\n            newline = unicode(\"\\n\")\n            space = unicode(\" \")\n        else:\n            newline = \"\\n\"\n            space = \" \"\n        if sep is None:\n            sep = space\n        if end is None:\n            end = newline\n        for i, arg in enumerate(args):\n            if i:\n                write(sep)\n            write(arg)\n        write(end)\nif sys.version_info[:2] < (3, 3):\n    _print = print_\n\n    def print_(*args, **kwargs):\n        fp = kwargs.get(\"file\", sys.stdout)\n        flush = kwargs.pop(\"flush\", False)\n        _print(*args, **kwargs)\n        if flush and fp is not None:\n            fp.flush()\n\n_add_doc(reraise, \"\"\"Reraise an exception.\"\"\")\n\nif sys.version_info[0:2] < (3, 4):\n    def wraps(wrapped, assigned=functools.WRAPPER_ASSIGNMENTS,\n              updated=functools.WRAPPER_UPDATES):\n        def wrapper(f):\n            f = functools.wraps(wrapped, assigned, updated)(f)\n            f.__wrapped__ = wrapped\n            return f\n        return wrapper\nelse:\n    wraps = functools.wraps\n\n\ndef with_metaclass(meta, *bases):\n    \"\"\"Create a base class with a metaclass.\"\"\"\n    # This requires a bit of explanation: the basic idea is to make a dummy\n    # metaclass for one level of class instantiation that replaces itself with\n    # the actual metaclass.\n    class metaclass(meta):\n\n        def __new__(cls, name, this_bases, d):\n            return meta(name, bases, d)\n    return type.__new__(metaclass, 'temporary_class', (), {})\n\n\ndef add_metaclass(metaclass):\n    \"\"\"Class decorator for creating a class with a metaclass.\"\"\"\n    def wrapper(cls):\n        orig_vars = cls.__dict__.copy()\n        slots = orig_vars.get('__slots__')\n        if slots is not None:\n            if isinstance(slots, str):\n                slots = [slots]\n            for slots_var in slots:\n                orig_vars.pop(slots_var)\n        orig_vars.pop('__dict__', None)\n        orig_vars.pop('__weakref__', None)\n        return metaclass(cls.__name__, cls.__bases__, orig_vars)\n    return wrapper\n\n\ndef python_2_unicode_compatible(klass):\n    \"\"\"\n    A decorator that defines __unicode__ and __str__ methods under Python 2.\n    Under Python 3 it does nothing.\n\n    To support Python 2 and 3 with a single code base, define a __str__ method\n    returning text and apply this decorator to the class.\n    \"\"\"\n    if PY2:\n        if '__str__' not in klass.__dict__:\n            raise ValueError(\"@python_2_unicode_compatible cannot be applied \"\n                             \"to %s because it doesn't define __str__().\" %\n                             klass.__name__)\n        klass.__unicode__ = klass.__str__\n        klass.__str__ = lambda self: self.__unicode__().encode('utf-8')\n    return klass\n\n\n# Complete the moves implementation.\n# This code is at the end of this module to speed up module loading.\n# Turn this module into a package.\n__path__ = []  # required for PEP 302 and PEP 451\n__package__ = __name__  # see PEP 366 @ReservedAssignment\nif globals().get(\"__spec__\") is not None:\n    __spec__.submodule_search_locations = []  # PEP 451 @UndefinedVariable\n# Remove other six meta path importers, since they cause problems. This can\n# happen if six is removed from sys.modules and then reloaded. (Setuptools does\n# this for some reason.)\nif sys.meta_path:\n    for i, importer in enumerate(sys.meta_path):\n        # Here's some real nastiness: Another \"instance\" of the six module might\n        # be floating around. Therefore, we can't use isinstance() to check for\n        # the six meta path importer, since the other six instance will have\n        # inserted an importer with different class.\n        if (type(importer).__name__ == \"_SixMetaPathImporter\" and\n                importer.name == __name__):\n            del sys.meta_path[i]\n            break\n    del i, importer\n# Finally, add the importer to the meta path import hook.\nsys.meta_path.append(_importer)\n", 
    "sre_compile": "# -*- coding: utf-8 -*-\n#\n# Secret Labs' Regular Expression Engine\n#\n# convert template to internal format\n#\n# Copyright (c) 1997-2001 by Secret Labs AB.  All rights reserved.\n#\n# See the sre.py file for information on usage and redistribution.\n#\n\n\"\"\"Internal support module for sre\"\"\"\n\nimport _sre, sys\nimport sre_parse\nfrom sre_constants import *\n\nassert _sre.MAGIC == MAGIC, \"SRE module mismatch\"\n\nif _sre.CODESIZE == 2:\n    MAXCODE = 65535\nelse:\n    MAXCODE = 0xFFFFFFFFL\n\n_LITERAL_CODES = set([LITERAL, NOT_LITERAL])\n_REPEATING_CODES = set([REPEAT, MIN_REPEAT, MAX_REPEAT])\n_SUCCESS_CODES = set([SUCCESS, FAILURE])\n_ASSERT_CODES = set([ASSERT, ASSERT_NOT])\n\n# Sets of lowercase characters which have the same uppercase.\n_equivalences = (\n    # LATIN SMALL LETTER I, LATIN SMALL LETTER DOTLESS I\n    (0x69, 0x131), # i\u0131\n    # LATIN SMALL LETTER S, LATIN SMALL LETTER LONG S\n    (0x73, 0x17f), # s\u017f\n    # MICRO SIGN, GREEK SMALL LETTER MU\n    (0xb5, 0x3bc), # \u00b5\u03bc\n    # COMBINING GREEK YPOGEGRAMMENI, GREEK SMALL LETTER IOTA, GREEK PROSGEGRAMMENI\n    (0x345, 0x3b9, 0x1fbe), # \\u0345\u03b9\u1fbe\n    # GREEK SMALL LETTER BETA, GREEK BETA SYMBOL\n    (0x3b2, 0x3d0), # \u03b2\u03d0\n    # GREEK SMALL LETTER EPSILON, GREEK LUNATE EPSILON SYMBOL\n    (0x3b5, 0x3f5), # \u03b5\u03f5\n    # GREEK SMALL LETTER THETA, GREEK THETA SYMBOL\n    (0x3b8, 0x3d1), # \u03b8\u03d1\n    # GREEK SMALL LETTER KAPPA, GREEK KAPPA SYMBOL\n    (0x3ba, 0x3f0), # \u03ba\u03f0\n    # GREEK SMALL LETTER PI, GREEK PI SYMBOL\n    (0x3c0, 0x3d6), # \u03c0\u03d6\n    # GREEK SMALL LETTER RHO, GREEK RHO SYMBOL\n    (0x3c1, 0x3f1), # \u03c1\u03f1\n    # GREEK SMALL LETTER FINAL SIGMA, GREEK SMALL LETTER SIGMA\n    (0x3c2, 0x3c3), # \u03c2\u03c3\n    # GREEK SMALL LETTER PHI, GREEK PHI SYMBOL\n    (0x3c6, 0x3d5), # \u03c6\u03d5\n    # LATIN SMALL LETTER S WITH DOT ABOVE, LATIN SMALL LETTER LONG S WITH DOT ABOVE\n    (0x1e61, 0x1e9b), # \u1e61\u1e9b\n)\n\n# Maps the lowercase code to lowercase codes which have the same uppercase.\n_ignorecase_fixes = {i: tuple(j for j in t if i != j)\n                     for t in _equivalences for i in t}\n\ndef _compile(code, pattern, flags):\n    # internal: compile a (sub)pattern\n    emit = code.append\n    _len = len\n    LITERAL_CODES = _LITERAL_CODES\n    REPEATING_CODES = _REPEATING_CODES\n    SUCCESS_CODES = _SUCCESS_CODES\n    ASSERT_CODES = _ASSERT_CODES\n    if (flags & SRE_FLAG_IGNORECASE and\n            not (flags & SRE_FLAG_LOCALE) and\n            flags & SRE_FLAG_UNICODE):\n        fixes = _ignorecase_fixes\n    else:\n        fixes = None\n    for op, av in pattern:\n        if op in LITERAL_CODES:\n            if flags & SRE_FLAG_IGNORECASE:\n                lo = _sre.getlower(av, flags)\n                if fixes and lo in fixes:\n                    emit(OPCODES[IN_IGNORE])\n                    skip = _len(code); emit(0)\n                    if op is NOT_LITERAL:\n                        emit(OPCODES[NEGATE])\n                    for k in (lo,) + fixes[lo]:\n                        emit(OPCODES[LITERAL])\n                        emit(k)\n                    emit(OPCODES[FAILURE])\n                    code[skip] = _len(code) - skip\n                else:\n                    emit(OPCODES[OP_IGNORE[op]])\n                    emit(lo)\n            else:\n                emit(OPCODES[op])\n                emit(av)\n        elif op is IN:\n            if flags & SRE_FLAG_IGNORECASE:\n                emit(OPCODES[OP_IGNORE[op]])\n                def fixup(literal, flags=flags):\n                    return _sre.getlower(literal, flags)\n            else:\n                emit(OPCODES[op])\n                fixup = None\n            skip = _len(code); emit(0)\n            _compile_charset(av, flags, code, fixup, fixes)\n            code[skip] = _len(code) - skip\n        elif op is ANY:\n            if flags & SRE_FLAG_DOTALL:\n                emit(OPCODES[ANY_ALL])\n            else:\n                emit(OPCODES[ANY])\n        elif op in REPEATING_CODES:\n            if flags & SRE_FLAG_TEMPLATE:\n                raise error, \"internal: unsupported template operator\"\n                emit(OPCODES[REPEAT])\n                skip = _len(code); emit(0)\n                emit(av[0])\n                emit(av[1])\n                _compile(code, av[2], flags)\n                emit(OPCODES[SUCCESS])\n                code[skip] = _len(code) - skip\n            elif _simple(av) and op is not REPEAT:\n                if op is MAX_REPEAT:\n                    emit(OPCODES[REPEAT_ONE])\n                else:\n                    emit(OPCODES[MIN_REPEAT_ONE])\n                skip = _len(code); emit(0)\n                emit(av[0])\n                emit(av[1])\n                _compile(code, av[2], flags)\n                emit(OPCODES[SUCCESS])\n                code[skip] = _len(code) - skip\n            else:\n                emit(OPCODES[REPEAT])\n                skip = _len(code); emit(0)\n                emit(av[0])\n                emit(av[1])\n                _compile(code, av[2], flags)\n                code[skip] = _len(code) - skip\n                if op is MAX_REPEAT:\n                    emit(OPCODES[MAX_UNTIL])\n                else:\n                    emit(OPCODES[MIN_UNTIL])\n        elif op is SUBPATTERN:\n            if av[0]:\n                emit(OPCODES[MARK])\n                emit((av[0]-1)*2)\n            # _compile_info(code, av[1], flags)\n            _compile(code, av[1], flags)\n            if av[0]:\n                emit(OPCODES[MARK])\n                emit((av[0]-1)*2+1)\n        elif op in SUCCESS_CODES:\n            emit(OPCODES[op])\n        elif op in ASSERT_CODES:\n            emit(OPCODES[op])\n            skip = _len(code); emit(0)\n            if av[0] >= 0:\n                emit(0) # look ahead\n            else:\n                lo, hi = av[1].getwidth()\n                if lo != hi:\n                    raise error, \"look-behind requires fixed-width pattern\"\n                emit(lo) # look behind\n            _compile(code, av[1], flags)\n            emit(OPCODES[SUCCESS])\n            code[skip] = _len(code) - skip\n        elif op is CALL:\n            emit(OPCODES[op])\n            skip = _len(code); emit(0)\n            _compile(code, av, flags)\n            emit(OPCODES[SUCCESS])\n            code[skip] = _len(code) - skip\n        elif op is AT:\n            emit(OPCODES[op])\n            if flags & SRE_FLAG_MULTILINE:\n                av = AT_MULTILINE.get(av, av)\n            if flags & SRE_FLAG_LOCALE:\n                av = AT_LOCALE.get(av, av)\n            elif flags & SRE_FLAG_UNICODE:\n                av = AT_UNICODE.get(av, av)\n            emit(ATCODES[av])\n        elif op is BRANCH:\n            emit(OPCODES[op])\n            tail = []\n            tailappend = tail.append\n            for av in av[1]:\n                skip = _len(code); emit(0)\n                # _compile_info(code, av, flags)\n                _compile(code, av, flags)\n                emit(OPCODES[JUMP])\n                tailappend(_len(code)); emit(0)\n                code[skip] = _len(code) - skip\n            emit(0) # end of branch\n            for tail in tail:\n                code[tail] = _len(code) - tail\n        elif op is CATEGORY:\n            emit(OPCODES[op])\n            if flags & SRE_FLAG_LOCALE:\n                av = CH_LOCALE[av]\n            elif flags & SRE_FLAG_UNICODE:\n                av = CH_UNICODE[av]\n            emit(CHCODES[av])\n        elif op is GROUPREF:\n            if flags & SRE_FLAG_IGNORECASE:\n                emit(OPCODES[OP_IGNORE[op]])\n            else:\n                emit(OPCODES[op])\n            emit(av-1)\n        elif op is GROUPREF_EXISTS:\n            emit(OPCODES[op])\n            emit(av[0]-1)\n            skipyes = _len(code); emit(0)\n            _compile(code, av[1], flags)\n            if av[2]:\n                emit(OPCODES[JUMP])\n                skipno = _len(code); emit(0)\n                code[skipyes] = _len(code) - skipyes + 1\n                _compile(code, av[2], flags)\n                code[skipno] = _len(code) - skipno\n            else:\n                code[skipyes] = _len(code) - skipyes + 1\n        else:\n            raise ValueError, (\"unsupported operand type\", op)\n\ndef _compile_charset(charset, flags, code, fixup=None, fixes=None):\n    # compile charset subprogram\n    emit = code.append\n    for op, av in _optimize_charset(charset, fixup, fixes,\n                                    flags & SRE_FLAG_UNICODE):\n        emit(OPCODES[op])\n        if op is NEGATE:\n            pass\n        elif op is LITERAL:\n            emit(av)\n        elif op is RANGE:\n            emit(av[0])\n            emit(av[1])\n        elif op is CHARSET:\n            code.extend(av)\n        elif op is BIGCHARSET:\n            code.extend(av)\n        elif op is CATEGORY:\n            if flags & SRE_FLAG_LOCALE:\n                emit(CHCODES[CH_LOCALE[av]])\n            elif flags & SRE_FLAG_UNICODE:\n                emit(CHCODES[CH_UNICODE[av]])\n            else:\n                emit(CHCODES[av])\n        else:\n            raise error, \"internal: unsupported set operator\"\n    emit(OPCODES[FAILURE])\n\ndef _optimize_charset(charset, fixup, fixes, isunicode):\n    # internal: optimize character set\n    out = []\n    tail = []\n    charmap = bytearray(256)\n    for op, av in charset:\n        while True:\n            try:\n                if op is LITERAL:\n                    if fixup:\n                        i = fixup(av)\n                        charmap[i] = 1\n                        if fixes and i in fixes:\n                            for k in fixes[i]:\n                                charmap[k] = 1\n                    else:\n                        charmap[av] = 1\n                elif op is RANGE:\n                    r = range(av[0], av[1]+1)\n                    if fixup:\n                        r = map(fixup, r)\n                    if fixup and fixes:\n                        for i in r:\n                            charmap[i] = 1\n                            if i in fixes:\n                                for k in fixes[i]:\n                                    charmap[k] = 1\n                    else:\n                        for i in r:\n                            charmap[i] = 1\n                elif op is NEGATE:\n                    out.append((op, av))\n                else:\n                    tail.append((op, av))\n            except IndexError:\n                if len(charmap) == 256:\n                    # character set contains non-UCS1 character codes\n                    charmap += b'\\0' * 0xff00\n                    continue\n                # character set contains non-BMP character codes\n                if fixup and isunicode and op is RANGE:\n                    lo, hi = av\n                    ranges = [av]\n                    # There are only two ranges of cased astral characters:\n                    # 10400-1044F (Deseret) and 118A0-118DF (Warang Citi).\n                    _fixup_range(max(0x10000, lo), min(0x11fff, hi),\n                                 ranges, fixup)\n                    for lo, hi in ranges:\n                        if lo == hi:\n                            tail.append((LITERAL, hi))\n                        else:\n                            tail.append((RANGE, (lo, hi)))\n                else:\n                    tail.append((op, av))\n            break\n\n    # compress character map\n    runs = []\n    q = 0\n    while True:\n        p = charmap.find(b'\\1', q)\n        if p < 0:\n            break\n        if len(runs) >= 2:\n            runs = None\n            break\n        q = charmap.find(b'\\0', p)\n        if q < 0:\n            runs.append((p, len(charmap)))\n            break\n        runs.append((p, q))\n    if runs is not None:\n        # use literal/range\n        for p, q in runs:\n            if q - p == 1:\n                out.append((LITERAL, p))\n            else:\n                out.append((RANGE, (p, q - 1)))\n        out += tail\n        # if the case was changed or new representation is more compact\n        if fixup or len(out) < len(charset):\n            return out\n        # else original character set is good enough\n        return charset\n\n    # use bitmap\n    if len(charmap) == 256:\n        data = _mk_bitmap(charmap)\n        out.append((CHARSET, data))\n        out += tail\n        return out\n\n    # To represent a big charset, first a bitmap of all characters in the\n    # set is constructed. Then, this bitmap is sliced into chunks of 256\n    # characters, duplicate chunks are eliminated, and each chunk is\n    # given a number. In the compiled expression, the charset is\n    # represented by a 32-bit word sequence, consisting of one word for\n    # the number of different chunks, a sequence of 256 bytes (64 words)\n    # of chunk numbers indexed by their original chunk position, and a\n    # sequence of 256-bit chunks (8 words each).\n\n    # Compression is normally good: in a typical charset, large ranges of\n    # Unicode will be either completely excluded (e.g. if only cyrillic\n    # letters are to be matched), or completely included (e.g. if large\n    # subranges of Kanji match). These ranges will be represented by\n    # chunks of all one-bits or all zero-bits.\n\n    # Matching can be also done efficiently: the more significant byte of\n    # the Unicode character is an index into the chunk number, and the\n    # less significant byte is a bit index in the chunk (just like the\n    # CHARSET matching).\n\n    # In UCS-4 mode, the BIGCHARSET opcode still supports only subsets\n    # of the basic multilingual plane; an efficient representation\n    # for all of Unicode has not yet been developed.\n\n    charmap = bytes(charmap) # should be hashable\n    comps = {}\n    mapping = bytearray(256)\n    block = 0\n    data = bytearray()\n    for i in range(0, 65536, 256):\n        chunk = charmap[i: i + 256]\n        if chunk in comps:\n            mapping[i // 256] = comps[chunk]\n        else:\n            mapping[i // 256] = comps[chunk] = block\n            block += 1\n            data += chunk\n    data = _mk_bitmap(data)\n    data[0:0] = [block] + _bytes_to_codes(mapping)\n    out.append((BIGCHARSET, data))\n    out += tail\n    return out\n\ndef _fixup_range(lo, hi, ranges, fixup):\n    for i in map(fixup, range(lo, hi+1)):\n        for k, (lo, hi) in enumerate(ranges):\n            if i < lo:\n                if l == lo - 1:\n                    ranges[k] = (i, hi)\n                else:\n                    ranges.insert(k, (i, i))\n                break\n            elif i > hi:\n                if i == hi + 1:\n                    ranges[k] = (lo, i)\n                    break\n            else:\n                break\n        else:\n            ranges.append((i, i))\n\n_CODEBITS = _sre.CODESIZE * 8\n_BITS_TRANS = b'0' + b'1' * 255\ndef _mk_bitmap(bits, _CODEBITS=_CODEBITS, _int=int):\n    s = bytes(bits).translate(_BITS_TRANS)[::-1]\n    return [_int(s[i - _CODEBITS: i], 2)\n            for i in range(len(s), 0, -_CODEBITS)]\n\ndef _bytes_to_codes(b):\n    # Convert block indices to word array\n    import array\n    if _sre.CODESIZE == 2:\n        code = 'H'\n    else:\n        code = 'I'\n    a = array.array(code, bytes(b))\n    assert a.itemsize == _sre.CODESIZE\n    assert len(a) * a.itemsize == len(b)\n    return a.tolist()\n\ndef _simple(av):\n    # check if av is a \"simple\" operator\n    lo, hi = av[2].getwidth()\n    return lo == hi == 1 and av[2][0][0] != SUBPATTERN\n\ndef _compile_info(code, pattern, flags):\n    # internal: compile an info block.  in the current version,\n    # this contains min/max pattern width, and an optional literal\n    # prefix or a character map\n    lo, hi = pattern.getwidth()\n    if lo == 0:\n        return # not worth it\n    # look for a literal prefix\n    prefix = []\n    prefixappend = prefix.append\n    prefix_skip = 0\n    charset = [] # not used\n    charsetappend = charset.append\n    if not (flags & SRE_FLAG_IGNORECASE):\n        # look for literal prefix\n        for op, av in pattern.data:\n            if op is LITERAL:\n                if len(prefix) == prefix_skip:\n                    prefix_skip = prefix_skip + 1\n                prefixappend(av)\n            elif op is SUBPATTERN and len(av[1]) == 1:\n                op, av = av[1][0]\n                if op is LITERAL:\n                    prefixappend(av)\n                else:\n                    break\n            else:\n                break\n        # if no prefix, look for charset prefix\n        if not prefix and pattern.data:\n            op, av = pattern.data[0]\n            if op is SUBPATTERN and av[1]:\n                op, av = av[1][0]\n                if op is LITERAL:\n                    charsetappend((op, av))\n                elif op is BRANCH:\n                    c = []\n                    cappend = c.append\n                    for p in av[1]:\n                        if not p:\n                            break\n                        op, av = p[0]\n                        if op is LITERAL:\n                            cappend((op, av))\n                        else:\n                            break\n                    else:\n                        charset = c\n            elif op is BRANCH:\n                c = []\n                cappend = c.append\n                for p in av[1]:\n                    if not p:\n                        break\n                    op, av = p[0]\n                    if op is LITERAL:\n                        cappend((op, av))\n                    else:\n                        break\n                else:\n                    charset = c\n            elif op is IN:\n                charset = av\n##     if prefix:\n##         print \"*** PREFIX\", prefix, prefix_skip\n##     if charset:\n##         print \"*** CHARSET\", charset\n    # add an info block\n    emit = code.append\n    emit(OPCODES[INFO])\n    skip = len(code); emit(0)\n    # literal flag\n    mask = 0\n    if prefix:\n        mask = SRE_INFO_PREFIX\n        if len(prefix) == prefix_skip == len(pattern.data):\n            mask = mask + SRE_INFO_LITERAL\n    elif charset:\n        mask = mask + SRE_INFO_CHARSET\n    emit(mask)\n    # pattern length\n    if lo < MAXCODE:\n        emit(lo)\n    else:\n        emit(MAXCODE)\n        prefix = prefix[:MAXCODE]\n    if hi < MAXCODE:\n        emit(hi)\n    else:\n        emit(0)\n    # add literal prefix\n    if prefix:\n        emit(len(prefix)) # length\n        emit(prefix_skip) # skip\n        code.extend(prefix)\n        # generate overlap table\n        table = [-1] + ([0]*len(prefix))\n        for i in xrange(len(prefix)):\n            table[i+1] = table[i]+1\n            while table[i+1] > 0 and prefix[i] != prefix[table[i+1]-1]:\n                table[i+1] = table[table[i+1]-1]+1\n        code.extend(table[1:]) # don't store first entry\n    elif charset:\n        _compile_charset(charset, flags, code)\n    code[skip] = len(code) - skip\n\ntry:\n    unicode\nexcept NameError:\n    STRING_TYPES = (type(\"\"),)\nelse:\n    STRING_TYPES = (type(\"\"), type(unicode(\"\")))\n\ndef isstring(obj):\n    for tp in STRING_TYPES:\n        if isinstance(obj, tp):\n            return 1\n    return 0\n\ndef _code(p, flags):\n\n    flags = p.pattern.flags | flags\n    code = []\n\n    # compile info block\n    _compile_info(code, p, flags)\n\n    # compile the pattern\n    _compile(code, p.data, flags)\n\n    code.append(OPCODES[SUCCESS])\n\n    return code\n\ndef compile(p, flags=0):\n    # internal: convert pattern list to internal format\n\n    if isstring(p):\n        pattern = p\n        p = sre_parse.parse(p, flags)\n    else:\n        pattern = None\n\n    code = _code(p, flags)\n\n    # print code\n\n    # XXX: <fl> get rid of this limitation!\n    if p.pattern.groups > 100:\n        raise AssertionError(\n            \"sorry, but this version only supports 100 named groups\"\n            )\n\n    # map in either direction\n    groupindex = p.pattern.groupdict\n    indexgroup = [None] * p.pattern.groups\n    for k, i in groupindex.items():\n        indexgroup[i] = k\n\n    return _sre.compile(\n        pattern, flags | p.pattern.flags, code,\n        p.pattern.groups-1,\n        groupindex, indexgroup\n        )\n", 
    "sre_constants": "#\n# Secret Labs' Regular Expression Engine\n#\n# various symbols used by the regular expression engine.\n# run this script to update the _sre include files!\n#\n# Copyright (c) 1998-2001 by Secret Labs AB.  All rights reserved.\n#\n# See the sre.py file for information on usage and redistribution.\n#\n\n\"\"\"Internal support module for sre\"\"\"\n\n# update when constants are added or removed\n\nMAGIC = 20031017\n\ntry:\n    from _sre import MAXREPEAT\nexcept ImportError:\n    import _sre\n    MAXREPEAT = _sre.MAXREPEAT = 65535\n\n# SRE standard exception (access as sre.error)\n# should this really be here?\n\nclass error(Exception):\n    pass\n\n# operators\n\nFAILURE = \"failure\"\nSUCCESS = \"success\"\n\nANY = \"any\"\nANY_ALL = \"any_all\"\nASSERT = \"assert\"\nASSERT_NOT = \"assert_not\"\nAT = \"at\"\nBIGCHARSET = \"bigcharset\"\nBRANCH = \"branch\"\nCALL = \"call\"\nCATEGORY = \"category\"\nCHARSET = \"charset\"\nGROUPREF = \"groupref\"\nGROUPREF_IGNORE = \"groupref_ignore\"\nGROUPREF_EXISTS = \"groupref_exists\"\nIN = \"in\"\nIN_IGNORE = \"in_ignore\"\nINFO = \"info\"\nJUMP = \"jump\"\nLITERAL = \"literal\"\nLITERAL_IGNORE = \"literal_ignore\"\nMARK = \"mark\"\nMAX_REPEAT = \"max_repeat\"\nMAX_UNTIL = \"max_until\"\nMIN_REPEAT = \"min_repeat\"\nMIN_UNTIL = \"min_until\"\nNEGATE = \"negate\"\nNOT_LITERAL = \"not_literal\"\nNOT_LITERAL_IGNORE = \"not_literal_ignore\"\nRANGE = \"range\"\nREPEAT = \"repeat\"\nREPEAT_ONE = \"repeat_one\"\nSUBPATTERN = \"subpattern\"\nMIN_REPEAT_ONE = \"min_repeat_one\"\n\n# positions\nAT_BEGINNING = \"at_beginning\"\nAT_BEGINNING_LINE = \"at_beginning_line\"\nAT_BEGINNING_STRING = \"at_beginning_string\"\nAT_BOUNDARY = \"at_boundary\"\nAT_NON_BOUNDARY = \"at_non_boundary\"\nAT_END = \"at_end\"\nAT_END_LINE = \"at_end_line\"\nAT_END_STRING = \"at_end_string\"\nAT_LOC_BOUNDARY = \"at_loc_boundary\"\nAT_LOC_NON_BOUNDARY = \"at_loc_non_boundary\"\nAT_UNI_BOUNDARY = \"at_uni_boundary\"\nAT_UNI_NON_BOUNDARY = \"at_uni_non_boundary\"\n\n# categories\nCATEGORY_DIGIT = \"category_digit\"\nCATEGORY_NOT_DIGIT = \"category_not_digit\"\nCATEGORY_SPACE = \"category_space\"\nCATEGORY_NOT_SPACE = \"category_not_space\"\nCATEGORY_WORD = \"category_word\"\nCATEGORY_NOT_WORD = \"category_not_word\"\nCATEGORY_LINEBREAK = \"category_linebreak\"\nCATEGORY_NOT_LINEBREAK = \"category_not_linebreak\"\nCATEGORY_LOC_WORD = \"category_loc_word\"\nCATEGORY_LOC_NOT_WORD = \"category_loc_not_word\"\nCATEGORY_UNI_DIGIT = \"category_uni_digit\"\nCATEGORY_UNI_NOT_DIGIT = \"category_uni_not_digit\"\nCATEGORY_UNI_SPACE = \"category_uni_space\"\nCATEGORY_UNI_NOT_SPACE = \"category_uni_not_space\"\nCATEGORY_UNI_WORD = \"category_uni_word\"\nCATEGORY_UNI_NOT_WORD = \"category_uni_not_word\"\nCATEGORY_UNI_LINEBREAK = \"category_uni_linebreak\"\nCATEGORY_UNI_NOT_LINEBREAK = \"category_uni_not_linebreak\"\n\nOPCODES = [\n\n    # failure=0 success=1 (just because it looks better that way :-)\n    FAILURE, SUCCESS,\n\n    ANY, ANY_ALL,\n    ASSERT, ASSERT_NOT,\n    AT,\n    BRANCH,\n    CALL,\n    CATEGORY,\n    CHARSET, BIGCHARSET,\n    GROUPREF, GROUPREF_EXISTS, GROUPREF_IGNORE,\n    IN, IN_IGNORE,\n    INFO,\n    JUMP,\n    LITERAL, LITERAL_IGNORE,\n    MARK,\n    MAX_UNTIL,\n    MIN_UNTIL,\n    NOT_LITERAL, NOT_LITERAL_IGNORE,\n    NEGATE,\n    RANGE,\n    REPEAT,\n    REPEAT_ONE,\n    SUBPATTERN,\n    MIN_REPEAT_ONE\n\n]\n\nATCODES = [\n    AT_BEGINNING, AT_BEGINNING_LINE, AT_BEGINNING_STRING, AT_BOUNDARY,\n    AT_NON_BOUNDARY, AT_END, AT_END_LINE, AT_END_STRING,\n    AT_LOC_BOUNDARY, AT_LOC_NON_BOUNDARY, AT_UNI_BOUNDARY,\n    AT_UNI_NON_BOUNDARY\n]\n\nCHCODES = [\n    CATEGORY_DIGIT, CATEGORY_NOT_DIGIT, CATEGORY_SPACE,\n    CATEGORY_NOT_SPACE, CATEGORY_WORD, CATEGORY_NOT_WORD,\n    CATEGORY_LINEBREAK, CATEGORY_NOT_LINEBREAK, CATEGORY_LOC_WORD,\n    CATEGORY_LOC_NOT_WORD, CATEGORY_UNI_DIGIT, CATEGORY_UNI_NOT_DIGIT,\n    CATEGORY_UNI_SPACE, CATEGORY_UNI_NOT_SPACE, CATEGORY_UNI_WORD,\n    CATEGORY_UNI_NOT_WORD, CATEGORY_UNI_LINEBREAK,\n    CATEGORY_UNI_NOT_LINEBREAK\n]\n\ndef makedict(list):\n    d = {}\n    i = 0\n    for item in list:\n        d[item] = i\n        i = i + 1\n    return d\n\nOPCODES = makedict(OPCODES)\nATCODES = makedict(ATCODES)\nCHCODES = makedict(CHCODES)\n\n# replacement operations for \"ignore case\" mode\nOP_IGNORE = {\n    GROUPREF: GROUPREF_IGNORE,\n    IN: IN_IGNORE,\n    LITERAL: LITERAL_IGNORE,\n    NOT_LITERAL: NOT_LITERAL_IGNORE\n}\n\nAT_MULTILINE = {\n    AT_BEGINNING: AT_BEGINNING_LINE,\n    AT_END: AT_END_LINE\n}\n\nAT_LOCALE = {\n    AT_BOUNDARY: AT_LOC_BOUNDARY,\n    AT_NON_BOUNDARY: AT_LOC_NON_BOUNDARY\n}\n\nAT_UNICODE = {\n    AT_BOUNDARY: AT_UNI_BOUNDARY,\n    AT_NON_BOUNDARY: AT_UNI_NON_BOUNDARY\n}\n\nCH_LOCALE = {\n    CATEGORY_DIGIT: CATEGORY_DIGIT,\n    CATEGORY_NOT_DIGIT: CATEGORY_NOT_DIGIT,\n    CATEGORY_SPACE: CATEGORY_SPACE,\n    CATEGORY_NOT_SPACE: CATEGORY_NOT_SPACE,\n    CATEGORY_WORD: CATEGORY_LOC_WORD,\n    CATEGORY_NOT_WORD: CATEGORY_LOC_NOT_WORD,\n    CATEGORY_LINEBREAK: CATEGORY_LINEBREAK,\n    CATEGORY_NOT_LINEBREAK: CATEGORY_NOT_LINEBREAK\n}\n\nCH_UNICODE = {\n    CATEGORY_DIGIT: CATEGORY_UNI_DIGIT,\n    CATEGORY_NOT_DIGIT: CATEGORY_UNI_NOT_DIGIT,\n    CATEGORY_SPACE: CATEGORY_UNI_SPACE,\n    CATEGORY_NOT_SPACE: CATEGORY_UNI_NOT_SPACE,\n    CATEGORY_WORD: CATEGORY_UNI_WORD,\n    CATEGORY_NOT_WORD: CATEGORY_UNI_NOT_WORD,\n    CATEGORY_LINEBREAK: CATEGORY_UNI_LINEBREAK,\n    CATEGORY_NOT_LINEBREAK: CATEGORY_UNI_NOT_LINEBREAK\n}\n\n# flags\nSRE_FLAG_TEMPLATE = 1 # template mode (disable backtracking)\nSRE_FLAG_IGNORECASE = 2 # case insensitive\nSRE_FLAG_LOCALE = 4 # honour system locale\nSRE_FLAG_MULTILINE = 8 # treat target as multiline string\nSRE_FLAG_DOTALL = 16 # treat target as a single string\nSRE_FLAG_UNICODE = 32 # use unicode locale\nSRE_FLAG_VERBOSE = 64 # ignore whitespace and comments\nSRE_FLAG_DEBUG = 128 # debugging\n\n# flags for INFO primitive\nSRE_INFO_PREFIX = 1 # has prefix\nSRE_INFO_LITERAL = 2 # entire pattern is literal (given by prefix)\nSRE_INFO_CHARSET = 4 # pattern starts with character from given set\n\nif __name__ == \"__main__\":\n    def dump(f, d, prefix):\n        items = d.items()\n        items.sort(key=lambda a: a[1])\n        for k, v in items:\n            f.write(\"#define %s_%s %s\\n\" % (prefix, k.upper(), v))\n    f = open(\"sre_constants.h\", \"w\")\n    f.write(\"\"\"\\\n/*\n * Secret Labs' Regular Expression Engine\n *\n * regular expression matching engine\n *\n * NOTE: This file is generated by sre_constants.py.  If you need\n * to change anything in here, edit sre_constants.py and run it.\n *\n * Copyright (c) 1997-2001 by Secret Labs AB.  All rights reserved.\n *\n * See the _sre.c file for information on usage and redistribution.\n */\n\n\"\"\")\n\n    f.write(\"#define SRE_MAGIC %d\\n\" % MAGIC)\n\n    dump(f, OPCODES, \"SRE_OP\")\n    dump(f, ATCODES, \"SRE\")\n    dump(f, CHCODES, \"SRE\")\n\n    f.write(\"#define SRE_FLAG_TEMPLATE %d\\n\" % SRE_FLAG_TEMPLATE)\n    f.write(\"#define SRE_FLAG_IGNORECASE %d\\n\" % SRE_FLAG_IGNORECASE)\n    f.write(\"#define SRE_FLAG_LOCALE %d\\n\" % SRE_FLAG_LOCALE)\n    f.write(\"#define SRE_FLAG_MULTILINE %d\\n\" % SRE_FLAG_MULTILINE)\n    f.write(\"#define SRE_FLAG_DOTALL %d\\n\" % SRE_FLAG_DOTALL)\n    f.write(\"#define SRE_FLAG_UNICODE %d\\n\" % SRE_FLAG_UNICODE)\n    f.write(\"#define SRE_FLAG_VERBOSE %d\\n\" % SRE_FLAG_VERBOSE)\n\n    f.write(\"#define SRE_INFO_PREFIX %d\\n\" % SRE_INFO_PREFIX)\n    f.write(\"#define SRE_INFO_LITERAL %d\\n\" % SRE_INFO_LITERAL)\n    f.write(\"#define SRE_INFO_CHARSET %d\\n\" % SRE_INFO_CHARSET)\n\n    f.close()\n    print \"done\"\n", 
    "sre_parse": "#\n# Secret Labs' Regular Expression Engine\n#\n# convert re-style regular expression to sre pattern\n#\n# Copyright (c) 1998-2001 by Secret Labs AB.  All rights reserved.\n#\n# See the sre.py file for information on usage and redistribution.\n#\n\n\"\"\"Internal support module for sre\"\"\"\n\n# XXX: show string offset and offending character for all errors\n\nimport sys\n\nfrom sre_constants import *\n\ntry:\n    from __pypy__ import newdict\nexcept ImportError:\n    assert '__pypy__' not in sys.builtin_module_names\n    newdict = lambda _ : {}\n\nSPECIAL_CHARS = \".\\\\[{()*+?^$|\"\nREPEAT_CHARS = \"*+?{\"\n\nDIGITS = set(\"0123456789\")\n\nOCTDIGITS = set(\"01234567\")\nHEXDIGITS = set(\"0123456789abcdefABCDEF\")\n\nWHITESPACE = set(\" \\t\\n\\r\\v\\f\")\n\nESCAPES = {\n    r\"\\a\": (LITERAL, ord(\"\\a\")),\n    r\"\\b\": (LITERAL, ord(\"\\b\")),\n    r\"\\f\": (LITERAL, ord(\"\\f\")),\n    r\"\\n\": (LITERAL, ord(\"\\n\")),\n    r\"\\r\": (LITERAL, ord(\"\\r\")),\n    r\"\\t\": (LITERAL, ord(\"\\t\")),\n    r\"\\v\": (LITERAL, ord(\"\\v\")),\n    r\"\\\\\": (LITERAL, ord(\"\\\\\"))\n}\n\nCATEGORIES = {\n    r\"\\A\": (AT, AT_BEGINNING_STRING), # start of string\n    r\"\\b\": (AT, AT_BOUNDARY),\n    r\"\\B\": (AT, AT_NON_BOUNDARY),\n    r\"\\d\": (IN, [(CATEGORY, CATEGORY_DIGIT)]),\n    r\"\\D\": (IN, [(CATEGORY, CATEGORY_NOT_DIGIT)]),\n    r\"\\s\": (IN, [(CATEGORY, CATEGORY_SPACE)]),\n    r\"\\S\": (IN, [(CATEGORY, CATEGORY_NOT_SPACE)]),\n    r\"\\w\": (IN, [(CATEGORY, CATEGORY_WORD)]),\n    r\"\\W\": (IN, [(CATEGORY, CATEGORY_NOT_WORD)]),\n    r\"\\Z\": (AT, AT_END_STRING), # end of string\n}\n\nFLAGS = {\n    # standard flags\n    \"i\": SRE_FLAG_IGNORECASE,\n    \"L\": SRE_FLAG_LOCALE,\n    \"m\": SRE_FLAG_MULTILINE,\n    \"s\": SRE_FLAG_DOTALL,\n    \"x\": SRE_FLAG_VERBOSE,\n    # extensions\n    \"t\": SRE_FLAG_TEMPLATE,\n    \"u\": SRE_FLAG_UNICODE,\n}\n\nclass Pattern:\n    # master pattern object.  keeps track of global attributes\n    def __init__(self):\n        self.flags = 0\n        self.open = []\n        self.groups = 1\n        self.groupdict = newdict(\"module\")\n    def opengroup(self, name=None):\n        gid = self.groups\n        self.groups = gid + 1\n        if name is not None:\n            ogid = self.groupdict.get(name, None)\n            if ogid is not None:\n                raise error, (\"redefinition of group name %s as group %d; \"\n                              \"was group %d\" % (repr(name), gid,  ogid))\n            self.groupdict[name] = gid\n        self.open.append(gid)\n        return gid\n    def closegroup(self, gid):\n        self.open.remove(gid)\n    def checkgroup(self, gid):\n        return gid < self.groups and gid not in self.open\n\nclass SubPattern:\n    # a subpattern, in intermediate form\n    def __init__(self, pattern, data=None):\n        self.pattern = pattern\n        if data is None:\n            data = []\n        self.data = data\n        self.width = None\n    def dump(self, level=0):\n        seqtypes = (tuple, list)\n        for op, av in self.data:\n            print level*\"  \" + op,\n            if op == IN:\n                # member sublanguage\n                print\n                for op, a in av:\n                    print (level+1)*\"  \" + op, a\n            elif op == BRANCH:\n                print\n                for i, a in enumerate(av[1]):\n                    if i:\n                        print level*\"  \" + \"or\"\n                    a.dump(level+1)\n            elif op == GROUPREF_EXISTS:\n                condgroup, item_yes, item_no = av\n                print condgroup\n                item_yes.dump(level+1)\n                if item_no:\n                    print level*\"  \" + \"else\"\n                    item_no.dump(level+1)\n            elif isinstance(av, seqtypes):\n                nl = 0\n                for a in av:\n                    if isinstance(a, SubPattern):\n                        if not nl:\n                            print\n                        a.dump(level+1)\n                        nl = 1\n                    else:\n                        print a,\n                        nl = 0\n                if not nl:\n                    print\n            else:\n                print av\n    def __repr__(self):\n        return repr(self.data)\n    def __len__(self):\n        return len(self.data)\n    def __delitem__(self, index):\n        del self.data[index]\n    def __getitem__(self, index):\n        if isinstance(index, slice):\n            return SubPattern(self.pattern, self.data[index])\n        return self.data[index]\n    def __setitem__(self, index, code):\n        self.data[index] = code\n    def insert(self, index, code):\n        self.data.insert(index, code)\n    def append(self, code):\n        self.data.append(code)\n    def getwidth(self):\n        # determine the width (min, max) for this subpattern\n        if self.width:\n            return self.width\n        lo = hi = 0\n        UNITCODES = (ANY, RANGE, IN, LITERAL, NOT_LITERAL, CATEGORY)\n        REPEATCODES = (MIN_REPEAT, MAX_REPEAT)\n        for op, av in self.data:\n            if op is BRANCH:\n                i = MAXREPEAT - 1\n                j = 0\n                for av in av[1]:\n                    l, h = av.getwidth()\n                    i = min(i, l)\n                    j = max(j, h)\n                lo = lo + i\n                hi = hi + j\n            elif op is CALL:\n                i, j = av.getwidth()\n                lo = lo + i\n                hi = hi + j\n            elif op is SUBPATTERN:\n                i, j = av[1].getwidth()\n                lo = lo + i\n                hi = hi + j\n            elif op in REPEATCODES:\n                i, j = av[2].getwidth()\n                lo = lo + i * av[0]\n                hi = hi + j * av[1]\n            elif op in UNITCODES:\n                lo = lo + 1\n                hi = hi + 1\n            elif op == SUCCESS:\n                break\n        self.width = min(lo, MAXREPEAT - 1), min(hi, MAXREPEAT)\n        return self.width\n\nclass Tokenizer:\n    def __init__(self, string):\n        self.string = string\n        self.index = 0\n        self.__next()\n    def __next(self):\n        if self.index >= len(self.string):\n            self.next = None\n            return\n        char = self.string[self.index]\n        if char[0] == \"\\\\\":\n            try:\n                c = self.string[self.index + 1]\n            except IndexError:\n                raise error, \"bogus escape (end of line)\"\n            char = char + c\n        self.index = self.index + len(char)\n        self.next = char\n    def match(self, char, skip=1):\n        if char == self.next:\n            if skip:\n                self.__next()\n            return 1\n        return 0\n    def get(self):\n        this = self.next\n        self.__next()\n        return this\n    def tell(self):\n        return self.index, self.next\n    def seek(self, index):\n        self.index, self.next = index\n\ndef isident(char):\n    return \"a\" <= char <= \"z\" or \"A\" <= char <= \"Z\" or char == \"_\"\n\ndef isdigit(char):\n    return \"0\" <= char <= \"9\"\n\ndef isname(name):\n    # check that group name is a valid string\n    if not isident(name[0]):\n        return False\n    for char in name[1:]:\n        if not isident(char) and not isdigit(char):\n            return False\n    return True\n\ndef _class_escape(source, escape):\n    # handle escape code inside character class\n    code = ESCAPES.get(escape)\n    if code:\n        return code\n    code = CATEGORIES.get(escape)\n    if code and code[0] == IN:\n        return code\n    try:\n        c = escape[1:2]\n        if c == \"x\":\n            # hexadecimal escape (exactly two digits)\n            while source.next in HEXDIGITS and len(escape) < 4:\n                escape = escape + source.get()\n            escape = escape[2:]\n            if len(escape) != 2:\n                raise error, \"bogus escape: %s\" % repr(\"\\\\\" + escape)\n            return LITERAL, int(escape, 16) & 0xff\n        elif c in OCTDIGITS:\n            # octal escape (up to three digits)\n            while source.next in OCTDIGITS and len(escape) < 4:\n                escape = escape + source.get()\n            escape = escape[1:]\n            return LITERAL, int(escape, 8) & 0xff\n        elif c in DIGITS:\n            raise error, \"bogus escape: %s\" % repr(escape)\n        if len(escape) == 2:\n            return LITERAL, ord(escape[1])\n    except ValueError:\n        pass\n    raise error, \"bogus escape: %s\" % repr(escape)\n\ndef _escape(source, escape, state):\n    # handle escape code in expression\n    code = CATEGORIES.get(escape)\n    if code:\n        return code\n    code = ESCAPES.get(escape)\n    if code:\n        return code\n    try:\n        c = escape[1:2]\n        if c == \"x\":\n            # hexadecimal escape\n            while source.next in HEXDIGITS and len(escape) < 4:\n                escape = escape + source.get()\n            if len(escape) != 4:\n                raise ValueError\n            return LITERAL, int(escape[2:], 16) & 0xff\n        elif c == \"0\":\n            # octal escape\n            while source.next in OCTDIGITS and len(escape) < 4:\n                escape = escape + source.get()\n            return LITERAL, int(escape[1:], 8) & 0xff\n        elif c in DIGITS:\n            # octal escape *or* decimal group reference (sigh)\n            if source.next in DIGITS:\n                escape = escape + source.get()\n                if (escape[1] in OCTDIGITS and escape[2] in OCTDIGITS and\n                    source.next in OCTDIGITS):\n                    # got three octal digits; this is an octal escape\n                    escape = escape + source.get()\n                    return LITERAL, int(escape[1:], 8) & 0xff\n            # not an octal escape, so this is a group reference\n            group = int(escape[1:])\n            if group < state.groups:\n                if not state.checkgroup(group):\n                    raise error, \"cannot refer to open group\"\n                return GROUPREF, group\n            raise ValueError\n        if len(escape) == 2:\n            return LITERAL, ord(escape[1])\n    except ValueError:\n        pass\n    raise error, \"bogus escape: %s\" % repr(escape)\n\ndef _parse_sub(source, state, nested=1):\n    # parse an alternation: a|b|c\n\n    items = []\n    itemsappend = items.append\n    sourcematch = source.match\n    while 1:\n        itemsappend(_parse(source, state))\n        if sourcematch(\"|\"):\n            continue\n        if not nested:\n            break\n        if not source.next or sourcematch(\")\", 0):\n            break\n        else:\n            raise error, \"pattern not properly closed\"\n\n    if len(items) == 1:\n        return items[0]\n\n    subpattern = SubPattern(state)\n    subpatternappend = subpattern.append\n\n    # check if all items share a common prefix\n    while 1:\n        prefix = None\n        for item in items:\n            if not item:\n                break\n            if prefix is None:\n                prefix = item[0]\n            elif item[0] != prefix:\n                break\n        else:\n            # all subitems start with a common \"prefix\".\n            # move it out of the branch\n            for item in items:\n                del item[0]\n            subpatternappend(prefix)\n            continue # check next one\n        break\n\n    # check if the branch can be replaced by a character set\n    for item in items:\n        if len(item) != 1 or item[0][0] != LITERAL:\n            break\n    else:\n        # we can store this as a character set instead of a\n        # branch (the compiler may optimize this even more)\n        set = []\n        setappend = set.append\n        for item in items:\n            setappend(item[0])\n        subpatternappend((IN, set))\n        return subpattern\n\n    subpattern.append((BRANCH, (None, items)))\n    return subpattern\n\ndef _parse_sub_cond(source, state, condgroup):\n    item_yes = _parse(source, state)\n    if source.match(\"|\"):\n        item_no = _parse(source, state)\n        if source.match(\"|\"):\n            raise error, \"conditional backref with more than two branches\"\n    else:\n        item_no = None\n    if source.next and not source.match(\")\", 0):\n        raise error, \"pattern not properly closed\"\n    subpattern = SubPattern(state)\n    subpattern.append((GROUPREF_EXISTS, (condgroup, item_yes, item_no)))\n    return subpattern\n\n_PATTERNENDERS = set(\"|)\")\n_ASSERTCHARS = set(\"=!<\")\n_LOOKBEHINDASSERTCHARS = set(\"=!\")\n_REPEATCODES = set([MIN_REPEAT, MAX_REPEAT])\n\ndef _parse(source, state):\n    # parse a simple pattern\n    subpattern = SubPattern(state)\n\n    # precompute constants into local variables\n    subpatternappend = subpattern.append\n    sourceget = source.get\n    sourcematch = source.match\n    _len = len\n    PATTERNENDERS = _PATTERNENDERS\n    ASSERTCHARS = _ASSERTCHARS\n    LOOKBEHINDASSERTCHARS = _LOOKBEHINDASSERTCHARS\n    REPEATCODES = _REPEATCODES\n\n    while 1:\n\n        if source.next in PATTERNENDERS:\n            break # end of subpattern\n        this = sourceget()\n        if this is None:\n            break # end of pattern\n\n        if state.flags & SRE_FLAG_VERBOSE:\n            # skip whitespace and comments\n            if this in WHITESPACE:\n                continue\n            if this == \"#\":\n                while 1:\n                    this = sourceget()\n                    if this in (None, \"\\n\"):\n                        break\n                continue\n\n        if this and this[0] not in SPECIAL_CHARS:\n            subpatternappend((LITERAL, ord(this)))\n\n        elif this == \"[\":\n            # character set\n            set = []\n            setappend = set.append\n##          if sourcematch(\":\"):\n##              pass # handle character classes\n            if sourcematch(\"^\"):\n                setappend((NEGATE, None))\n            # check remaining characters\n            start = set[:]\n            while 1:\n                this = sourceget()\n                if this == \"]\" and set != start:\n                    break\n                elif this and this[0] == \"\\\\\":\n                    code1 = _class_escape(source, this)\n                elif this:\n                    code1 = LITERAL, ord(this)\n                else:\n                    raise error, \"unexpected end of regular expression\"\n                if sourcematch(\"-\"):\n                    # potential range\n                    this = sourceget()\n                    if this == \"]\":\n                        if code1[0] is IN:\n                            code1 = code1[1][0]\n                        setappend(code1)\n                        setappend((LITERAL, ord(\"-\")))\n                        break\n                    elif this:\n                        if this[0] == \"\\\\\":\n                            code2 = _class_escape(source, this)\n                        else:\n                            code2 = LITERAL, ord(this)\n                        if code1[0] != LITERAL or code2[0] != LITERAL:\n                            raise error, \"bad character range\"\n                        lo = code1[1]\n                        hi = code2[1]\n                        if hi < lo:\n                            raise error, \"bad character range\"\n                        setappend((RANGE, (lo, hi)))\n                    else:\n                        raise error, \"unexpected end of regular expression\"\n                else:\n                    if code1[0] is IN:\n                        code1 = code1[1][0]\n                    setappend(code1)\n\n            # XXX: <fl> should move set optimization to compiler!\n            if _len(set)==1 and set[0][0] is LITERAL:\n                subpatternappend(set[0]) # optimization\n            elif _len(set)==2 and set[0][0] is NEGATE and set[1][0] is LITERAL:\n                subpatternappend((NOT_LITERAL, set[1][1])) # optimization\n            else:\n                # XXX: <fl> should add charmap optimization here\n                subpatternappend((IN, set))\n\n        elif this and this[0] in REPEAT_CHARS:\n            # repeat previous item\n            if this == \"?\":\n                min, max = 0, 1\n            elif this == \"*\":\n                min, max = 0, MAXREPEAT\n\n            elif this == \"+\":\n                min, max = 1, MAXREPEAT\n            elif this == \"{\":\n                if source.next == \"}\":\n                    subpatternappend((LITERAL, ord(this)))\n                    continue\n                here = source.tell()\n                min, max = 0, MAXREPEAT\n                lo = hi = \"\"\n                while source.next in DIGITS:\n                    lo = lo + source.get()\n                if sourcematch(\",\"):\n                    while source.next in DIGITS:\n                        hi = hi + sourceget()\n                else:\n                    hi = lo\n                if not sourcematch(\"}\"):\n                    subpatternappend((LITERAL, ord(this)))\n                    source.seek(here)\n                    continue\n                if lo:\n                    min = int(lo)\n                    if min >= MAXREPEAT:\n                        raise OverflowError(\"the repetition number is too large\")\n                if hi:\n                    max = int(hi)\n                    if max >= MAXREPEAT:\n                        raise OverflowError(\"the repetition number is too large\")\n                    if max < min:\n                        raise error(\"bad repeat interval\")\n            else:\n                raise error, \"not supported\"\n            # figure out which item to repeat\n            if subpattern:\n                item = subpattern[-1:]\n            else:\n                item = None\n            if not item or (_len(item) == 1 and item[0][0] == AT):\n                raise error, \"nothing to repeat\"\n            if item[0][0] in REPEATCODES:\n                raise error, \"multiple repeat\"\n            if sourcematch(\"?\"):\n                subpattern[-1] = (MIN_REPEAT, (min, max, item))\n            else:\n                subpattern[-1] = (MAX_REPEAT, (min, max, item))\n\n        elif this == \".\":\n            subpatternappend((ANY, None))\n\n        elif this == \"(\":\n            group = 1\n            name = None\n            condgroup = None\n            if sourcematch(\"?\"):\n                group = 0\n                # options\n                if sourcematch(\"P\"):\n                    # python extensions\n                    if sourcematch(\"<\"):\n                        # named group: skip forward to end of name\n                        name = \"\"\n                        while 1:\n                            char = sourceget()\n                            if char is None:\n                                raise error, \"unterminated name\"\n                            if char == \">\":\n                                break\n                            name = name + char\n                        group = 1\n                        if not name:\n                            raise error(\"missing group name\")\n                        if not isname(name):\n                            raise error(\"bad character in group name %r\" %\n                                        name)\n                    elif sourcematch(\"=\"):\n                        # named backreference\n                        name = \"\"\n                        while 1:\n                            char = sourceget()\n                            if char is None:\n                                raise error, \"unterminated name\"\n                            if char == \")\":\n                                break\n                            name = name + char\n                        if not name:\n                            raise error(\"missing group name\")\n                        if not isname(name):\n                            raise error(\"bad character in backref group name \"\n                                        \"%r\" % name)\n                        gid = state.groupdict.get(name)\n                        if gid is None:\n                            msg = \"unknown group name: {0!r}\".format(name)\n                            raise error(msg)\n                        subpatternappend((GROUPREF, gid))\n                        continue\n                    else:\n                        char = sourceget()\n                        if char is None:\n                            raise error, \"unexpected end of pattern\"\n                        raise error, \"unknown specifier: ?P%s\" % char\n                elif sourcematch(\":\"):\n                    # non-capturing group\n                    group = 2\n                elif sourcematch(\"#\"):\n                    # comment\n                    while 1:\n                        if source.next is None or source.next == \")\":\n                            break\n                        sourceget()\n                    if not sourcematch(\")\"):\n                        raise error, \"unbalanced parenthesis\"\n                    continue\n                elif source.next in ASSERTCHARS:\n                    # lookahead assertions\n                    char = sourceget()\n                    dir = 1\n                    if char == \"<\":\n                        if source.next not in LOOKBEHINDASSERTCHARS:\n                            raise error, \"syntax error\"\n                        dir = -1 # lookbehind\n                        char = sourceget()\n                    p = _parse_sub(source, state)\n                    if not sourcematch(\")\"):\n                        raise error, \"unbalanced parenthesis\"\n                    if char == \"=\":\n                        subpatternappend((ASSERT, (dir, p)))\n                    else:\n                        subpatternappend((ASSERT_NOT, (dir, p)))\n                    continue\n                elif sourcematch(\"(\"):\n                    # conditional backreference group\n                    condname = \"\"\n                    while 1:\n                        char = sourceget()\n                        if char is None:\n                            raise error, \"unterminated name\"\n                        if char == \")\":\n                            break\n                        condname = condname + char\n                    group = 2\n                    if not condname:\n                        raise error(\"missing group name\")\n                    if isname(condname):\n                        condgroup = state.groupdict.get(condname)\n                        if condgroup is None:\n                            msg = \"unknown group name: {0!r}\".format(condname)\n                            raise error(msg)\n                    else:\n                        try:\n                            condgroup = int(condname)\n                        except ValueError:\n                            raise error, \"bad character in group name\"\n                else:\n                    # flags\n                    if not source.next in FLAGS:\n                        raise error, \"unexpected end of pattern\"\n                    while source.next in FLAGS:\n                        state.flags = state.flags | FLAGS[sourceget()]\n            if group:\n                # parse group contents\n                if group == 2:\n                    # anonymous group\n                    group = None\n                else:\n                    group = state.opengroup(name)\n                if condgroup:\n                    p = _parse_sub_cond(source, state, condgroup)\n                else:\n                    p = _parse_sub(source, state)\n                if not sourcematch(\")\"):\n                    raise error, \"unbalanced parenthesis\"\n                if group is not None:\n                    state.closegroup(group)\n                subpatternappend((SUBPATTERN, (group, p)))\n            else:\n                while 1:\n                    char = sourceget()\n                    if char is None:\n                        raise error, \"unexpected end of pattern\"\n                    if char == \")\":\n                        break\n                    raise error, \"unknown extension\"\n\n        elif this == \"^\":\n            subpatternappend((AT, AT_BEGINNING))\n\n        elif this == \"$\":\n            subpattern.append((AT, AT_END))\n\n        elif this and this[0] == \"\\\\\":\n            code = _escape(source, this, state)\n            subpatternappend(code)\n\n        else:\n            raise error, \"parser error\"\n\n    return subpattern\n\ndef parse(str, flags=0, pattern=None):\n    # parse 're' pattern into list of (opcode, argument) tuples\n\n    source = Tokenizer(str)\n\n    if pattern is None:\n        pattern = Pattern()\n    pattern.flags = flags\n    pattern.str = str\n\n    p = _parse_sub(source, pattern, 0)\n\n    tail = source.get()\n    if tail == \")\":\n        raise error, \"unbalanced parenthesis\"\n    elif tail:\n        raise error, \"bogus characters at end of regular expression\"\n\n    if flags & SRE_FLAG_DEBUG:\n        p.dump()\n\n    if not (flags & SRE_FLAG_VERBOSE) and p.pattern.flags & SRE_FLAG_VERBOSE:\n        # the VERBOSE flag was switched on inside the pattern.  to be\n        # on the safe side, we'll parse the whole thing again...\n        return parse(str, p.pattern.flags)\n\n    return p\n\ndef parse_template(source, pattern):\n    # parse 're' replacement string into list of literals and\n    # group references\n    s = Tokenizer(source)\n    sget = s.get\n    p = []\n    a = p.append\n    def literal(literal, p=p, pappend=a):\n        if p and p[-1][0] is LITERAL:\n            p[-1] = LITERAL, p[-1][1] + literal\n        else:\n            pappend((LITERAL, literal))\n    sep = source[:0]\n    if type(sep) is type(\"\"):\n        makechar = chr\n    else:\n        makechar = unichr\n    while 1:\n        this = sget()\n        if this is None:\n            break # end of replacement string\n        if this and this[0] == \"\\\\\":\n            # group\n            c = this[1:2]\n            if c == \"g\":\n                name = \"\"\n                if s.match(\"<\"):\n                    while 1:\n                        char = sget()\n                        if char is None:\n                            raise error, \"unterminated group name\"\n                        if char == \">\":\n                            break\n                        name = name + char\n                if not name:\n                    raise error, \"missing group name\"\n                try:\n                    index = int(name)\n                    if index < 0:\n                        raise error, \"negative group number\"\n                except ValueError:\n                    if not isname(name):\n                        raise error, \"bad character in group name\"\n                    try:\n                        index = pattern.groupindex[name]\n                    except KeyError:\n                        msg = \"unknown group name: {0!r}\".format(name)\n                        raise IndexError(msg)\n                a((MARK, index))\n            elif c == \"0\":\n                if s.next in OCTDIGITS:\n                    this = this + sget()\n                    if s.next in OCTDIGITS:\n                        this = this + sget()\n                literal(makechar(int(this[1:], 8) & 0xff))\n            elif c in DIGITS:\n                isoctal = False\n                if s.next in DIGITS:\n                    this = this + sget()\n                    if (c in OCTDIGITS and this[2] in OCTDIGITS and\n                        s.next in OCTDIGITS):\n                        this = this + sget()\n                        isoctal = True\n                        literal(makechar(int(this[1:], 8) & 0xff))\n                if not isoctal:\n                    a((MARK, int(this[1:])))\n            else:\n                try:\n                    this = makechar(ESCAPES[this][1])\n                except KeyError:\n                    pass\n                literal(this)\n        else:\n            literal(this)\n    # convert template to groups and literals lists\n    i = 0\n    groups = []\n    groupsappend = groups.append\n    literals = [None] * len(p)\n    for c, s in p:\n        if c is MARK:\n            groupsappend((i, s))\n            # literal[i] is already None\n        else:\n            literals[i] = s\n        i = i + 1\n    return groups, literals\n\ndef expand_template(template, match):\n    g = match.group\n    sep = match.string[:0]\n    groups, literals = template\n    literals = literals[:]\n    try:\n        for index, group in groups:\n            literals[index] = s = g(group)\n            if s is None:\n                raise error, \"unmatched group\"\n    except IndexError:\n        raise error, \"invalid group reference\"\n    return sep.join(literals)\n", 
    "stat": "\"\"\"Constants/functions for interpreting results of os.stat() and os.lstat().\n\nSuggested usage: from stat import *\n\"\"\"\n\n# Indices for stat struct members in the tuple returned by os.stat()\n\nST_MODE  = 0\nST_INO   = 1\nST_DEV   = 2\nST_NLINK = 3\nST_UID   = 4\nST_GID   = 5\nST_SIZE  = 6\nST_ATIME = 7\nST_MTIME = 8\nST_CTIME = 9\n\n# Extract bits from the mode\n\ndef S_IMODE(mode):\n    return mode & 07777\n\ndef S_IFMT(mode):\n    return mode & 0170000\n\n# Constants used as S_IFMT() for various file types\n# (not all are implemented on all systems)\n\nS_IFDIR  = 0040000\nS_IFCHR  = 0020000\nS_IFBLK  = 0060000\nS_IFREG  = 0100000\nS_IFIFO  = 0010000\nS_IFLNK  = 0120000\nS_IFSOCK = 0140000\n\n# Functions to test for each file type\n\ndef S_ISDIR(mode):\n    return S_IFMT(mode) == S_IFDIR\n\ndef S_ISCHR(mode):\n    return S_IFMT(mode) == S_IFCHR\n\ndef S_ISBLK(mode):\n    return S_IFMT(mode) == S_IFBLK\n\ndef S_ISREG(mode):\n    return S_IFMT(mode) == S_IFREG\n\ndef S_ISFIFO(mode):\n    return S_IFMT(mode) == S_IFIFO\n\ndef S_ISLNK(mode):\n    return S_IFMT(mode) == S_IFLNK\n\ndef S_ISSOCK(mode):\n    return S_IFMT(mode) == S_IFSOCK\n\n# Names for permission bits\n\nS_ISUID = 04000\nS_ISGID = 02000\nS_ENFMT = S_ISGID\nS_ISVTX = 01000\nS_IREAD = 00400\nS_IWRITE = 00200\nS_IEXEC = 00100\nS_IRWXU = 00700\nS_IRUSR = 00400\nS_IWUSR = 00200\nS_IXUSR = 00100\nS_IRWXG = 00070\nS_IRGRP = 00040\nS_IWGRP = 00020\nS_IXGRP = 00010\nS_IRWXO = 00007\nS_IROTH = 00004\nS_IWOTH = 00002\nS_IXOTH = 00001\n\n# Names for file flags\n\nUF_NODUMP    = 0x00000001\nUF_IMMUTABLE = 0x00000002\nUF_APPEND    = 0x00000004\nUF_OPAQUE    = 0x00000008\nUF_NOUNLINK  = 0x00000010\nUF_COMPRESSED = 0x00000020  # OS X: file is hfs-compressed\nUF_HIDDEN    = 0x00008000   # OS X: file should not be displayed\nSF_ARCHIVED  = 0x00010000\nSF_IMMUTABLE = 0x00020000\nSF_APPEND    = 0x00040000\nSF_NOUNLINK  = 0x00100000\nSF_SNAPSHOT  = 0x00200000\n", 
    "string": "\"\"\"A collection of string operations (most are no longer used).\n\nWarning: most of the code you see here isn't normally used nowadays.\nBeginning with Python 1.6, many of these functions are implemented as\nmethods on the standard string object. They used to be implemented by\na built-in module called strop, but strop is now obsolete itself.\n\nPublic module variables:\n\nwhitespace -- a string containing all characters considered whitespace\nlowercase -- a string containing all characters considered lowercase letters\nuppercase -- a string containing all characters considered uppercase letters\nletters -- a string containing all characters considered letters\ndigits -- a string containing all characters considered decimal digits\nhexdigits -- a string containing all characters considered hexadecimal digits\noctdigits -- a string containing all characters considered octal digits\npunctuation -- a string containing all characters considered punctuation\nprintable -- a string containing all characters considered printable\n\n\"\"\"\n\n# Some strings for ctype-style character classification\nwhitespace = ' \\t\\n\\r\\v\\f'\nlowercase = 'abcdefghijklmnopqrstuvwxyz'\nuppercase = 'ABCDEFGHIJKLMNOPQRSTUVWXYZ'\nletters = lowercase + uppercase\nascii_lowercase = lowercase\nascii_uppercase = uppercase\nascii_letters = ascii_lowercase + ascii_uppercase\ndigits = '0123456789'\nhexdigits = digits + 'abcdef' + 'ABCDEF'\noctdigits = '01234567'\npunctuation = \"\"\"!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\"\"\"\nprintable = digits + letters + punctuation + whitespace\n\n# Case conversion helpers\n# Use str to convert Unicode literal in case of -U\nl = map(chr, xrange(256))\n_idmap = str('').join(l)\ndel l\n\n# Functions which aren't available as string methods.\n\n# Capitalize the words in a string, e.g. \" aBc  dEf \" -> \"Abc Def\".\ndef capwords(s, sep=None):\n    \"\"\"capwords(s [,sep]) -> string\n\n    Split the argument into words using split, capitalize each\n    word using capitalize, and join the capitalized words using\n    join.  If the optional second argument sep is absent or None,\n    runs of whitespace characters are replaced by a single space\n    and leading and trailing whitespace are removed, otherwise\n    sep is used to split and join the words.\n\n    \"\"\"\n    return (sep or ' ').join(x.capitalize() for x in s.split(sep))\n\n\n# Construct a translation string\n_idmapL = None\ndef maketrans(fromstr, tostr):\n    \"\"\"maketrans(frm, to) -> string\n\n    Return a translation table (a string of 256 bytes long)\n    suitable for use in string.translate.  The strings frm and to\n    must be of the same length.\n\n    \"\"\"\n    n = len(fromstr)\n    if n != len(tostr):\n        raise ValueError, \"maketrans arguments must have same length\"\n    # this function has been rewritten to suit PyPy better; it is\n    # almost 10x faster than the original.\n    buf = bytearray(256)\n    for i in range(256):\n        buf[i] = i\n    for i in range(n):\n        buf[ord(fromstr[i])] = tostr[i]\n    return str(buf)\n\n\n\n####################################################################\nimport re as _re\n\nclass _multimap:\n    \"\"\"Helper class for combining multiple mappings.\n\n    Used by .{safe_,}substitute() to combine the mapping and keyword\n    arguments.\n    \"\"\"\n    def __init__(self, primary, secondary):\n        self._primary = primary\n        self._secondary = secondary\n\n    def __getitem__(self, key):\n        try:\n            return self._primary[key]\n        except KeyError:\n            return self._secondary[key]\n\n\nclass _TemplateMetaclass(type):\n    pattern = r\"\"\"\n    %(delim)s(?:\n      (?P<escaped>%(delim)s) |   # Escape sequence of two delimiters\n      (?P<named>%(id)s)      |   # delimiter and a Python identifier\n      {(?P<braced>%(id)s)}   |   # delimiter and a braced identifier\n      (?P<invalid>)              # Other ill-formed delimiter exprs\n    )\n    \"\"\"\n\n    def __init__(cls, name, bases, dct):\n        super(_TemplateMetaclass, cls).__init__(name, bases, dct)\n        if 'pattern' in dct:\n            pattern = cls.pattern\n        else:\n            pattern = _TemplateMetaclass.pattern % {\n                'delim' : _re.escape(cls.delimiter),\n                'id'    : cls.idpattern,\n                }\n        cls.pattern = _re.compile(pattern, _re.IGNORECASE | _re.VERBOSE)\n\n\nclass Template:\n    \"\"\"A string class for supporting $-substitutions.\"\"\"\n    __metaclass__ = _TemplateMetaclass\n\n    delimiter = '$'\n    idpattern = r'[_a-z][_a-z0-9]*'\n\n    def __init__(self, template):\n        self.template = template\n\n    # Search for $$, $identifier, ${identifier}, and any bare $'s\n\n    def _invalid(self, mo):\n        i = mo.start('invalid')\n        lines = self.template[:i].splitlines(True)\n        if not lines:\n            colno = 1\n            lineno = 1\n        else:\n            colno = i - len(''.join(lines[:-1]))\n            lineno = len(lines)\n        raise ValueError('Invalid placeholder in string: line %d, col %d' %\n                         (lineno, colno))\n\n    def substitute(self, *args, **kws):\n        if len(args) > 1:\n            raise TypeError('Too many positional arguments')\n        if not args:\n            mapping = kws\n        elif kws:\n            mapping = _multimap(kws, args[0])\n        else:\n            mapping = args[0]\n        # Helper function for .sub()\n        def convert(mo):\n            # Check the most common path first.\n            named = mo.group('named') or mo.group('braced')\n            if named is not None:\n                val = mapping[named]\n                # We use this idiom instead of str() because the latter will\n                # fail if val is a Unicode containing non-ASCII characters.\n                return '%s' % (val,)\n            if mo.group('escaped') is not None:\n                return self.delimiter\n            if mo.group('invalid') is not None:\n                self._invalid(mo)\n            raise ValueError('Unrecognized named group in pattern',\n                             self.pattern)\n        return self.pattern.sub(convert, self.template)\n\n    def safe_substitute(self, *args, **kws):\n        if len(args) > 1:\n            raise TypeError('Too many positional arguments')\n        if not args:\n            mapping = kws\n        elif kws:\n            mapping = _multimap(kws, args[0])\n        else:\n            mapping = args[0]\n        # Helper function for .sub()\n        def convert(mo):\n            named = mo.group('named') or mo.group('braced')\n            if named is not None:\n                try:\n                    # We use this idiom instead of str() because the latter\n                    # will fail if val is a Unicode containing non-ASCII\n                    return '%s' % (mapping[named],)\n                except KeyError:\n                    return mo.group()\n            if mo.group('escaped') is not None:\n                return self.delimiter\n            if mo.group('invalid') is not None:\n                return mo.group()\n            raise ValueError('Unrecognized named group in pattern',\n                             self.pattern)\n        return self.pattern.sub(convert, self.template)\n\n\n\n####################################################################\n# NOTE: Everything below here is deprecated.  Use string methods instead.\n# This stuff will go away in Python 3.0.\n\n# Backward compatible names for exceptions\nindex_error = ValueError\natoi_error = ValueError\natof_error = ValueError\natol_error = ValueError\n\n# convert UPPER CASE letters to lower case\ndef lower(s):\n    \"\"\"lower(s) -> string\n\n    Return a copy of the string s converted to lowercase.\n\n    \"\"\"\n    return s.lower()\n\n# Convert lower case letters to UPPER CASE\ndef upper(s):\n    \"\"\"upper(s) -> string\n\n    Return a copy of the string s converted to uppercase.\n\n    \"\"\"\n    return s.upper()\n\n# Swap lower case letters and UPPER CASE\ndef swapcase(s):\n    \"\"\"swapcase(s) -> string\n\n    Return a copy of the string s with upper case characters\n    converted to lowercase and vice versa.\n\n    \"\"\"\n    return s.swapcase()\n\n# Strip leading and trailing tabs and spaces\ndef strip(s, chars=None):\n    \"\"\"strip(s [,chars]) -> string\n\n    Return a copy of the string s with leading and trailing\n    whitespace removed.\n    If chars is given and not None, remove characters in chars instead.\n    If chars is unicode, S will be converted to unicode before stripping.\n\n    \"\"\"\n    return s.strip(chars)\n\n# Strip leading tabs and spaces\ndef lstrip(s, chars=None):\n    \"\"\"lstrip(s [,chars]) -> string\n\n    Return a copy of the string s with leading whitespace removed.\n    If chars is given and not None, remove characters in chars instead.\n\n    \"\"\"\n    return s.lstrip(chars)\n\n# Strip trailing tabs and spaces\ndef rstrip(s, chars=None):\n    \"\"\"rstrip(s [,chars]) -> string\n\n    Return a copy of the string s with trailing whitespace removed.\n    If chars is given and not None, remove characters in chars instead.\n\n    \"\"\"\n    return s.rstrip(chars)\n\n\n# Split a string into a list of space/tab-separated words\ndef split(s, sep=None, maxsplit=-1):\n    \"\"\"split(s [,sep [,maxsplit]]) -> list of strings\n\n    Return a list of the words in the string s, using sep as the\n    delimiter string.  If maxsplit is given, splits at no more than\n    maxsplit places (resulting in at most maxsplit+1 words).  If sep\n    is not specified or is None, any whitespace string is a separator.\n\n    (split and splitfields are synonymous)\n\n    \"\"\"\n    return s.split(sep, maxsplit)\nsplitfields = split\n\n# Split a string into a list of space/tab-separated words\ndef rsplit(s, sep=None, maxsplit=-1):\n    \"\"\"rsplit(s [,sep [,maxsplit]]) -> list of strings\n\n    Return a list of the words in the string s, using sep as the\n    delimiter string, starting at the end of the string and working\n    to the front.  If maxsplit is given, at most maxsplit splits are\n    done. If sep is not specified or is None, any whitespace string\n    is a separator.\n    \"\"\"\n    return s.rsplit(sep, maxsplit)\n\n# Join fields with optional separator\ndef join(words, sep = ' '):\n    \"\"\"join(list [,sep]) -> string\n\n    Return a string composed of the words in list, with\n    intervening occurrences of sep.  The default separator is a\n    single space.\n\n    (joinfields and join are synonymous)\n\n    \"\"\"\n    return sep.join(words)\njoinfields = join\n\n# Find substring, raise exception if not found\ndef index(s, *args):\n    \"\"\"index(s, sub [,start [,end]]) -> int\n\n    Like find but raises ValueError when the substring is not found.\n\n    \"\"\"\n    return s.index(*args)\n\n# Find last substring, raise exception if not found\ndef rindex(s, *args):\n    \"\"\"rindex(s, sub [,start [,end]]) -> int\n\n    Like rfind but raises ValueError when the substring is not found.\n\n    \"\"\"\n    return s.rindex(*args)\n\n# Count non-overlapping occurrences of substring\ndef count(s, *args):\n    \"\"\"count(s, sub[, start[,end]]) -> int\n\n    Return the number of occurrences of substring sub in string\n    s[start:end].  Optional arguments start and end are\n    interpreted as in slice notation.\n\n    \"\"\"\n    return s.count(*args)\n\n# Find substring, return -1 if not found\ndef find(s, *args):\n    \"\"\"find(s, sub [,start [,end]]) -> in\n\n    Return the lowest index in s where substring sub is found,\n    such that sub is contained within s[start,end].  Optional\n    arguments start and end are interpreted as in slice notation.\n\n    Return -1 on failure.\n\n    \"\"\"\n    return s.find(*args)\n\n# Find last substring, return -1 if not found\ndef rfind(s, *args):\n    \"\"\"rfind(s, sub [,start [,end]]) -> int\n\n    Return the highest index in s where substring sub is found,\n    such that sub is contained within s[start,end].  Optional\n    arguments start and end are interpreted as in slice notation.\n\n    Return -1 on failure.\n\n    \"\"\"\n    return s.rfind(*args)\n\n# for a bit of speed\n_float = float\n_int = int\n_long = long\n\n# Convert string to float\ndef atof(s):\n    \"\"\"atof(s) -> float\n\n    Return the floating point number represented by the string s.\n\n    \"\"\"\n    return _float(s)\n\n\n# Convert string to integer\ndef atoi(s , base=10):\n    \"\"\"atoi(s [,base]) -> int\n\n    Return the integer represented by the string s in the given\n    base, which defaults to 10.  The string s must consist of one\n    or more digits, possibly preceded by a sign.  If base is 0, it\n    is chosen from the leading characters of s, 0 for octal, 0x or\n    0X for hexadecimal.  If base is 16, a preceding 0x or 0X is\n    accepted.\n\n    \"\"\"\n    return _int(s, base)\n\n\n# Convert string to long integer\ndef atol(s, base=10):\n    \"\"\"atol(s [,base]) -> long\n\n    Return the long integer represented by the string s in the\n    given base, which defaults to 10.  The string s must consist\n    of one or more digits, possibly preceded by a sign.  If base\n    is 0, it is chosen from the leading characters of s, 0 for\n    octal, 0x or 0X for hexadecimal.  If base is 16, a preceding\n    0x or 0X is accepted.  A trailing L or l is not accepted,\n    unless base is 0.\n\n    \"\"\"\n    return _long(s, base)\n\n\n# Left-justify a string\ndef ljust(s, width, *args):\n    \"\"\"ljust(s, width[, fillchar]) -> string\n\n    Return a left-justified version of s, in a field of the\n    specified width, padded with spaces as needed.  The string is\n    never truncated.  If specified the fillchar is used instead of spaces.\n\n    \"\"\"\n    return s.ljust(width, *args)\n\n# Right-justify a string\ndef rjust(s, width, *args):\n    \"\"\"rjust(s, width[, fillchar]) -> string\n\n    Return a right-justified version of s, in a field of the\n    specified width, padded with spaces as needed.  The string is\n    never truncated.  If specified the fillchar is used instead of spaces.\n\n    \"\"\"\n    return s.rjust(width, *args)\n\n# Center a string\ndef center(s, width, *args):\n    \"\"\"center(s, width[, fillchar]) -> string\n\n    Return a center version of s, in a field of the specified\n    width. padded with spaces as needed.  The string is never\n    truncated.  If specified the fillchar is used instead of spaces.\n\n    \"\"\"\n    return s.center(width, *args)\n\n# Zero-fill a number, e.g., (12, 3) --> '012' and (-3, 3) --> '-03'\n# Decadent feature: the argument may be a string or a number\n# (Use of this is deprecated; it should be a string as with ljust c.s.)\ndef zfill(x, width):\n    \"\"\"zfill(x, width) -> string\n\n    Pad a numeric string x with zeros on the left, to fill a field\n    of the specified width.  The string x is never truncated.\n\n    \"\"\"\n    if not isinstance(x, basestring):\n        x = repr(x)\n    return x.zfill(width)\n\n# Expand tabs in a string.\n# Doesn't take non-printing chars into account, but does understand \\n.\ndef expandtabs(s, tabsize=8):\n    \"\"\"expandtabs(s [,tabsize]) -> string\n\n    Return a copy of the string s with all tab characters replaced\n    by the appropriate number of spaces, depending on the current\n    column, and the tabsize (default 8).\n\n    \"\"\"\n    return s.expandtabs(tabsize)\n\n# Character translation through look-up table.\ndef translate(s, table, deletions=\"\"):\n    \"\"\"translate(s,table [,deletions]) -> string\n\n    Return a copy of the string s, where all characters occurring\n    in the optional argument deletions are removed, and the\n    remaining characters have been mapped through the given\n    translation table, which must be a string of length 256.  The\n    deletions argument is not allowed for Unicode strings.\n\n    \"\"\"\n    if deletions or table is None:\n        return s.translate(table, deletions)\n    else:\n        # Add s[:0] so that if s is Unicode and table is an 8-bit string,\n        # table is converted to Unicode.  This means that table *cannot*\n        # be a dictionary -- for that feature, use u.translate() directly.\n        return s.translate(table + s[:0])\n\n# Capitalize a string, e.g. \"aBc  dEf\" -> \"Abc  def\".\ndef capitalize(s):\n    \"\"\"capitalize(s) -> string\n\n    Return a copy of the string s with only its first character\n    capitalized.\n\n    \"\"\"\n    return s.capitalize()\n\n# Substring replacement (global)\ndef replace(s, old, new, maxreplace=-1):\n    \"\"\"replace (str, old, new[, maxreplace]) -> string\n\n    Return a copy of string str with all occurrences of substring\n    old replaced by new. If the optional argument maxreplace is\n    given, only the first maxreplace occurrences are replaced.\n\n    \"\"\"\n    return s.replace(old, new, maxreplace)\n\n\n# Try importing optional built-in module \"strop\" -- if it exists,\n# it redefines some string operations that are 100-1000 times faster.\n# It also defines values for whitespace, lowercase and uppercase\n# that match <ctype.h>'s definitions.\n\ntry:\n    from strop import maketrans, lowercase, uppercase, whitespace\n    letters = lowercase + uppercase\nexcept ImportError:\n    pass                                          # Use the original versions\n\n########################################################################\n# the Formatter class\n# see PEP 3101 for details and purpose of this class\n\n# The hard parts are reused from the C implementation.  They're exposed as \"_\"\n# prefixed methods of str and unicode.\n\n# The overall parser is implemented in str._formatter_parser.\n# The field name parser is implemented in str._formatter_field_name_split\n\nclass Formatter(object):\n    def format(self, format_string, *args, **kwargs):\n        return self.vformat(format_string, args, kwargs)\n\n    def vformat(self, format_string, args, kwargs):\n        used_args = set()\n        result = self._vformat(format_string, args, kwargs, used_args, 2)\n        self.check_unused_args(used_args, args, kwargs)\n        return result\n\n    def _vformat(self, format_string, args, kwargs, used_args, recursion_depth):\n        if recursion_depth < 0:\n            raise ValueError('Max string recursion exceeded')\n        result = []\n        for literal_text, field_name, format_spec, conversion in \\\n                self.parse(format_string):\n\n            # output the literal text\n            if literal_text:\n                result.append(literal_text)\n\n            # if there's a field, output it\n            if field_name is not None:\n                # this is some markup, find the object and do\n                #  the formatting\n\n                # given the field_name, find the object it references\n                #  and the argument it came from\n                obj, arg_used = self.get_field(field_name, args, kwargs)\n                used_args.add(arg_used)\n\n                # do any conversion on the resulting object\n                obj = self.convert_field(obj, conversion)\n\n                # expand the format spec, if needed\n                format_spec = self._vformat(format_spec, args, kwargs,\n                                            used_args, recursion_depth-1)\n\n                # format the object and append to the result\n                result.append(self.format_field(obj, format_spec))\n\n        return ''.join(result)\n\n\n    def get_value(self, key, args, kwargs):\n        if isinstance(key, (int, long)):\n            return args[key]\n        else:\n            return kwargs[key]\n\n\n    def check_unused_args(self, used_args, args, kwargs):\n        pass\n\n\n    def format_field(self, value, format_spec):\n        return format(value, format_spec)\n\n\n    def convert_field(self, value, conversion):\n        # do any conversion on the resulting object\n        if conversion is None:\n            return value\n        elif conversion == 's':\n            return str(value)\n        elif conversion == 'r':\n            return repr(value)\n        raise ValueError(\"Unknown conversion specifier {0!s}\".format(conversion))\n\n\n    # returns an iterable that contains tuples of the form:\n    # (literal_text, field_name, format_spec, conversion)\n    # literal_text can be zero length\n    # field_name can be None, in which case there's no\n    #  object to format and output\n    # if field_name is not None, it is looked up, formatted\n    #  with format_spec and conversion and then used\n    def parse(self, format_string):\n        return format_string._formatter_parser()\n\n\n    # given a field_name, find the object it references.\n    #  field_name:   the field being looked up, e.g. \"0.name\"\n    #                 or \"lookup[3]\"\n    #  used_args:    a set of which args have been used\n    #  args, kwargs: as passed in to vformat\n    def get_field(self, field_name, args, kwargs):\n        first, rest = field_name._formatter_field_name_split()\n\n        obj = self.get_value(first, args, kwargs)\n\n        # loop through the rest of the field_name, doing\n        #  getattr or getitem as needed\n        for is_attr, i in rest:\n            if is_attr:\n                obj = getattr(obj, i)\n            else:\n                obj = obj[i]\n\n        return obj, first\n", 
    "struct": "from _struct import *\nfrom _struct import _clearcache\nfrom _struct import __doc__\n", 
    "subprocess": "# Fake subprocess file\n", 
    "tempfile": "\"\"\"Temporary files.\n\nThis module provides generic, low- and high-level interfaces for\ncreating temporary files and directories.  All of the interfaces\nprovided by this module can be used without fear of race conditions\nexcept for 'mktemp'.  'mktemp' is subject to race conditions and\nshould not be used; it is provided for backward compatibility only.\n\nThis module also provides some data items to the user:\n\n  TMP_MAX  - maximum number of names that will be tried before\n             giving up.\n  template - the default prefix for all temporary names.\n             You may change this to control the default prefix.\n  tempdir  - If this is set to a string before the first use of\n             any routine from this module, it will be considered as\n             another candidate location to store temporary files.\n\"\"\"\n\n__all__ = [\n    \"NamedTemporaryFile\", \"TemporaryFile\", # high level safe interfaces\n    \"SpooledTemporaryFile\",\n    \"mkstemp\", \"mkdtemp\",                  # low level safe interfaces\n    \"mktemp\",                              # deprecated unsafe interface\n    \"TMP_MAX\", \"gettempprefix\",            # constants\n    \"tempdir\", \"gettempdir\"\n   ]\n\n\n# Imports.\n\nimport io as _io\nimport os as _os\nimport errno as _errno\nfrom random import Random as _Random\n\ntry:\n    from cStringIO import StringIO as _StringIO\nexcept ImportError:\n    from StringIO import StringIO as _StringIO\n\ntry:\n    import fcntl as _fcntl\nexcept ImportError:\n    def _set_cloexec(fd):\n        pass\nelse:\n    def _set_cloexec(fd):\n        try:\n            flags = _fcntl.fcntl(fd, _fcntl.F_GETFD, 0)\n        except IOError:\n            pass\n        else:\n            # flags read successfully, modify\n            flags |= _fcntl.FD_CLOEXEC\n            _fcntl.fcntl(fd, _fcntl.F_SETFD, flags)\n\n\ntry:\n    import thread as _thread\nexcept ImportError:\n    import dummy_thread as _thread\n_allocate_lock = _thread.allocate_lock\n\n_text_openflags = _os.O_RDWR | _os.O_CREAT | _os.O_EXCL\nif hasattr(_os, 'O_NOINHERIT'):\n    _text_openflags |= _os.O_NOINHERIT\nif hasattr(_os, 'O_NOFOLLOW'):\n    _text_openflags |= _os.O_NOFOLLOW\n\n_bin_openflags = _text_openflags\nif hasattr(_os, 'O_BINARY'):\n    _bin_openflags |= _os.O_BINARY\n\nif hasattr(_os, 'TMP_MAX'):\n    TMP_MAX = _os.TMP_MAX\nelse:\n    TMP_MAX = 10000\n\ntemplate = \"tmp\"\n\n# Internal routines.\n\n_once_lock = _allocate_lock()\n\nif hasattr(_os, \"lstat\"):\n    _stat = _os.lstat\nelif hasattr(_os, \"stat\"):\n    _stat = _os.stat\nelse:\n    # Fallback.  All we need is something that raises os.error if the\n    # file doesn't exist.\n    def _stat(fn):\n        try:\n            f = open(fn)\n        except IOError:\n            raise _os.error\n        f.close()\n\ndef _exists(fn):\n    try:\n        _stat(fn)\n    except _os.error:\n        return False\n    else:\n        return True\n\nclass _RandomNameSequence:\n    \"\"\"An instance of _RandomNameSequence generates an endless\n    sequence of unpredictable strings which can safely be incorporated\n    into file names.  Each string is six characters long.  Multiple\n    threads can safely use the same instance at the same time.\n\n    _RandomNameSequence is an iterator.\"\"\"\n\n    characters = (\"abcdefghijklmnopqrstuvwxyz\" +\n                  \"ABCDEFGHIJKLMNOPQRSTUVWXYZ\" +\n                  \"0123456789_\")\n\n    def __init__(self):\n        self.mutex = _allocate_lock()\n        self.normcase = _os.path.normcase\n\n    @property\n    def rng(self):\n        cur_pid = _os.getpid()\n        if cur_pid != getattr(self, '_rng_pid', None):\n            self._rng = _Random()\n            self._rng_pid = cur_pid\n        return self._rng\n\n    def __iter__(self):\n        return self\n\n    def next(self):\n        m = self.mutex\n        c = self.characters\n        choose = self.rng.choice\n\n        m.acquire()\n        try:\n            letters = [choose(c) for dummy in \"123456\"]\n        finally:\n            m.release()\n\n        return self.normcase(''.join(letters))\n\ndef _candidate_tempdir_list():\n    \"\"\"Generate a list of candidate temporary directories which\n    _get_default_tempdir will try.\"\"\"\n\n    dirlist = []\n\n    # First, try the environment.\n    for envname in 'TMPDIR', 'TEMP', 'TMP':\n        dirname = _os.getenv(envname)\n        if dirname: dirlist.append(dirname)\n\n    # Failing that, try OS-specific locations.\n    if _os.name == 'riscos':\n        dirname = _os.getenv('Wimp$ScrapDir')\n        if dirname: dirlist.append(dirname)\n    elif _os.name == 'nt':\n        dirlist.extend([ r'c:\\temp', r'c:\\tmp', r'\\temp', r'\\tmp' ])\n    else:\n        dirlist.extend([ '/tmp', '/var/tmp', '/usr/tmp' ])\n\n    # As a last resort, the current directory.\n    try:\n        dirlist.append(_os.getcwd())\n    except (AttributeError, _os.error):\n        dirlist.append(_os.curdir)\n\n    return dirlist\n\ndef _get_default_tempdir():\n    \"\"\"Calculate the default directory to use for temporary files.\n    This routine should be called exactly once.\n\n    We determine whether or not a candidate temp dir is usable by\n    trying to create and write to a file in that directory.  If this\n    is successful, the test file is deleted.  To prevent denial of\n    service, the name of the test file must be randomized.\"\"\"\n\n    namer = _RandomNameSequence()\n    dirlist = _candidate_tempdir_list()\n    flags = _text_openflags\n\n    for dir in dirlist:\n        if dir != _os.curdir:\n            dir = _os.path.normcase(_os.path.abspath(dir))\n        # Try only a few names per directory.\n        for seq in xrange(100):\n            name = namer.next()\n            filename = _os.path.join(dir, name)\n            try:\n                fd = _os.open(filename, flags, 0o600)\n                try:\n                    try:\n                        with _io.open(fd, 'wb', closefd=False) as fp:\n                            fp.write(b'blat')\n                    finally:\n                        _os.close(fd)\n                finally:\n                    _os.unlink(filename)\n                return dir\n            except (OSError, IOError) as e:\n                if e.args[0] != _errno.EEXIST:\n                    break # no point trying more names in this directory\n                pass\n    raise IOError, (_errno.ENOENT,\n                    (\"No usable temporary directory found in %s\" % dirlist))\n\n_name_sequence = None\n\ndef _get_candidate_names():\n    \"\"\"Common setup sequence for all user-callable interfaces.\"\"\"\n\n    global _name_sequence\n    if _name_sequence is None:\n        _once_lock.acquire()\n        try:\n            if _name_sequence is None:\n                _name_sequence = _RandomNameSequence()\n        finally:\n            _once_lock.release()\n    return _name_sequence\n\n\ndef _mkstemp_inner(dir, pre, suf, flags):\n    \"\"\"Code common to mkstemp, TemporaryFile, and NamedTemporaryFile.\"\"\"\n\n    names = _get_candidate_names()\n\n    for seq in xrange(TMP_MAX):\n        name = names.next()\n        file = _os.path.join(dir, pre + name + suf)\n        try:\n            fd = _os.open(file, flags, 0600)\n            _set_cloexec(fd)\n            return (fd, _os.path.abspath(file))\n        except OSError, e:\n            if e.errno == _errno.EEXIST:\n                continue # try again\n            if _os.name == 'nt' and e.errno == _errno.EACCES:\n                # On windows, when a directory with the chosen name already\n                # exists, EACCES error code is returned instead of EEXIST.\n                continue\n            raise\n\n    raise IOError, (_errno.EEXIST, \"No usable temporary file name found\")\n\n\n# User visible interfaces.\n\ndef gettempprefix():\n    \"\"\"Accessor for tempdir.template.\"\"\"\n    return template\n\ntempdir = None\n\ndef gettempdir():\n    \"\"\"Accessor for tempfile.tempdir.\"\"\"\n    global tempdir\n    if tempdir is None:\n        _once_lock.acquire()\n        try:\n            if tempdir is None:\n                tempdir = _get_default_tempdir()\n        finally:\n            _once_lock.release()\n    return tempdir\n\ndef mkstemp(suffix=\"\", prefix=template, dir=None, text=False):\n    \"\"\"User-callable function to create and return a unique temporary\n    file.  The return value is a pair (fd, name) where fd is the\n    file descriptor returned by os.open, and name is the filename.\n\n    If 'suffix' is specified, the file name will end with that suffix,\n    otherwise there will be no suffix.\n\n    If 'prefix' is specified, the file name will begin with that prefix,\n    otherwise a default prefix is used.\n\n    If 'dir' is specified, the file will be created in that directory,\n    otherwise a default directory is used.\n\n    If 'text' is specified and true, the file is opened in text\n    mode.  Else (the default) the file is opened in binary mode.  On\n    some operating systems, this makes no difference.\n\n    The file is readable and writable only by the creating user ID.\n    If the operating system uses permission bits to indicate whether a\n    file is executable, the file is executable by no one. The file\n    descriptor is not inherited by children of this process.\n\n    Caller is responsible for deleting the file when done with it.\n    \"\"\"\n\n    if dir is None:\n        dir = gettempdir()\n\n    if text:\n        flags = _text_openflags\n    else:\n        flags = _bin_openflags\n\n    return _mkstemp_inner(dir, prefix, suffix, flags)\n\n\ndef mkdtemp(suffix=\"\", prefix=template, dir=None):\n    \"\"\"User-callable function to create and return a unique temporary\n    directory.  The return value is the pathname of the directory.\n\n    Arguments are as for mkstemp, except that the 'text' argument is\n    not accepted.\n\n    The directory is readable, writable, and searchable only by the\n    creating user.\n\n    Caller is responsible for deleting the directory when done with it.\n    \"\"\"\n\n    if dir is None:\n        dir = gettempdir()\n\n    names = _get_candidate_names()\n\n    for seq in xrange(TMP_MAX):\n        name = names.next()\n        file = _os.path.join(dir, prefix + name + suffix)\n        try:\n            _os.mkdir(file, 0700)\n            return file\n        except OSError, e:\n            if e.errno == _errno.EEXIST:\n                continue # try again\n            raise\n\n    raise IOError, (_errno.EEXIST, \"No usable temporary directory name found\")\n\ndef mktemp(suffix=\"\", prefix=template, dir=None):\n    \"\"\"User-callable function to return a unique temporary file name.  The\n    file is not created.\n\n    Arguments are as for mkstemp, except that the 'text' argument is\n    not accepted.\n\n    This function is unsafe and should not be used.  The file name\n    refers to a file that did not exist at some point, but by the time\n    you get around to creating it, someone else may have beaten you to\n    the punch.\n    \"\"\"\n\n##    from warnings import warn as _warn\n##    _warn(\"mktemp is a potential security risk to your program\",\n##          RuntimeWarning, stacklevel=2)\n\n    if dir is None:\n        dir = gettempdir()\n\n    names = _get_candidate_names()\n    for seq in xrange(TMP_MAX):\n        name = names.next()\n        file = _os.path.join(dir, prefix + name + suffix)\n        if not _exists(file):\n            return file\n\n    raise IOError, (_errno.EEXIST, \"No usable temporary filename found\")\n\n\nclass _TemporaryFileWrapper:\n    \"\"\"Temporary file wrapper\n\n    This class provides a wrapper around files opened for\n    temporary use.  In particular, it seeks to automatically\n    remove the file when it is no longer needed.\n    \"\"\"\n\n    def __init__(self, file, name, delete=True):\n        self.file = file\n        self.name = name\n        self.close_called = False\n        self.delete = delete\n\n    def __getattr__(self, name):\n        # Attribute lookups are delegated to the underlying file\n        # and cached for non-numeric results\n        # (i.e. methods are cached, closed and friends are not)\n        file = self.__dict__['file']\n        a = getattr(file, name)\n        if not issubclass(type(a), type(0)):\n            setattr(self, name, a)\n        return a\n\n    # The underlying __enter__ method returns the wrong object\n    # (self.file) so override it to return the wrapper\n    def __enter__(self):\n        self.file.__enter__()\n        return self\n\n    # NT provides delete-on-close as a primitive, so we don't need\n    # the wrapper to do anything special.  We still use it so that\n    # file.name is useful (i.e. not \"(fdopen)\") with NamedTemporaryFile.\n    if _os.name != 'nt':\n        # Cache the unlinker so we don't get spurious errors at\n        # shutdown when the module-level \"os\" is None'd out.  Note\n        # that this must be referenced as self.unlink, because the\n        # name TemporaryFileWrapper may also get None'd out before\n        # __del__ is called.\n        unlink = _os.unlink\n\n        def close(self):\n            if not self.close_called:\n                self.close_called = True\n                self.file.close()\n                if self.delete:\n                    self.unlink(self.name)\n\n        def __del__(self):\n            self.close()\n\n        # Need to trap __exit__ as well to ensure the file gets\n        # deleted when used in a with statement\n        def __exit__(self, exc, value, tb):\n            result = self.file.__exit__(exc, value, tb)\n            self.close()\n            return result\n    else:\n        def __exit__(self, exc, value, tb):\n            self.file.__exit__(exc, value, tb)\n\n\ndef NamedTemporaryFile(mode='w+b', bufsize=-1, suffix=\"\",\n                       prefix=template, dir=None, delete=True):\n    \"\"\"Create and return a temporary file.\n    Arguments:\n    'prefix', 'suffix', 'dir' -- as for mkstemp.\n    'mode' -- the mode argument to os.fdopen (default \"w+b\").\n    'bufsize' -- the buffer size argument to os.fdopen (default -1).\n    'delete' -- whether the file is deleted on close (default True).\n    The file is created as mkstemp() would do it.\n\n    Returns an object with a file-like interface; the name of the file\n    is accessible as file.name.  The file will be automatically deleted\n    when it is closed unless the 'delete' argument is set to False.\n    \"\"\"\n\n    if dir is None:\n        dir = gettempdir()\n\n    if 'b' in mode:\n        flags = _bin_openflags\n    else:\n        flags = _text_openflags\n\n    # Setting O_TEMPORARY in the flags causes the OS to delete\n    # the file when it is closed.  This is only supported by Windows.\n    if _os.name == 'nt' and delete:\n        flags |= _os.O_TEMPORARY\n\n    (fd, name) = _mkstemp_inner(dir, prefix, suffix, flags)\n    try:\n        file = _os.fdopen(fd, mode, bufsize)\n        return _TemporaryFileWrapper(file, name, delete)\n    except:\n        _os.close(fd)\n        raise\n\nif _os.name != 'posix' or _os.sys.platform == 'cygwin':\n    # On non-POSIX and Cygwin systems, assume that we cannot unlink a file\n    # while it is open.\n    TemporaryFile = NamedTemporaryFile\n\nelse:\n    def TemporaryFile(mode='w+b', bufsize=-1, suffix=\"\",\n                      prefix=template, dir=None):\n        \"\"\"Create and return a temporary file.\n        Arguments:\n        'prefix', 'suffix', 'dir' -- as for mkstemp.\n        'mode' -- the mode argument to os.fdopen (default \"w+b\").\n        'bufsize' -- the buffer size argument to os.fdopen (default -1).\n        The file is created as mkstemp() would do it.\n\n        Returns an object with a file-like interface.  The file has no\n        name, and will cease to exist when it is closed.\n        \"\"\"\n\n        if dir is None:\n            dir = gettempdir()\n\n        if 'b' in mode:\n            flags = _bin_openflags\n        else:\n            flags = _text_openflags\n\n        (fd, name) = _mkstemp_inner(dir, prefix, suffix, flags)\n        try:\n            _os.unlink(name)\n            return _os.fdopen(fd, mode, bufsize)\n        except:\n            _os.close(fd)\n            raise\n\nclass SpooledTemporaryFile:\n    \"\"\"Temporary file wrapper, specialized to switch from\n    StringIO to a real file when it exceeds a certain size or\n    when a fileno is needed.\n    \"\"\"\n    _rolled = False\n\n    def __init__(self, max_size=0, mode='w+b', bufsize=-1,\n                 suffix=\"\", prefix=template, dir=None):\n        self._file = _StringIO()\n        self._max_size = max_size\n        self._rolled = False\n        self._TemporaryFileArgs = (mode, bufsize, suffix, prefix, dir)\n\n    def _check(self, file):\n        if self._rolled: return\n        max_size = self._max_size\n        if max_size and file.tell() > max_size:\n            self.rollover()\n\n    def rollover(self):\n        if self._rolled: return\n        file = self._file\n        newfile = self._file = TemporaryFile(*self._TemporaryFileArgs)\n        del self._TemporaryFileArgs\n\n        newfile.write(file.getvalue())\n        newfile.seek(file.tell(), 0)\n\n        self._rolled = True\n\n    # The method caching trick from NamedTemporaryFile\n    # won't work here, because _file may change from a\n    # _StringIO instance to a real file. So we list\n    # all the methods directly.\n\n    # Context management protocol\n    def __enter__(self):\n        if self._file.closed:\n            raise ValueError(\"Cannot enter context with closed file\")\n        return self\n\n    def __exit__(self, exc, value, tb):\n        self._file.close()\n\n    # file protocol\n    def __iter__(self):\n        return self._file.__iter__()\n\n    def close(self):\n        self._file.close()\n\n    @property\n    def closed(self):\n        return self._file.closed\n\n    def fileno(self):\n        self.rollover()\n        return self._file.fileno()\n\n    def flush(self):\n        self._file.flush()\n\n    def isatty(self):\n        return self._file.isatty()\n\n    @property\n    def mode(self):\n        try:\n            return self._file.mode\n        except AttributeError:\n            return self._TemporaryFileArgs[0]\n\n    @property\n    def name(self):\n        try:\n            return self._file.name\n        except AttributeError:\n            return None\n\n    def next(self):\n        return self._file.next\n\n    def read(self, *args):\n        return self._file.read(*args)\n\n    def readline(self, *args):\n        return self._file.readline(*args)\n\n    def readlines(self, *args):\n        return self._file.readlines(*args)\n\n    def seek(self, *args):\n        self._file.seek(*args)\n\n    @property\n    def softspace(self):\n        return self._file.softspace\n\n    def tell(self):\n        return self._file.tell()\n\n    def truncate(self):\n        self._file.truncate()\n\n    def write(self, s):\n        file = self._file\n        rv = file.write(s)\n        self._check(file)\n        return rv\n\n    def writelines(self, iterable):\n        file = self._file\n        rv = file.writelines(iterable)\n        self._check(file)\n        return rv\n\n    def xreadlines(self, *args):\n        if hasattr(self._file, 'xreadlines'):  # real file\n            return iter(self._file)\n        else:  # StringIO()\n            return iter(self._file.readlines(*args))\n", 
    "textwrap": "\"\"\"Text wrapping and filling.\n\"\"\"\n\n# Copyright (C) 1999-2001 Gregory P. Ward.\n# Copyright (C) 2002, 2003 Python Software Foundation.\n# Written by Greg Ward <gward@python.net>\n\n__revision__ = \"$Id$\"\n\nimport string, re\n\ntry:\n    _unicode = unicode\nexcept NameError:\n    # If Python is built without Unicode support, the unicode type\n    # will not exist. Fake one.\n    class _unicode(object):\n        pass\n\n# Do the right thing with boolean values for all known Python versions\n# (so this module can be copied to projects that don't depend on Python\n# 2.3, e.g. Optik and Docutils) by uncommenting the block of code below.\n#try:\n#    True, False\n#except NameError:\n#    (True, False) = (1, 0)\n\n__all__ = ['TextWrapper', 'wrap', 'fill', 'dedent']\n\n# Hardcode the recognized whitespace characters to the US-ASCII\n# whitespace characters.  The main reason for doing this is that in\n# ISO-8859-1, 0xa0 is non-breaking whitespace, so in certain locales\n# that character winds up in string.whitespace.  Respecting\n# string.whitespace in those cases would 1) make textwrap treat 0xa0 the\n# same as any other whitespace char, which is clearly wrong (it's a\n# *non-breaking* space), 2) possibly cause problems with Unicode,\n# since 0xa0 is not in range(128).\n_whitespace = '\\t\\n\\x0b\\x0c\\r '\n\nclass TextWrapper:\n    \"\"\"\n    Object for wrapping/filling text.  The public interface consists of\n    the wrap() and fill() methods; the other methods are just there for\n    subclasses to override in order to tweak the default behaviour.\n    If you want to completely replace the main wrapping algorithm,\n    you'll probably have to override _wrap_chunks().\n\n    Several instance attributes control various aspects of wrapping:\n      width (default: 70)\n        the maximum width of wrapped lines (unless break_long_words\n        is false)\n      initial_indent (default: \"\")\n        string that will be prepended to the first line of wrapped\n        output.  Counts towards the line's width.\n      subsequent_indent (default: \"\")\n        string that will be prepended to all lines save the first\n        of wrapped output; also counts towards each line's width.\n      expand_tabs (default: true)\n        Expand tabs in input text to spaces before further processing.\n        Each tab will become 1 .. 8 spaces, depending on its position in\n        its line.  If false, each tab is treated as a single character.\n      replace_whitespace (default: true)\n        Replace all whitespace characters in the input text by spaces\n        after tab expansion.  Note that if expand_tabs is false and\n        replace_whitespace is true, every tab will be converted to a\n        single space!\n      fix_sentence_endings (default: false)\n        Ensure that sentence-ending punctuation is always followed\n        by two spaces.  Off by default because the algorithm is\n        (unavoidably) imperfect.\n      break_long_words (default: true)\n        Break words longer than 'width'.  If false, those words will not\n        be broken, and some lines might be longer than 'width'.\n      break_on_hyphens (default: true)\n        Allow breaking hyphenated words. If true, wrapping will occur\n        preferably on whitespaces and right after hyphens part of\n        compound words.\n      drop_whitespace (default: true)\n        Drop leading and trailing whitespace from lines.\n    \"\"\"\n\n    whitespace_trans = string.maketrans(_whitespace, ' ' * len(_whitespace))\n\n    unicode_whitespace_trans = {}\n    uspace = ord(u' ')\n    for x in map(ord, _whitespace):\n        unicode_whitespace_trans[x] = uspace\n\n    # This funky little regex is just the trick for splitting\n    # text up into word-wrappable chunks.  E.g.\n    #   \"Hello there -- you goof-ball, use the -b option!\"\n    # splits into\n    #   Hello/ /there/ /--/ /you/ /goof-/ball,/ /use/ /the/ /-b/ /option!\n    # (after stripping out empty strings).\n    wordsep_re = re.compile(\n        r'(\\s+|'                                  # any whitespace\n        r'[^\\s\\w]*\\w+[^0-9\\W]-(?=\\w+[^0-9\\W])|'   # hyphenated words\n        r'(?<=[\\w\\!\\\"\\'\\&\\.\\,\\?])-{2,}(?=\\w))')   # em-dash\n\n    # This less funky little regex just split on recognized spaces. E.g.\n    #   \"Hello there -- you goof-ball, use the -b option!\"\n    # splits into\n    #   Hello/ /there/ /--/ /you/ /goof-ball,/ /use/ /the/ /-b/ /option!/\n    wordsep_simple_re = re.compile(r'(\\s+)')\n\n    # XXX this is not locale- or charset-aware -- string.lowercase\n    # is US-ASCII only (and therefore English-only)\n    sentence_end_re = re.compile(r'[%s]'              # lowercase letter\n                                 r'[\\.\\!\\?]'          # sentence-ending punct.\n                                 r'[\\\"\\']?'           # optional end-of-quote\n                                 r'\\Z'                # end of chunk\n                                 % string.lowercase)\n\n\n    def __init__(self,\n                 width=70,\n                 initial_indent=\"\",\n                 subsequent_indent=\"\",\n                 expand_tabs=True,\n                 replace_whitespace=True,\n                 fix_sentence_endings=False,\n                 break_long_words=True,\n                 drop_whitespace=True,\n                 break_on_hyphens=True):\n        self.width = width\n        self.initial_indent = initial_indent\n        self.subsequent_indent = subsequent_indent\n        self.expand_tabs = expand_tabs\n        self.replace_whitespace = replace_whitespace\n        self.fix_sentence_endings = fix_sentence_endings\n        self.break_long_words = break_long_words\n        self.drop_whitespace = drop_whitespace\n        self.break_on_hyphens = break_on_hyphens\n\n        # recompile the regexes for Unicode mode -- done in this clumsy way for\n        # backwards compatibility because it's rather common to monkey-patch\n        # the TextWrapper class' wordsep_re attribute.\n        self.wordsep_re_uni = re.compile(self.wordsep_re.pattern, re.U)\n        self.wordsep_simple_re_uni = re.compile(\n            self.wordsep_simple_re.pattern, re.U)\n\n\n    # -- Private methods -----------------------------------------------\n    # (possibly useful for subclasses to override)\n\n    def _munge_whitespace(self, text):\n        \"\"\"_munge_whitespace(text : string) -> string\n\n        Munge whitespace in text: expand tabs and convert all other\n        whitespace characters to spaces.  Eg. \" foo\\tbar\\n\\nbaz\"\n        becomes \" foo    bar  baz\".\n        \"\"\"\n        if self.expand_tabs:\n            text = text.expandtabs()\n        if self.replace_whitespace:\n            if isinstance(text, str):\n                text = text.translate(self.whitespace_trans)\n            elif isinstance(text, _unicode):\n                text = text.translate(self.unicode_whitespace_trans)\n        return text\n\n\n    def _split(self, text):\n        \"\"\"_split(text : string) -> [string]\n\n        Split the text to wrap into indivisible chunks.  Chunks are\n        not quite the same as words; see _wrap_chunks() for full\n        details.  As an example, the text\n          Look, goof-ball -- use the -b option!\n        breaks into the following chunks:\n          'Look,', ' ', 'goof-', 'ball', ' ', '--', ' ',\n          'use', ' ', 'the', ' ', '-b', ' ', 'option!'\n        if break_on_hyphens is True, or in:\n          'Look,', ' ', 'goof-ball', ' ', '--', ' ',\n          'use', ' ', 'the', ' ', '-b', ' ', option!'\n        otherwise.\n        \"\"\"\n        if isinstance(text, _unicode):\n            if self.break_on_hyphens:\n                pat = self.wordsep_re_uni\n            else:\n                pat = self.wordsep_simple_re_uni\n        else:\n            if self.break_on_hyphens:\n                pat = self.wordsep_re\n            else:\n                pat = self.wordsep_simple_re\n        chunks = pat.split(text)\n        chunks = filter(None, chunks)  # remove empty chunks\n        return chunks\n\n    def _fix_sentence_endings(self, chunks):\n        \"\"\"_fix_sentence_endings(chunks : [string])\n\n        Correct for sentence endings buried in 'chunks'.  Eg. when the\n        original text contains \"... foo.\\nBar ...\", munge_whitespace()\n        and split() will convert that to [..., \"foo.\", \" \", \"Bar\", ...]\n        which has one too few spaces; this method simply changes the one\n        space to two.\n        \"\"\"\n        i = 0\n        patsearch = self.sentence_end_re.search\n        while i < len(chunks)-1:\n            if chunks[i+1] == \" \" and patsearch(chunks[i]):\n                chunks[i+1] = \"  \"\n                i += 2\n            else:\n                i += 1\n\n    def _handle_long_word(self, reversed_chunks, cur_line, cur_len, width):\n        \"\"\"_handle_long_word(chunks : [string],\n                             cur_line : [string],\n                             cur_len : int, width : int)\n\n        Handle a chunk of text (most likely a word, not whitespace) that\n        is too long to fit in any line.\n        \"\"\"\n        # Figure out when indent is larger than the specified width, and make\n        # sure at least one character is stripped off on every pass\n        if width < 1:\n            space_left = 1\n        else:\n            space_left = width - cur_len\n\n        # If we're allowed to break long words, then do so: put as much\n        # of the next chunk onto the current line as will fit.\n        if self.break_long_words:\n            cur_line.append(reversed_chunks[-1][:space_left])\n            reversed_chunks[-1] = reversed_chunks[-1][space_left:]\n\n        # Otherwise, we have to preserve the long word intact.  Only add\n        # it to the current line if there's nothing already there --\n        # that minimizes how much we violate the width constraint.\n        elif not cur_line:\n            cur_line.append(reversed_chunks.pop())\n\n        # If we're not allowed to break long words, and there's already\n        # text on the current line, do nothing.  Next time through the\n        # main loop of _wrap_chunks(), we'll wind up here again, but\n        # cur_len will be zero, so the next line will be entirely\n        # devoted to the long word that we can't handle right now.\n\n    def _wrap_chunks(self, chunks):\n        \"\"\"_wrap_chunks(chunks : [string]) -> [string]\n\n        Wrap a sequence of text chunks and return a list of lines of\n        length 'self.width' or less.  (If 'break_long_words' is false,\n        some lines may be longer than this.)  Chunks correspond roughly\n        to words and the whitespace between them: each chunk is\n        indivisible (modulo 'break_long_words'), but a line break can\n        come between any two chunks.  Chunks should not have internal\n        whitespace; ie. a chunk is either all whitespace or a \"word\".\n        Whitespace chunks will be removed from the beginning and end of\n        lines, but apart from that whitespace is preserved.\n        \"\"\"\n        lines = []\n        if self.width <= 0:\n            raise ValueError(\"invalid width %r (must be > 0)\" % self.width)\n\n        # Arrange in reverse order so items can be efficiently popped\n        # from a stack of chucks.\n        chunks.reverse()\n\n        while chunks:\n\n            # Start the list of chunks that will make up the current line.\n            # cur_len is just the length of all the chunks in cur_line.\n            cur_line = []\n            cur_len = 0\n\n            # Figure out which static string will prefix this line.\n            if lines:\n                indent = self.subsequent_indent\n            else:\n                indent = self.initial_indent\n\n            # Maximum width for this line.\n            width = self.width - len(indent)\n\n            # First chunk on line is whitespace -- drop it, unless this\n            # is the very beginning of the text (ie. no lines started yet).\n            if self.drop_whitespace and chunks[-1].strip() == '' and lines:\n                del chunks[-1]\n\n            while chunks:\n                l = len(chunks[-1])\n\n                # Can at least squeeze this chunk onto the current line.\n                if cur_len + l <= width:\n                    cur_line.append(chunks.pop())\n                    cur_len += l\n\n                # Nope, this line is full.\n                else:\n                    break\n\n            # The current line is full, and the next chunk is too big to\n            # fit on *any* line (not just this one).\n            if chunks and len(chunks[-1]) > width:\n                self._handle_long_word(chunks, cur_line, cur_len, width)\n\n            # If the last chunk on this line is all whitespace, drop it.\n            if self.drop_whitespace and cur_line and cur_line[-1].strip() == '':\n                del cur_line[-1]\n\n            # Convert current line back to a string and store it in list\n            # of all lines (return value).\n            if cur_line:\n                lines.append(indent + ''.join(cur_line))\n\n        return lines\n\n\n    # -- Public interface ----------------------------------------------\n\n    def wrap(self, text):\n        \"\"\"wrap(text : string) -> [string]\n\n        Reformat the single paragraph in 'text' so it fits in lines of\n        no more than 'self.width' columns, and return a list of wrapped\n        lines.  Tabs in 'text' are expanded with string.expandtabs(),\n        and all other whitespace characters (including newline) are\n        converted to space.\n        \"\"\"\n        text = self._munge_whitespace(text)\n        chunks = self._split(text)\n        if self.fix_sentence_endings:\n            self._fix_sentence_endings(chunks)\n        return self._wrap_chunks(chunks)\n\n    def fill(self, text):\n        \"\"\"fill(text : string) -> string\n\n        Reformat the single paragraph in 'text' to fit in lines of no\n        more than 'self.width' columns, and return a new string\n        containing the entire wrapped paragraph.\n        \"\"\"\n        return \"\\n\".join(self.wrap(text))\n\n\n# -- Convenience interface ---------------------------------------------\n\ndef wrap(text, width=70, **kwargs):\n    \"\"\"Wrap a single paragraph of text, returning a list of wrapped lines.\n\n    Reformat the single paragraph in 'text' so it fits in lines of no\n    more than 'width' columns, and return a list of wrapped lines.  By\n    default, tabs in 'text' are expanded with string.expandtabs(), and\n    all other whitespace characters (including newline) are converted to\n    space.  See TextWrapper class for available keyword args to customize\n    wrapping behaviour.\n    \"\"\"\n    w = TextWrapper(width=width, **kwargs)\n    return w.wrap(text)\n\ndef fill(text, width=70, **kwargs):\n    \"\"\"Fill a single paragraph of text, returning a new string.\n\n    Reformat the single paragraph in 'text' to fit in lines of no more\n    than 'width' columns, and return a new string containing the entire\n    wrapped paragraph.  As with wrap(), tabs are expanded and other\n    whitespace characters converted to space.  See TextWrapper class for\n    available keyword args to customize wrapping behaviour.\n    \"\"\"\n    w = TextWrapper(width=width, **kwargs)\n    return w.fill(text)\n\n\n# -- Loosely related functionality -------------------------------------\n\n_whitespace_only_re = re.compile('^[ \\t]+$', re.MULTILINE)\n_leading_whitespace_re = re.compile('(^[ \\t]*)(?:[^ \\t\\n])', re.MULTILINE)\n\ndef dedent(text):\n    \"\"\"Remove any common leading whitespace from every line in `text`.\n\n    This can be used to make triple-quoted strings line up with the left\n    edge of the display, while still presenting them in the source code\n    in indented form.\n\n    Note that tabs and spaces are both treated as whitespace, but they\n    are not equal: the lines \"  hello\" and \"\\thello\" are\n    considered to have no common leading whitespace.  (This behaviour is\n    new in Python 2.5; older versions of this module incorrectly\n    expanded tabs before searching for common leading whitespace.)\n    \"\"\"\n    # Look for the longest leading string of spaces and tabs common to\n    # all lines.\n    margin = None\n    text = _whitespace_only_re.sub('', text)\n    indents = _leading_whitespace_re.findall(text)\n    for indent in indents:\n        if margin is None:\n            margin = indent\n\n        # Current line more deeply indented than previous winner:\n        # no change (previous winner is still on top).\n        elif indent.startswith(margin):\n            pass\n\n        # Current line consistent with and no deeper than previous winner:\n        # it's the new winner.\n        elif margin.startswith(indent):\n            margin = indent\n\n        # Current line and previous winner have no common whitespace:\n        # there is no margin.\n        else:\n            margin = \"\"\n            break\n\n    # sanity check (testing/debugging only)\n    if 0 and margin:\n        for line in text.split(\"\\n\"):\n            assert not line or line.startswith(margin), \\\n                   \"line = %r, margin = %r\" % (line, margin)\n\n    if margin:\n        text = re.sub(r'(?m)^' + margin, '', text)\n    return text\n\nif __name__ == \"__main__\":\n    #print dedent(\"\\tfoo\\n\\tbar\")\n    #print dedent(\"  \\thello there\\n  \\t  how are you?\")\n    print dedent(\"Hello there.\\n  This is indented.\")\n", 
    "threading": "# Hack for making promise work in pypyjs\n\ndef do_nothing(*args, **kwargs):\n\tpass\n\nclass RLock(object):\n    __init__ = do_nothing\n    __enter__ = do_nothing\n    __exit__ = do_nothing\n    acquire = do_nothing\n    release = do_nothing\n\nclass Event(object):\n    __init__ = do_nothing\n    set = do_nothing\n    wait = do_nothing\n\n_shutdown = do_nothing\n", 
    "token": "\"\"\"Token constants (from \"token.h\").\"\"\"\n\n#  This file is automatically generated; please don't muck it up!\n#\n#  To update the symbols in this file, 'cd' to the top directory of\n#  the python source tree after building the interpreter and run:\n#\n#    ./python Lib/token.py\n\n#--start constants--\nENDMARKER = 0\nNAME = 1\nNUMBER = 2\nSTRING = 3\nNEWLINE = 4\nINDENT = 5\nDEDENT = 6\nLPAR = 7\nRPAR = 8\nLSQB = 9\nRSQB = 10\nCOLON = 11\nCOMMA = 12\nSEMI = 13\nPLUS = 14\nMINUS = 15\nSTAR = 16\nSLASH = 17\nVBAR = 18\nAMPER = 19\nLESS = 20\nGREATER = 21\nEQUAL = 22\nDOT = 23\nPERCENT = 24\nBACKQUOTE = 25\nLBRACE = 26\nRBRACE = 27\nEQEQUAL = 28\nNOTEQUAL = 29\nLESSEQUAL = 30\nGREATEREQUAL = 31\nTILDE = 32\nCIRCUMFLEX = 33\nLEFTSHIFT = 34\nRIGHTSHIFT = 35\nDOUBLESTAR = 36\nPLUSEQUAL = 37\nMINEQUAL = 38\nSTAREQUAL = 39\nSLASHEQUAL = 40\nPERCENTEQUAL = 41\nAMPEREQUAL = 42\nVBAREQUAL = 43\nCIRCUMFLEXEQUAL = 44\nLEFTSHIFTEQUAL = 45\nRIGHTSHIFTEQUAL = 46\nDOUBLESTAREQUAL = 47\nDOUBLESLASH = 48\nDOUBLESLASHEQUAL = 49\nAT = 50\nOP = 51\nERRORTOKEN = 52\nN_TOKENS = 53\nNT_OFFSET = 256\n#--end constants--\n\ntok_name = {}\nfor _name, _value in globals().items():\n    if type(_value) is type(0):\n        tok_name[_value] = _name\ndel _name, _value\n\n\ndef ISTERMINAL(x):\n    return x < NT_OFFSET\n\ndef ISNONTERMINAL(x):\n    return x >= NT_OFFSET\n\ndef ISEOF(x):\n    return x == ENDMARKER\n\n\ndef main():\n    import re\n    import sys\n    args = sys.argv[1:]\n    inFileName = args and args[0] or \"Include/token.h\"\n    outFileName = \"Lib/token.py\"\n    if len(args) > 1:\n        outFileName = args[1]\n    try:\n        fp = open(inFileName)\n    except IOError, err:\n        sys.stdout.write(\"I/O error: %s\\n\" % str(err))\n        sys.exit(1)\n    lines = fp.read().split(\"\\n\")\n    fp.close()\n    prog = re.compile(\n        \"#define[ \\t][ \\t]*([A-Z0-9][A-Z0-9_]*)[ \\t][ \\t]*([0-9][0-9]*)\",\n        re.IGNORECASE)\n    tokens = {}\n    for line in lines:\n        match = prog.match(line)\n        if match:\n            name, val = match.group(1, 2)\n            val = int(val)\n            tokens[val] = name          # reverse so we can sort them...\n    keys = tokens.keys()\n    keys.sort()\n    # load the output skeleton from the target:\n    try:\n        fp = open(outFileName)\n    except IOError, err:\n        sys.stderr.write(\"I/O error: %s\\n\" % str(err))\n        sys.exit(2)\n    format = fp.read().split(\"\\n\")\n    fp.close()\n    try:\n        start = format.index(\"#--start constants--\") + 1\n        end = format.index(\"#--end constants--\")\n    except ValueError:\n        sys.stderr.write(\"target does not contain format markers\")\n        sys.exit(3)\n    lines = []\n    for val in keys:\n        lines.append(\"%s = %d\" % (tokens[val], val))\n    format[start:end] = lines\n    try:\n        fp = open(outFileName, 'w')\n    except IOError, err:\n        sys.stderr.write(\"I/O error: %s\\n\" % str(err))\n        sys.exit(4)\n    fp.write(\"\\n\".join(format))\n    fp.close()\n\n\nif __name__ == \"__main__\":\n    main()\n", 
    "tokenize": "\"\"\"Tokenization help for Python programs.\n\ngenerate_tokens(readline) is a generator that breaks a stream of\ntext into Python tokens.  It accepts a readline-like method which is called\nrepeatedly to get the next line of input (or \"\" for EOF).  It generates\n5-tuples with these members:\n\n    the token type (see token.py)\n    the token (a string)\n    the starting (row, column) indices of the token (a 2-tuple of ints)\n    the ending (row, column) indices of the token (a 2-tuple of ints)\n    the original line (string)\n\nIt is designed to match the working of the Python tokenizer exactly, except\nthat it produces COMMENT tokens for comments and gives type OP for all\noperators\n\nOlder entry points\n    tokenize_loop(readline, tokeneater)\n    tokenize(readline, tokeneater=printtoken)\nare the same, except instead of generating tokens, tokeneater is a callback\nfunction to which the 5 fields described above are passed as 5 arguments,\neach time a new token is found.\"\"\"\n\n__author__ = 'Ka-Ping Yee <ping@lfw.org>'\n__credits__ = ('GvR, ESR, Tim Peters, Thomas Wouters, Fred Drake, '\n               'Skip Montanaro, Raymond Hettinger')\n\nfrom itertools import chain\nimport string, re\nfrom token import *\n\nimport token\n__all__ = [x for x in dir(token) if not x.startswith(\"_\")]\n__all__ += [\"COMMENT\", \"tokenize\", \"generate_tokens\", \"NL\", \"untokenize\"]\ndel x\ndel token\n\nCOMMENT = N_TOKENS\ntok_name[COMMENT] = 'COMMENT'\nNL = N_TOKENS + 1\ntok_name[NL] = 'NL'\nN_TOKENS += 2\n\ndef group(*choices): return '(' + '|'.join(choices) + ')'\ndef any(*choices): return group(*choices) + '*'\ndef maybe(*choices): return group(*choices) + '?'\n\nWhitespace = r'[ \\f\\t]*'\nComment = r'#[^\\r\\n]*'\nIgnore = Whitespace + any(r'\\\\\\r?\\n' + Whitespace) + maybe(Comment)\nName = r'[a-zA-Z_]\\w*'\n\nHexnumber = r'0[xX][\\da-fA-F]+[lL]?'\nOctnumber = r'(0[oO][0-7]+)|(0[0-7]*)[lL]?'\nBinnumber = r'0[bB][01]+[lL]?'\nDecnumber = r'[1-9]\\d*[lL]?'\nIntnumber = group(Hexnumber, Binnumber, Octnumber, Decnumber)\nExponent = r'[eE][-+]?\\d+'\nPointfloat = group(r'\\d+\\.\\d*', r'\\.\\d+') + maybe(Exponent)\nExpfloat = r'\\d+' + Exponent\nFloatnumber = group(Pointfloat, Expfloat)\nImagnumber = group(r'\\d+[jJ]', Floatnumber + r'[jJ]')\nNumber = group(Imagnumber, Floatnumber, Intnumber)\n\n# Tail end of ' string.\nSingle = r\"[^'\\\\]*(?:\\\\.[^'\\\\]*)*'\"\n# Tail end of \" string.\nDouble = r'[^\"\\\\]*(?:\\\\.[^\"\\\\]*)*\"'\n# Tail end of ''' string.\nSingle3 = r\"[^'\\\\]*(?:(?:\\\\.|'(?!''))[^'\\\\]*)*'''\"\n# Tail end of \"\"\" string.\nDouble3 = r'[^\"\\\\]*(?:(?:\\\\.|\"(?!\"\"))[^\"\\\\]*)*\"\"\"'\nTriple = group(\"[uUbB]?[rR]?'''\", '[uUbB]?[rR]?\"\"\"')\n# Single-line ' or \" string.\nString = group(r\"[uUbB]?[rR]?'[^\\n'\\\\]*(?:\\\\.[^\\n'\\\\]*)*'\",\n               r'[uUbB]?[rR]?\"[^\\n\"\\\\]*(?:\\\\.[^\\n\"\\\\]*)*\"')\n\n# Because of leftmost-then-longest match semantics, be sure to put the\n# longest operators first (e.g., if = came before ==, == would get\n# recognized as two instances of =).\nOperator = group(r\"\\*\\*=?\", r\">>=?\", r\"<<=?\", r\"<>\", r\"!=\",\n                 r\"//=?\",\n                 r\"[+\\-*/%&|^=<>]=?\",\n                 r\"~\")\n\nBracket = '[][(){}]'\nSpecial = group(r'\\r?\\n', r'[:;.,`@]')\nFunny = group(Operator, Bracket, Special)\n\nPlainToken = group(Number, Funny, String, Name)\nToken = Ignore + PlainToken\n\n# First (or only) line of ' or \" string.\nContStr = group(r\"[uUbB]?[rR]?'[^\\n'\\\\]*(?:\\\\.[^\\n'\\\\]*)*\" +\n                group(\"'\", r'\\\\\\r?\\n'),\n                r'[uUbB]?[rR]?\"[^\\n\"\\\\]*(?:\\\\.[^\\n\"\\\\]*)*' +\n                group('\"', r'\\\\\\r?\\n'))\nPseudoExtras = group(r'\\\\\\r?\\n|\\Z', Comment, Triple)\nPseudoToken = Whitespace + group(PseudoExtras, Number, Funny, ContStr, Name)\n\ntokenprog, pseudoprog, single3prog, double3prog = map(\n    re.compile, (Token, PseudoToken, Single3, Double3))\nendprogs = {\"'\": re.compile(Single), '\"': re.compile(Double),\n            \"'''\": single3prog, '\"\"\"': double3prog,\n            \"r'''\": single3prog, 'r\"\"\"': double3prog,\n            \"u'''\": single3prog, 'u\"\"\"': double3prog,\n            \"ur'''\": single3prog, 'ur\"\"\"': double3prog,\n            \"R'''\": single3prog, 'R\"\"\"': double3prog,\n            \"U'''\": single3prog, 'U\"\"\"': double3prog,\n            \"uR'''\": single3prog, 'uR\"\"\"': double3prog,\n            \"Ur'''\": single3prog, 'Ur\"\"\"': double3prog,\n            \"UR'''\": single3prog, 'UR\"\"\"': double3prog,\n            \"b'''\": single3prog, 'b\"\"\"': double3prog,\n            \"br'''\": single3prog, 'br\"\"\"': double3prog,\n            \"B'''\": single3prog, 'B\"\"\"': double3prog,\n            \"bR'''\": single3prog, 'bR\"\"\"': double3prog,\n            \"Br'''\": single3prog, 'Br\"\"\"': double3prog,\n            \"BR'''\": single3prog, 'BR\"\"\"': double3prog,\n            'r': None, 'R': None, 'u': None, 'U': None,\n            'b': None, 'B': None}\n\ntriple_quoted = {}\nfor t in (\"'''\", '\"\"\"',\n          \"r'''\", 'r\"\"\"', \"R'''\", 'R\"\"\"',\n          \"u'''\", 'u\"\"\"', \"U'''\", 'U\"\"\"',\n          \"ur'''\", 'ur\"\"\"', \"Ur'''\", 'Ur\"\"\"',\n          \"uR'''\", 'uR\"\"\"', \"UR'''\", 'UR\"\"\"',\n          \"b'''\", 'b\"\"\"', \"B'''\", 'B\"\"\"',\n          \"br'''\", 'br\"\"\"', \"Br'''\", 'Br\"\"\"',\n          \"bR'''\", 'bR\"\"\"', \"BR'''\", 'BR\"\"\"'):\n    triple_quoted[t] = t\nsingle_quoted = {}\nfor t in (\"'\", '\"',\n          \"r'\", 'r\"', \"R'\", 'R\"',\n          \"u'\", 'u\"', \"U'\", 'U\"',\n          \"ur'\", 'ur\"', \"Ur'\", 'Ur\"',\n          \"uR'\", 'uR\"', \"UR'\", 'UR\"',\n          \"b'\", 'b\"', \"B'\", 'B\"',\n          \"br'\", 'br\"', \"Br'\", 'Br\"',\n          \"bR'\", 'bR\"', \"BR'\", 'BR\"' ):\n    single_quoted[t] = t\n\ntabsize = 8\n\nclass TokenError(Exception): pass\n\nclass StopTokenizing(Exception): pass\n\ndef printtoken(type, token, srow_scol, erow_ecol, line): # for testing\n    srow, scol = srow_scol\n    erow, ecol = erow_ecol\n    print \"%d,%d-%d,%d:\\t%s\\t%s\" % \\\n        (srow, scol, erow, ecol, tok_name[type], repr(token))\n\ndef tokenize(readline, tokeneater=printtoken):\n    \"\"\"\n    The tokenize() function accepts two parameters: one representing the\n    input stream, and one providing an output mechanism for tokenize().\n\n    The first parameter, readline, must be a callable object which provides\n    the same interface as the readline() method of built-in file objects.\n    Each call to the function should return one line of input as a string.\n\n    The second parameter, tokeneater, must also be a callable object. It is\n    called once for each token, with five arguments, corresponding to the\n    tuples generated by generate_tokens().\n    \"\"\"\n    try:\n        tokenize_loop(readline, tokeneater)\n    except StopTokenizing:\n        pass\n\n# backwards compatible interface\ndef tokenize_loop(readline, tokeneater):\n    for token_info in generate_tokens(readline):\n        tokeneater(*token_info)\n\nclass Untokenizer:\n\n    def __init__(self):\n        self.tokens = []\n        self.prev_row = 1\n        self.prev_col = 0\n\n    def add_whitespace(self, start):\n        row, col = start\n        if row < self.prev_row or row == self.prev_row and col < self.prev_col:\n            raise ValueError(\"start ({},{}) precedes previous end ({},{})\"\n                             .format(row, col, self.prev_row, self.prev_col))\n        row_offset = row - self.prev_row\n        if row_offset:\n            self.tokens.append(\"\\\\\\n\" * row_offset)\n            self.prev_col = 0\n        col_offset = col - self.prev_col\n        if col_offset:\n            self.tokens.append(\" \" * col_offset)\n\n    def untokenize(self, iterable):\n        it = iter(iterable)\n        for t in it:\n            if len(t) == 2:\n                self.compat(t, it)\n                break\n            tok_type, token, start, end, line = t\n            if tok_type == ENDMARKER:\n                break\n            self.add_whitespace(start)\n            self.tokens.append(token)\n            self.prev_row, self.prev_col = end\n            if tok_type in (NEWLINE, NL):\n                self.prev_row += 1\n                self.prev_col = 0\n        return \"\".join(self.tokens)\n\n    def compat(self, token, iterable):\n        indents = []\n        toks_append = self.tokens.append\n        startline = token[0] in (NEWLINE, NL)\n        prevstring = False\n\n        for tok in chain([token], iterable):\n            toknum, tokval = tok[:2]\n\n            if toknum in (NAME, NUMBER):\n                tokval += ' '\n\n            # Insert a space between two consecutive strings\n            if toknum == STRING:\n                if prevstring:\n                    tokval = ' ' + tokval\n                prevstring = True\n            else:\n                prevstring = False\n\n            if toknum == INDENT:\n                indents.append(tokval)\n                continue\n            elif toknum == DEDENT:\n                indents.pop()\n                continue\n            elif toknum in (NEWLINE, NL):\n                startline = True\n            elif startline and indents:\n                toks_append(indents[-1])\n                startline = False\n            toks_append(tokval)\n\ndef untokenize(iterable):\n    \"\"\"Transform tokens back into Python source code.\n\n    Each element returned by the iterable must be a token sequence\n    with at least two elements, a token number and token value.  If\n    only two tokens are passed, the resulting output is poor.\n\n    Round-trip invariant for full input:\n        Untokenized source will match input source exactly\n\n    Round-trip invariant for limited intput:\n        # Output text will tokenize the back to the input\n        t1 = [tok[:2] for tok in generate_tokens(f.readline)]\n        newcode = untokenize(t1)\n        readline = iter(newcode.splitlines(1)).next\n        t2 = [tok[:2] for tok in generate_tokens(readline)]\n        assert t1 == t2\n    \"\"\"\n    ut = Untokenizer()\n    return ut.untokenize(iterable)\n\ndef generate_tokens(readline):\n    \"\"\"\n    The generate_tokens() generator requires one argument, readline, which\n    must be a callable object which provides the same interface as the\n    readline() method of built-in file objects. Each call to the function\n    should return one line of input as a string.  Alternately, readline\n    can be a callable function terminating with StopIteration:\n        readline = open(myfile).next    # Example of alternate readline\n\n    The generator produces 5-tuples with these members: the token type; the\n    token string; a 2-tuple (srow, scol) of ints specifying the row and\n    column where the token begins in the source; a 2-tuple (erow, ecol) of\n    ints specifying the row and column where the token ends in the source;\n    and the line on which the token was found. The line passed is the\n    logical line; continuation lines are included.\n    \"\"\"\n    lnum = parenlev = continued = 0\n    namechars, numchars = string.ascii_letters + '_', '0123456789'\n    contstr, needcont = '', 0\n    contline = None\n    indents = [0]\n\n    while 1:                                   # loop over lines in stream\n        try:\n            line = readline()\n        except StopIteration:\n            line = ''\n        lnum += 1\n        pos, max = 0, len(line)\n\n        if contstr:                            # continued string\n            if not line:\n                raise TokenError, (\"EOF in multi-line string\", strstart)\n            endmatch = endprog.match(line)\n            if endmatch:\n                pos = end = endmatch.end(0)\n                yield (STRING, contstr + line[:end],\n                       strstart, (lnum, end), contline + line)\n                contstr, needcont = '', 0\n                contline = None\n            elif needcont and line[-2:] != '\\\\\\n' and line[-3:] != '\\\\\\r\\n':\n                yield (ERRORTOKEN, contstr + line,\n                           strstart, (lnum, len(line)), contline)\n                contstr = ''\n                contline = None\n                continue\n            else:\n                contstr = contstr + line\n                contline = contline + line\n                continue\n\n        elif parenlev == 0 and not continued:  # new statement\n            if not line: break\n            column = 0\n            while pos < max:                   # measure leading whitespace\n                if line[pos] == ' ':\n                    column += 1\n                elif line[pos] == '\\t':\n                    column = (column//tabsize + 1)*tabsize\n                elif line[pos] == '\\f':\n                    column = 0\n                else:\n                    break\n                pos += 1\n            if pos == max:\n                break\n\n            if line[pos] in '#\\r\\n':           # skip comments or blank lines\n                if line[pos] == '#':\n                    comment_token = line[pos:].rstrip('\\r\\n')\n                    nl_pos = pos + len(comment_token)\n                    yield (COMMENT, comment_token,\n                           (lnum, pos), (lnum, pos + len(comment_token)), line)\n                    yield (NL, line[nl_pos:],\n                           (lnum, nl_pos), (lnum, len(line)), line)\n                else:\n                    yield ((NL, COMMENT)[line[pos] == '#'], line[pos:],\n                           (lnum, pos), (lnum, len(line)), line)\n                continue\n\n            if column > indents[-1]:           # count indents or dedents\n                indents.append(column)\n                yield (INDENT, line[:pos], (lnum, 0), (lnum, pos), line)\n            while column < indents[-1]:\n                if column not in indents:\n                    raise IndentationError(\n                        \"unindent does not match any outer indentation level\",\n                        (\"<tokenize>\", lnum, pos, line))\n                indents = indents[:-1]\n                yield (DEDENT, '', (lnum, pos), (lnum, pos), line)\n\n        else:                                  # continued statement\n            if not line:\n                raise TokenError, (\"EOF in multi-line statement\", (lnum, 0))\n            continued = 0\n\n        while pos < max:\n            pseudomatch = pseudoprog.match(line, pos)\n            if pseudomatch:                                # scan for tokens\n                start, end = pseudomatch.span(1)\n                spos, epos, pos = (lnum, start), (lnum, end), end\n                if start == end:\n                    continue\n                token, initial = line[start:end], line[start]\n\n                if initial in numchars or \\\n                   (initial == '.' and token != '.'):      # ordinary number\n                    yield (NUMBER, token, spos, epos, line)\n                elif initial in '\\r\\n':\n                    yield (NL if parenlev > 0 else NEWLINE,\n                           token, spos, epos, line)\n                elif initial == '#':\n                    assert not token.endswith(\"\\n\")\n                    yield (COMMENT, token, spos, epos, line)\n                elif token in triple_quoted:\n                    endprog = endprogs[token]\n                    endmatch = endprog.match(line, pos)\n                    if endmatch:                           # all on one line\n                        pos = endmatch.end(0)\n                        token = line[start:pos]\n                        yield (STRING, token, spos, (lnum, pos), line)\n                    else:\n                        strstart = (lnum, start)           # multiple lines\n                        contstr = line[start:]\n                        contline = line\n                        break\n                elif initial in single_quoted or \\\n                    token[:2] in single_quoted or \\\n                    token[:3] in single_quoted:\n                    if token[-1] == '\\n':                  # continued string\n                        strstart = (lnum, start)\n                        endprog = (endprogs[initial] or endprogs[token[1]] or\n                                   endprogs[token[2]])\n                        contstr, needcont = line[start:], 1\n                        contline = line\n                        break\n                    else:                                  # ordinary string\n                        yield (STRING, token, spos, epos, line)\n                elif initial in namechars:                 # ordinary name\n                    yield (NAME, token, spos, epos, line)\n                elif initial == '\\\\':                      # continued stmt\n                    continued = 1\n                else:\n                    if initial in '([{':\n                        parenlev += 1\n                    elif initial in ')]}':\n                        parenlev -= 1\n                    yield (OP, token, spos, epos, line)\n            else:\n                yield (ERRORTOKEN, line[pos],\n                           (lnum, pos), (lnum, pos+1), line)\n                pos += 1\n\n    for indent in indents[1:]:                 # pop remaining indent levels\n        yield (DEDENT, '', (lnum, 0), (lnum, 0), '')\n    yield (ENDMARKER, '', (lnum, 0), (lnum, 0), '')\n\nif __name__ == '__main__':                     # testing\n    import sys\n    if len(sys.argv) > 1:\n        tokenize(open(sys.argv[1]).readline)\n    else:\n        tokenize(sys.stdin.readline)\n", 
    "traceback": "\"\"\"Extract, format and print information about Python stack traces.\"\"\"\n\nimport linecache\nimport sys\nimport types\n\n__all__ = ['extract_stack', 'extract_tb', 'format_exception',\n           'format_exception_only', 'format_list', 'format_stack',\n           'format_tb', 'print_exc', 'format_exc', 'print_exception',\n           'print_last', 'print_stack', 'print_tb', 'tb_lineno']\n\ndef _print(file, str='', terminator='\\n'):\n    file.write(str+terminator)\n\n\ndef print_list(extracted_list, file=None):\n    \"\"\"Print the list of tuples as returned by extract_tb() or\n    extract_stack() as a formatted stack trace to the given file.\"\"\"\n    if file is None:\n        file = sys.stderr\n    for filename, lineno, name, line in extracted_list:\n        _print(file,\n               '  File \"%s\", line %d, in %s' % (filename,lineno,name))\n        if line:\n            _print(file, '    %s' % line.strip())\n\ndef format_list(extracted_list):\n    \"\"\"Format a list of traceback entry tuples for printing.\n\n    Given a list of tuples as returned by extract_tb() or\n    extract_stack(), return a list of strings ready for printing.\n    Each string in the resulting list corresponds to the item with the\n    same index in the argument list.  Each string ends in a newline;\n    the strings may contain internal newlines as well, for those items\n    whose source text line is not None.\n    \"\"\"\n    list = []\n    for filename, lineno, name, line in extracted_list:\n        item = '  File \"%s\", line %d, in %s\\n' % (filename,lineno,name)\n        if line:\n            item = item + '    %s\\n' % line.strip()\n        list.append(item)\n    return list\n\n\ndef print_tb(tb, limit=None, file=None):\n    \"\"\"Print up to 'limit' stack trace entries from the traceback 'tb'.\n\n    If 'limit' is omitted or None, all entries are printed.  If 'file'\n    is omitted or None, the output goes to sys.stderr; otherwise\n    'file' should be an open file or file-like object with a write()\n    method.\n    \"\"\"\n    if file is None:\n        file = sys.stderr\n    if limit is None:\n        if hasattr(sys, 'tracebacklimit'):\n            limit = sys.tracebacklimit\n    n = 0\n    while tb is not None and (limit is None or n < limit):\n        f = tb.tb_frame\n        lineno = tb.tb_lineno\n        co = f.f_code\n        filename = co.co_filename\n        name = co.co_name\n        _print(file,\n               '  File \"%s\", line %d, in %s' % (filename, lineno, name))\n        linecache.checkcache(filename)\n        line = linecache.getline(filename, lineno, f.f_globals)\n        if line: _print(file, '    ' + line.strip())\n        tb = tb.tb_next\n        n = n+1\n\ndef format_tb(tb, limit = None):\n    \"\"\"A shorthand for 'format_list(extract_tb(tb, limit))'.\"\"\"\n    return format_list(extract_tb(tb, limit))\n\ndef extract_tb(tb, limit = None):\n    \"\"\"Return list of up to limit pre-processed entries from traceback.\n\n    This is useful for alternate formatting of stack traces.  If\n    'limit' is omitted or None, all entries are extracted.  A\n    pre-processed stack trace entry is a quadruple (filename, line\n    number, function name, text) representing the information that is\n    usually printed for a stack trace.  The text is a string with\n    leading and trailing whitespace stripped; if the source is not\n    available it is None.\n    \"\"\"\n    if limit is None:\n        if hasattr(sys, 'tracebacklimit'):\n            limit = sys.tracebacklimit\n    list = []\n    n = 0\n    while tb is not None and (limit is None or n < limit):\n        f = tb.tb_frame\n        lineno = tb.tb_lineno\n        co = f.f_code\n        filename = co.co_filename\n        name = co.co_name\n        linecache.checkcache(filename)\n        line = linecache.getline(filename, lineno, f.f_globals)\n        if line: line = line.strip()\n        else: line = None\n        list.append((filename, lineno, name, line))\n        tb = tb.tb_next\n        n = n+1\n    return list\n\n\ndef print_exception(etype, value, tb, limit=None, file=None, _encoding=None):\n    \"\"\"Print exception up to 'limit' stack trace entries from 'tb' to 'file'.\n\n    This differs from print_tb() in the following ways: (1) if\n    traceback is not None, it prints a header \"Traceback (most recent\n    call last):\"; (2) it prints the exception type and value after the\n    stack trace; (3) if type is SyntaxError and value has the\n    appropriate format, it prints the line where the syntax error\n    occurred with a caret on the next line indicating the approximate\n    position of the error.\n    \"\"\"\n    if file is None:\n        file = sys.stderr\n    if tb:\n        _print(file, 'Traceback (most recent call last):')\n        print_tb(tb, limit, file)\n    lines = format_exception_only(etype, value, _encoding)\n    for line in lines:\n        _print(file, line, '')\n\ndef format_exception(etype, value, tb, limit = None):\n    \"\"\"Format a stack trace and the exception information.\n\n    The arguments have the same meaning as the corresponding arguments\n    to print_exception().  The return value is a list of strings, each\n    ending in a newline and some containing internal newlines.  When\n    these lines are concatenated and printed, exactly the same text is\n    printed as does print_exception().\n    \"\"\"\n    if tb:\n        list = ['Traceback (most recent call last):\\n']\n        list = list + format_tb(tb, limit)\n    else:\n        list = []\n    list = list + format_exception_only(etype, value)\n    return list\n\ndef format_exception_only(etype, value, _encoding=None):\n    \"\"\"Format the exception part of a traceback.\n\n    The arguments are the exception type and value such as given by\n    sys.last_type and sys.last_value. The return value is a list of\n    strings, each ending in a newline.\n\n    Normally, the list contains a single string; however, for\n    SyntaxError exceptions, it contains several lines that (when\n    printed) display detailed information about where the syntax\n    error occurred.\n\n    The message indicating which exception occurred is always the last\n    string in the list.\n\n    \"\"\"\n\n    # An instance should not have a meaningful value parameter, but\n    # sometimes does, particularly for string exceptions, such as\n    # >>> raise string1, string2  # deprecated\n    #\n    # Clear these out first because issubtype(string1, SyntaxError)\n    # would raise another exception and mask the original problem.\n    if (isinstance(etype, BaseException) or\n        isinstance(etype, types.InstanceType) or\n        etype is None or type(etype) is str):\n        return [_format_final_exc_line(etype, value, _encoding)]\n\n    stype = etype.__name__\n\n    if not issubclass(etype, SyntaxError):\n        return [_format_final_exc_line(stype, value, _encoding)]\n\n    # It was a syntax error; show exactly where the problem was found.\n    lines = []\n    try:\n        msg, (filename, lineno, offset, badline) = value.args\n    except Exception:\n        pass\n    else:\n        filename = filename or \"<string>\"\n        lines.append('  File \"%s\", line %d\\n' % (filename, lineno))\n        if badline is not None:\n            lines.append('    %s\\n' % badline.strip())\n            if offset is not None:\n                caretspace = badline.rstrip('\\n')\n                offset = min(len(caretspace), offset) - 1\n                caretspace = caretspace[:offset].lstrip()\n                # non-space whitespace (likes tabs) must be kept for alignment\n                caretspace = ((c.isspace() and c or ' ') for c in caretspace)\n                lines.append('    %s^\\n' % ''.join(caretspace))\n        value = msg\n\n    lines.append(_format_final_exc_line(stype, value, _encoding))\n    return lines\n\ndef _format_final_exc_line(etype, value, _encoding=None):\n    \"\"\"Return a list of a single line -- normal case for format_exception_only\"\"\"\n    valuestr = _some_str(value, _encoding)\n    if value is None or not valuestr:\n        line = \"%s\\n\" % etype\n    else:\n        line = \"%s: %s\\n\" % (etype, valuestr)\n    return line\n\ndef _some_str(value, _encoding=None):\n    try:\n        return str(value)\n    except Exception:\n        pass\n    try:\n        value = unicode(value)\n        return value.encode(_encoding or \"ascii\", \"backslashreplace\")\n    except Exception:\n        pass\n    return '<unprintable %s object>' % type(value).__name__\n\n\ndef print_exc(limit=None, file=None):\n    \"\"\"Shorthand for 'print_exception(sys.exc_type, sys.exc_value, sys.exc_traceback, limit, file)'.\n    (In fact, it uses sys.exc_info() to retrieve the same information\n    in a thread-safe way.)\"\"\"\n    if file is None:\n        file = sys.stderr\n    try:\n        etype, value, tb = sys.exc_info()\n        print_exception(etype, value, tb, limit, file)\n    finally:\n        etype = value = tb = None\n\n\ndef format_exc(limit=None):\n    \"\"\"Like print_exc() but return a string.\"\"\"\n    try:\n        etype, value, tb = sys.exc_info()\n        return ''.join(format_exception(etype, value, tb, limit))\n    finally:\n        etype = value = tb = None\n\n\ndef print_last(limit=None, file=None):\n    \"\"\"This is a shorthand for 'print_exception(sys.last_type,\n    sys.last_value, sys.last_traceback, limit, file)'.\"\"\"\n    if not hasattr(sys, \"last_type\"):\n        raise ValueError(\"no last exception\")\n    if file is None:\n        file = sys.stderr\n    print_exception(sys.last_type, sys.last_value, sys.last_traceback,\n                    limit, file)\n\n\ndef print_stack(f=None, limit=None, file=None):\n    \"\"\"Print a stack trace from its invocation point.\n\n    The optional 'f' argument can be used to specify an alternate\n    stack frame at which to start. The optional 'limit' and 'file'\n    arguments have the same meaning as for print_exception().\n    \"\"\"\n    if f is None:\n        try:\n            raise ZeroDivisionError\n        except ZeroDivisionError:\n            f = sys.exc_info()[2].tb_frame.f_back\n    print_list(extract_stack(f, limit), file)\n\ndef format_stack(f=None, limit=None):\n    \"\"\"Shorthand for 'format_list(extract_stack(f, limit))'.\"\"\"\n    if f is None:\n        try:\n            raise ZeroDivisionError\n        except ZeroDivisionError:\n            f = sys.exc_info()[2].tb_frame.f_back\n    return format_list(extract_stack(f, limit))\n\ndef extract_stack(f=None, limit = None):\n    \"\"\"Extract the raw traceback from the current stack frame.\n\n    The return value has the same format as for extract_tb().  The\n    optional 'f' and 'limit' arguments have the same meaning as for\n    print_stack().  Each item in the list is a quadruple (filename,\n    line number, function name, text), and the entries are in order\n    from oldest to newest stack frame.\n    \"\"\"\n    if f is None:\n        try:\n            raise ZeroDivisionError\n        except ZeroDivisionError:\n            f = sys.exc_info()[2].tb_frame.f_back\n    if limit is None:\n        if hasattr(sys, 'tracebacklimit'):\n            limit = sys.tracebacklimit\n    list = []\n    n = 0\n    while f is not None and (limit is None or n < limit):\n        lineno = f.f_lineno\n        co = f.f_code\n        filename = co.co_filename\n        name = co.co_name\n        linecache.checkcache(filename)\n        line = linecache.getline(filename, lineno, f.f_globals)\n        if line: line = line.strip()\n        else: line = None\n        list.append((filename, lineno, name, line))\n        f = f.f_back\n        n = n+1\n    list.reverse()\n    return list\n\ndef tb_lineno(tb):\n    \"\"\"Calculate correct line number of traceback given in tb.\n\n    Obsolete in 2.3.\n    \"\"\"\n    return tb.tb_lineno\n", 
    "types": "\"\"\"Define names for all type symbols known in the standard interpreter.\n\nTypes that are part of optional modules (e.g. array) are not listed.\n\"\"\"\nimport sys\n\n# Iterators in Python aren't a matter of type but of protocol.  A large\n# and changing number of builtin types implement *some* flavor of\n# iterator.  Don't check the type!  Use hasattr to check for both\n# \"__iter__\" and \"next\" attributes instead.\n\nNoneType = type(None)\nTypeType = type\nObjectType = object\n\nIntType = int\nLongType = long\nFloatType = float\nBooleanType = bool\ntry:\n    ComplexType = complex\nexcept NameError:\n    pass\n\nStringType = str\n\n# StringTypes is already outdated.  Instead of writing \"type(x) in\n# types.StringTypes\", you should use \"isinstance(x, basestring)\".  But\n# we keep around for compatibility with Python 2.2.\ntry:\n    UnicodeType = unicode\n    StringTypes = (StringType, UnicodeType)\nexcept NameError:\n    StringTypes = (StringType,)\n\nBufferType = buffer\n\nTupleType = tuple\nListType = list\nDictType = DictionaryType = dict\n\ndef _f(): pass\nFunctionType = type(_f)\nLambdaType = type(lambda: None)         # Same as FunctionType\nCodeType = type(_f.func_code)\n\ndef _g():\n    yield 1\nGeneratorType = type(_g())\n\nclass _C:\n    def _m(self): pass\nClassType = type(_C)\nUnboundMethodType = type(_C._m)         # Same as MethodType\n_x = _C()\nInstanceType = type(_x)\nMethodType = type(_x._m)\n\nBuiltinFunctionType = type(len)\nBuiltinMethodType = type([].append)     # Same as BuiltinFunctionType\n\nModuleType = type(sys)\nFileType = file\nXRangeType = xrange\n\ntry:\n    raise TypeError\nexcept TypeError:\n    tb = sys.exc_info()[2]\n    TracebackType = type(tb)\n    FrameType = type(tb.tb_frame)\n    del tb\n\nSliceType = slice\nEllipsisType = type(Ellipsis)\n\nDictProxyType = type(TypeType.__dict__)\nNotImplementedType = type(NotImplemented)\n\n# For Jython, the following two types are identical\nGetSetDescriptorType = type(FunctionType.func_code)\nMemberDescriptorType = type(FunctionType.func_globals)\n\ndel sys, _f, _g, _C, _x                           # Not for export\n", 
    "typing": "from __future__ import absolute_import, unicode_literals\n\nimport abc\nfrom abc import abstractmethod, abstractproperty\nimport collections\nimport functools\nimport re as stdlib_re  # Avoid confusion with the re we export.\nimport sys\nimport types\ntry:\n    import collections.abc as collections_abc\nexcept ImportError:\n    import collections as collections_abc  # Fallback for PY3.2.\n\n\n# Please keep __all__ alphabetized within each category.\n__all__ = [\n    # Super-special typing primitives.\n    'Any',\n    'Callable',\n    'ClassVar',\n    'Generic',\n    'Optional',\n    'Tuple',\n    'Type',\n    'TypeVar',\n    'Union',\n\n    # ABCs (from collections.abc).\n    'AbstractSet',  # collections.abc.Set.\n    'GenericMeta',  # subclass of abc.ABCMeta and a metaclass\n                    # for 'Generic' and ABCs below.\n    'ByteString',\n    'Container',\n    'Hashable',\n    'ItemsView',\n    'Iterable',\n    'Iterator',\n    'KeysView',\n    'Mapping',\n    'MappingView',\n    'MutableMapping',\n    'MutableSequence',\n    'MutableSet',\n    'Sequence',\n    'Sized',\n    'ValuesView',\n\n    # Structural checks, a.k.a. protocols.\n    'Reversible',\n    'SupportsAbs',\n    'SupportsFloat',\n    'SupportsInt',\n\n    # Concrete collection types.\n    'Counter',\n    'Deque',\n    'Dict',\n    'DefaultDict',\n    'List',\n    'Set',\n    'FrozenSet',\n    'NamedTuple',  # Not really a type.\n    'Generator',\n\n    # One-off things.\n    'AnyStr',\n    'cast',\n    'get_type_hints',\n    'NewType',\n    'no_type_check',\n    'no_type_check_decorator',\n    'overload',\n    'Text',\n    'TYPE_CHECKING',\n]\n\n# The pseudo-submodules 're' and 'io' are part of the public\n# namespace, but excluded from __all__ because they might stomp on\n# legitimate imports of those modules.\n\n\ndef _qualname(x):\n    if sys.version_info[:2] >= (3, 3):\n        return x.__qualname__\n    else:\n        # Fall back to just name.\n        return x.__name__\n\n\ndef _trim_name(nm):\n    whitelist = ('_TypeAlias', '_ForwardRef', '_TypingBase', '_FinalTypingBase')\n    if nm.startswith('_') and nm not in whitelist:\n        nm = nm[1:]\n    return nm\n\n\nclass TypingMeta(type):\n    \"\"\"Metaclass for most types defined in typing module\n    (not a part of public API).\n\n    This also defines a dummy constructor (all the work for most typing\n    constructs is done in __new__) and a nicer repr().\n    \"\"\"\n\n    _is_protocol = False\n\n    def __new__(cls, name, bases, namespace):\n        return super(TypingMeta, cls).__new__(cls, str(name), bases, namespace)\n\n    @classmethod\n    def assert_no_subclassing(cls, bases):\n        for base in bases:\n            if isinstance(base, cls):\n                raise TypeError(\"Cannot subclass %s\" %\n                                (', '.join(map(_type_repr, bases)) or '()'))\n\n    def __init__(self, *args, **kwds):\n        pass\n\n    def _eval_type(self, globalns, localns):\n        \"\"\"Override this in subclasses to interpret forward references.\n\n        For example, List['C'] is internally stored as\n        List[_ForwardRef('C')], which should evaluate to List[C],\n        where C is an object found in globalns or localns (searching\n        localns first, of course).\n        \"\"\"\n        return self\n\n    def _get_type_vars(self, tvars):\n        pass\n\n    def __repr__(self):\n        qname = _trim_name(_qualname(self))\n        return '%s.%s' % (self.__module__, qname)\n\n\nclass _TypingBase(object):\n    \"\"\"Internal indicator of special typing constructs.\"\"\"\n    __metaclass__ = TypingMeta\n    __slots__ = ('__weakref__',)\n\n    def __init__(self, *args, **kwds):\n        pass\n\n    def __new__(cls, *args, **kwds):\n        \"\"\"Constructor.\n\n        This only exists to give a better error message in case\n        someone tries to subclass a special typing object (not a good idea).\n        \"\"\"\n        if (len(args) == 3 and\n                isinstance(args[0], str) and\n                isinstance(args[1], tuple)):\n            # Close enough.\n            raise TypeError(\"Cannot subclass %r\" % cls)\n        return super(_TypingBase, cls).__new__(cls)\n\n    # Things that are not classes also need these.\n    def _eval_type(self, globalns, localns):\n        return self\n\n    def _get_type_vars(self, tvars):\n        pass\n\n    def __repr__(self):\n        cls = type(self)\n        qname = _trim_name(_qualname(cls))\n        return '%s.%s' % (cls.__module__, qname)\n\n    def __call__(self, *args, **kwds):\n        raise TypeError(\"Cannot instantiate %r\" % type(self))\n\n\nclass _FinalTypingBase(_TypingBase):\n    \"\"\"Internal mix-in class to prevent instantiation.\n\n    Prevents instantiation unless _root=True is given in class call.\n    It is used to create pseudo-singleton instances Any, Union, Optional, etc.\n    \"\"\"\n\n    __slots__ = ()\n\n    def __new__(cls, *args, **kwds):\n        self = super(_FinalTypingBase, cls).__new__(cls, *args, **kwds)\n        if '_root' in kwds and kwds['_root'] is True:\n            return self\n        raise TypeError(\"Cannot instantiate %r\" % cls)\n\n    def __reduce__(self):\n        return _trim_name(type(self).__name__)\n\n\nclass _ForwardRef(_TypingBase):\n    \"\"\"Internal wrapper to hold a forward reference.\"\"\"\n\n    __slots__ = ('__forward_arg__', '__forward_code__',\n                 '__forward_evaluated__', '__forward_value__')\n\n    def __init__(self, arg):\n        super(_ForwardRef, self).__init__(arg)\n        if not isinstance(arg, basestring):\n            raise TypeError('Forward reference must be a string -- got %r' % (arg,))\n        try:\n            code = compile(arg, '<string>', 'eval')\n        except SyntaxError:\n            raise SyntaxError('Forward reference must be an expression -- got %r' %\n                              (arg,))\n        self.__forward_arg__ = arg\n        self.__forward_code__ = code\n        self.__forward_evaluated__ = False\n        self.__forward_value__ = None\n\n    def _eval_type(self, globalns, localns):\n        if not self.__forward_evaluated__ or localns is not globalns:\n            if globalns is None and localns is None:\n                globalns = localns = {}\n            elif globalns is None:\n                globalns = localns\n            elif localns is None:\n                localns = globalns\n            self.__forward_value__ = _type_check(\n                eval(self.__forward_code__, globalns, localns),\n                \"Forward references must evaluate to types.\")\n            self.__forward_evaluated__ = True\n        return self.__forward_value__\n\n    def __eq__(self, other):\n        if not isinstance(other, _ForwardRef):\n            return NotImplemented\n        return (self.__forward_arg__ == other.__forward_arg__ and\n                self.__forward_value__ == other.__forward_value__)\n\n    def __hash__(self):\n        return hash((self.__forward_arg__, self.__forward_value__))\n\n    def __instancecheck__(self, obj):\n        raise TypeError(\"Forward references cannot be used with isinstance().\")\n\n    def __subclasscheck__(self, cls):\n        raise TypeError(\"Forward references cannot be used with issubclass().\")\n\n    def __repr__(self):\n        return '_ForwardRef(%r)' % (self.__forward_arg__,)\n\n\nclass _TypeAlias(_TypingBase):\n    \"\"\"Internal helper class for defining generic variants of concrete types.\n\n    Note that this is not a type; let's call it a pseudo-type.  It cannot\n    be used in instance and subclass checks in parameterized form, i.e.\n    ``isinstance(42, Match[str])`` raises ``TypeError`` instead of returning\n    ``False``.\n    \"\"\"\n\n    __slots__ = ('name', 'type_var', 'impl_type', 'type_checker')\n\n    def __init__(self, name, type_var, impl_type, type_checker):\n        \"\"\"Initializer.\n\n        Args:\n            name: The name, e.g. 'Pattern'.\n            type_var: The type parameter, e.g. AnyStr, or the\n                specific type, e.g. str.\n            impl_type: The implementation type.\n            type_checker: Function that takes an impl_type instance.\n                and returns a value that should be a type_var instance.\n        \"\"\"\n        assert isinstance(name, basestring), repr(name)\n        assert isinstance(impl_type, type), repr(impl_type)\n        assert not isinstance(impl_type, TypingMeta), repr(impl_type)\n        assert isinstance(type_var, (type, _TypingBase)), repr(type_var)\n        self.name = name\n        self.type_var = type_var\n        self.impl_type = impl_type\n        self.type_checker = type_checker\n\n    def __repr__(self):\n        return \"%s[%s]\" % (self.name, _type_repr(self.type_var))\n\n    def __getitem__(self, parameter):\n        if not isinstance(self.type_var, TypeVar):\n            raise TypeError(\"%s cannot be further parameterized.\" % self)\n        if self.type_var.__constraints__ and isinstance(parameter, type):\n            if not issubclass(parameter, self.type_var.__constraints__):\n                raise TypeError(\"%s is not a valid substitution for %s.\" %\n                                (parameter, self.type_var))\n        if isinstance(parameter, TypeVar) and parameter is not self.type_var:\n            raise TypeError(\"%s cannot be re-parameterized.\" % self)\n        return self.__class__(self.name, parameter,\n                              self.impl_type, self.type_checker)\n\n    def __eq__(self, other):\n        if not isinstance(other, _TypeAlias):\n            return NotImplemented\n        return self.name == other.name and self.type_var == other.type_var\n\n    def __hash__(self):\n        return hash((self.name, self.type_var))\n\n    def __instancecheck__(self, obj):\n        if not isinstance(self.type_var, TypeVar):\n            raise TypeError(\"Parameterized type aliases cannot be used \"\n                            \"with isinstance().\")\n        return isinstance(obj, self.impl_type)\n\n    def __subclasscheck__(self, cls):\n        if not isinstance(self.type_var, TypeVar):\n            raise TypeError(\"Parameterized type aliases cannot be used \"\n                            \"with issubclass().\")\n        return issubclass(cls, self.impl_type)\n\n\ndef _get_type_vars(types, tvars):\n    for t in types:\n        if isinstance(t, TypingMeta) or isinstance(t, _TypingBase):\n            t._get_type_vars(tvars)\n\n\ndef _type_vars(types):\n    tvars = []\n    _get_type_vars(types, tvars)\n    return tuple(tvars)\n\n\ndef _eval_type(t, globalns, localns):\n    if isinstance(t, TypingMeta) or isinstance(t, _TypingBase):\n        return t._eval_type(globalns, localns)\n    return t\n\n\ndef _type_check(arg, msg):\n    \"\"\"Check that the argument is a type, and return it (internal helper).\n\n    As a special case, accept None and return type(None) instead.\n    Also, _TypeAlias instances (e.g. Match, Pattern) are acceptable.\n\n    The msg argument is a human-readable error message, e.g.\n\n        \"Union[arg, ...]: arg should be a type.\"\n\n    We append the repr() of the actual value (truncated to 100 chars).\n    \"\"\"\n    if arg is None:\n        return type(None)\n    if isinstance(arg, basestring):\n        arg = _ForwardRef(arg)\n    if (\n        isinstance(arg, _TypingBase) and type(arg).__name__ == '_ClassVar' or\n        not isinstance(arg, (type, _TypingBase)) and not callable(arg)\n    ):\n        raise TypeError(msg + \" Got %.100r.\" % (arg,))\n    # Bare Union etc. are not valid as type arguments\n    if (\n        type(arg).__name__ in ('_Union', '_Optional') and\n        not getattr(arg, '__origin__', None) or\n        isinstance(arg, TypingMeta) and _gorg(arg) in (Generic, _Protocol)\n    ):\n        raise TypeError(\"Plain %s is not valid as type argument\" % arg)\n    return arg\n\n\ndef _type_repr(obj):\n    \"\"\"Return the repr() of an object, special-casing types (internal helper).\n\n    If obj is a type, we return a shorter version than the default\n    type.__repr__, based on the module and qualified name, which is\n    typically enough to uniquely identify a type.  For everything\n    else, we fall back on repr(obj).\n    \"\"\"\n    if isinstance(obj, type) and not isinstance(obj, TypingMeta):\n        if obj.__module__ == '__builtin__':\n            return _qualname(obj)\n        return '%s.%s' % (obj.__module__, _qualname(obj))\n    if obj is Ellipsis:\n        return('...')\n    if isinstance(obj, types.FunctionType):\n        return obj.__name__\n    return repr(obj)\n\n\nclass ClassVarMeta(TypingMeta):\n    \"\"\"Metaclass for _ClassVar\"\"\"\n\n    def __new__(cls, name, bases, namespace):\n        cls.assert_no_subclassing(bases)\n        self = super(ClassVarMeta, cls).__new__(cls, name, bases, namespace)\n        return self\n\n\nclass _ClassVar(_FinalTypingBase):\n    \"\"\"Special type construct to mark class variables.\n\n    An annotation wrapped in ClassVar indicates that a given\n    attribute is intended to be used as a class variable and\n    should not be set on instances of that class. Usage::\n\n      class Starship:\n          stats = {}  # type: ClassVar[Dict[str, int]] # class variable\n          damage = 10 # type: int                      # instance variable\n\n    ClassVar accepts only types and cannot be further subscribed.\n\n    Note that ClassVar is not a class itself, and should not\n    be used with isinstance() or issubclass().\n    \"\"\"\n\n    __metaclass__ = ClassVarMeta\n    __slots__ = ('__type__',)\n\n    def __init__(self, tp=None, _root=False):\n        self.__type__ = tp\n\n    def __getitem__(self, item):\n        cls = type(self)\n        if self.__type__ is None:\n            return cls(_type_check(item,\n                       '{} accepts only types.'.format(cls.__name__[1:])),\n                       _root=True)\n        raise TypeError('{} cannot be further subscripted'\n                        .format(cls.__name__[1:]))\n\n    def _eval_type(self, globalns, localns):\n        return type(self)(_eval_type(self.__type__, globalns, localns),\n                          _root=True)\n\n    def __repr__(self):\n        r = super(_ClassVar, self).__repr__()\n        if self.__type__ is not None:\n            r += '[{}]'.format(_type_repr(self.__type__))\n        return r\n\n    def __hash__(self):\n        return hash((type(self).__name__, self.__type__))\n\n    def __eq__(self, other):\n        if not isinstance(other, _ClassVar):\n            return NotImplemented\n        if self.__type__ is not None:\n            return self.__type__ == other.__type__\n        return self is other\n\n\nClassVar = _ClassVar(_root=True)\n\n\nclass AnyMeta(TypingMeta):\n    \"\"\"Metaclass for Any.\"\"\"\n\n    def __new__(cls, name, bases, namespace):\n        cls.assert_no_subclassing(bases)\n        self = super(AnyMeta, cls).__new__(cls, name, bases, namespace)\n        return self\n\n\nclass _Any(_FinalTypingBase):\n    \"\"\"Special type indicating an unconstrained type.\n\n    - Any is compatible with every type.\n    - Any assumed to have all methods.\n    - All values assumed to be instances of Any.\n\n    Note that all the above statements are true from the point of view of\n    static type checkers. At runtime, Any should not be used with instance\n    or class checks.\n    \"\"\"\n    __metaclass__ = AnyMeta\n    __slots__ = ()\n\n    def __instancecheck__(self, obj):\n        raise TypeError(\"Any cannot be used with isinstance().\")\n\n    def __subclasscheck__(self, cls):\n        raise TypeError(\"Any cannot be used with issubclass().\")\n\n\nAny = _Any(_root=True)\n\n\nclass TypeVarMeta(TypingMeta):\n    def __new__(cls, name, bases, namespace):\n        cls.assert_no_subclassing(bases)\n        return super(TypeVarMeta, cls).__new__(cls, name, bases, namespace)\n\n\nclass TypeVar(_TypingBase):\n    \"\"\"Type variable.\n\n    Usage::\n\n      T = TypeVar('T')  # Can be anything\n      A = TypeVar('A', str, bytes)  # Must be str or bytes\n\n    Type variables exist primarily for the benefit of static type\n    checkers.  They serve as the parameters for generic types as well\n    as for generic function definitions.  See class Generic for more\n    information on generic types.  Generic functions work as follows:\n\n      def repeat(x: T, n: int) -> List[T]:\n          '''Return a list containing n references to x.'''\n          return [x]*n\n\n      def longest(x: A, y: A) -> A:\n          '''Return the longest of two strings.'''\n          return x if len(x) >= len(y) else y\n\n    The latter example's signature is essentially the overloading\n    of (str, str) -> str and (bytes, bytes) -> bytes.  Also note\n    that if the arguments are instances of some subclass of str,\n    the return type is still plain str.\n\n    At runtime, isinstance(x, T) and issubclass(C, T) will raise TypeError.\n\n    Type variables defined with covariant=True or contravariant=True\n    can be used do declare covariant or contravariant generic types.\n    See PEP 484 for more details. By default generic types are invariant\n    in all type variables.\n\n    Type variables can be introspected. e.g.:\n\n      T.__name__ == 'T'\n      T.__constraints__ == ()\n      T.__covariant__ == False\n      T.__contravariant__ = False\n      A.__constraints__ == (str, bytes)\n    \"\"\"\n\n    __metaclass__ = TypeVarMeta\n    __slots__ = ('__name__', '__bound__', '__constraints__',\n                 '__covariant__', '__contravariant__')\n\n    def __init__(self, name, *constraints, **kwargs):\n        super(TypeVar, self).__init__(name, *constraints, **kwargs)\n        bound = kwargs.get('bound', None)\n        covariant = kwargs.get('covariant', False)\n        contravariant = kwargs.get('contravariant', False)\n        self.__name__ = name\n        if covariant and contravariant:\n            raise ValueError(\"Bivariant types are not supported.\")\n        self.__covariant__ = bool(covariant)\n        self.__contravariant__ = bool(contravariant)\n        if constraints and bound is not None:\n            raise TypeError(\"Constraints cannot be combined with bound=...\")\n        if constraints and len(constraints) == 1:\n            raise TypeError(\"A single constraint is not allowed\")\n        msg = \"TypeVar(name, constraint, ...): constraints must be types.\"\n        self.__constraints__ = tuple(_type_check(t, msg) for t in constraints)\n        if bound:\n            self.__bound__ = _type_check(bound, \"Bound must be a type.\")\n        else:\n            self.__bound__ = None\n\n    def _get_type_vars(self, tvars):\n        if self not in tvars:\n            tvars.append(self)\n\n    def __repr__(self):\n        if self.__covariant__:\n            prefix = '+'\n        elif self.__contravariant__:\n            prefix = '-'\n        else:\n            prefix = '~'\n        return prefix + self.__name__\n\n    def __instancecheck__(self, instance):\n        raise TypeError(\"Type variables cannot be used with isinstance().\")\n\n    def __subclasscheck__(self, cls):\n        raise TypeError(\"Type variables cannot be used with issubclass().\")\n\n\n# Some unconstrained type variables.  These are used by the container types.\n# (These are not for export.)\nT = TypeVar('T')  # Any type.\nKT = TypeVar('KT')  # Key type.\nVT = TypeVar('VT')  # Value type.\nT_co = TypeVar('T_co', covariant=True)  # Any type covariant containers.\nV_co = TypeVar('V_co', covariant=True)  # Any type covariant containers.\nVT_co = TypeVar('VT_co', covariant=True)  # Value type covariant containers.\nT_contra = TypeVar('T_contra', contravariant=True)  # Ditto contravariant.\n\n# A useful type variable with constraints.  This represents string types.\n# (This one *is* for export!)\nAnyStr = TypeVar('AnyStr', bytes, unicode)\n\n\ndef _replace_arg(arg, tvars, args):\n    \"\"\"An internal helper function: replace arg if it is a type variable\n    found in tvars with corresponding substitution from args or\n    with corresponding substitution sub-tree if arg is a generic type.\n    \"\"\"\n\n    if tvars is None:\n        tvars = []\n    if hasattr(arg, '_subs_tree') and isinstance(arg, (GenericMeta, _TypingBase)):\n        return arg._subs_tree(tvars, args)\n    if isinstance(arg, TypeVar):\n        for i, tvar in enumerate(tvars):\n            if arg == tvar:\n                return args[i]\n    return arg\n\n\n# Special typing constructs Union, Optional, Generic, Callable and Tuple\n# use three special attributes for internal bookkeeping of generic types:\n# * __parameters__ is a tuple of unique free type parameters of a generic\n#   type, for example, Dict[T, T].__parameters__ == (T,);\n# * __origin__ keeps a reference to a type that was subscripted,\n#   e.g., Union[T, int].__origin__ == Union;\n# * __args__ is a tuple of all arguments used in subscripting,\n#   e.g., Dict[T, int].__args__ == (T, int).\n\n\ndef _subs_tree(cls, tvars=None, args=None):\n    \"\"\"An internal helper function: calculate substitution tree\n    for generic cls after replacing its type parameters with\n    substitutions in tvars -> args (if any).\n    Repeat the same following __origin__'s.\n\n    Return a list of arguments with all possible substitutions\n    performed. Arguments that are generic classes themselves are represented\n    as tuples (so that no new classes are created by this function).\n    For example: _subs_tree(List[Tuple[int, T]][str]) == [(Tuple, int, str)]\n    \"\"\"\n\n    if cls.__origin__ is None:\n        return cls\n    # Make of chain of origins (i.e. cls -> cls.__origin__)\n    current = cls.__origin__\n    orig_chain = []\n    while current.__origin__ is not None:\n        orig_chain.append(current)\n        current = current.__origin__\n    # Replace type variables in __args__ if asked ...\n    tree_args = []\n    for arg in cls.__args__:\n        tree_args.append(_replace_arg(arg, tvars, args))\n    # ... then continue replacing down the origin chain.\n    for ocls in orig_chain:\n        new_tree_args = []\n        for arg in ocls.__args__:\n            new_tree_args.append(_replace_arg(arg, ocls.__parameters__, tree_args))\n        tree_args = new_tree_args\n    return tree_args\n\n\ndef _remove_dups_flatten(parameters):\n    \"\"\"An internal helper for Union creation and substitution: flatten Union's\n    among parameters, then remove duplicates and strict subclasses.\n    \"\"\"\n\n    # Flatten out Union[Union[...], ...].\n    params = []\n    for p in parameters:\n        if isinstance(p, _Union) and p.__origin__ is Union:\n            params.extend(p.__args__)\n        elif isinstance(p, tuple) and len(p) > 0 and p[0] is Union:\n            params.extend(p[1:])\n        else:\n            params.append(p)\n    # Weed out strict duplicates, preserving the first of each occurrence.\n    all_params = set(params)\n    if len(all_params) < len(params):\n        new_params = []\n        for t in params:\n            if t in all_params:\n                new_params.append(t)\n                all_params.remove(t)\n        params = new_params\n        assert not all_params, all_params\n    # Weed out subclasses.\n    # E.g. Union[int, Employee, Manager] == Union[int, Employee].\n    # If object is present it will be sole survivor among proper classes.\n    # Never discard type variables.\n    # (In particular, Union[str, AnyStr] != AnyStr.)\n    all_params = set(params)\n    for t1 in params:\n        if not isinstance(t1, type):\n            continue\n        if any(isinstance(t2, type) and issubclass(t1, t2)\n               for t2 in all_params - {t1}\n               if not (isinstance(t2, GenericMeta) and\n                       t2.__origin__ is not None)):\n            all_params.remove(t1)\n    return tuple(t for t in params if t in all_params)\n\n\ndef _check_generic(cls, parameters):\n    # Check correct count for parameters of a generic cls (internal helper).\n    if not cls.__parameters__:\n        raise TypeError(\"%s is not a generic class\" % repr(cls))\n    alen = len(parameters)\n    elen = len(cls.__parameters__)\n    if alen != elen:\n        raise TypeError(\"Too %s parameters for %s; actual %s, expected %s\" %\n                        (\"many\" if alen > elen else \"few\", repr(cls), alen, elen))\n\n\n_cleanups = []\n\n\ndef _tp_cache(func):\n    maxsize = 128\n    cache = {}\n    _cleanups.append(cache.clear)\n\n    @functools.wraps(func)\n    def inner(*args):\n        key = args\n        try:\n            return cache[key]\n        except TypeError:\n            # Assume it's an unhashable argument.\n            return func(*args)\n        except KeyError:\n            value = func(*args)\n            if len(cache) >= maxsize:\n                # If the cache grows too much, just start over.\n                cache.clear()\n            cache[key] = value\n            return value\n\n    return inner\n\n\nclass UnionMeta(TypingMeta):\n    \"\"\"Metaclass for Union.\"\"\"\n\n    def __new__(cls, name, bases, namespace):\n        cls.assert_no_subclassing(bases)\n        return super(UnionMeta, cls).__new__(cls, name, bases, namespace)\n\n\nclass _Union(_FinalTypingBase):\n    \"\"\"Union type; Union[X, Y] means either X or Y.\n\n    To define a union, use e.g. Union[int, str].  Details:\n\n    - The arguments must be types and there must be at least one.\n\n    - None as an argument is a special case and is replaced by\n      type(None).\n\n    - Unions of unions are flattened, e.g.::\n\n        Union[Union[int, str], float] == Union[int, str, float]\n\n    - Unions of a single argument vanish, e.g.::\n\n        Union[int] == int  # The constructor actually returns int\n\n    - Redundant arguments are skipped, e.g.::\n\n        Union[int, str, int] == Union[int, str]\n\n    - When comparing unions, the argument order is ignored, e.g.::\n\n        Union[int, str] == Union[str, int]\n\n    - When two arguments have a subclass relationship, the least\n      derived argument is kept, e.g.::\n\n        class Employee: pass\n        class Manager(Employee): pass\n        Union[int, Employee, Manager] == Union[int, Employee]\n        Union[Manager, int, Employee] == Union[int, Employee]\n        Union[Employee, Manager] == Employee\n\n    - Similar for object::\n\n        Union[int, object] == object\n\n    - You cannot subclass or instantiate a union.\n\n    - You can use Optional[X] as a shorthand for Union[X, None].\n    \"\"\"\n\n    __metaclass__ = UnionMeta\n    __slots__ = ('__parameters__', '__args__', '__origin__', '__tree_hash__')\n\n    def __new__(cls, parameters=None, origin=None, *args, **kwds):\n        self = super(_Union, cls).__new__(cls, parameters, origin, *args, **kwds)\n        if origin is None:\n            self.__parameters__ = None\n            self.__args__ = None\n            self.__origin__ = None\n            self.__tree_hash__ = hash(frozenset(('Union',)))\n            return self\n        if not isinstance(parameters, tuple):\n            raise TypeError(\"Expected parameters=<tuple>\")\n        if origin is Union:\n            parameters = _remove_dups_flatten(parameters)\n            # It's not a union if there's only one type left.\n            if len(parameters) == 1:\n                return parameters[0]\n        self.__parameters__ = _type_vars(parameters)\n        self.__args__ = parameters\n        self.__origin__ = origin\n        # Pre-calculate the __hash__ on instantiation.\n        # This improves speed for complex substitutions.\n        subs_tree = self._subs_tree()\n        if isinstance(subs_tree, tuple):\n            self.__tree_hash__ = hash(frozenset(subs_tree))\n        else:\n            self.__tree_hash__ = hash(subs_tree)\n        return self\n\n    def _eval_type(self, globalns, localns):\n        if self.__args__ is None:\n            return self\n        ev_args = tuple(_eval_type(t, globalns, localns) for t in self.__args__)\n        ev_origin = _eval_type(self.__origin__, globalns, localns)\n        if ev_args == self.__args__ and ev_origin == self.__origin__:\n            # Everything is already evaluated.\n            return self\n        return self.__class__(ev_args, ev_origin, _root=True)\n\n    def _get_type_vars(self, tvars):\n        if self.__origin__ and self.__parameters__:\n            _get_type_vars(self.__parameters__, tvars)\n\n    def __repr__(self):\n        if self.__origin__ is None:\n            return super(_Union, self).__repr__()\n        tree = self._subs_tree()\n        if not isinstance(tree, tuple):\n            return repr(tree)\n        return tree[0]._tree_repr(tree)\n\n    def _tree_repr(self, tree):\n        arg_list = []\n        for arg in tree[1:]:\n            if not isinstance(arg, tuple):\n                arg_list.append(_type_repr(arg))\n            else:\n                arg_list.append(arg[0]._tree_repr(arg))\n        return super(_Union, self).__repr__() + '[%s]' % ', '.join(arg_list)\n\n    @_tp_cache\n    def __getitem__(self, parameters):\n        if parameters == ():\n            raise TypeError(\"Cannot take a Union of no types.\")\n        if not isinstance(parameters, tuple):\n            parameters = (parameters,)\n        if self.__origin__ is None:\n            msg = \"Union[arg, ...]: each arg must be a type.\"\n        else:\n            msg = \"Parameters to generic types must be types.\"\n        parameters = tuple(_type_check(p, msg) for p in parameters)\n        if self is not Union:\n            _check_generic(self, parameters)\n        return self.__class__(parameters, origin=self, _root=True)\n\n    def _subs_tree(self, tvars=None, args=None):\n        if self is Union:\n            return Union  # Nothing to substitute\n        tree_args = _subs_tree(self, tvars, args)\n        tree_args = _remove_dups_flatten(tree_args)\n        if len(tree_args) == 1:\n            return tree_args[0]  # Union of a single type is that type\n        return (Union,) + tree_args\n\n    def __eq__(self, other):\n        if isinstance(other, _Union):\n            return self.__tree_hash__ == other.__tree_hash__\n        elif self is not Union:\n            return self._subs_tree() == other\n        else:\n            return self is other\n\n    def __hash__(self):\n        return self.__tree_hash__\n\n    def __instancecheck__(self, obj):\n        raise TypeError(\"Unions cannot be used with isinstance().\")\n\n    def __subclasscheck__(self, cls):\n        raise TypeError(\"Unions cannot be used with issubclass().\")\n\n\nUnion = _Union(_root=True)\n\n\nclass OptionalMeta(TypingMeta):\n    \"\"\"Metaclass for Optional.\"\"\"\n\n    def __new__(cls, name, bases, namespace):\n        cls.assert_no_subclassing(bases)\n        return super(OptionalMeta, cls).__new__(cls, name, bases, namespace)\n\n\nclass _Optional(_FinalTypingBase):\n    \"\"\"Optional type.\n\n    Optional[X] is equivalent to Union[X, None].\n    \"\"\"\n\n    __metaclass__ = OptionalMeta\n    __slots__ = ()\n\n    @_tp_cache\n    def __getitem__(self, arg):\n        arg = _type_check(arg, \"Optional[t] requires a single type.\")\n        return Union[arg, type(None)]\n\n\nOptional = _Optional(_root=True)\n\n\ndef _gorg(a):\n    \"\"\"Return the farthest origin of a generic class (internal helper).\"\"\"\n    assert isinstance(a, GenericMeta)\n    while a.__origin__ is not None:\n        a = a.__origin__\n    return a\n\n\ndef _geqv(a, b):\n    \"\"\"Return whether two generic classes are equivalent (internal helper).\n\n    The intention is to consider generic class X and any of its\n    parameterized forms (X[T], X[int], etc.) as equivalent.\n\n    However, X is not equivalent to a subclass of X.\n\n    The relation is reflexive, symmetric and transitive.\n    \"\"\"\n    assert isinstance(a, GenericMeta) and isinstance(b, GenericMeta)\n    # Reduce each to its origin.\n    return _gorg(a) is _gorg(b)\n\n\ndef _next_in_mro(cls):\n    \"\"\"Helper for Generic.__new__.\n\n    Returns the class after the last occurrence of Generic or\n    Generic[...] in cls.__mro__.\n    \"\"\"\n    next_in_mro = object\n    # Look for the last occurrence of Generic or Generic[...].\n    for i, c in enumerate(cls.__mro__[:-1]):\n        if isinstance(c, GenericMeta) and _gorg(c) is Generic:\n            next_in_mro = cls.__mro__[i + 1]\n    return next_in_mro\n\n\ndef _make_subclasshook(cls):\n    \"\"\"Construct a __subclasshook__ callable that incorporates\n    the associated __extra__ class in subclass checks performed\n    against cls.\n    \"\"\"\n    if isinstance(cls.__extra__, abc.ABCMeta):\n        # The logic mirrors that of ABCMeta.__subclasscheck__.\n        # Registered classes need not be checked here because\n        # cls and its extra share the same _abc_registry.\n        def __extrahook__(cls, subclass):\n            res = cls.__extra__.__subclasshook__(subclass)\n            if res is not NotImplemented:\n                return res\n            if cls.__extra__ in getattr(subclass, '__mro__', ()):\n                return True\n            for scls in cls.__extra__.__subclasses__():\n                if isinstance(scls, GenericMeta):\n                    continue\n                if issubclass(subclass, scls):\n                    return True\n            return NotImplemented\n    else:\n        # For non-ABC extras we'll just call issubclass().\n        def __extrahook__(cls, subclass):\n            if cls.__extra__ and issubclass(subclass, cls.__extra__):\n                return True\n            return NotImplemented\n    return classmethod(__extrahook__)\n\n\nclass GenericMeta(TypingMeta, abc.ABCMeta):\n    \"\"\"Metaclass for generic types.\n\n    This is a metaclass for typing.Generic and generic ABCs defined in\n    typing module. User defined subclasses of GenericMeta can override\n    __new__ and invoke super().__new__. Note that GenericMeta.__new__\n    has strict rules on what is allowed in its bases argument:\n    * plain Generic is disallowed in bases;\n    * Generic[...] should appear in bases at most once;\n    * if Generic[...] is present, then it should list all type variables\n      that appear in other bases.\n    In addition, type of all generic bases is erased, e.g., C[int] is\n    stripped to plain C.\n    \"\"\"\n\n    def __new__(cls, name, bases, namespace,\n                tvars=None, args=None, origin=None, extra=None, orig_bases=None):\n        \"\"\"Create a new generic class. GenericMeta.__new__ accepts\n        keyword arguments that are used for internal bookkeeping, therefore\n        an override should pass unused keyword arguments to super().\n        \"\"\"\n        if tvars is not None:\n            # Called from __getitem__() below.\n            assert origin is not None\n            assert all(isinstance(t, TypeVar) for t in tvars), tvars\n        else:\n            # Called from class statement.\n            assert tvars is None, tvars\n            assert args is None, args\n            assert origin is None, origin\n\n            # Get the full set of tvars from the bases.\n            tvars = _type_vars(bases)\n            # Look for Generic[T1, ..., Tn].\n            # If found, tvars must be a subset of it.\n            # If not found, tvars is it.\n            # Also check for and reject plain Generic,\n            # and reject multiple Generic[...].\n            gvars = None\n            for base in bases:\n                if base is Generic:\n                    raise TypeError(\"Cannot inherit from plain Generic\")\n                if (isinstance(base, GenericMeta) and\n                        base.__origin__ is Generic):\n                    if gvars is not None:\n                        raise TypeError(\n                            \"Cannot inherit from Generic[...] multiple types.\")\n                    gvars = base.__parameters__\n            if gvars is None:\n                gvars = tvars\n            else:\n                tvarset = set(tvars)\n                gvarset = set(gvars)\n                if not tvarset <= gvarset:\n                    raise TypeError(\n                        \"Some type variables (%s) \"\n                        \"are not listed in Generic[%s]\" %\n                        (\", \".join(str(t) for t in tvars if t not in gvarset),\n                         \", \".join(str(g) for g in gvars)))\n                tvars = gvars\n\n        initial_bases = bases\n        if extra is None:\n            extra = namespace.get('__extra__')\n        if extra is not None and type(extra) is abc.ABCMeta and extra not in bases:\n            bases = (extra,) + bases\n        bases = tuple(_gorg(b) if isinstance(b, GenericMeta) else b for b in bases)\n\n        # remove bare Generic from bases if there are other generic bases\n        if any(isinstance(b, GenericMeta) and b is not Generic for b in bases):\n            bases = tuple(b for b in bases if b is not Generic)\n        namespace.update({'__origin__': origin, '__extra__': extra})\n        self = super(GenericMeta, cls).__new__(cls, name, bases, namespace)\n\n        self.__parameters__ = tvars\n        # Be prepared that GenericMeta will be subclassed by TupleMeta\n        # and CallableMeta, those two allow ..., (), or [] in __args___.\n        self.__args__ = tuple(Ellipsis if a is _TypingEllipsis else\n                              () if a is _TypingEmpty else\n                              a for a in args) if args else None\n        # Speed hack (https://github.com/python/typing/issues/196).\n        self.__next_in_mro__ = _next_in_mro(self)\n        # Preserve base classes on subclassing (__bases__ are type erased now).\n        if orig_bases is None:\n            self.__orig_bases__ = initial_bases\n\n        # This allows unparameterized generic collections to be used\n        # with issubclass() and isinstance() in the same way as their\n        # collections.abc counterparts (e.g., isinstance([], Iterable)).\n        if (\n            '__subclasshook__' not in namespace and extra or\n            # allow overriding\n            getattr(self.__subclasshook__, '__name__', '') == '__extrahook__'\n        ):\n            self.__subclasshook__ = _make_subclasshook(self)\n\n        if origin and hasattr(origin, '__qualname__'):  # Fix for Python 3.2.\n            self.__qualname__ = origin.__qualname__\n        self.__tree_hash__ = (hash(self._subs_tree()) if origin else\n                              super(GenericMeta, self).__hash__())\n        return self\n\n    def __init__(self, *args, **kwargs):\n        super(GenericMeta, self).__init__(*args, **kwargs)\n        if isinstance(self.__extra__, abc.ABCMeta):\n            self._abc_registry = self.__extra__._abc_registry\n            self._abc_cache = self.__extra__._abc_cache\n        elif self.__origin__ is not None:\n            self._abc_registry = self.__origin__._abc_registry\n            self._abc_cache = self.__origin__._abc_cache\n\n    # _abc_negative_cache and _abc_negative_cache_version\n    # realised as descriptors, since GenClass[t1, t2, ...] always\n    # share subclass info with GenClass.\n    # This is an important memory optimization.\n    @property\n    def _abc_negative_cache(self):\n        if isinstance(self.__extra__, abc.ABCMeta):\n            return self.__extra__._abc_negative_cache\n        return _gorg(self)._abc_generic_negative_cache\n\n    @_abc_negative_cache.setter\n    def _abc_negative_cache(self, value):\n        if self.__origin__ is None:\n            if isinstance(self.__extra__, abc.ABCMeta):\n                self.__extra__._abc_negative_cache = value\n            else:\n                self._abc_generic_negative_cache = value\n\n    @property\n    def _abc_negative_cache_version(self):\n        if isinstance(self.__extra__, abc.ABCMeta):\n            return self.__extra__._abc_negative_cache_version\n        return _gorg(self)._abc_generic_negative_cache_version\n\n    @_abc_negative_cache_version.setter\n    def _abc_negative_cache_version(self, value):\n        if self.__origin__ is None:\n            if isinstance(self.__extra__, abc.ABCMeta):\n                self.__extra__._abc_negative_cache_version = value\n            else:\n                self._abc_generic_negative_cache_version = value\n\n    def _get_type_vars(self, tvars):\n        if self.__origin__ and self.__parameters__:\n            _get_type_vars(self.__parameters__, tvars)\n\n    def _eval_type(self, globalns, localns):\n        ev_origin = (self.__origin__._eval_type(globalns, localns)\n                     if self.__origin__ else None)\n        ev_args = tuple(_eval_type(a, globalns, localns) for a\n                        in self.__args__) if self.__args__ else None\n        if ev_origin == self.__origin__ and ev_args == self.__args__:\n            return self\n        return self.__class__(self.__name__,\n                              self.__bases__,\n                              dict(self.__dict__),\n                              tvars=_type_vars(ev_args) if ev_args else None,\n                              args=ev_args,\n                              origin=ev_origin,\n                              extra=self.__extra__,\n                              orig_bases=self.__orig_bases__)\n\n    def __repr__(self):\n        if self.__origin__ is None:\n            return super(GenericMeta, self).__repr__()\n        return self._tree_repr(self._subs_tree())\n\n    def _tree_repr(self, tree):\n        arg_list = []\n        for arg in tree[1:]:\n            if arg == ():\n                arg_list.append('()')\n            elif not isinstance(arg, tuple):\n                arg_list.append(_type_repr(arg))\n            else:\n                arg_list.append(arg[0]._tree_repr(arg))\n        return super(GenericMeta, self).__repr__() + '[%s]' % ', '.join(arg_list)\n\n    def _subs_tree(self, tvars=None, args=None):\n        if self.__origin__ is None:\n            return self\n        tree_args = _subs_tree(self, tvars, args)\n        return (_gorg(self),) + tuple(tree_args)\n\n    def __eq__(self, other):\n        if not isinstance(other, GenericMeta):\n            return NotImplemented\n        if self.__origin__ is None or other.__origin__ is None:\n            return self is other\n        return self.__tree_hash__ == other.__tree_hash__\n\n    def __hash__(self):\n        return self.__tree_hash__\n\n    @_tp_cache\n    def __getitem__(self, params):\n        if not isinstance(params, tuple):\n            params = (params,)\n        if not params and not _gorg(self) is Tuple:\n            raise TypeError(\n                \"Parameter list to %s[...] cannot be empty\" % _qualname(self))\n        msg = \"Parameters to generic types must be types.\"\n        params = tuple(_type_check(p, msg) for p in params)\n        if self is Generic:\n            # Generic can only be subscripted with unique type variables.\n            if not all(isinstance(p, TypeVar) for p in params):\n                raise TypeError(\n                    \"Parameters to Generic[...] must all be type variables\")\n            if len(set(params)) != len(params):\n                raise TypeError(\n                    \"Parameters to Generic[...] must all be unique\")\n            tvars = params\n            args = params\n        elif self in (Tuple, Callable):\n            tvars = _type_vars(params)\n            args = params\n        elif self is _Protocol:\n            # _Protocol is internal, don't check anything.\n            tvars = params\n            args = params\n        elif self.__origin__ in (Generic, _Protocol):\n            # Can't subscript Generic[...] or _Protocol[...].\n            raise TypeError(\"Cannot subscript already-subscripted %s\" %\n                            repr(self))\n        else:\n            # Subscripting a regular Generic subclass.\n            _check_generic(self, params)\n            tvars = _type_vars(params)\n            args = params\n\n        prepend = (self,) if self.__origin__ is None else ()\n        return self.__class__(self.__name__,\n                              prepend + self.__bases__,\n                              dict(self.__dict__),\n                              tvars=tvars,\n                              args=args,\n                              origin=self,\n                              extra=self.__extra__,\n                              orig_bases=self.__orig_bases__)\n\n    def __subclasscheck__(self, cls):\n        if self.__origin__ is not None:\n            if sys._getframe(1).f_globals['__name__'] not in ['abc', 'functools']:\n                raise TypeError(\"Parameterized generics cannot be used with class \"\n                                \"or instance checks\")\n            return False\n        if self is Generic:\n            raise TypeError(\"Class %r cannot be used with class \"\n                            \"or instance checks\" % self)\n        return super(GenericMeta, self).__subclasscheck__(cls)\n\n    def __instancecheck__(self, instance):\n        # Since we extend ABC.__subclasscheck__ and\n        # ABC.__instancecheck__ inlines the cache checking done by the\n        # latter, we must extend __instancecheck__ too. For simplicity\n        # we just skip the cache check -- instance checks for generic\n        # classes are supposed to be rare anyways.\n        if not isinstance(instance, type):\n            return issubclass(instance.__class__, self)\n        return False\n\n    def __copy__(self):\n        return self.__class__(self.__name__, self.__bases__, dict(self.__dict__),\n                              self.__parameters__, self.__args__, self.__origin__,\n                              self.__extra__, self.__orig_bases__)\n\n    def __setattr__(self, attr, value):\n        # We consider all the subscripted genrics as proxies for original class\n        if (\n            attr.startswith('__') and attr.endswith('__') or\n            attr.startswith('_abc_')\n        ):\n            super(GenericMeta, self).__setattr__(attr, value)\n        else:\n            super(GenericMeta, _gorg(self)).__setattr__(attr, value)\n\n\n# Prevent checks for Generic to crash when defining Generic.\nGeneric = None\n\n\ndef _generic_new(base_cls, cls, *args, **kwds):\n    # Assure type is erased on instantiation,\n    # but attempt to store it in __orig_class__\n    if cls.__origin__ is None:\n        return base_cls.__new__(cls)\n    else:\n        origin = _gorg(cls)\n        obj = base_cls.__new__(origin)\n        try:\n            obj.__orig_class__ = cls\n        except AttributeError:\n            pass\n        obj.__init__(*args, **kwds)\n        return obj\n\n\nclass Generic(object):\n    \"\"\"Abstract base class for generic types.\n\n    A generic type is typically declared by inheriting from\n    this class parameterized with one or more type variables.\n    For example, a generic mapping type might be defined as::\n\n      class Mapping(Generic[KT, VT]):\n          def __getitem__(self, key: KT) -> VT:\n              ...\n          # Etc.\n\n    This class can then be used as follows::\n\n      def lookup_name(mapping: Mapping[KT, VT], key: KT, default: VT) -> VT:\n          try:\n              return mapping[key]\n          except KeyError:\n              return default\n    \"\"\"\n\n    __metaclass__ = GenericMeta\n    __slots__ = ()\n\n    def __new__(cls, *args, **kwds):\n        if _geqv(cls, Generic):\n            raise TypeError(\"Type Generic cannot be instantiated; \"\n                            \"it can be used only as a base class\")\n        return _generic_new(cls.__next_in_mro__, cls, *args, **kwds)\n\n\nclass _TypingEmpty(object):\n    \"\"\"Internal placeholder for () or []. Used by TupleMeta and CallableMeta\n    to allow empty list/tuple in specific places, without allowing them\n    to sneak in where prohibited.\n    \"\"\"\n\n\nclass _TypingEllipsis(object):\n    \"\"\"Internal placeholder for ... (ellipsis).\"\"\"\n\n\nclass TupleMeta(GenericMeta):\n    \"\"\"Metaclass for Tuple (internal).\"\"\"\n\n    @_tp_cache\n    def __getitem__(self, parameters):\n        if self.__origin__ is not None or not _geqv(self, Tuple):\n            # Normal generic rules apply if this is not the first subscription\n            # or a subscription of a subclass.\n            return super(TupleMeta, self).__getitem__(parameters)\n        if parameters == ():\n            return super(TupleMeta, self).__getitem__((_TypingEmpty,))\n        if not isinstance(parameters, tuple):\n            parameters = (parameters,)\n        if len(parameters) == 2 and parameters[1] is Ellipsis:\n            msg = \"Tuple[t, ...]: t must be a type.\"\n            p = _type_check(parameters[0], msg)\n            return super(TupleMeta, self).__getitem__((p, _TypingEllipsis))\n        msg = \"Tuple[t0, t1, ...]: each t must be a type.\"\n        parameters = tuple(_type_check(p, msg) for p in parameters)\n        return super(TupleMeta, self).__getitem__(parameters)\n\n    def __instancecheck__(self, obj):\n        if self.__args__ is None:\n            return isinstance(obj, tuple)\n        raise TypeError(\"Parameterized Tuple cannot be used \"\n                        \"with isinstance().\")\n\n    def __subclasscheck__(self, cls):\n        if self.__args__ is None:\n            return issubclass(cls, tuple)\n        raise TypeError(\"Parameterized Tuple cannot be used \"\n                        \"with issubclass().\")\n\n\nclass Tuple(tuple):\n    \"\"\"Tuple type; Tuple[X, Y] is the cross-product type of X and Y.\n\n    Example: Tuple[T1, T2] is a tuple of two elements corresponding\n    to type variables T1 and T2.  Tuple[int, float, str] is a tuple\n    of an int, a float and a string.\n\n    To specify a variable-length tuple of homogeneous type, use Tuple[T, ...].\n    \"\"\"\n\n    __metaclass__ = TupleMeta\n    __extra__ = tuple\n    __slots__ = ()\n\n    def __new__(cls, *args, **kwds):\n        if _geqv(cls, Tuple):\n            raise TypeError(\"Type Tuple cannot be instantiated; \"\n                            \"use tuple() instead\")\n        return _generic_new(tuple, cls, *args, **kwds)\n\n\nclass CallableMeta(GenericMeta):\n    \"\"\" Metaclass for Callable.\"\"\"\n\n    def __repr__(self):\n        if self.__origin__ is None:\n            return super(CallableMeta, self).__repr__()\n        return self._tree_repr(self._subs_tree())\n\n    def _tree_repr(self, tree):\n        if _gorg(self) is not Callable:\n            return super(CallableMeta, self)._tree_repr(tree)\n        # For actual Callable (not its subclass) we override\n        # super(CallableMeta, self)._tree_repr() for nice formatting.\n        arg_list = []\n        for arg in tree[1:]:\n            if not isinstance(arg, tuple):\n                arg_list.append(_type_repr(arg))\n            else:\n                arg_list.append(arg[0]._tree_repr(arg))\n        if arg_list[0] == '...':\n            return repr(tree[0]) + '[..., %s]' % arg_list[1]\n        return (repr(tree[0]) +\n                '[[%s], %s]' % (', '.join(arg_list[:-1]), arg_list[-1]))\n\n    def __getitem__(self, parameters):\n        \"\"\"A thin wrapper around __getitem_inner__ to provide the latter\n        with hashable arguments to improve speed.\n        \"\"\"\n\n        if self.__origin__ is not None or not _geqv(self, Callable):\n            return super(CallableMeta, self).__getitem__(parameters)\n        if not isinstance(parameters, tuple) or len(parameters) != 2:\n            raise TypeError(\"Callable must be used as \"\n                            \"Callable[[arg, ...], result].\")\n        args, result = parameters\n        if args is Ellipsis:\n            parameters = (Ellipsis, result)\n        else:\n            if not isinstance(args, list):\n                raise TypeError(\"Callable[args, result]: args must be a list.\"\n                                \" Got %.100r.\" % (args,))\n            parameters = (tuple(args), result)\n        return self.__getitem_inner__(parameters)\n\n    @_tp_cache\n    def __getitem_inner__(self, parameters):\n        args, result = parameters\n        msg = \"Callable[args, result]: result must be a type.\"\n        result = _type_check(result, msg)\n        if args is Ellipsis:\n            return super(CallableMeta, self).__getitem__((_TypingEllipsis, result))\n        msg = \"Callable[[arg, ...], result]: each arg must be a type.\"\n        args = tuple(_type_check(arg, msg) for arg in args)\n        parameters = args + (result,)\n        return super(CallableMeta, self).__getitem__(parameters)\n\n\nclass Callable(object):\n    \"\"\"Callable type; Callable[[int], str] is a function of (int) -> str.\n\n    The subscription syntax must always be used with exactly two\n    values: the argument list and the return type.  The argument list\n    must be a list of types or ellipsis; the return type must be a single type.\n\n    There is no syntax to indicate optional or keyword arguments,\n    such function types are rarely used as callback types.\n    \"\"\"\n\n    __metaclass__ = CallableMeta\n    __extra__ = collections_abc.Callable\n    __slots__ = ()\n\n    def __new__(cls, *args, **kwds):\n        if _geqv(cls, Callable):\n            raise TypeError(\"Type Callable cannot be instantiated; \"\n                            \"use a non-abstract subclass instead\")\n        return _generic_new(cls.__next_in_mro__, cls, *args, **kwds)\n\n\ndef cast(typ, val):\n    \"\"\"Cast a value to a type.\n\n    This returns the value unchanged.  To the type checker this\n    signals that the return value has the designated type, but at\n    runtime we intentionally don't check anything (we want this\n    to be as fast as possible).\n    \"\"\"\n    return val\n\n\ndef _get_defaults(func):\n    \"\"\"Internal helper to extract the default arguments, by name.\"\"\"\n    code = func.__code__\n    pos_count = code.co_argcount\n    arg_names = code.co_varnames\n    arg_names = arg_names[:pos_count]\n    defaults = func.__defaults__ or ()\n    kwdefaults = func.__kwdefaults__\n    res = dict(kwdefaults) if kwdefaults else {}\n    pos_offset = pos_count - len(defaults)\n    for name, value in zip(arg_names[pos_offset:], defaults):\n        assert name not in res\n        res[name] = value\n    return res\n\n\ndef get_type_hints(obj, globalns=None, localns=None):\n    \"\"\"In Python 2 this is not supported and always returns None.\"\"\"\n    return None\n\n\ndef no_type_check(arg):\n    \"\"\"Decorator to indicate that annotations are not type hints.\n\n    The argument must be a class or function; if it is a class, it\n    applies recursively to all methods and classes defined in that class\n    (but not to methods defined in its superclasses or subclasses).\n\n    This mutates the function(s) or class(es) in place.\n    \"\"\"\n    if isinstance(arg, type):\n        arg_attrs = arg.__dict__.copy()\n        for attr, val in arg.__dict__.items():\n            if val in arg.__bases__:\n                arg_attrs.pop(attr)\n        for obj in arg_attrs.values():\n            if isinstance(obj, types.FunctionType):\n                obj.__no_type_check__ = True\n            if isinstance(obj, type):\n                no_type_check(obj)\n    try:\n        arg.__no_type_check__ = True\n    except TypeError:  # built-in classes\n        pass\n    return arg\n\n\ndef no_type_check_decorator(decorator):\n    \"\"\"Decorator to give another decorator the @no_type_check effect.\n\n    This wraps the decorator with something that wraps the decorated\n    function in @no_type_check.\n    \"\"\"\n\n    @functools.wraps(decorator)\n    def wrapped_decorator(*args, **kwds):\n        func = decorator(*args, **kwds)\n        func = no_type_check(func)\n        return func\n\n    return wrapped_decorator\n\n\ndef _overload_dummy(*args, **kwds):\n    \"\"\"Helper for @overload to raise when called.\"\"\"\n    raise NotImplementedError(\n        \"You should not call an overloaded function. \"\n        \"A series of @overload-decorated functions \"\n        \"outside a stub module should always be followed \"\n        \"by an implementation that is not @overload-ed.\")\n\n\ndef overload(func):\n    \"\"\"Decorator for overloaded functions/methods.\n\n    In a stub file, place two or more stub definitions for the same\n    function in a row, each decorated with @overload.  For example:\n\n      @overload\n      def utf8(value: None) -> None: ...\n      @overload\n      def utf8(value: bytes) -> bytes: ...\n      @overload\n      def utf8(value: str) -> bytes: ...\n\n    In a non-stub file (i.e. a regular .py file), do the same but\n    follow it with an implementation.  The implementation should *not*\n    be decorated with @overload.  For example:\n\n      @overload\n      def utf8(value: None) -> None: ...\n      @overload\n      def utf8(value: bytes) -> bytes: ...\n      @overload\n      def utf8(value: str) -> bytes: ...\n      def utf8(value):\n          # implementation goes here\n    \"\"\"\n    return _overload_dummy\n\n\nclass _ProtocolMeta(GenericMeta):\n    \"\"\"Internal metaclass for _Protocol.\n\n    This exists so _Protocol classes can be generic without deriving\n    from Generic.\n    \"\"\"\n\n    def __instancecheck__(self, obj):\n        if _Protocol not in self.__bases__:\n            return super(_ProtocolMeta, self).__instancecheck__(obj)\n        raise TypeError(\"Protocols cannot be used with isinstance().\")\n\n    def __subclasscheck__(self, cls):\n        if not self._is_protocol:\n            # No structural checks since this isn't a protocol.\n            return NotImplemented\n\n        if self is _Protocol:\n            # Every class is a subclass of the empty protocol.\n            return True\n\n        # Find all attributes defined in the protocol.\n        attrs = self._get_protocol_attrs()\n\n        for attr in attrs:\n            if not any(attr in d.__dict__ for d in cls.__mro__):\n                return False\n        return True\n\n    def _get_protocol_attrs(self):\n        # Get all Protocol base classes.\n        protocol_bases = []\n        for c in self.__mro__:\n            if getattr(c, '_is_protocol', False) and c.__name__ != '_Protocol':\n                protocol_bases.append(c)\n\n        # Get attributes included in protocol.\n        attrs = set()\n        for base in protocol_bases:\n            for attr in base.__dict__.keys():\n                # Include attributes not defined in any non-protocol bases.\n                for c in self.__mro__:\n                    if (c is not base and attr in c.__dict__ and\n                            not getattr(c, '_is_protocol', False)):\n                        break\n                else:\n                    if (not attr.startswith('_abc_') and\n                            attr != '__abstractmethods__' and\n                            attr != '_is_protocol' and\n                            attr != '__dict__' and\n                            attr != '__args__' and\n                            attr != '__slots__' and\n                            attr != '_get_protocol_attrs' and\n                            attr != '__next_in_mro__' and\n                            attr != '__parameters__' and\n                            attr != '__origin__' and\n                            attr != '__orig_bases__' and\n                            attr != '__extra__' and\n                            attr != '__tree_hash__' and\n                            attr != '__module__'):\n                        attrs.add(attr)\n\n        return attrs\n\n\nclass _Protocol(object):\n    \"\"\"Internal base class for protocol classes.\n\n    This implements a simple-minded structural issubclass check\n    (similar but more general than the one-offs in collections.abc\n    such as Hashable).\n    \"\"\"\n\n    __metaclass__ = _ProtocolMeta\n    __slots__ = ()\n\n    _is_protocol = True\n\n\n# Various ABCs mimicking those in collections.abc.\n# A few are simply re-exported for completeness.\n\nHashable = collections_abc.Hashable  # Not generic.\n\n\nclass Iterable(Generic[T_co]):\n    __slots__ = ()\n    __extra__ = collections_abc.Iterable\n\n\nclass Iterator(Iterable[T_co]):\n    __slots__ = ()\n    __extra__ = collections_abc.Iterator\n\n\nclass SupportsInt(_Protocol):\n    __slots__ = ()\n\n    @abstractmethod\n    def __int__(self):\n        pass\n\n\nclass SupportsFloat(_Protocol):\n    __slots__ = ()\n\n    @abstractmethod\n    def __float__(self):\n        pass\n\n\nclass SupportsComplex(_Protocol):\n    __slots__ = ()\n\n    @abstractmethod\n    def __complex__(self):\n        pass\n\n\nclass SupportsAbs(_Protocol[T_co]):\n    __slots__ = ()\n\n    @abstractmethod\n    def __abs__(self):\n        pass\n\n\nif hasattr(collections_abc, 'Reversible'):\n    class Reversible(Iterable[T_co]):\n        __slots__ = ()\n        __extra__ = collections_abc.Reversible\nelse:\n    class Reversible(_Protocol[T_co]):\n        __slots__ = ()\n\n        @abstractmethod\n        def __reversed__(self):\n            pass\n\n\nSized = collections_abc.Sized  # Not generic.\n\n\nclass Container(Generic[T_co]):\n    __slots__ = ()\n    __extra__ = collections_abc.Container\n\n\n# Callable was defined earlier.\n\n\nclass AbstractSet(Sized, Iterable[T_co], Container[T_co]):\n    __slots__ = ()\n    __extra__ = collections_abc.Set\n\n\nclass MutableSet(AbstractSet[T]):\n    __slots__ = ()\n    __extra__ = collections_abc.MutableSet\n\n\n# NOTE: It is only covariant in the value type.\nclass Mapping(Sized, Iterable[KT], Container[KT], Generic[KT, VT_co]):\n    __slots__ = ()\n    __extra__ = collections_abc.Mapping\n\n\nclass MutableMapping(Mapping[KT, VT]):\n    __slots__ = ()\n    __extra__ = collections_abc.MutableMapping\n\n\nif hasattr(collections_abc, 'Reversible'):\n    class Sequence(Sized, Reversible[T_co], Container[T_co]):\n        __slots__ = ()\n        __extra__ = collections_abc.Sequence\nelse:\n    class Sequence(Sized, Iterable[T_co], Container[T_co]):\n        __slots__ = ()\n        __extra__ = collections_abc.Sequence\n\n\nclass MutableSequence(Sequence[T]):\n    __slots__ = ()\n    __extra__ = collections_abc.MutableSequence\n\n\nclass ByteString(Sequence[int]):\n    pass\n\n\nByteString.register(str)\nByteString.register(bytearray)\n\n\nclass List(list, MutableSequence[T]):\n    __slots__ = ()\n    __extra__ = list\n\n    def __new__(cls, *args, **kwds):\n        if _geqv(cls, List):\n            raise TypeError(\"Type List cannot be instantiated; \"\n                            \"use list() instead\")\n        return _generic_new(list, cls, *args, **kwds)\n\n\nclass Deque(collections.deque, MutableSequence[T]):\n    __slots__ = ()\n    __extra__ = collections.deque\n\n    def __new__(cls, *args, **kwds):\n        if _geqv(cls, Deque):\n            return collections.deque(*args, **kwds)\n        return _generic_new(collections.deque, cls, *args, **kwds)\n\n\nclass Set(set, MutableSet[T]):\n    __slots__ = ()\n    __extra__ = set\n\n    def __new__(cls, *args, **kwds):\n        if _geqv(cls, Set):\n            raise TypeError(\"Type Set cannot be instantiated; \"\n                            \"use set() instead\")\n        return _generic_new(set, cls, *args, **kwds)\n\n\nclass FrozenSet(frozenset, AbstractSet[T_co]):\n    __slots__ = ()\n    __extra__ = frozenset\n\n    def __new__(cls, *args, **kwds):\n        if _geqv(cls, FrozenSet):\n            raise TypeError(\"Type FrozenSet cannot be instantiated; \"\n                            \"use frozenset() instead\")\n        return _generic_new(frozenset, cls, *args, **kwds)\n\n\nclass MappingView(Sized, Iterable[T_co]):\n    __slots__ = ()\n    __extra__ = collections_abc.MappingView\n\n\nclass KeysView(MappingView[KT], AbstractSet[KT]):\n    __slots__ = ()\n    __extra__ = collections_abc.KeysView\n\n\nclass ItemsView(MappingView[Tuple[KT, VT_co]],\n                AbstractSet[Tuple[KT, VT_co]],\n                Generic[KT, VT_co]):\n    __slots__ = ()\n    __extra__ = collections_abc.ItemsView\n\n\nclass ValuesView(MappingView[VT_co]):\n    __slots__ = ()\n    __extra__ = collections_abc.ValuesView\n\n\nclass Dict(dict, MutableMapping[KT, VT]):\n    __slots__ = ()\n    __extra__ = dict\n\n    def __new__(cls, *args, **kwds):\n        if _geqv(cls, Dict):\n            raise TypeError(\"Type Dict cannot be instantiated; \"\n                            \"use dict() instead\")\n        return _generic_new(dict, cls, *args, **kwds)\n\n\nclass DefaultDict(collections.defaultdict, MutableMapping[KT, VT]):\n    __slots__ = ()\n    __extra__ = collections.defaultdict\n\n    def __new__(cls, *args, **kwds):\n        if _geqv(cls, DefaultDict):\n            return collections.defaultdict(*args, **kwds)\n        return _generic_new(collections.defaultdict, cls, *args, **kwds)\n\n\nclass Counter(collections.Counter, Dict[T, int]):\n    __slots__ = ()\n    __extra__ = collections.Counter\n\n    def __new__(cls, *args, **kwds):\n        if _geqv(cls, Counter):\n            return collections.Counter(*args, **kwds)\n        return _generic_new(collections.Counter, cls, *args, **kwds)\n\n\n# Determine what base class to use for Generator.\nif hasattr(collections_abc, 'Generator'):\n    # Sufficiently recent versions of 3.5 have a Generator ABC.\n    _G_base = collections_abc.Generator\nelse:\n    # Fall back on the exact type.\n    _G_base = types.GeneratorType\n\n\nclass Generator(Iterator[T_co], Generic[T_co, T_contra, V_co]):\n    __slots__ = ()\n    __extra__ = _G_base\n\n    def __new__(cls, *args, **kwds):\n        if _geqv(cls, Generator):\n            raise TypeError(\"Type Generator cannot be instantiated; \"\n                            \"create a subclass instead\")\n        return _generic_new(_G_base, cls, *args, **kwds)\n\n\n# Internal type variable used for Type[].\nCT_co = TypeVar('CT_co', covariant=True, bound=type)\n\n\n# This is not a real generic class.  Don't use outside annotations.\nclass Type(Generic[CT_co]):\n    \"\"\"A special construct usable to annotate class objects.\n\n    For example, suppose we have the following classes::\n\n      class User: ...  # Abstract base for User classes\n      class BasicUser(User): ...\n      class ProUser(User): ...\n      class TeamUser(User): ...\n\n    And a function that takes a class argument that's a subclass of\n    User and returns an instance of the corresponding class::\n\n      U = TypeVar('U', bound=User)\n      def new_user(user_class: Type[U]) -> U:\n          user = user_class()\n          # (Here we could write the user object to a database)\n          return user\n\n      joe = new_user(BasicUser)\n\n    At this point the type checker knows that joe has type BasicUser.\n    \"\"\"\n    __slots__ = ()\n    __extra__ = type\n\n\ndef NamedTuple(typename, fields):\n    \"\"\"Typed version of namedtuple.\n\n    Usage::\n\n        Employee = typing.NamedTuple('Employee', [('name', str), ('id', int)])\n\n    This is equivalent to::\n\n        Employee = collections.namedtuple('Employee', ['name', 'id'])\n\n    The resulting class has one extra attribute: _field_types,\n    giving a dict mapping field names to types.  (The field names\n    are in the _fields attribute, which is part of the namedtuple\n    API.)\n    \"\"\"\n    fields = [(n, t) for n, t in fields]\n    cls = collections.namedtuple(typename, [n for n, t in fields])\n    cls._field_types = dict(fields)\n    # Set the module to the caller's module (otherwise it'd be 'typing').\n    try:\n        cls.__module__ = sys._getframe(1).f_globals.get('__name__', '__main__')\n    except (AttributeError, ValueError):\n        pass\n    return cls\n\n\ndef NewType(name, tp):\n    \"\"\"NewType creates simple unique types with almost zero\n    runtime overhead. NewType(name, tp) is considered a subtype of tp\n    by static type checkers. At runtime, NewType(name, tp) returns\n    a dummy function that simply returns its argument. Usage::\n\n        UserId = NewType('UserId', int)\n\n        def name_by_id(user_id):\n            # type: (UserId) -> str\n            ...\n\n        UserId('user')          # Fails type check\n\n        name_by_id(42)          # Fails type check\n        name_by_id(UserId(42))  # OK\n\n        num = UserId(5) + 1     # type: int\n    \"\"\"\n\n    def new_type(x):\n        return x\n\n    # Some versions of Python 2 complain because of making all strings unicode\n    new_type.__name__ = str(name)\n    new_type.__supertype__ = tp\n    return new_type\n\n\n# Python-version-specific alias (Python 2: unicode; Python 3: str)\nText = unicode\n\n\n# Constant that's True when type checking, but False here.\nTYPE_CHECKING = False\n\n\nclass IO(Generic[AnyStr]):\n    \"\"\"Generic base class for TextIO and BinaryIO.\n\n    This is an abstract, generic version of the return of open().\n\n    NOTE: This does not distinguish between the different possible\n    classes (text vs. binary, read vs. write vs. read/write,\n    append-only, unbuffered).  The TextIO and BinaryIO subclasses\n    below capture the distinctions between text vs. binary, which is\n    pervasive in the interface; however we currently do not offer a\n    way to track the other distinctions in the type system.\n    \"\"\"\n\n    __slots__ = ()\n\n    @abstractproperty\n    def mode(self):\n        pass\n\n    @abstractproperty\n    def name(self):\n        pass\n\n    @abstractmethod\n    def close(self):\n        pass\n\n    @abstractmethod\n    def closed(self):\n        pass\n\n    @abstractmethod\n    def fileno(self):\n        pass\n\n    @abstractmethod\n    def flush(self):\n        pass\n\n    @abstractmethod\n    def isatty(self):\n        pass\n\n    @abstractmethod\n    def read(self, n=-1):\n        pass\n\n    @abstractmethod\n    def readable(self):\n        pass\n\n    @abstractmethod\n    def readline(self, limit=-1):\n        pass\n\n    @abstractmethod\n    def readlines(self, hint=-1):\n        pass\n\n    @abstractmethod\n    def seek(self, offset, whence=0):\n        pass\n\n    @abstractmethod\n    def seekable(self):\n        pass\n\n    @abstractmethod\n    def tell(self):\n        pass\n\n    @abstractmethod\n    def truncate(self, size=None):\n        pass\n\n    @abstractmethod\n    def writable(self):\n        pass\n\n    @abstractmethod\n    def write(self, s):\n        pass\n\n    @abstractmethod\n    def writelines(self, lines):\n        pass\n\n    @abstractmethod\n    def __enter__(self):\n        pass\n\n    @abstractmethod\n    def __exit__(self, type, value, traceback):\n        pass\n\n\nclass BinaryIO(IO[bytes]):\n    \"\"\"Typed version of the return of open() in binary mode.\"\"\"\n\n    __slots__ = ()\n\n    @abstractmethod\n    def write(self, s):\n        pass\n\n    @abstractmethod\n    def __enter__(self):\n        pass\n\n\nclass TextIO(IO[unicode]):\n    \"\"\"Typed version of the return of open() in text mode.\"\"\"\n\n    __slots__ = ()\n\n    @abstractproperty\n    def buffer(self):\n        pass\n\n    @abstractproperty\n    def encoding(self):\n        pass\n\n    @abstractproperty\n    def errors(self):\n        pass\n\n    @abstractproperty\n    def line_buffering(self):\n        pass\n\n    @abstractproperty\n    def newlines(self):\n        pass\n\n    @abstractmethod\n    def __enter__(self):\n        pass\n\n\nclass io(object):\n    \"\"\"Wrapper namespace for IO generic classes.\"\"\"\n\n    __all__ = ['IO', 'TextIO', 'BinaryIO']\n    IO = IO\n    TextIO = TextIO\n    BinaryIO = BinaryIO\n\n\nio.__name__ = __name__ + b'.io'\nsys.modules[io.__name__] = io\n\n\nPattern = _TypeAlias('Pattern', AnyStr, type(stdlib_re.compile('')),\n                     lambda p: p.pattern)\nMatch = _TypeAlias('Match', AnyStr, type(stdlib_re.match('', '')),\n                   lambda m: m.re.pattern)\n\n\nclass re(object):\n    \"\"\"Wrapper namespace for re type aliases.\"\"\"\n\n    __all__ = ['Pattern', 'Match']\n    Pattern = Pattern\n    Match = Match\n\n\nre.__name__ = __name__ + b'.re'\nsys.modules[re.__name__] = re\n", 
    "warnings": "\"\"\"Python part of the warnings subsystem.\"\"\"\n\n# Note: function level imports should *not* be used\n# in this module as it may cause import lock deadlock.\n# See bug 683658.\nimport linecache\nimport sys\nimport types\n\n__all__ = [\"warn\", \"warn_explicit\", \"showwarning\",\n           \"formatwarning\", \"filterwarnings\", \"simplefilter\",\n           \"resetwarnings\", \"catch_warnings\"]\n\n\ndef warnpy3k(message, category=None, stacklevel=1):\n    \"\"\"Issue a deprecation warning for Python 3.x related changes.\n\n    Warnings are omitted unless Python is started with the -3 option.\n    \"\"\"\n    if sys.py3kwarning:\n        if category is None:\n            category = DeprecationWarning\n        warn(message, category, stacklevel+1)\n\ndef _show_warning(message, category, filename, lineno, file=None, line=None):\n    \"\"\"Hook to write a warning to a file; replace if you like.\"\"\"\n    if file is None:\n        file = sys.stderr\n    try:\n        file.write(formatwarning(message, category, filename, lineno, line))\n    except IOError:\n        pass # the file (probably stderr) is invalid - this warning gets lost.\n# Keep a working version around in case the deprecation of the old API is\n# triggered.\nshowwarning = _show_warning\n\ndef formatwarning(message, category, filename, lineno, line=None):\n    \"\"\"Function to format a warning the standard way.\"\"\"\n    s =  \"%s:%s: %s: %s\\n\" % (filename, lineno, category.__name__, message)\n    line = linecache.getline(filename, lineno) if line is None else line\n    if line:\n        line = line.strip()\n        s += \"  %s\\n\" % line\n    return s\n\ndef filterwarnings(action, message=\"\", category=Warning, module=\"\", lineno=0,\n                   append=0):\n    \"\"\"Insert an entry into the list of warnings filters (at the front).\n\n    'action' -- one of \"error\", \"ignore\", \"always\", \"default\", \"module\",\n                or \"once\"\n    'message' -- a regex that the warning message must match\n    'category' -- a class that the warning must be a subclass of\n    'module' -- a regex that the module name must match\n    'lineno' -- an integer line number, 0 matches all warnings\n    'append' -- if true, append to the list of filters\n    \"\"\"\n    import re\n    assert action in (\"error\", \"ignore\", \"always\", \"default\", \"module\",\n                      \"once\"), \"invalid action: %r\" % (action,)\n    assert isinstance(message, basestring), \"message must be a string\"\n    assert isinstance(category, (type, types.ClassType)), \\\n           \"category must be a class\"\n    assert issubclass(category, Warning), \"category must be a Warning subclass\"\n    assert isinstance(module, basestring), \"module must be a string\"\n    assert isinstance(lineno, int) and lineno >= 0, \\\n           \"lineno must be an int >= 0\"\n    item = (action, re.compile(message, re.I), category,\n            re.compile(module), lineno)\n    if append:\n        filters.append(item)\n    else:\n        filters.insert(0, item)\n\ndef simplefilter(action, category=Warning, lineno=0, append=0):\n    \"\"\"Insert a simple entry into the list of warnings filters (at the front).\n\n    A simple filter matches all modules and messages.\n    'action' -- one of \"error\", \"ignore\", \"always\", \"default\", \"module\",\n                or \"once\"\n    'category' -- a class that the warning must be a subclass of\n    'lineno' -- an integer line number, 0 matches all warnings\n    'append' -- if true, append to the list of filters\n    \"\"\"\n    assert action in (\"error\", \"ignore\", \"always\", \"default\", \"module\",\n                      \"once\"), \"invalid action: %r\" % (action,)\n    assert isinstance(lineno, int) and lineno >= 0, \\\n           \"lineno must be an int >= 0\"\n    item = (action, None, category, None, lineno)\n    if append:\n        filters.append(item)\n    else:\n        filters.insert(0, item)\n\ndef resetwarnings():\n    \"\"\"Clear the list of warning filters, so that no filters are active.\"\"\"\n    filters[:] = []\n\nclass _OptionError(Exception):\n    \"\"\"Exception used by option processing helpers.\"\"\"\n    pass\n\n# Helper to process -W options passed via sys.warnoptions\ndef _processoptions(args):\n    for arg in args:\n        try:\n            _setoption(arg)\n        except _OptionError, msg:\n            print >>sys.stderr, \"Invalid -W option ignored:\", msg\n\n# Helper for _processoptions()\ndef _setoption(arg):\n    import re\n    parts = arg.split(':')\n    if len(parts) > 5:\n        raise _OptionError(\"too many fields (max 5): %r\" % (arg,))\n    while len(parts) < 5:\n        parts.append('')\n    action, message, category, module, lineno = [s.strip()\n                                                 for s in parts]\n    action = _getaction(action)\n    message = re.escape(message)\n    category = _getcategory(category)\n    module = re.escape(module)\n    if module:\n        module = module + '$'\n    if lineno:\n        try:\n            lineno = int(lineno)\n            if lineno < 0:\n                raise ValueError\n        except (ValueError, OverflowError):\n            raise _OptionError(\"invalid lineno %r\" % (lineno,))\n    else:\n        lineno = 0\n    filterwarnings(action, message, category, module, lineno)\n\n# Helper for _setoption()\ndef _getaction(action):\n    if not action:\n        return \"default\"\n    if action == \"all\": return \"always\" # Alias\n    for a in ('default', 'always', 'ignore', 'module', 'once', 'error'):\n        if a.startswith(action):\n            return a\n    raise _OptionError(\"invalid action: %r\" % (action,))\n\n# Helper for _setoption()\ndef _getcategory(category):\n    import re\n    if not category:\n        return Warning\n    if re.match(\"^[a-zA-Z0-9_]+$\", category):\n        try:\n            cat = eval(category)\n        except NameError:\n            raise _OptionError(\"unknown warning category: %r\" % (category,))\n    else:\n        i = category.rfind(\".\")\n        module = category[:i]\n        klass = category[i+1:]\n        try:\n            m = __import__(module, None, None, [klass])\n        except ImportError:\n            raise _OptionError(\"invalid module name: %r\" % (module,))\n        try:\n            cat = getattr(m, klass)\n        except AttributeError:\n            raise _OptionError(\"unknown warning category: %r\" % (category,))\n    if not issubclass(cat, Warning):\n        raise _OptionError(\"invalid warning category: %r\" % (category,))\n    return cat\n\n\n# Code typically replaced by _warnings\ndef warn(message, category=None, stacklevel=1):\n    \"\"\"Issue a warning, or maybe ignore it or raise an exception.\"\"\"\n    # Check if message is already a Warning object\n    if isinstance(message, Warning):\n        category = message.__class__\n    # Check category argument\n    if category is None:\n        category = UserWarning\n    assert issubclass(category, Warning)\n    # Get context information\n    try:\n        caller = sys._getframe(stacklevel)\n    except ValueError:\n        globals = sys.__dict__\n        lineno = 1\n    else:\n        globals = caller.f_globals\n        lineno = caller.f_lineno\n    if '__name__' in globals:\n        module = globals['__name__']\n    else:\n        module = \"<string>\"\n    filename = globals.get('__file__')\n    if filename:\n        fnl = filename.lower()\n        if fnl.endswith((\".pyc\", \".pyo\")):\n            filename = filename[:-1]\n    else:\n        if module == \"__main__\":\n            try:\n                filename = sys.argv[0]\n            except AttributeError:\n                # embedded interpreters don't have sys.argv, see bug #839151\n                filename = '__main__'\n        if not filename:\n            filename = module\n    registry = globals.setdefault(\"__warningregistry__\", {})\n    warn_explicit(message, category, filename, lineno, module, registry,\n                  globals)\n\ndef warn_explicit(message, category, filename, lineno,\n                  module=None, registry=None, module_globals=None):\n    lineno = int(lineno)\n    if module is None:\n        module = filename or \"<unknown>\"\n        if module[-3:].lower() == \".py\":\n            module = module[:-3] # XXX What about leading pathname?\n    if registry is None:\n        registry = {}\n    if isinstance(message, Warning):\n        text = str(message)\n        category = message.__class__\n    else:\n        text = message\n        message = category(message)\n    key = (text, category, lineno)\n    # Quick test for common case\n    if registry.get(key):\n        return\n    # Search the filters\n    for item in filters:\n        action, msg, cat, mod, ln = item\n        if ((msg is None or msg.match(text)) and\n            issubclass(category, cat) and\n            (mod is None or mod.match(module)) and\n            (ln == 0 or lineno == ln)):\n            break\n    else:\n        action = defaultaction\n    # Early exit actions\n    if action == \"ignore\":\n        registry[key] = 1\n        return\n\n    # Prime the linecache for formatting, in case the\n    # \"file\" is actually in a zipfile or something.\n    linecache.getlines(filename, module_globals)\n\n    if action == \"error\":\n        raise message\n    # Other actions\n    if action == \"once\":\n        registry[key] = 1\n        oncekey = (text, category)\n        if onceregistry.get(oncekey):\n            return\n        onceregistry[oncekey] = 1\n    elif action == \"always\":\n        pass\n    elif action == \"module\":\n        registry[key] = 1\n        altkey = (text, category, 0)\n        if registry.get(altkey):\n            return\n        registry[altkey] = 1\n    elif action == \"default\":\n        registry[key] = 1\n    else:\n        # Unrecognized actions are errors\n        raise RuntimeError(\n              \"Unrecognized action (%r) in warnings.filters:\\n %s\" %\n              (action, item))\n    # Print message and context\n    showwarning(message, category, filename, lineno)\n\n\nclass WarningMessage(object):\n\n    \"\"\"Holds the result of a single showwarning() call.\"\"\"\n\n    _WARNING_DETAILS = (\"message\", \"category\", \"filename\", \"lineno\", \"file\",\n                        \"line\")\n\n    def __init__(self, message, category, filename, lineno, file=None,\n                    line=None):\n        local_values = locals()\n        for attr in self._WARNING_DETAILS:\n            setattr(self, attr, local_values[attr])\n        self._category_name = category.__name__ if category else None\n\n    def __str__(self):\n        return (\"{message : %r, category : %r, filename : %r, lineno : %s, \"\n                    \"line : %r}\" % (self.message, self._category_name,\n                                    self.filename, self.lineno, self.line))\n\n\nclass catch_warnings(object):\n\n    \"\"\"A context manager that copies and restores the warnings filter upon\n    exiting the context.\n\n    The 'record' argument specifies whether warnings should be captured by a\n    custom implementation of warnings.showwarning() and be appended to a list\n    returned by the context manager. Otherwise None is returned by the context\n    manager. The objects appended to the list are arguments whose attributes\n    mirror the arguments to showwarning().\n\n    The 'module' argument is to specify an alternative module to the module\n    named 'warnings' and imported under that name. This argument is only useful\n    when testing the warnings module itself.\n\n    \"\"\"\n\n    def __init__(self, record=False, module=None):\n        \"\"\"Specify whether to record warnings and if an alternative module\n        should be used other than sys.modules['warnings'].\n\n        For compatibility with Python 3.0, please consider all arguments to be\n        keyword-only.\n\n        \"\"\"\n        self._record = record\n        self._module = sys.modules['warnings'] if module is None else module\n        self._entered = False\n\n    def __repr__(self):\n        args = []\n        if self._record:\n            args.append(\"record=True\")\n        if self._module is not sys.modules['warnings']:\n            args.append(\"module=%r\" % self._module)\n        name = type(self).__name__\n        return \"%s(%s)\" % (name, \", \".join(args))\n\n    def __enter__(self):\n        if self._entered:\n            raise RuntimeError(\"Cannot enter %r twice\" % self)\n        self._entered = True\n        self._filters = self._module.filters\n        self._module.filters = self._filters[:]\n        self._showwarning = self._module.showwarning\n        if self._record:\n            log = []\n            def showwarning(*args, **kwargs):\n                log.append(WarningMessage(*args, **kwargs))\n            self._module.showwarning = showwarning\n            return log\n        else:\n            return None\n\n    def __exit__(self, *exc_info):\n        if not self._entered:\n            raise RuntimeError(\"Cannot exit %r without entering first\" % self)\n        self._module.filters = self._filters\n        self._module.showwarning = self._showwarning\n\n\n# filters contains a sequence of filter 5-tuples\n# The components of the 5-tuple are:\n# - an action: error, ignore, always, default, module, or once\n# - a compiled regex that must match the warning message\n# - a class representing the warning category\n# - a compiled regex that must match the module that is being warned\n# - a line number for the line being warning, or 0 to mean any line\n# If either if the compiled regexs are None, match anything.\n_warnings_defaults = False\ntry:\n    from _warnings import (filters, default_action, once_registry,\n                            warn, warn_explicit)\n    defaultaction = default_action\n    onceregistry = once_registry\n    _warnings_defaults = True\nexcept ImportError:\n    filters = []\n    defaultaction = \"default\"\n    onceregistry = {}\n\n\n# Module initialization\n_processoptions(sys.warnoptions)\nif not _warnings_defaults:\n    silence = [ImportWarning, PendingDeprecationWarning]\n    # Don't silence DeprecationWarning if -3 or -Q was used.\n    if not sys.py3kwarning and not sys.flags.division_warning:\n        silence.append(DeprecationWarning)\n    for cls in silence:\n        simplefilter(\"ignore\", category=cls)\n    bytes_warning = sys.flags.bytes_warning\n    if bytes_warning > 1:\n        bytes_action = \"error\"\n    elif bytes_warning:\n        bytes_action = \"default\"\n    else:\n        bytes_action = \"ignore\"\n    simplefilter(bytes_action, category=BytesWarning, append=1)\ndel _warnings_defaults\n", 
    "weakref": "\"\"\"Weak reference support for Python.\n\nThis module is an implementation of PEP 205:\n\nhttp://www.python.org/dev/peps/pep-0205/\n\"\"\"\n\n# Naming convention: Variables named \"wr\" are weak reference objects;\n# they are called this instead of \"ref\" to avoid name collisions with\n# the module-global ref() function imported from _weakref.\n\nimport UserDict\n\nfrom _weakref import (\n     getweakrefcount,\n     getweakrefs,\n     ref,\n     proxy,\n     CallableProxyType,\n     ProxyType,\n     ReferenceType)\n\nfrom _weakrefset import WeakSet, _IterationGuard\n\nfrom exceptions import ReferenceError\n\n\nProxyTypes = (ProxyType, CallableProxyType)\n\n__all__ = [\"ref\", \"proxy\", \"getweakrefcount\", \"getweakrefs\",\n           \"WeakKeyDictionary\", \"ReferenceError\", \"ReferenceType\", \"ProxyType\",\n           \"CallableProxyType\", \"ProxyTypes\", \"WeakValueDictionary\", 'WeakSet']\n\n\nclass WeakValueDictionary(UserDict.UserDict):\n    \"\"\"Mapping class that references values weakly.\n\n    Entries in the dictionary will be discarded when no strong\n    reference to the value exists anymore\n    \"\"\"\n    # We inherit the constructor without worrying about the input\n    # dictionary; since it uses our .update() method, we get the right\n    # checks (if the other dictionary is a WeakValueDictionary,\n    # objects are unwrapped on the way out, and we always wrap on the\n    # way in).\n\n    def __init__(self, *args, **kw):\n        def remove(wr, selfref=ref(self)):\n            self = selfref()\n            if self is not None:\n                if self._iterating:\n                    self._pending_removals.append(wr.key)\n                else:\n                    # Changed this for PyPy: made more resistent.  The\n                    # issue is that in some corner cases, self.data\n                    # might already be changed or removed by the time\n                    # this weakref's callback is called.  If that is\n                    # the case, we don't want to randomly kill an\n                    # unrelated entry.\n                    if self.data.get(wr.key) is wr:\n                        del self.data[wr.key]\n        self._remove = remove\n        # A list of keys to be removed\n        self._pending_removals = []\n        self._iterating = set()\n        UserDict.UserDict.__init__(self, *args, **kw)\n\n    def _commit_removals(self):\n        l = self._pending_removals\n        d = self.data\n        # We shouldn't encounter any KeyError, because this method should\n        # always be called *before* mutating the dict.\n        while l:\n            del d[l.pop()]\n\n    def __getitem__(self, key):\n        o = self.data[key]()\n        if o is None:\n            raise KeyError, key\n        else:\n            return o\n\n    def __delitem__(self, key):\n        if self._pending_removals:\n            self._commit_removals()\n        del self.data[key]\n\n    def __contains__(self, key):\n        try:\n            o = self.data[key]()\n        except KeyError:\n            return False\n        return o is not None\n\n    def has_key(self, key):\n        try:\n            o = self.data[key]()\n        except KeyError:\n            return False\n        return o is not None\n\n    def __repr__(self):\n        return \"<WeakValueDictionary at %s>\" % id(self)\n\n    def __setitem__(self, key, value):\n        if self._pending_removals:\n            self._commit_removals()\n        self.data[key] = KeyedRef(value, self._remove, key)\n\n    def clear(self):\n        if self._pending_removals:\n            self._commit_removals()\n        self.data.clear()\n\n    def copy(self):\n        new = WeakValueDictionary()\n        for key, wr in self.data.items():\n            o = wr()\n            if o is not None:\n                new[key] = o\n        return new\n\n    __copy__ = copy\n\n    def __deepcopy__(self, memo):\n        from copy import deepcopy\n        new = self.__class__()\n        for key, wr in self.data.items():\n            o = wr()\n            if o is not None:\n                new[deepcopy(key, memo)] = o\n        return new\n\n    def get(self, key, default=None):\n        try:\n            wr = self.data[key]\n        except KeyError:\n            return default\n        else:\n            o = wr()\n            if o is None:\n                # This should only happen\n                return default\n            else:\n                return o\n\n    def items(self):\n        L = []\n        for key, wr in self.data.items():\n            o = wr()\n            if o is not None:\n                L.append((key, o))\n        return L\n\n    def iteritems(self):\n        with _IterationGuard(self):\n            for wr in self.data.itervalues():\n                value = wr()\n                if value is not None:\n                    yield wr.key, value\n\n    def iterkeys(self):\n        with _IterationGuard(self):\n            for k in self.data.iterkeys():\n                yield k\n\n    __iter__ = iterkeys\n\n    def itervaluerefs(self):\n        \"\"\"Return an iterator that yields the weak references to the values.\n\n        The references are not guaranteed to be 'live' at the time\n        they are used, so the result of calling the references needs\n        to be checked before being used.  This can be used to avoid\n        creating references that will cause the garbage collector to\n        keep the values around longer than needed.\n\n        \"\"\"\n        with _IterationGuard(self):\n            for wr in self.data.itervalues():\n                yield wr\n\n    def itervalues(self):\n        with _IterationGuard(self):\n            for wr in self.data.itervalues():\n                obj = wr()\n                if obj is not None:\n                    yield obj\n\n    def popitem(self):\n        if self._pending_removals:\n            self._commit_removals()\n        while 1:\n            key, wr = self.data.popitem()\n            o = wr()\n            if o is not None:\n                return key, o\n\n    def pop(self, key, *args):\n        if self._pending_removals:\n            self._commit_removals()\n        try:\n            o = self.data.pop(key)()\n        except KeyError:\n            o = None\n        if o is None:\n            if args:\n                return args[0]\n            raise KeyError, key\n        else:\n            return o\n        # The logic above was fixed in PyPy\n\n    def setdefault(self, key, default=None):\n        try:\n            o = self.data[key]()\n        except KeyError:\n            o = None\n        if o is None:\n            if self._pending_removals:\n                self._commit_removals()\n            self.data[key] = KeyedRef(default, self._remove, key)\n            return default\n        else:\n            return o\n        # The logic above was fixed in PyPy\n\n    def update(self, dict=None, **kwargs):\n        if self._pending_removals:\n            self._commit_removals()\n        d = self.data\n        if dict is not None:\n            if not hasattr(dict, \"items\"):\n                dict = type({})(dict)\n            for key, o in dict.items():\n                d[key] = KeyedRef(o, self._remove, key)\n        if len(kwargs):\n            self.update(kwargs)\n\n    def valuerefs(self):\n        \"\"\"Return a list of weak references to the values.\n\n        The references are not guaranteed to be 'live' at the time\n        they are used, so the result of calling the references needs\n        to be checked before being used.  This can be used to avoid\n        creating references that will cause the garbage collector to\n        keep the values around longer than needed.\n\n        \"\"\"\n        return self.data.values()\n\n    def values(self):\n        L = []\n        for wr in self.data.values():\n            o = wr()\n            if o is not None:\n                L.append(o)\n        return L\n\n\nclass KeyedRef(ref):\n    \"\"\"Specialized reference that includes a key corresponding to the value.\n\n    This is used in the WeakValueDictionary to avoid having to create\n    a function object for each key stored in the mapping.  A shared\n    callback object can use the 'key' attribute of a KeyedRef instead\n    of getting a reference to the key from an enclosing scope.\n\n    \"\"\"\n\n    __slots__ = \"key\",\n\n    def __new__(type, ob, callback, key):\n        self = ref.__new__(type, ob, callback)\n        self.key = key\n        return self\n\n    def __init__(self, ob, callback, key):\n        super(KeyedRef,  self).__init__(ob, callback)\n\n\nclass WeakKeyDictionary(UserDict.UserDict):\n    \"\"\" Mapping class that references keys weakly.\n\n    Entries in the dictionary will be discarded when there is no\n    longer a strong reference to the key. This can be used to\n    associate additional data with an object owned by other parts of\n    an application without adding attributes to those objects. This\n    can be especially useful with objects that override attribute\n    accesses.\n    \"\"\"\n\n    def __init__(self, dict=None):\n        self.data = {}\n        def remove(k, selfref=ref(self)):\n            self = selfref()\n            if self is not None:\n                if self._iterating:\n                    self._pending_removals.append(k)\n                else:\n                    del self.data[k]\n        self._remove = remove\n        # A list of dead weakrefs (keys to be removed)\n        self._pending_removals = []\n        self._iterating = set()\n        if dict is not None:\n            self.update(dict)\n\n    def _commit_removals(self):\n        # NOTE: We don't need to call this method before mutating the dict,\n        # because a dead weakref never compares equal to a live weakref,\n        # even if they happened to refer to equal objects.\n        # However, it means keys may already have been removed.\n        l = self._pending_removals\n        d = self.data\n        while l:\n            try:\n                del d[l.pop()]\n            except KeyError:\n                pass\n\n    def __delitem__(self, key):\n        del self.data[ref(key)]\n\n    def __getitem__(self, key):\n        return self.data[ref(key)]\n\n    def __repr__(self):\n        return \"<WeakKeyDictionary at %s>\" % id(self)\n\n    def __setitem__(self, key, value):\n        self.data[ref(key, self._remove)] = value\n\n    def copy(self):\n        new = WeakKeyDictionary()\n        for key, value in self.data.items():\n            o = key()\n            if o is not None:\n                new[o] = value\n        return new\n\n    __copy__ = copy\n\n    def __deepcopy__(self, memo):\n        from copy import deepcopy\n        new = self.__class__()\n        for key, value in self.data.items():\n            o = key()\n            if o is not None:\n                new[o] = deepcopy(value, memo)\n        return new\n\n    def get(self, key, default=None):\n        return self.data.get(ref(key),default)\n\n    def has_key(self, key):\n        try:\n            wr = ref(key)\n        except TypeError:\n            return 0\n        return wr in self.data\n\n    def __contains__(self, key):\n        try:\n            wr = ref(key)\n        except TypeError:\n            return 0\n        return wr in self.data\n\n    def items(self):\n        L = []\n        for key, value in self.data.items():\n            o = key()\n            if o is not None:\n                L.append((o, value))\n        return L\n\n    def iteritems(self):\n        with _IterationGuard(self):\n            for wr, value in self.data.iteritems():\n                key = wr()\n                if key is not None:\n                    yield key, value\n\n    def iterkeyrefs(self):\n        \"\"\"Return an iterator that yields the weak references to the keys.\n\n        The references are not guaranteed to be 'live' at the time\n        they are used, so the result of calling the references needs\n        to be checked before being used.  This can be used to avoid\n        creating references that will cause the garbage collector to\n        keep the keys around longer than needed.\n\n        \"\"\"\n        with _IterationGuard(self):\n            for wr in self.data.iterkeys():\n                yield wr\n\n    def iterkeys(self):\n        with _IterationGuard(self):\n            for wr in self.data.iterkeys():\n                obj = wr()\n                if obj is not None:\n                    yield obj\n\n    __iter__ = iterkeys\n\n    def itervalues(self):\n        with _IterationGuard(self):\n            for value in self.data.itervalues():\n                yield value\n\n    def keyrefs(self):\n        \"\"\"Return a list of weak references to the keys.\n\n        The references are not guaranteed to be 'live' at the time\n        they are used, so the result of calling the references needs\n        to be checked before being used.  This can be used to avoid\n        creating references that will cause the garbage collector to\n        keep the keys around longer than needed.\n\n        \"\"\"\n        return self.data.keys()\n\n    def keys(self):\n        L = []\n        for wr in self.data.keys():\n            o = wr()\n            if o is not None:\n                L.append(o)\n        return L\n\n    def popitem(self):\n        while 1:\n            key, value = self.data.popitem()\n            o = key()\n            if o is not None:\n                return o, value\n\n    def pop(self, key, *args):\n        return self.data.pop(ref(key), *args)\n\n    def setdefault(self, key, default=None):\n        return self.data.setdefault(ref(key, self._remove),default)\n\n    def update(self, dict=None, **kwargs):\n        d = self.data\n        if dict is not None:\n            if not hasattr(dict, \"items\"):\n                dict = type({})(dict)\n            for key, value in dict.items():\n                d[ref(key, self._remove)] = value\n        if len(kwargs):\n            self.update(kwargs)\n"
  }
}