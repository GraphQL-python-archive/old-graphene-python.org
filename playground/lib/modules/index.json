{
  "modules": {
    "BaseHTTPServer": {
      "file": "BaseHTTPServer.py", 
      "imports": [
        "sys", 
        "time", 
        "mimetools", 
        "socket", 
        "SocketServer", 
        "warnings"
      ]
    }, 
    "Bastion": {
      "file": "Bastion.py", 
      "imports": [
        "rexec", 
        "types", 
        "warnings"
      ]
    }, 
    "CGIHTTPServer": {
      "file": "CGIHTTPServer.py", 
      "imports": [
        "BaseHTTPServer", 
        "base64", 
        "binascii", 
        "select", 
        "subprocess", 
        "sys", 
        "copy", 
        "os", 
        "SimpleHTTPServer", 
        "urllib", 
        "pwd"
      ]
    }, 
    "ConfigParser": {
      "file": "ConfigParser.py", 
      "imports": [
        "collections", 
        "re", 
        "UserDict"
      ]
    }, 
    "Cookie": {
      "file": "Cookie.py", 
      "imports": [
        "Cookie", 
        "time.gmtime", 
        "time.time", 
        "doctest", 
        "pickle", 
        "re", 
        "string", 
        "warnings", 
        "cPickle"
      ]
    }, 
    "DocXMLRPCServer": {
      "file": "DocXMLRPCServer.py", 
      "imports": [
        "sys", 
        "inspect", 
        "pydoc", 
        "re", 
        "SimpleXMLRPCServer"
      ]
    }, 
    "HTMLParser": {
      "file": "HTMLParser.py", 
      "imports": [
        "htmlentitydefs", 
        "markupbase", 
        "re"
      ]
    }, 
    "MimeWriter": {
      "file": "MimeWriter.py", 
      "imports": [
        "mimetools", 
        "test.test_MimeWriter", 
        "warnings"
      ]
    }, 
    "Queue": {
      "file": "Queue.py", 
      "imports": [
        "collections", 
        "dummy_threading", 
        "heapq", 
        "threading", 
        "time.time"
      ]
    }, 
    "SimpleHTTPServer": {
      "file": "SimpleHTTPServer.py", 
      "imports": [
        "BaseHTTPServer", 
        "cStringIO.StringIO", 
        "cgi", 
        "mimetypes", 
        "os", 
        "posixpath", 
        "shutil", 
        "sys", 
        "StringIO", 
        "urllib"
      ]
    }, 
    "SimpleXMLRPCServer": {
      "file": "SimpleXMLRPCServer.py", 
      "imports": [
        "BaseHTTPServer", 
        "fcntl", 
        "os", 
        "pydoc", 
        "re", 
        "sys", 
        "SocketServer", 
        "traceback", 
        "xmlrpclib"
      ]
    }, 
    "SocketServer": {
      "file": "SocketServer.py", 
      "imports": [
        "cStringIO.StringIO", 
        "dummy_threading", 
        "errno", 
        "os", 
        "select", 
        "socket", 
        "sys", 
        "threading", 
        "StringIO", 
        "traceback"
      ]
    }, 
    "StringIO": {
      "file": "StringIO.py", 
      "imports": [
        "errno.EINVAL", 
        "sys"
      ]
    }, 
    "UserDict": {
      "file": "UserDict.py", 
      "imports": [
        "_abcoll", 
        "copy"
      ]
    }, 
    "UserList": {
      "file": "UserList.py", 
      "imports": [
        "collections"
      ]
    }, 
    "UserString": {
      "file": "UserString.py", 
      "imports": [
        "collections", 
        "os", 
        "sys", 
        "test.test_support", 
        "warnings"
      ]
    }, 
    "_LWPCookieJar": {
      "file": "_LWPCookieJar.py", 
      "imports": [
        "time", 
        "cookielib", 
        "re"
      ]
    }, 
    "_MozillaCookieJar": {
      "file": "_MozillaCookieJar.py", 
      "imports": [
        "time", 
        "cookielib", 
        "re"
      ]
    }, 
    "__future__": {
      "file": "__future__.py", 
      "imports": []
    }, 
    "__init__": {
      "file": "__init__.py", 
      "imports": []
    }, 
    "__phello__.foo": {
      "file": "__phello__.foo.py", 
      "imports": []
    }, 
    "_abcoll": {
      "file": "_abcoll.py", 
      "imports": [
        "sys", 
        "abc"
      ]
    }, 
    "_audioop_build": {
      "file": "_audioop_build.py", 
      "imports": [
        "cffi.FFI"
      ]
    }, 
    "_codecs_cn": {
      "file": "_codecs_cn.py", 
      "imports": [
        "_multibytecodec.__getcodec"
      ]
    }, 
    "_codecs_hk": {
      "file": "_codecs_hk.py", 
      "imports": [
        "_multibytecodec.__getcodec"
      ]
    }, 
    "_codecs_iso2022": {
      "file": "_codecs_iso2022.py", 
      "imports": [
        "_multibytecodec.__getcodec"
      ]
    }, 
    "_codecs_jp": {
      "file": "_codecs_jp.py", 
      "imports": [
        "_multibytecodec.__getcodec"
      ]
    }, 
    "_codecs_kr": {
      "file": "_codecs_kr.py", 
      "imports": [
        "_multibytecodec.__getcodec"
      ]
    }, 
    "_codecs_tw": {
      "file": "_codecs_tw.py", 
      "imports": [
        "_multibytecodec.__getcodec"
      ]
    }, 
    "_collections": {
      "file": "_collections.py", 
      "imports": [
        "threading._get_ident"
      ]
    }, 
    "_csv": {
      "file": "_csv.py", 
      "imports": []
    }, 
    "_ctypes_test": {
      "file": "_ctypes_test.py", 
      "imports": [
        "_ctypes", 
        "cpyext", 
        "imp", 
        "os", 
        "_pypy_testcapi"
      ]
    }, 
    "_curses": {
      "file": "_curses.py", 
      "imports": [
        "_curses_cffi.ffi", 
        "_curses_cffi.lib", 
        "functools", 
        "sys"
      ]
    }, 
    "_curses_build": {
      "file": "_curses_build.py", 
      "imports": [
        "cffi.FFI"
      ]
    }, 
    "_curses_panel": {
      "file": "_curses_panel.py", 
      "imports": [
        "_curses"
      ]
    }, 
    "_elementtree": {
      "file": "_elementtree.py", 
      "imports": [
        "xml.etree.ElementTree"
      ]
    }, 
    "_functools": {
      "file": "_functools.py", 
      "imports": []
    }, 
    "_gdbm_build": {
      "file": "_gdbm_build.py", 
      "imports": [
        "cffi", 
        "os", 
        "sys"
      ]
    }, 
    "_marshal": {
      "file": "_marshal.py", 
      "imports": [
        "__pypy__.builtinify", 
        "sys.intern", 
        "types"
      ]
    }, 
    "_md5": {
      "file": "_md5.py", 
      "imports": [
        "copy", 
        "struct"
      ]
    }, 
    "_osx_support": {
      "file": "_osx_support.py", 
      "imports": [
        "sys", 
        "contextlib", 
        "distutils.log", 
        "os", 
        "re", 
        "tempfile"
      ]
    }, 
    "_pwdgrp_build": {
      "file": "_pwdgrp_build.py", 
      "imports": [
        "cffi.FFI"
      ]
    }, 
    "_pyio": {
      "file": "_pyio.py", 
      "imports": [
        "__future__", 
        "_io.FileIO", 
        "array", 
        "errno", 
        "errno.EINTR", 
        "thread.allocate_lock", 
        "abc", 
        "codecs", 
        "dummy_thread", 
        "io", 
        "locale", 
        "os", 
        "warnings"
      ]
    }, 
    "_pypy_interact": {
      "file": "_pypy_interact.py", 
      "imports": [
        "__main__", 
        "code", 
        "os", 
        "sys", 
        "_pypy_irc_topic", 
        "pyrepl.simple_interact"
      ]
    }, 
    "_pypy_irc_topic": {
      "file": "_pypy_irc_topic.py", 
      "imports": [
        "string", 
        "time"
      ]
    }, 
    "_pypy_testcapi": {
      "file": "_pypy_testcapi.py", 
      "imports": [
        "binascii", 
        "distutils.ccompiler", 
        "imp", 
        "os", 
        "sys", 
        "tempfile"
      ]
    }, 
    "_pypy_wait": {
      "file": "_pypy_wait.py", 
      "imports": [
        "ctypes.CDLL", 
        "ctypes.POINTER", 
        "ctypes.byref", 
        "ctypes.c_int", 
        "ctypes.util.find_library", 
        "resource"
      ]
    }, 
    "_scproxy": {
      "file": "_scproxy.py", 
      "imports": [
        "ctypes.c_char_p", 
        "ctypes.c_int", 
        "ctypes.c_int32", 
        "ctypes.c_int64", 
        "ctypes.c_void_p", 
        "ctypes.cdll", 
        "ctypes.create_string_buffer", 
        "ctypes.pointer", 
        "ctypes.util.find_library", 
        "sys"
      ]
    }, 
    "_sha": {
      "file": "_sha.py", 
      "imports": [
        "copy", 
        "struct"
      ]
    }, 
    "_sha256": {
      "file": "_sha256.py", 
      "imports": [
        "struct"
      ]
    }, 
    "_sha512": {
      "file": "_sha512.py", 
      "imports": [
        "_sha512", 
        "struct"
      ]
    }, 
    "_sqlite3": {
      "file": "_sqlite3.py", 
      "imports": [
        "__pypy__.newlist_hint", 
        "_sqlite3_cffi.ffi", 
        "_sqlite3_cffi.lib", 
        "collections", 
        "functools", 
        "sqlite3.dump._iterdump", 
        "string", 
        "sys", 
        "threading._get_ident", 
        "weakref", 
        "datetime"
      ]
    }, 
    "_sqlite3_build": {
      "file": "_sqlite3_build.py", 
      "imports": [
        "cffi.FFI", 
        "os", 
        "sys"
      ]
    }, 
    "_strptime": {
      "file": "_strptime.py", 
      "imports": [
        "thread.allocate_lock", 
        "time", 
        "calendar", 
        "dummy_thread", 
        "locale", 
        "re", 
        "datetime"
      ]
    }, 
    "_structseq": {
      "file": "_structseq.py", 
      "imports": []
    }, 
    "_syslog_build": {
      "file": "_syslog_build.py", 
      "imports": [
        "cffi.FFI"
      ]
    }, 
    "_testcapi": {
      "file": "_testcapi.py", 
      "imports": [
        "_pypy_testcapi", 
        "cpyext", 
        "imp", 
        "os"
      ]
    }, 
    "_threading_local": {
      "file": "_threading_local.py", 
      "imports": [
        "threading", 
        "threading.RLock", 
        "threading.current_thread"
      ]
    }, 
    "_weakrefset": {
      "file": "_weakrefset.py", 
      "imports": [
        "_weakref.ref"
      ]
    }, 
    "abc": {
      "file": "abc.py", 
      "imports": [
        "_weakrefset", 
        "types"
      ]
    }, 
    "aifc": {
      "file": "aifc.py", 
      "imports": [
        "__builtin__", 
        "audioop", 
        "cl", 
        "math", 
        "sys", 
        "chunk", 
        "struct"
      ]
    }, 
    "antigravity": {
      "file": "antigravity.py", 
      "imports": [
        "webbrowser"
      ]
    }, 
    "anydbm": {
      "file": "anydbm.py", 
      "imports": [
        "whichdb"
      ]
    }, 
    "argparse": {
      "file": "argparse.py", 
      "imports": [
        "sys", 
        "collections", 
        "copy", 
        "gettext", 
        "os", 
        "re", 
        "textwrap", 
        "warnings"
      ]
    }, 
    "ast": {
      "file": "ast.py", 
      "imports": [
        "_ast.*", 
        "_ast.__version__", 
        "collections", 
        "inspect"
      ]
    }, 
    "asynchat": {
      "file": "asynchat.py", 
      "imports": [
        "errno", 
        "sys.py3kwarning", 
        "asyncore", 
        "collections", 
        "socket", 
        "warnings"
      ]
    }, 
    "asyncore": {
      "file": "asyncore.py", 
      "imports": [
        "errno.EAGAIN", 
        "errno.EALREADY", 
        "errno.EBADF", 
        "errno.ECONNABORTED", 
        "errno.ECONNRESET", 
        "errno.EINPROGRESS", 
        "errno.EINTR", 
        "errno.EINVAL", 
        "errno.EISCONN", 
        "errno.ENOTCONN", 
        "errno.EPIPE", 
        "errno.ESHUTDOWN", 
        "errno.EWOULDBLOCK", 
        "errno.errorcode", 
        "fcntl", 
        "select", 
        "sys", 
        "time", 
        "os", 
        "socket", 
        "warnings"
      ]
    }, 
    "atexit": {
      "file": "atexit.py", 
      "imports": [
        "sys", 
        "traceback"
      ]
    }, 
    "base64": {
      "file": "base64.py", 
      "imports": [
        "binascii", 
        "sys", 
        "getopt", 
        "re", 
        "struct"
      ]
    }, 
    "bdb": {
      "file": "bdb.py", 
      "imports": [
        "__main__", 
        "sys", 
        "fnmatch", 
        "linecache", 
        "os", 
        "repr", 
        "types"
      ]
    }, 
    "binhex": {
      "file": "binhex.py", 
      "imports": [
        "Carbon.File.FInfo", 
        "Carbon.File.FSSpec", 
        "MacOS.openrf", 
        "binascii", 
        "sys", 
        "os", 
        "struct"
      ]
    }, 
    "bisect": {
      "file": "bisect.py", 
      "imports": [
        "_bisect.*"
      ]
    }, 
    "cPickle": {
      "file": "cPickle.py", 
      "imports": [
        "__pypy__.builtinify", 
        "copy_reg", 
        "marshal", 
        "pickle", 
        "struct", 
        "sys", 
        "types"
      ]
    }, 
    "cProfile": {
      "file": "cProfile.py", 
      "imports": [
        "__main__", 
        "_lsprof", 
        "marshal", 
        "sys", 
        "optparse", 
        "os", 
        "pstats", 
        "types"
      ]
    }, 
    "cStringIO": {
      "file": "cStringIO.py", 
      "imports": [
        "StringIO"
      ]
    }, 
    "calendar": {
      "file": "calendar.py", 
      "imports": [
        "sys", 
        "locale", 
        "optparse", 
        "datetime"
      ]
    }, 
    "cgi": {
      "file": "cgi.py", 
      "imports": [
        "cStringIO.StringIO", 
        "operator.attrgetter", 
        "sys", 
        "mimetools", 
        "os", 
        "re", 
        "rfc822", 
        "StringIO", 
        "tempfile", 
        "traceback", 
        "urlparse", 
        "UserDict", 
        "warnings"
      ]
    }, 
    "cgitb": {
      "file": "cgitb.py", 
      "imports": [
        "sys", 
        "time", 
        "inspect", 
        "keyword", 
        "linecache", 
        "os", 
        "pydoc", 
        "tempfile", 
        "tokenize", 
        "traceback", 
        "types"
      ]
    }, 
    "chunk": {
      "file": "chunk.py", 
      "imports": [
        "struct"
      ]
    }, 
    "cmd": {
      "file": "cmd.py", 
      "imports": [
        "readline", 
        "sys", 
        "string"
      ]
    }, 
    "code": {
      "file": "code.py", 
      "imports": [
        "readline", 
        "sys", 
        "codeop", 
        "traceback"
      ]
    }, 
    "codecs": {
      "file": "codecs.py", 
      "imports": [
        "__builtin__", 
        "_codecs.*", 
        "sys", 
        "encodings"
      ]
    }, 
    "codeop": {
      "file": "codeop.py", 
      "imports": [
        "__future__"
      ]
    }, 
    "collections": {
      "file": "collections.py", 
      "imports": [
        "__pypy__.newdict", 
        "__pypy__.reversed_dict", 
        "_abcoll", 
        "_collections.defaultdict", 
        "_collections.deque", 
        "itertools.chain", 
        "itertools.imap", 
        "itertools.repeat", 
        "itertools.starmap", 
        "operator.eq", 
        "operator.itemgetter", 
        "sys", 
        "thread.get_ident", 
        "doctest", 
        "dummy_thread", 
        "heapq", 
        "keyword", 
        "cPickle"
      ]
    }, 
    "colorsys": {
      "file": "colorsys.py", 
      "imports": []
    }, 
    "commands": {
      "file": "commands.py", 
      "imports": [
        "os", 
        "warnings"
      ]
    }, 
    "compileall": {
      "file": "compileall.py", 
      "imports": [
        "imp", 
        "sys", 
        "getopt", 
        "os", 
        "py_compile", 
        "re", 
        "struct"
      ]
    }, 
    "compiler": {
      "dir": "compiler"
    }, 
    "compiler.__init__": {
      "file": "compiler/__init__.py", 
      "imports": [
        "compiler.pycodegen", 
        "compiler.transformer", 
        "compiler.visitor", 
        "warnings"
      ]
    }, 
    "compiler.ast": {
      "file": "compiler/ast.py", 
      "imports": [
        "compiler.consts"
      ]
    }, 
    "compiler.consts": {
      "file": "compiler/consts.py", 
      "imports": []
    }, 
    "compiler.future": {
      "file": "compiler/future.py", 
      "imports": [
        "compiler", 
        "compiler.ast", 
        "sys"
      ]
    }, 
    "compiler.misc": {
      "file": "compiler/misc.py", 
      "imports": []
    }, 
    "compiler.pyassem": {
      "file": "compiler/pyassem.py", 
      "imports": [
        "compiler.consts", 
        "compiler.misc", 
        "sys", 
        "dis", 
        "types"
      ]
    }, 
    "compiler.pycodegen": {
      "file": "compiler/pycodegen.py", 
      "imports": [
        "cStringIO.StringIO", 
        "compiler", 
        "compiler.ast", 
        "compiler.consts", 
        "compiler.future", 
        "compiler.misc", 
        "compiler.pyassem", 
        "compiler.symbols", 
        "compiler.syntax", 
        "imp", 
        "marshal", 
        "sys", 
        "os", 
        "pprint", 
        "struct"
      ]
    }, 
    "compiler.symbols": {
      "file": "compiler/symbols.py", 
      "imports": [
        "compiler", 
        "compiler.ast", 
        "compiler.consts", 
        "compiler.misc", 
        "symtable", 
        "sys", 
        "types"
      ]
    }, 
    "compiler.syntax": {
      "file": "compiler/syntax.py", 
      "imports": [
        "compiler", 
        "compiler.ast"
      ]
    }, 
    "compiler.transformer": {
      "file": "compiler/transformer.py", 
      "imports": [
        "compiler.ast", 
        "compiler.consts", 
        "parser", 
        "symbol", 
        "token"
      ]
    }, 
    "compiler.visitor": {
      "file": "compiler/visitor.py", 
      "imports": [
        "compiler.ast"
      ]
    }, 
    "contextlib": {
      "file": "contextlib.py", 
      "imports": [
        "sys", 
        "functools", 
        "warnings"
      ]
    }, 
    "cookielib": {
      "file": "cookielib.py", 
      "imports": [
        "_LWPCookieJar", 
        "_MozillaCookieJar", 
        "calendar", 
        "threading", 
        "time", 
        "copy", 
        "dummy_threading", 
        "httplib", 
        "logging", 
        "re", 
        "StringIO", 
        "traceback", 
        "urllib", 
        "urlparse", 
        "warnings"
      ]
    }, 
    "copy": {
      "file": "copy.py", 
      "imports": [
        "org.python.core.PyStringMap", 
        "sys", 
        "copy_reg", 
        "repr", 
        "types", 
        "weakref"
      ]
    }, 
    "copy_reg": {
      "file": "copy_reg.py", 
      "imports": [
        "types"
      ]
    }, 
    "csv": {
      "file": "csv.py", 
      "imports": [
        "_csv.Dialect", 
        "_csv.Error", 
        "_csv.QUOTE_ALL", 
        "_csv.QUOTE_MINIMAL", 
        "_csv.QUOTE_NONE", 
        "_csv.QUOTE_NONNUMERIC", 
        "_csv.__doc__", 
        "_csv.__version__", 
        "_csv.field_size_limit", 
        "_csv.get_dialect", 
        "_csv.list_dialects", 
        "_csv.reader", 
        "_csv.register_dialect", 
        "_csv.unregister_dialect", 
        "_csv.writer", 
        "cStringIO.StringIO", 
        "functools", 
        "re", 
        "StringIO"
      ]
    }, 
    "ctypes_config_cache": {
      "dir": "ctypes_config_cache"
    }, 
    "ctypes_config_cache.__init__": {
      "file": "ctypes_config_cache/__init__.py", 
      "imports": []
    }, 
    "ctypes_config_cache._locale_32_": {
      "file": "ctypes_config_cache/_locale_32_.py", 
      "imports": [
        "ctypes"
      ]
    }, 
    "ctypes_config_cache._locale_cache": {
      "file": "ctypes_config_cache/_locale_cache.py", 
      "imports": [
        "sys"
      ]
    }, 
    "ctypes_config_cache._resource_32_": {
      "file": "ctypes_config_cache/_resource_32_.py", 
      "imports": [
        "ctypes"
      ]
    }, 
    "ctypes_config_cache._resource_cache": {
      "file": "ctypes_config_cache/_resource_cache.py", 
      "imports": [
        "sys"
      ]
    }, 
    "ctypes_config_cache.dumpcache": {
      "file": "ctypes_config_cache/dumpcache.py", 
      "imports": [
        "ctypes_configure.dumpcache", 
        "os", 
        "sys"
      ]
    }, 
    "ctypes_config_cache.locale.ctc": {
      "file": "ctypes_config_cache/locale.ctc.py", 
      "imports": [
        "ctypes_config_cache.dumpcache", 
        "ctypes_configure.configure.ConstantInteger", 
        "ctypes_configure.configure.DefinedConstantInteger", 
        "ctypes_configure.configure.ExternalCompilationInfo", 
        "ctypes_configure.configure.SimpleType", 
        "ctypes_configure.configure.check_eci", 
        "ctypes_configure.configure.configure"
      ]
    }, 
    "ctypes_config_cache.rebuild": {
      "file": "ctypes_config_cache/rebuild.py", 
      "imports": [
        "os", 
        "py", 
        "rpython.tool.ansi_print.ansi_log", 
        "sys"
      ]
    }, 
    "ctypes_config_cache.resource.ctc": {
      "file": "ctypes_config_cache/resource.ctc.py", 
      "imports": [
        "ctypes.sizeof", 
        "ctypes_config_cache.dumpcache", 
        "ctypes_configure.configure.ConstantInteger", 
        "ctypes_configure.configure.DefinedConstantInteger", 
        "ctypes_configure.configure.ExternalCompilationInfo", 
        "ctypes_configure.configure.SimpleType", 
        "ctypes_configure.configure.configure"
      ]
    }, 
    "datetime": {
      "file": "datetime.py", 
      "imports": [
        "__future__", 
        "_strptime", 
        "math", 
        "struct", 
        "time"
      ]
    }, 
    "dbhash": {
      "file": "dbhash.py", 
      "imports": [
        "bsddb", 
        "sys", 
        "warnings"
      ]
    }, 
    "dbm": {
      "file": "dbm.py", 
      "imports": [
        "ctypes.CDLL", 
        "ctypes.POINTER", 
        "ctypes.Structure", 
        "ctypes.c_char", 
        "ctypes.c_char_p", 
        "ctypes.c_int", 
        "ctypes.c_void_p", 
        "ctypes.util", 
        "os", 
        "sys"
      ]
    }, 
    "decimal": {
      "file": "decimal.py", 
      "imports": [
        "collections", 
        "itertools.chain", 
        "itertools.repeat", 
        "math", 
        "sys", 
        "threading", 
        "doctest", 
        "locale", 
        "numbers", 
        "re"
      ]
    }, 
    "difflib": {
      "file": "difflib.py", 
      "imports": [
        "collections", 
        "difflib", 
        "doctest", 
        "functools", 
        "heapq", 
        "re"
      ]
    }, 
    "dircache": {
      "file": "dircache.py", 
      "imports": [
        "os", 
        "warnings"
      ]
    }, 
    "dis": {
      "file": "dis.py", 
      "imports": [
        "sys", 
        "opcode", 
        "types"
      ]
    }, 
    "distutils": {
      "dir": "distutils"
    }, 
    "distutils.__init__": {
      "file": "distutils/__init__.py", 
      "imports": []
    }, 
    "distutils.archive_util": {
      "file": "distutils/archive_util.py", 
      "imports": [
        "distutils.dir_util", 
        "distutils.errors", 
        "distutils.log", 
        "distutils.spawn", 
        "sys", 
        "os", 
        "tarfile", 
        "warnings", 
        "zipfile", 
        "grp", 
        "pwd"
      ]
    }, 
    "distutils.bcppcompiler": {
      "file": "distutils/bcppcompiler.py", 
      "imports": [
        "distutils.ccompiler", 
        "distutils.dep_util", 
        "distutils.errors", 
        "distutils.file_util", 
        "distutils.log", 
        "os"
      ]
    }, 
    "distutils.ccompiler": {
      "file": "distutils/ccompiler.py", 
      "imports": [
        "distutils.debug", 
        "distutils.dep_util", 
        "distutils.dir_util", 
        "distutils.errors", 
        "distutils.fancy_getopt", 
        "distutils.file_util", 
        "distutils.log", 
        "distutils.spawn", 
        "distutils.sysconfig", 
        "distutils.util", 
        "sys", 
        "os", 
        "re", 
        "tempfile"
      ]
    }, 
    "distutils.cmd": {
      "file": "distutils/cmd.py", 
      "imports": [
        "distutils.archive_util", 
        "distutils.debug", 
        "distutils.dep_util", 
        "distutils.dir_util", 
        "distutils.dist", 
        "distutils.errors", 
        "distutils.fancy_getopt", 
        "distutils.file_util", 
        "distutils.log", 
        "distutils.spawn", 
        "distutils.util", 
        "sys", 
        "os", 
        "re"
      ]
    }, 
    "distutils.command": {
      "dir": "distutils/command"
    }, 
    "distutils.command.__init__": {
      "file": "distutils/command/__init__.py", 
      "imports": []
    }, 
    "distutils.command.bdist": {
      "file": "distutils/command/bdist.py", 
      "imports": [
        "distutils.core", 
        "distutils.errors", 
        "distutils.fancy_getopt", 
        "distutils.util", 
        "os"
      ]
    }, 
    "distutils.command.bdist_dumb": {
      "file": "distutils/command/bdist_dumb.py", 
      "imports": [
        "distutils.core", 
        "distutils.dir_util", 
        "distutils.errors", 
        "distutils.log", 
        "distutils.sysconfig", 
        "distutils.util", 
        "os"
      ]
    }, 
    "distutils.command.bdist_msi": {
      "file": "distutils/command/bdist_msi.py", 
      "imports": [
        "distutils.core", 
        "distutils.dir_util", 
        "distutils.errors", 
        "distutils.log", 
        "distutils.sysconfig", 
        "distutils.util", 
        "distutils.version", 
        "msilib", 
        "msilib.Dialog", 
        "msilib.Directory", 
        "msilib.Feature", 
        "msilib.add_data", 
        "msilib.schema", 
        "msilib.sequence", 
        "msilib.text", 
        "sys", 
        "os"
      ]
    }, 
    "distutils.command.bdist_rpm": {
      "file": "distutils/command/bdist_rpm.py", 
      "imports": [
        "distutils.core", 
        "distutils.debug", 
        "distutils.errors", 
        "distutils.file_util", 
        "distutils.log", 
        "distutils.sysconfig", 
        "sys", 
        "os", 
        "string"
      ]
    }, 
    "distutils.command.bdist_wininst": {
      "file": "distutils/command/bdist_wininst.py", 
      "imports": [
        "distutils", 
        "distutils.core", 
        "distutils.dir_util", 
        "distutils.errors", 
        "distutils.log", 
        "distutils.msvccompiler", 
        "distutils.sysconfig", 
        "distutils.util", 
        "sys", 
        "time", 
        "os", 
        "string", 
        "struct", 
        "tempfile"
      ]
    }, 
    "distutils.command.build": {
      "file": "distutils/command/build.py", 
      "imports": [
        "distutils.ccompiler", 
        "distutils.core", 
        "distutils.errors", 
        "distutils.util", 
        "sys", 
        "os"
      ]
    }, 
    "distutils.command.build_clib": {
      "file": "distutils/command/build_clib.py", 
      "imports": [
        "distutils.ccompiler", 
        "distutils.core", 
        "distutils.errors", 
        "distutils.log", 
        "distutils.sysconfig", 
        "os"
      ]
    }, 
    "distutils.command.build_ext": {
      "file": "distutils/command/build_ext.py", 
      "imports": [
        "distutils.ccompiler", 
        "distutils.core", 
        "distutils.dep_util", 
        "distutils.errors", 
        "distutils.extension", 
        "distutils.log", 
        "distutils.msvccompiler", 
        "distutils.sysconfig", 
        "distutils.util", 
        "imp", 
        "sys", 
        "os", 
        "re", 
        "site", 
        "string", 
        "types"
      ]
    }, 
    "distutils.command.build_py": {
      "file": "distutils/command/build_py.py", 
      "imports": [
        "distutils.core", 
        "distutils.errors", 
        "distutils.log", 
        "distutils.util", 
        "sys", 
        "glob", 
        "os"
      ]
    }, 
    "distutils.command.build_scripts": {
      "file": "distutils/command/build_scripts.py", 
      "imports": [
        "distutils.core", 
        "distutils.dep_util", 
        "distutils.log", 
        "distutils.util", 
        "os", 
        "re", 
        "stat"
      ]
    }, 
    "distutils.command.check": {
      "file": "distutils/command/check.py", 
      "imports": [
        "distutils.core", 
        "distutils.dist", 
        "distutils.errors", 
        "docutils.frontend", 
        "docutils.nodes", 
        "docutils.parsers.rst.Parser", 
        "docutils.utils.Reporter", 
        "StringIO"
      ]
    }, 
    "distutils.command.clean": {
      "file": "distutils/command/clean.py", 
      "imports": [
        "distutils.core", 
        "distutils.dir_util", 
        "distutils.log", 
        "os"
      ]
    }, 
    "distutils.command.config": {
      "file": "distutils/command/config.py", 
      "imports": [
        "distutils.ccompiler", 
        "distutils.core", 
        "distutils.errors", 
        "distutils.log", 
        "distutils.sysconfig", 
        "os", 
        "re"
      ]
    }, 
    "distutils.command.install": {
      "file": "distutils/command/install.py", 
      "imports": [
        "distutils.core", 
        "distutils.debug", 
        "distutils.errors", 
        "distutils.fancy_getopt", 
        "distutils.file_util", 
        "distutils.log", 
        "distutils.sysconfig", 
        "distutils.util", 
        "sys", 
        "os", 
        "pprint", 
        "site", 
        "string", 
        "types"
      ]
    }, 
    "distutils.command.install_data": {
      "file": "distutils/command/install_data.py", 
      "imports": [
        "distutils.core", 
        "distutils.util", 
        "os"
      ]
    }, 
    "distutils.command.install_egg_info": {
      "file": "distutils/command/install_egg_info.py", 
      "imports": [
        "distutils.cmd", 
        "distutils.dir_util", 
        "distutils.log", 
        "sys", 
        "os", 
        "re"
      ]
    }, 
    "distutils.command.install_headers": {
      "file": "distutils/command/install_headers.py", 
      "imports": [
        "distutils.core"
      ]
    }, 
    "distutils.command.install_lib": {
      "file": "distutils/command/install_lib.py", 
      "imports": [
        "distutils.core", 
        "distutils.errors", 
        "distutils.util", 
        "sys", 
        "os"
      ]
    }, 
    "distutils.command.install_scripts": {
      "file": "distutils/command/install_scripts.py", 
      "imports": [
        "distutils.core", 
        "distutils.log", 
        "os", 
        "stat"
      ]
    }, 
    "distutils.command.register": {
      "file": "distutils/command/register.py", 
      "imports": [
        "distutils.core", 
        "distutils.log", 
        "getpass", 
        "urllib2", 
        "urlparse", 
        "warnings"
      ]
    }, 
    "distutils.command.sdist": {
      "file": "distutils/command/sdist.py", 
      "imports": [
        "distutils.archive_util", 
        "distutils.core", 
        "distutils.dep_util", 
        "distutils.dir_util", 
        "distutils.errors", 
        "distutils.fancy_getopt", 
        "distutils.file_util", 
        "distutils.filelist", 
        "distutils.log", 
        "distutils.text_file", 
        "distutils.util", 
        "sys", 
        "glob", 
        "os", 
        "string", 
        "warnings"
      ]
    }, 
    "distutils.command.upload": {
      "file": "distutils/command/upload.py", 
      "imports": [
        "base64", 
        "cStringIO", 
        "distutils.core", 
        "distutils.errors", 
        "distutils.log", 
        "distutils.spawn", 
        "hashlib", 
        "os", 
        "platform", 
        "socket", 
        "urllib2", 
        "urlparse"
      ]
    }, 
    "distutils.config": {
      "file": "distutils/config.py", 
      "imports": [
        "ConfigParser", 
        "distutils.cmd", 
        "os"
      ]
    }, 
    "distutils.core": {
      "file": "distutils/core.py", 
      "imports": [
        "distutils.cmd", 
        "distutils.config", 
        "distutils.debug", 
        "distutils.dist", 
        "distutils.errors", 
        "distutils.extension", 
        "sys", 
        "os"
      ]
    }, 
    "distutils.cygwinccompiler": {
      "file": "distutils/cygwinccompiler.py", 
      "imports": [
        "copy", 
        "distutils.ccompiler", 
        "distutils.errors", 
        "distutils.file_util", 
        "distutils.log", 
        "distutils.spawn", 
        "distutils.sysconfig", 
        "distutils.unixccompiler", 
        "distutils.version", 
        "sys", 
        "os", 
        "re", 
        "string"
      ]
    }, 
    "distutils.debug": {
      "file": "distutils/debug.py", 
      "imports": [
        "os"
      ]
    }, 
    "distutils.dep_util": {
      "file": "distutils/dep_util.py", 
      "imports": [
        "distutils.errors", 
        "os", 
        "stat"
      ]
    }, 
    "distutils.dir_util": {
      "file": "distutils/dir_util.py", 
      "imports": [
        "distutils.errors", 
        "distutils.file_util", 
        "distutils.log", 
        "errno", 
        "os"
      ]
    }, 
    "distutils.dist": {
      "file": "distutils/dist.py", 
      "imports": [
        "ConfigParser", 
        "distutils.cmd", 
        "distutils.command", 
        "distutils.core", 
        "distutils.debug", 
        "distutils.errors", 
        "distutils.fancy_getopt", 
        "distutils.log", 
        "distutils.util", 
        "distutils.versionpredicate", 
        "sys", 
        "email", 
        "os", 
        "pprint", 
        "re", 
        "warnings"
      ]
    }, 
    "distutils.emxccompiler": {
      "file": "distutils/emxccompiler.py", 
      "imports": [
        "copy", 
        "distutils.ccompiler", 
        "distutils.errors", 
        "distutils.file_util", 
        "distutils.log", 
        "distutils.spawn", 
        "distutils.sysconfig", 
        "distutils.unixccompiler", 
        "distutils.version", 
        "sys", 
        "os", 
        "re", 
        "string"
      ]
    }, 
    "distutils.errors": {
      "file": "distutils/errors.py", 
      "imports": []
    }, 
    "distutils.extension": {
      "file": "distutils/extension.py", 
      "imports": [
        "distutils.sysconfig", 
        "distutils.text_file", 
        "distutils.util", 
        "sys", 
        "os", 
        "string", 
        "types", 
        "warnings"
      ]
    }, 
    "distutils.fancy_getopt": {
      "file": "distutils/fancy_getopt.py", 
      "imports": [
        "distutils.errors", 
        "sys", 
        "getopt", 
        "re", 
        "string"
      ]
    }, 
    "distutils.file_util": {
      "file": "distutils/file_util.py", 
      "imports": [
        "distutils.dep_util", 
        "distutils.errors", 
        "distutils.log", 
        "errno", 
        "os", 
        "stat"
      ]
    }, 
    "distutils.filelist": {
      "file": "distutils/filelist.py", 
      "imports": [
        "distutils.debug", 
        "distutils.errors", 
        "distutils.log", 
        "distutils.util", 
        "fnmatch", 
        "os", 
        "re", 
        "stat"
      ]
    }, 
    "distutils.log": {
      "file": "distutils/log.py", 
      "imports": [
        "sys"
      ]
    }, 
    "distutils.msvc9compiler": {
      "file": "distutils/msvc9compiler.py", 
      "imports": [
        "_winreg", 
        "distutils.ccompiler", 
        "distutils.errors", 
        "distutils.log", 
        "distutils.util", 
        "subprocess", 
        "sys", 
        "os", 
        "re"
      ]
    }, 
    "distutils.msvccompiler": {
      "file": "distutils/msvccompiler.py", 
      "imports": [
        "_winreg", 
        "distutils.ccompiler", 
        "distutils.errors", 
        "distutils.log", 
        "distutils.msvc9compiler", 
        "sys", 
        "win32api", 
        "win32con", 
        "os", 
        "string"
      ]
    }, 
    "distutils.spawn": {
      "file": "distutils/spawn.py", 
      "imports": [
        "distutils.debug", 
        "distutils.errors", 
        "distutils.log", 
        "distutils.sysconfig", 
        "errno", 
        "subprocess", 
        "sys", 
        "os"
      ]
    }, 
    "distutils.sysconfig": {
      "file": "distutils/sysconfig.py", 
      "imports": [
        "distutils.sysconfig_cpython", 
        "distutils.sysconfig_pypy", 
        "sys"
      ]
    }, 
    "distutils.sysconfig_cpython": {
      "file": "distutils/sysconfig_cpython.py", 
      "imports": [
        "_osx_support", 
        "_sysconfigdata.build_time_vars", 
        "distutils.errors", 
        "distutils.text_file", 
        "sys", 
        "os", 
        "re", 
        "string"
      ]
    }, 
    "distutils.sysconfig_pypy": {
      "file": "distutils/sysconfig_pypy.py", 
      "imports": [
        "distutils.errors", 
        "distutils.sysconfig_cpython", 
        "sys", 
        "os", 
        "shlex"
      ]
    }, 
    "distutils.tests": {
      "dir": "distutils/tests"
    }, 
    "distutils.tests.__init__": {
      "file": "distutils/tests/__init__.py", 
      "imports": [
        "sys", 
        "os", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "distutils.tests.setuptools_build_ext": {
      "file": "distutils/tests/setuptools_build_ext.py", 
      "imports": [
        "Pyrex.Distutils.build_ext.build_ext", 
        "distutils.ccompiler", 
        "distutils.command.build_ext", 
        "distutils.errors", 
        "distutils.file_util", 
        "distutils.log", 
        "distutils.sysconfig", 
        "distutils.tests.setuptools_extension", 
        "distutils.util", 
        "dl.RTLD_NOW", 
        "sys", 
        "os"
      ]
    }, 
    "distutils.tests.setuptools_extension": {
      "file": "distutils/tests/setuptools_extension.py", 
      "imports": [
        "Pyrex.Distutils.build_ext.build_ext", 
        "distutils.core", 
        "distutils.extension", 
        "sys"
      ]
    }, 
    "distutils.tests.support": {
      "file": "distutils/tests/support.py", 
      "imports": [
        "copy", 
        "distutils.core", 
        "distutils.log", 
        "distutils.sysconfig", 
        "sys", 
        "os", 
        "shutil", 
        "tempfile", 
        "unittest", 
        "warnings"
      ]
    }, 
    "distutils.tests.test_archive_util": {
      "file": "distutils/tests/test_archive_util.py", 
      "imports": [
        "distutils.archive_util", 
        "distutils.spawn", 
        "distutils.tests.support", 
        "sys", 
        "zlib", 
        "os", 
        "tarfile", 
        "test.test_support", 
        "unittest", 
        "warnings", 
        "zipfile", 
        "grp", 
        "pwd"
      ]
    }, 
    "distutils.tests.test_bdist": {
      "file": "distutils/tests/test_bdist.py", 
      "imports": [
        "distutils.command.bdist", 
        "distutils.tests.support", 
        "os", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "distutils.tests.test_bdist_dumb": {
      "file": "distutils/tests/test_bdist_dumb.py", 
      "imports": [
        "distutils.command.bdist_dumb", 
        "distutils.core", 
        "distutils.tests.support", 
        "sys", 
        "zlib", 
        "os", 
        "test.test_support", 
        "unittest", 
        "zipfile"
      ]
    }, 
    "distutils.tests.test_bdist_msi": {
      "file": "distutils/tests/test_bdist_msi.py", 
      "imports": [
        "distutils.command.bdist_msi", 
        "distutils.tests.support", 
        "sys", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "distutils.tests.test_bdist_rpm": {
      "file": "distutils/tests/test_bdist_rpm.py", 
      "imports": [
        "distutils.command.bdist_rpm", 
        "distutils.core", 
        "distutils.errors", 
        "distutils.spawn", 
        "distutils.tests.support", 
        "sys", 
        "os", 
        "shutil", 
        "tempfile", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "distutils.tests.test_bdist_wininst": {
      "file": "distutils/tests/test_bdist_wininst.py", 
      "imports": [
        "distutils.command.bdist_wininst", 
        "distutils.tests.support", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "distutils.tests.test_build": {
      "file": "distutils/tests/test_build.py", 
      "imports": [
        "distutils.command.build", 
        "distutils.sysconfig", 
        "distutils.tests.support", 
        "sys", 
        "os", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "distutils.tests.test_build_clib": {
      "file": "distutils/tests/test_build_clib.py", 
      "imports": [
        "distutils.ccompiler", 
        "distutils.command.build_clib", 
        "distutils.errors", 
        "distutils.spawn", 
        "distutils.sysconfig", 
        "distutils.tests.support", 
        "sys", 
        "os", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "distutils.tests.test_build_ext": {
      "file": "distutils/tests/test_build_ext.py", 
      "imports": [
        "distutils.command.build_ext", 
        "distutils.core", 
        "distutils.errors", 
        "distutils.extension", 
        "distutils.sysconfig", 
        "distutils.tests.setuptools_build_ext", 
        "distutils.tests.setuptools_extension", 
        "distutils.tests.support", 
        "sys", 
        "xx", 
        "os", 
        "site", 
        "StringIO", 
        "test.test_support", 
        "textwrap", 
        "unittest"
      ]
    }, 
    "distutils.tests.test_build_py": {
      "file": "distutils/tests/test_build_py.py", 
      "imports": [
        "distutils.command.build_py", 
        "distutils.core", 
        "distutils.errors", 
        "distutils.tests.support", 
        "sys", 
        "os", 
        "StringIO", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "distutils.tests.test_build_scripts": {
      "file": "distutils/tests/test_build_scripts.py", 
      "imports": [
        "distutils.command.build_scripts", 
        "distutils.core", 
        "distutils.sysconfig", 
        "distutils.tests.support", 
        "sys", 
        "os", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "distutils.tests.test_ccompiler": {
      "file": "distutils/tests/test_ccompiler.py", 
      "imports": [
        "distutils.ccompiler", 
        "distutils.debug", 
        "distutils.sysconfig", 
        "distutils.tests.support", 
        "os", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "distutils.tests.test_check": {
      "file": "distutils/tests/test_check.py", 
      "imports": [
        "distutils.command.check", 
        "distutils.errors", 
        "distutils.tests.support", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "distutils.tests.test_clean": {
      "file": "distutils/tests/test_clean.py", 
      "imports": [
        "distutils.command.clean", 
        "distutils.tests.support", 
        "sys", 
        "getpass", 
        "os", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "distutils.tests.test_cmd": {
      "file": "distutils/tests/test_cmd.py", 
      "imports": [
        "distutils.cmd", 
        "distutils.debug", 
        "distutils.dist", 
        "distutils.errors", 
        "os", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "distutils.tests.test_config": {
      "file": "distutils/tests/test_config.py", 
      "imports": [
        "distutils.core", 
        "distutils.log", 
        "distutils.tests.support", 
        "sys", 
        "os", 
        "shutil", 
        "tempfile", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "distutils.tests.test_config_cmd": {
      "file": "distutils/tests/test_config_cmd.py", 
      "imports": [
        "distutils.command.config", 
        "distutils.log", 
        "distutils.tests.support", 
        "sys", 
        "os", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "distutils.tests.test_core": {
      "file": "distutils/tests/test_core.py", 
      "imports": [
        "distutils.core", 
        "distutils.tests.support", 
        "sys", 
        "os", 
        "shutil", 
        "StringIO", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "distutils.tests.test_dep_util": {
      "file": "distutils/tests/test_dep_util.py", 
      "imports": [
        "distutils.dep_util", 
        "distutils.errors", 
        "distutils.tests.support", 
        "time", 
        "os", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "distutils.tests.test_dir_util": {
      "file": "distutils/tests/test_dir_util.py", 
      "imports": [
        "distutils.dir_util", 
        "distutils.log", 
        "distutils.tests.support", 
        "sys", 
        "os", 
        "shutil", 
        "stat", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "distutils.tests.test_dist": {
      "file": "distutils/tests/test_dist.py", 
      "imports": [
        "distutils.cmd", 
        "distutils.dist", 
        "distutils.tests.support", 
        "distutils.tests.test_dist", 
        "sys", 
        "os", 
        "StringIO", 
        "test.test_support", 
        "textwrap", 
        "unittest", 
        "warnings"
      ]
    }, 
    "distutils.tests.test_file_util": {
      "file": "distutils/tests/test_file_util.py", 
      "imports": [
        "distutils.file_util", 
        "distutils.log", 
        "distutils.tests.support", 
        "os", 
        "shutil", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "distutils.tests.test_filelist": {
      "file": "distutils/tests/test_filelist.py", 
      "imports": [
        "distutils.debug", 
        "distutils.errors", 
        "distutils.filelist", 
        "distutils.log", 
        "distutils.tests.support", 
        "os", 
        "re", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "distutils.tests.test_install": {
      "file": "distutils/tests/test_install.py", 
      "imports": [
        "distutils.command.build_ext", 
        "distutils.command.install", 
        "distutils.core", 
        "distutils.errors", 
        "distutils.extension", 
        "distutils.sysconfig", 
        "distutils.tests.support", 
        "sys", 
        "os", 
        "site", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "distutils.tests.test_install_data": {
      "file": "distutils/tests/test_install_data.py", 
      "imports": [
        "distutils.command.install_data", 
        "distutils.tests.support", 
        "sys", 
        "getpass", 
        "os", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "distutils.tests.test_install_headers": {
      "file": "distutils/tests/test_install_headers.py", 
      "imports": [
        "distutils.command.install_headers", 
        "distutils.tests.support", 
        "sys", 
        "getpass", 
        "os", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "distutils.tests.test_install_lib": {
      "file": "distutils/tests/test_install_lib.py", 
      "imports": [
        "distutils.command.install_lib", 
        "distutils.errors", 
        "distutils.extension", 
        "distutils.tests.support", 
        "sys", 
        "os", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "distutils.tests.test_install_scripts": {
      "file": "distutils/tests/test_install_scripts.py", 
      "imports": [
        "distutils.command.install_scripts", 
        "distutils.core", 
        "distutils.tests.support", 
        "os", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "distutils.tests.test_msvc9compiler": {
      "file": "distutils/tests/test_msvc9compiler.py", 
      "imports": [
        "_winreg", 
        "distutils.errors", 
        "distutils.msvc9compiler", 
        "distutils.msvccompiler", 
        "distutils.tests.support", 
        "sys", 
        "os", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "distutils.tests.test_register": {
      "file": "distutils/tests/test_register.py", 
      "imports": [
        "distutils.command.register", 
        "distutils.errors", 
        "distutils.tests.test_config", 
        "docutils", 
        "getpass", 
        "os", 
        "test.test_support", 
        "unittest", 
        "urllib2", 
        "warnings"
      ]
    }, 
    "distutils.tests.test_sdist": {
      "file": "distutils/tests/test_sdist.py", 
      "imports": [
        "distutils.archive_util", 
        "distutils.command.sdist", 
        "distutils.core", 
        "distutils.errors", 
        "distutils.filelist", 
        "distutils.log", 
        "distutils.spawn", 
        "distutils.tests.test_config", 
        "zlib", 
        "os", 
        "tarfile", 
        "test.test_support", 
        "textwrap", 
        "unittest", 
        "warnings", 
        "zipfile", 
        "grp", 
        "pwd"
      ]
    }, 
    "distutils.tests.test_spawn": {
      "file": "distutils/tests/test_spawn.py", 
      "imports": [
        "distutils.errors", 
        "distutils.spawn", 
        "distutils.tests.support", 
        "time", 
        "os", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "distutils.tests.test_sysconfig": {
      "file": "distutils/tests/test_sysconfig.py", 
      "imports": [
        "distutils.sysconfig", 
        "distutils.tests.support", 
        "subprocess", 
        "sys", 
        "os", 
        "shutil", 
        "test.test_support", 
        "test", 
        "textwrap", 
        "unittest"
      ]
    }, 
    "distutils.tests.test_text_file": {
      "file": "distutils/tests/test_text_file.py", 
      "imports": [
        "distutils.tests.support", 
        "distutils.text_file", 
        "os", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "distutils.tests.test_unixccompiler": {
      "file": "distutils/tests/test_unixccompiler.py", 
      "imports": [
        "distutils.sysconfig", 
        "distutils.unixccompiler", 
        "sys", 
        "os", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "distutils.tests.test_upload": {
      "file": "distutils/tests/test_upload.py", 
      "imports": [
        "distutils.command.upload", 
        "distutils.core", 
        "distutils.errors", 
        "distutils.tests.test_config", 
        "os", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "distutils.tests.test_util": {
      "file": "distutils/tests/test_util.py", 
      "imports": [
        "distutils.errors", 
        "distutils.util", 
        "sys", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "distutils.tests.test_version": {
      "file": "distutils/tests/test_version.py", 
      "imports": [
        "distutils.version", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "distutils.tests.test_versionpredicate": {
      "file": "distutils/tests/test_versionpredicate.py", 
      "imports": [
        "distutils.versionpredicate", 
        "doctest", 
        "test.test_support"
      ]
    }, 
    "distutils.text_file": {
      "file": "distutils/text_file.py", 
      "imports": [
        "sys"
      ]
    }, 
    "distutils.unixccompiler": {
      "file": "distutils/unixccompiler.py", 
      "imports": [
        "_osx_support", 
        "distutils.ccompiler", 
        "distutils.dep_util", 
        "distutils.errors", 
        "distutils.log", 
        "distutils.sysconfig", 
        "sys", 
        "os", 
        "platform", 
        "re", 
        "types"
      ]
    }, 
    "distutils.util": {
      "file": "distutils/util.py", 
      "imports": [
        "_osx_support", 
        "distutils.dep_util", 
        "distutils.errors", 
        "distutils.log", 
        "distutils.spawn", 
        "distutils.sysconfig", 
        "sys", 
        "os", 
        "py_compile", 
        "re", 
        "string", 
        "tempfile", 
        "pwd"
      ]
    }, 
    "distutils.version": {
      "file": "distutils/version.py", 
      "imports": [
        "re", 
        "string", 
        "types"
      ]
    }, 
    "distutils.versionpredicate": {
      "file": "distutils/versionpredicate.py", 
      "imports": [
        "distutils.version", 
        "operator", 
        "re"
      ]
    }, 
    "doctest": {
      "file": "doctest.py", 
      "imports": [
        "__future__", 
        "collections", 
        "difflib", 
        "sys", 
        "inspect", 
        "linecache", 
        "os", 
        "pdb", 
        "re", 
        "StringIO", 
        "tempfile", 
        "traceback", 
        "types", 
        "unittest", 
        "warnings"
      ]
    }, 
    "dumbdbm": {
      "file": "dumbdbm.py", 
      "imports": [
        "__builtin__", 
        "os", 
        "UserDict"
      ]
    }, 
    "dummy_thread": {
      "file": "dummy_thread.py", 
      "imports": [
        "traceback"
      ]
    }, 
    "dummy_threading": {
      "file": "dummy_threading.py", 
      "imports": [
        "_dummy_threading.*", 
        "_dummy_threading.__all__", 
        "dummy_thread", 
        "sys.modules", 
        "threading"
      ]
    }, 
    "email": {
      "dir": "email"
    }, 
    "email.__init__": {
      "file": "email/__init__.py", 
      "imports": [
        "email.mime", 
        "email.parser", 
        "sys"
      ]
    }, 
    "email._parseaddr": {
      "file": "email/_parseaddr.py", 
      "imports": [
        "calendar", 
        "time"
      ]
    }, 
    "email.base64mime": {
      "file": "email/base64mime.py", 
      "imports": [
        "binascii.a2b_base64", 
        "binascii.b2a_base64", 
        "email.utils"
      ]
    }, 
    "email.charset": {
      "file": "email/charset.py", 
      "imports": [
        "codecs", 
        "email.base64mime", 
        "email.encoders", 
        "email.errors", 
        "email.quoprimime"
      ]
    }, 
    "email.encoders": {
      "file": "email/encoders.py", 
      "imports": [
        "base64", 
        "quopri"
      ]
    }, 
    "email.errors": {
      "file": "email/errors.py", 
      "imports": []
    }, 
    "email.feedparser": {
      "file": "email/feedparser.py", 
      "imports": [
        "email.errors", 
        "email.message", 
        "re"
      ]
    }, 
    "email.generator": {
      "file": "email/generator.py", 
      "imports": [
        "cStringIO.StringIO", 
        "email.header", 
        "sys", 
        "time", 
        "random", 
        "re", 
        "warnings"
      ]
    }, 
    "email.header": {
      "file": "email/header.py", 
      "imports": [
        "binascii", 
        "email.base64mime", 
        "email.charset", 
        "email.errors", 
        "email.quoprimime", 
        "re"
      ]
    }, 
    "email.iterators": {
      "file": "email/iterators.py", 
      "imports": [
        "cStringIO.StringIO", 
        "sys"
      ]
    }, 
    "email.message": {
      "file": "email/message.py", 
      "imports": [
        "binascii", 
        "cStringIO.StringIO", 
        "email.charset", 
        "email.errors", 
        "email.generator", 
        "email.iterators", 
        "email.utils", 
        "re", 
        "uu", 
        "warnings"
      ]
    }, 
    "email.mime": {
      "dir": "email/mime"
    }, 
    "email.mime.__init__": {
      "file": "email/mime/__init__.py", 
      "imports": []
    }, 
    "email.mime.application": {
      "file": "email/mime/application.py", 
      "imports": [
        "email.encoders", 
        "email.mime.nonmultipart"
      ]
    }, 
    "email.mime.audio": {
      "file": "email/mime/audio.py", 
      "imports": [
        "cStringIO.StringIO", 
        "email.encoders", 
        "email.mime.nonmultipart", 
        "sndhdr"
      ]
    }, 
    "email.mime.base": {
      "file": "email/mime/base.py", 
      "imports": [
        "email.message"
      ]
    }, 
    "email.mime.image": {
      "file": "email/mime/image.py", 
      "imports": [
        "email.encoders", 
        "email.mime.nonmultipart", 
        "imghdr"
      ]
    }, 
    "email.mime.message": {
      "file": "email/mime/message.py", 
      "imports": [
        "email.message", 
        "email.mime.nonmultipart"
      ]
    }, 
    "email.mime.multipart": {
      "file": "email/mime/multipart.py", 
      "imports": [
        "email.mime.base"
      ]
    }, 
    "email.mime.nonmultipart": {
      "file": "email/mime/nonmultipart.py", 
      "imports": [
        "email.errors", 
        "email.mime.base"
      ]
    }, 
    "email.mime.text": {
      "file": "email/mime/text.py", 
      "imports": [
        "email.encoders", 
        "email.mime.nonmultipart"
      ]
    }, 
    "email.parser": {
      "file": "email/parser.py", 
      "imports": [
        "cStringIO.StringIO", 
        "email.feedparser", 
        "email.message", 
        "warnings"
      ]
    }, 
    "email.quoprimime": {
      "file": "email/quoprimime.py", 
      "imports": [
        "email.utils", 
        "re", 
        "string"
      ]
    }, 
    "email.test": {
      "dir": "email/test"
    }, 
    "email.test.__init__": {
      "file": "email/test/__init__.py", 
      "imports": []
    }, 
    "email.test.test_email": {
      "file": "email/test/test_email.py", 
      "imports": [
        "base64", 
        "cStringIO.StringIO", 
        "difflib", 
        "email", 
        "email.feedparser", 
        "email.test", 
        "sys", 
        "time", 
        "os", 
        "random", 
        "re", 
        "test.test_support", 
        "textwrap", 
        "unittest", 
        "warnings"
      ]
    }, 
    "email.test.test_email_codecs": {
      "file": "email/test/test_email_codecs.py", 
      "imports": [
        "email.charset", 
        "email.header", 
        "email.message", 
        "email.test.test_email", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "email.test.test_email_codecs_renamed": {
      "file": "email/test/test_email_codecs_renamed.py", 
      "imports": [
        "email.charset", 
        "email.header", 
        "email.message", 
        "email.test.test_email", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "email.test.test_email_renamed": {
      "file": "email/test/test_email_renamed.py", 
      "imports": [
        "base64", 
        "cStringIO.StringIO", 
        "difflib", 
        "email", 
        "email.base64mime", 
        "email.charset", 
        "email.encoders", 
        "email.errors", 
        "email.generator", 
        "email.header", 
        "email.iterators", 
        "email.message", 
        "email.mime.application", 
        "email.mime.audio", 
        "email.mime.base", 
        "email.mime.image", 
        "email.mime.message", 
        "email.mime.multipart", 
        "email.mime.text", 
        "email.parser", 
        "email.quoprimime", 
        "email.test", 
        "email.utils", 
        "sys", 
        "time", 
        "os", 
        "test.test_support", 
        "unittest", 
        "warnings"
      ]
    }, 
    "email.test.test_email_torture": {
      "file": "email/test/test_email_torture.py", 
      "imports": [
        "cStringIO.StringIO", 
        "email", 
        "email.iterators", 
        "email.test.test_email", 
        "sys", 
        "os", 
        "test.test_support", 
        "types", 
        "unittest"
      ]
    }, 
    "email.utils": {
      "file": "email/utils.py", 
      "imports": [
        "base64", 
        "email._parseaddr", 
        "email.encoders", 
        "time", 
        "os", 
        "quopri", 
        "random", 
        "re", 
        "socket", 
        "urllib", 
        "warnings"
      ]
    }, 
    "encodings": {
      "dir": "encodings"
    }, 
    "encodings.__init__": {
      "file": "encodings/__init__.py", 
      "imports": [
        "__builtin__", 
        "codecs", 
        "encodings.aliases"
      ]
    }, 
    "encodings.aliases": {
      "file": "encodings/aliases.py", 
      "imports": []
    }, 
    "encodings.ascii": {
      "file": "encodings/ascii.py", 
      "imports": [
        "codecs"
      ]
    }, 
    "encodings.base64_codec": {
      "file": "encodings/base64_codec.py", 
      "imports": [
        "base64", 
        "codecs"
      ]
    }, 
    "encodings.big5": {
      "file": "encodings/big5.py", 
      "imports": [
        "_multibytecodec", 
        "codecs", 
        "_codecs_tw"
      ]
    }, 
    "encodings.big5hkscs": {
      "file": "encodings/big5hkscs.py", 
      "imports": [
        "_multibytecodec", 
        "codecs", 
        "_codecs_hk"
      ]
    }, 
    "encodings.bz2_codec": {
      "file": "encodings/bz2_codec.py", 
      "imports": [
        "bz2", 
        "codecs"
      ]
    }, 
    "encodings.charmap": {
      "file": "encodings/charmap.py", 
      "imports": [
        "codecs"
      ]
    }, 
    "encodings.cp037": {
      "file": "encodings/cp037.py", 
      "imports": [
        "codecs"
      ]
    }, 
    "encodings.cp1006": {
      "file": "encodings/cp1006.py", 
      "imports": [
        "codecs"
      ]
    }, 
    "encodings.cp1026": {
      "file": "encodings/cp1026.py", 
      "imports": [
        "codecs"
      ]
    }, 
    "encodings.cp1140": {
      "file": "encodings/cp1140.py", 
      "imports": [
        "codecs"
      ]
    }, 
    "encodings.cp1250": {
      "file": "encodings/cp1250.py", 
      "imports": [
        "codecs"
      ]
    }, 
    "encodings.cp1251": {
      "file": "encodings/cp1251.py", 
      "imports": [
        "codecs"
      ]
    }, 
    "encodings.cp1252": {
      "file": "encodings/cp1252.py", 
      "imports": [
        "codecs"
      ]
    }, 
    "encodings.cp1253": {
      "file": "encodings/cp1253.py", 
      "imports": [
        "codecs"
      ]
    }, 
    "encodings.cp1254": {
      "file": "encodings/cp1254.py", 
      "imports": [
        "codecs"
      ]
    }, 
    "encodings.cp1255": {
      "file": "encodings/cp1255.py", 
      "imports": [
        "codecs"
      ]
    }, 
    "encodings.cp1256": {
      "file": "encodings/cp1256.py", 
      "imports": [
        "codecs"
      ]
    }, 
    "encodings.cp1257": {
      "file": "encodings/cp1257.py", 
      "imports": [
        "codecs"
      ]
    }, 
    "encodings.cp1258": {
      "file": "encodings/cp1258.py", 
      "imports": [
        "codecs"
      ]
    }, 
    "encodings.cp424": {
      "file": "encodings/cp424.py", 
      "imports": [
        "codecs"
      ]
    }, 
    "encodings.cp437": {
      "file": "encodings/cp437.py", 
      "imports": [
        "codecs"
      ]
    }, 
    "encodings.cp500": {
      "file": "encodings/cp500.py", 
      "imports": [
        "codecs"
      ]
    }, 
    "encodings.cp720": {
      "file": "encodings/cp720.py", 
      "imports": [
        "codecs"
      ]
    }, 
    "encodings.cp737": {
      "file": "encodings/cp737.py", 
      "imports": [
        "codecs"
      ]
    }, 
    "encodings.cp775": {
      "file": "encodings/cp775.py", 
      "imports": [
        "codecs"
      ]
    }, 
    "encodings.cp850": {
      "file": "encodings/cp850.py", 
      "imports": [
        "codecs"
      ]
    }, 
    "encodings.cp852": {
      "file": "encodings/cp852.py", 
      "imports": [
        "codecs"
      ]
    }, 
    "encodings.cp855": {
      "file": "encodings/cp855.py", 
      "imports": [
        "codecs"
      ]
    }, 
    "encodings.cp856": {
      "file": "encodings/cp856.py", 
      "imports": [
        "codecs"
      ]
    }, 
    "encodings.cp857": {
      "file": "encodings/cp857.py", 
      "imports": [
        "codecs"
      ]
    }, 
    "encodings.cp858": {
      "file": "encodings/cp858.py", 
      "imports": [
        "codecs"
      ]
    }, 
    "encodings.cp860": {
      "file": "encodings/cp860.py", 
      "imports": [
        "codecs"
      ]
    }, 
    "encodings.cp861": {
      "file": "encodings/cp861.py", 
      "imports": [
        "codecs"
      ]
    }, 
    "encodings.cp862": {
      "file": "encodings/cp862.py", 
      "imports": [
        "codecs"
      ]
    }, 
    "encodings.cp863": {
      "file": "encodings/cp863.py", 
      "imports": [
        "codecs"
      ]
    }, 
    "encodings.cp864": {
      "file": "encodings/cp864.py", 
      "imports": [
        "codecs"
      ]
    }, 
    "encodings.cp865": {
      "file": "encodings/cp865.py", 
      "imports": [
        "codecs"
      ]
    }, 
    "encodings.cp866": {
      "file": "encodings/cp866.py", 
      "imports": [
        "codecs"
      ]
    }, 
    "encodings.cp869": {
      "file": "encodings/cp869.py", 
      "imports": [
        "codecs"
      ]
    }, 
    "encodings.cp874": {
      "file": "encodings/cp874.py", 
      "imports": [
        "codecs"
      ]
    }, 
    "encodings.cp875": {
      "file": "encodings/cp875.py", 
      "imports": [
        "codecs"
      ]
    }, 
    "encodings.cp932": {
      "file": "encodings/cp932.py", 
      "imports": [
        "_multibytecodec", 
        "codecs", 
        "_codecs_jp"
      ]
    }, 
    "encodings.cp949": {
      "file": "encodings/cp949.py", 
      "imports": [
        "_multibytecodec", 
        "codecs", 
        "_codecs_kr"
      ]
    }, 
    "encodings.cp950": {
      "file": "encodings/cp950.py", 
      "imports": [
        "_multibytecodec", 
        "codecs", 
        "_codecs_tw"
      ]
    }, 
    "encodings.euc_jis_2004": {
      "file": "encodings/euc_jis_2004.py", 
      "imports": [
        "_multibytecodec", 
        "codecs", 
        "_codecs_jp"
      ]
    }, 
    "encodings.euc_jisx0213": {
      "file": "encodings/euc_jisx0213.py", 
      "imports": [
        "_multibytecodec", 
        "codecs", 
        "_codecs_jp"
      ]
    }, 
    "encodings.euc_jp": {
      "file": "encodings/euc_jp.py", 
      "imports": [
        "_multibytecodec", 
        "codecs", 
        "_codecs_jp"
      ]
    }, 
    "encodings.euc_kr": {
      "file": "encodings/euc_kr.py", 
      "imports": [
        "_multibytecodec", 
        "codecs", 
        "_codecs_kr"
      ]
    }, 
    "encodings.gb18030": {
      "file": "encodings/gb18030.py", 
      "imports": [
        "_multibytecodec", 
        "codecs", 
        "_codecs_cn"
      ]
    }, 
    "encodings.gb2312": {
      "file": "encodings/gb2312.py", 
      "imports": [
        "_multibytecodec", 
        "codecs", 
        "_codecs_cn"
      ]
    }, 
    "encodings.gbk": {
      "file": "encodings/gbk.py", 
      "imports": [
        "_multibytecodec", 
        "codecs", 
        "_codecs_cn"
      ]
    }, 
    "encodings.hex_codec": {
      "file": "encodings/hex_codec.py", 
      "imports": [
        "binascii", 
        "codecs"
      ]
    }, 
    "encodings.hp_roman8": {
      "file": "encodings/hp_roman8.py", 
      "imports": [
        "codecs"
      ]
    }, 
    "encodings.hz": {
      "file": "encodings/hz.py", 
      "imports": [
        "_multibytecodec", 
        "codecs", 
        "_codecs_cn"
      ]
    }, 
    "encodings.idna": {
      "file": "encodings/idna.py", 
      "imports": [
        "codecs", 
        "unicodedata.ucd_3_2_0", 
        "re", 
        "stringprep"
      ]
    }, 
    "encodings.iso2022_jp": {
      "file": "encodings/iso2022_jp.py", 
      "imports": [
        "_multibytecodec", 
        "codecs", 
        "_codecs_iso2022"
      ]
    }, 
    "encodings.iso2022_jp_1": {
      "file": "encodings/iso2022_jp_1.py", 
      "imports": [
        "_multibytecodec", 
        "codecs", 
        "_codecs_iso2022"
      ]
    }, 
    "encodings.iso2022_jp_2": {
      "file": "encodings/iso2022_jp_2.py", 
      "imports": [
        "_multibytecodec", 
        "codecs", 
        "_codecs_iso2022"
      ]
    }, 
    "encodings.iso2022_jp_2004": {
      "file": "encodings/iso2022_jp_2004.py", 
      "imports": [
        "_multibytecodec", 
        "codecs", 
        "_codecs_iso2022"
      ]
    }, 
    "encodings.iso2022_jp_3": {
      "file": "encodings/iso2022_jp_3.py", 
      "imports": [
        "_multibytecodec", 
        "codecs", 
        "_codecs_iso2022"
      ]
    }, 
    "encodings.iso2022_jp_ext": {
      "file": "encodings/iso2022_jp_ext.py", 
      "imports": [
        "_multibytecodec", 
        "codecs", 
        "_codecs_iso2022"
      ]
    }, 
    "encodings.iso2022_kr": {
      "file": "encodings/iso2022_kr.py", 
      "imports": [
        "_multibytecodec", 
        "codecs", 
        "_codecs_iso2022"
      ]
    }, 
    "encodings.iso8859_1": {
      "file": "encodings/iso8859_1.py", 
      "imports": [
        "codecs"
      ]
    }, 
    "encodings.iso8859_10": {
      "file": "encodings/iso8859_10.py", 
      "imports": [
        "codecs"
      ]
    }, 
    "encodings.iso8859_11": {
      "file": "encodings/iso8859_11.py", 
      "imports": [
        "codecs"
      ]
    }, 
    "encodings.iso8859_13": {
      "file": "encodings/iso8859_13.py", 
      "imports": [
        "codecs"
      ]
    }, 
    "encodings.iso8859_14": {
      "file": "encodings/iso8859_14.py", 
      "imports": [
        "codecs"
      ]
    }, 
    "encodings.iso8859_15": {
      "file": "encodings/iso8859_15.py", 
      "imports": [
        "codecs"
      ]
    }, 
    "encodings.iso8859_16": {
      "file": "encodings/iso8859_16.py", 
      "imports": [
        "codecs"
      ]
    }, 
    "encodings.iso8859_2": {
      "file": "encodings/iso8859_2.py", 
      "imports": [
        "codecs"
      ]
    }, 
    "encodings.iso8859_3": {
      "file": "encodings/iso8859_3.py", 
      "imports": [
        "codecs"
      ]
    }, 
    "encodings.iso8859_4": {
      "file": "encodings/iso8859_4.py", 
      "imports": [
        "codecs"
      ]
    }, 
    "encodings.iso8859_5": {
      "file": "encodings/iso8859_5.py", 
      "imports": [
        "codecs"
      ]
    }, 
    "encodings.iso8859_6": {
      "file": "encodings/iso8859_6.py", 
      "imports": [
        "codecs"
      ]
    }, 
    "encodings.iso8859_7": {
      "file": "encodings/iso8859_7.py", 
      "imports": [
        "codecs"
      ]
    }, 
    "encodings.iso8859_8": {
      "file": "encodings/iso8859_8.py", 
      "imports": [
        "codecs"
      ]
    }, 
    "encodings.iso8859_9": {
      "file": "encodings/iso8859_9.py", 
      "imports": [
        "codecs"
      ]
    }, 
    "encodings.johab": {
      "file": "encodings/johab.py", 
      "imports": [
        "_multibytecodec", 
        "codecs", 
        "_codecs_kr"
      ]
    }, 
    "encodings.koi8_r": {
      "file": "encodings/koi8_r.py", 
      "imports": [
        "codecs"
      ]
    }, 
    "encodings.koi8_u": {
      "file": "encodings/koi8_u.py", 
      "imports": [
        "codecs"
      ]
    }, 
    "encodings.latin_1": {
      "file": "encodings/latin_1.py", 
      "imports": [
        "codecs"
      ]
    }, 
    "encodings.mac_arabic": {
      "file": "encodings/mac_arabic.py", 
      "imports": [
        "codecs"
      ]
    }, 
    "encodings.mac_centeuro": {
      "file": "encodings/mac_centeuro.py", 
      "imports": [
        "codecs"
      ]
    }, 
    "encodings.mac_croatian": {
      "file": "encodings/mac_croatian.py", 
      "imports": [
        "codecs"
      ]
    }, 
    "encodings.mac_cyrillic": {
      "file": "encodings/mac_cyrillic.py", 
      "imports": [
        "codecs"
      ]
    }, 
    "encodings.mac_farsi": {
      "file": "encodings/mac_farsi.py", 
      "imports": [
        "codecs"
      ]
    }, 
    "encodings.mac_greek": {
      "file": "encodings/mac_greek.py", 
      "imports": [
        "codecs"
      ]
    }, 
    "encodings.mac_iceland": {
      "file": "encodings/mac_iceland.py", 
      "imports": [
        "codecs"
      ]
    }, 
    "encodings.mac_latin2": {
      "file": "encodings/mac_latin2.py", 
      "imports": [
        "codecs"
      ]
    }, 
    "encodings.mac_roman": {
      "file": "encodings/mac_roman.py", 
      "imports": [
        "codecs"
      ]
    }, 
    "encodings.mac_romanian": {
      "file": "encodings/mac_romanian.py", 
      "imports": [
        "codecs"
      ]
    }, 
    "encodings.mac_turkish": {
      "file": "encodings/mac_turkish.py", 
      "imports": [
        "codecs"
      ]
    }, 
    "encodings.mbcs": {
      "file": "encodings/mbcs.py", 
      "imports": [
        "codecs"
      ]
    }, 
    "encodings.palmos": {
      "file": "encodings/palmos.py", 
      "imports": [
        "codecs"
      ]
    }, 
    "encodings.ptcp154": {
      "file": "encodings/ptcp154.py", 
      "imports": [
        "codecs"
      ]
    }, 
    "encodings.punycode": {
      "file": "encodings/punycode.py", 
      "imports": [
        "codecs"
      ]
    }, 
    "encodings.quopri_codec": {
      "file": "encodings/quopri_codec.py", 
      "imports": [
        "cStringIO.StringIO", 
        "codecs", 
        "quopri", 
        "StringIO"
      ]
    }, 
    "encodings.raw_unicode_escape": {
      "file": "encodings/raw_unicode_escape.py", 
      "imports": [
        "codecs"
      ]
    }, 
    "encodings.rot_13": {
      "file": "encodings/rot_13.py", 
      "imports": [
        "codecs", 
        "sys"
      ]
    }, 
    "encodings.shift_jis": {
      "file": "encodings/shift_jis.py", 
      "imports": [
        "_multibytecodec", 
        "codecs", 
        "_codecs_jp"
      ]
    }, 
    "encodings.shift_jis_2004": {
      "file": "encodings/shift_jis_2004.py", 
      "imports": [
        "_multibytecodec", 
        "codecs", 
        "_codecs_jp"
      ]
    }, 
    "encodings.shift_jisx0213": {
      "file": "encodings/shift_jisx0213.py", 
      "imports": [
        "_multibytecodec", 
        "codecs", 
        "_codecs_jp"
      ]
    }, 
    "encodings.string_escape": {
      "file": "encodings/string_escape.py", 
      "imports": [
        "codecs"
      ]
    }, 
    "encodings.tis_620": {
      "file": "encodings/tis_620.py", 
      "imports": [
        "codecs"
      ]
    }, 
    "encodings.undefined": {
      "file": "encodings/undefined.py", 
      "imports": [
        "codecs"
      ]
    }, 
    "encodings.unicode_escape": {
      "file": "encodings/unicode_escape.py", 
      "imports": [
        "codecs"
      ]
    }, 
    "encodings.unicode_internal": {
      "file": "encodings/unicode_internal.py", 
      "imports": [
        "codecs"
      ]
    }, 
    "encodings.utf_16": {
      "file": "encodings/utf_16.py", 
      "imports": [
        "codecs", 
        "sys"
      ]
    }, 
    "encodings.utf_16_be": {
      "file": "encodings/utf_16_be.py", 
      "imports": [
        "codecs"
      ]
    }, 
    "encodings.utf_16_le": {
      "file": "encodings/utf_16_le.py", 
      "imports": [
        "codecs"
      ]
    }, 
    "encodings.utf_32": {
      "file": "encodings/utf_32.py", 
      "imports": [
        "codecs", 
        "sys"
      ]
    }, 
    "encodings.utf_32_be": {
      "file": "encodings/utf_32_be.py", 
      "imports": [
        "codecs"
      ]
    }, 
    "encodings.utf_32_le": {
      "file": "encodings/utf_32_le.py", 
      "imports": [
        "codecs"
      ]
    }, 
    "encodings.utf_7": {
      "file": "encodings/utf_7.py", 
      "imports": [
        "codecs"
      ]
    }, 
    "encodings.utf_8": {
      "file": "encodings/utf_8.py", 
      "imports": [
        "codecs"
      ]
    }, 
    "encodings.utf_8_sig": {
      "file": "encodings/utf_8_sig.py", 
      "imports": [
        "codecs"
      ]
    }, 
    "encodings.uu_codec": {
      "file": "encodings/uu_codec.py", 
      "imports": [
        "binascii", 
        "binascii.a2b_uu", 
        "binascii.b2a_uu", 
        "cStringIO.StringIO", 
        "codecs"
      ]
    }, 
    "encodings.zlib_codec": {
      "file": "encodings/zlib_codec.py", 
      "imports": [
        "codecs", 
        "zlib"
      ]
    }, 
    "ensurepip": {
      "dir": "ensurepip"
    }, 
    "ensurepip.__init__": {
      "file": "ensurepip/__init__.py", 
      "imports": [
        "__future__", 
        "argparse", 
        "pip", 
        "ssl", 
        "sys", 
        "os", 
        "pkgutil", 
        "shutil", 
        "tempfile"
      ]
    }, 
    "ensurepip.__main__": {
      "file": "ensurepip/__main__.py", 
      "imports": [
        "ensurepip"
      ]
    }, 
    "ensurepip._uninstall": {
      "file": "ensurepip/_uninstall.py", 
      "imports": [
        "argparse", 
        "ensurepip"
      ]
    }, 
    "filecmp": {
      "file": "filecmp.py", 
      "imports": [
        "itertools.ifilter", 
        "itertools.ifilterfalse", 
        "itertools.imap", 
        "itertools.izip", 
        "sys", 
        "getopt", 
        "os", 
        "stat"
      ]
    }, 
    "fileinput": {
      "file": "fileinput.py", 
      "imports": [
        "bz2", 
        "sys", 
        "getopt", 
        "gzip", 
        "io", 
        "os"
      ]
    }, 
    "fnmatch": {
      "file": "fnmatch.py", 
      "imports": [
        "os", 
        "posixpath", 
        "re"
      ]
    }, 
    "formatter": {
      "file": "formatter.py", 
      "imports": [
        "sys"
      ]
    }, 
    "fpformat": {
      "file": "fpformat.py", 
      "imports": [
        "re", 
        "warnings"
      ]
    }, 
    "fractions": {
      "file": "fractions.py", 
      "imports": [
        "__future__", 
        "decimal", 
        "math", 
        "operator", 
        "numbers", 
        "re"
      ]
    }, 
    "ftplib": {
      "file": "ftplib.py", 
      "imports": [
        "SOCKS", 
        "ssl", 
        "sys", 
        "os", 
        "re", 
        "socket"
      ]
    }, 
    "functools": {
      "file": "functools.py", 
      "imports": [
        "_functools"
      ]
    }, 
    "future_builtins": {
      "file": "future_builtins.py", 
      "imports": [
        "itertools.ifilter", 
        "itertools.imap", 
        "itertools.izip"
      ]
    }, 
    "gdbm": {
      "file": "gdbm.py", 
      "imports": [
        "_gdbm_cffi.ffi", 
        "_gdbm_cffi.lib", 
        "os", 
        "thread"
      ]
    }, 
    "genericpath": {
      "file": "genericpath.py", 
      "imports": [
        "os", 
        "stat"
      ]
    }, 
    "getopt": {
      "file": "getopt.py", 
      "imports": [
        "sys", 
        "os"
      ]
    }, 
    "getpass": {
      "file": "getpass.py", 
      "imports": [
        "EasyDialogs.AskPassword", 
        "msvcrt", 
        "sys", 
        "termios", 
        "os", 
        "warnings", 
        "pwd"
      ]
    }, 
    "gettext": {
      "file": "gettext.py", 
      "imports": [
        "__builtin__", 
        "cStringIO.StringIO", 
        "copy", 
        "errno.ENOENT", 
        "sys", 
        "token", 
        "locale", 
        "os", 
        "re", 
        "StringIO", 
        "struct", 
        "tokenize"
      ]
    }, 
    "glob": {
      "file": "glob.py", 
      "imports": [
        "fnmatch", 
        "sys", 
        "os", 
        "re"
      ]
    }, 
    "graphene": {
      "dir": "graphene"
    }, 
    "graphene.__init__": {
      "file": "graphene/__init__.py", 
      "imports": [
        "graphene.pyutils.version", 
        "graphene.relay", 
        "graphene.types", 
        "graphene.utils.resolve_only_args"
      ]
    }, 
    "graphene.pyutils": {
      "dir": "graphene/pyutils"
    }, 
    "graphene.pyutils.__init__": {
      "file": "graphene/pyutils/__init__.py", 
      "imports": []
    }, 
    "graphene.pyutils.enum": {
      "file": "graphene/pyutils/enum.py", 
      "imports": [
        "collections", 
        "sys"
      ]
    }, 
    "graphene.pyutils.version": {
      "file": "graphene/pyutils/version.py", 
      "imports": [
        "__future__", 
        "datetime", 
        "graphene", 
        "os", 
        "subprocess"
      ]
    }, 
    "graphene.relay": {
      "dir": "graphene/relay"
    }, 
    "graphene.relay.__init__": {
      "file": "graphene/relay/__init__.py", 
      "imports": [
        "graphene.relay.connection", 
        "graphene.relay.mutation", 
        "graphene.relay.node"
      ]
    }, 
    "graphene.relay.connection": {
      "file": "graphene/relay/connection.py", 
      "imports": [
        "collections", 
        "functools", 
        "graphene.relay.node", 
        "graphene.types", 
        "graphene.types.field", 
        "graphene.types.objecttype", 
        "graphene.types.options", 
        "graphene.utils.is_base_type", 
        "graphene.utils.props", 
        "re", 
        "graphql_relay", 
        "six"
      ]
    }, 
    "graphene.relay.mutation": {
      "file": "graphene/relay/mutation.py", 
      "imports": [
        "functools", 
        "graphene.types", 
        "graphene.types.objecttype", 
        "graphene.utils.is_base_type", 
        "graphene.utils.props", 
        "re", 
        "promise", 
        "six"
      ]
    }, 
    "graphene.relay.node": {
      "file": "graphene/relay/node.py", 
      "imports": [
        "functools", 
        "graphene.relay.connection", 
        "graphene.types", 
        "graphene.types.interface", 
        "graphql_relay", 
        "six"
      ]
    }, 
    "graphene.types": {
      "dir": "graphene/types"
    }, 
    "graphene.types.__init__": {
      "file": "graphene/types/__init__.py", 
      "imports": [
        "graphene.types.abstracttype", 
        "graphene.types.argument", 
        "graphene.types.dynamic", 
        "graphene.types.enum", 
        "graphene.types.field", 
        "graphene.types.inputfield", 
        "graphene.types.inputobjecttype", 
        "graphene.types.interface", 
        "graphene.types.mutation", 
        "graphene.types.objecttype", 
        "graphene.types.scalars", 
        "graphene.types.schema", 
        "graphene.types.structures", 
        "graphene.types.union"
      ]
    }, 
    "graphene.types.abstracttype": {
      "file": "graphene/types/abstracttype.py", 
      "imports": [
        "graphene.types.options", 
        "graphene.types.utils", 
        "graphene.utils.is_base_type", 
        "six"
      ]
    }, 
    "graphene.types.argument": {
      "file": "graphene/types/argument.py", 
      "imports": [
        "collections", 
        "graphene.types.structures", 
        "graphene.types.unmountedtype", 
        "graphene.utils.orderedtype", 
        "itertools.chain"
      ]
    }, 
    "graphene.types.datetime": {
      "file": "graphene/types/datetime.py", 
      "imports": [
        "__future__", 
        "datetime", 
        "graphene.types.scalars", 
        "iso8601", 
        "graphql.language.ast"
      ]
    }, 
    "graphene.types.definitions": {
      "file": "graphene/types/definitions.py", 
      "imports": [
        "graphql"
      ]
    }, 
    "graphene.types.dynamic": {
      "file": "graphene/types/dynamic.py", 
      "imports": [
        "graphene.utils.orderedtype", 
        "inspect"
      ]
    }, 
    "graphene.types.enum": {
      "file": "graphene/types/enum.py", 
      "imports": [
        "collections", 
        "graphene.pyutils.enum", 
        "graphene.types.enum", 
        "graphene.types.options", 
        "graphene.types.unmountedtype", 
        "graphene.utils.is_base_type", 
        "six"
      ]
    }, 
    "graphene.types.field": {
      "file": "graphene/types/field.py", 
      "imports": [
        "collections", 
        "functools", 
        "graphene.types.argument", 
        "graphene.types.structures", 
        "graphene.types.unmountedtype", 
        "graphene.utils.orderedtype", 
        "inspect"
      ]
    }, 
    "graphene.types.inputfield": {
      "file": "graphene/types/inputfield.py", 
      "imports": [
        "graphene.types.structures", 
        "graphene.utils.orderedtype"
      ]
    }, 
    "graphene.types.inputobjecttype": {
      "file": "graphene/types/inputobjecttype.py", 
      "imports": [
        "graphene.types.abstracttype", 
        "graphene.types.inputfield", 
        "graphene.types.options", 
        "graphene.types.unmountedtype", 
        "graphene.types.utils", 
        "graphene.utils.is_base_type", 
        "six"
      ]
    }, 
    "graphene.types.interface": {
      "file": "graphene/types/interface.py", 
      "imports": [
        "graphene.types.abstracttype", 
        "graphene.types.field", 
        "graphene.types.options", 
        "graphene.types.utils", 
        "graphene.utils.is_base_type", 
        "six"
      ]
    }, 
    "graphene.types.json": {
      "file": "graphene/types/json.py", 
      "imports": [
        "__future__", 
        "graphene.types.scalars", 
        "json", 
        "graphql.language.ast"
      ]
    }, 
    "graphene.types.mutation": {
      "file": "graphene/types/mutation.py", 
      "imports": [
        "functools", 
        "graphene.types.field", 
        "graphene.types.objecttype", 
        "graphene.utils.is_base_type", 
        "graphene.utils.props", 
        "six"
      ]
    }, 
    "graphene.types.objecttype": {
      "file": "graphene/types/objecttype.py", 
      "imports": [
        "collections", 
        "graphene.types.abstracttype", 
        "graphene.types.field", 
        "graphene.types.interface", 
        "graphene.types.options", 
        "graphene.types.utils", 
        "graphene.utils.is_base_type", 
        "six"
      ]
    }, 
    "graphene.types.options": {
      "file": "graphene/types/options.py", 
      "imports": [
        "graphene.utils.props", 
        "inspect"
      ]
    }, 
    "graphene.types.scalars": {
      "file": "graphene/types/scalars.py", 
      "imports": [
        "graphene.types.options", 
        "graphene.types.unmountedtype", 
        "graphene.utils.is_base_type", 
        "graphql.language.ast", 
        "six"
      ]
    }, 
    "graphene.types.schema": {
      "file": "graphene/types/schema.py", 
      "imports": [
        "graphene.types.typemap", 
        "graphql.type.directives", 
        "graphql.type.introspection", 
        "graphql.graphql", 
        "graphql.utils.introspection_query", 
        "graphql.utils.schema_printer", 
        "graphql"
      ]
    }, 
    "graphene.types.structures": {
      "file": "graphene/types/structures.py", 
      "imports": [
        "graphene.types.unmountedtype"
      ]
    }, 
    "graphene.types.typemap": {
      "file": "graphene/types/typemap.py", 
      "imports": [
        "collections", 
        "functools", 
        "graphene.types.definitions", 
        "graphene.types.dynamic", 
        "graphene.types.enum", 
        "graphene.types.inputobjecttype", 
        "graphene.types.interface", 
        "graphene.types.objecttype", 
        "graphene.types.scalars", 
        "graphene.types.structures", 
        "graphene.types.union", 
        "graphene.utils.str_converters", 
        "inspect", 
        "graphql.type.typemap", 
        "graphql.type", 
        "graphql"
      ]
    }, 
    "graphene.types.union": {
      "file": "graphene/types/union.py", 
      "imports": [
        "graphene.types.options", 
        "graphene.utils.is_base_type", 
        "six"
      ]
    }, 
    "graphene.types.unmountedtype": {
      "file": "graphene/types/unmountedtype.py", 
      "imports": [
        "graphene.types.argument", 
        "graphene.types.field", 
        "graphene.types.inputfield", 
        "graphene.utils.orderedtype"
      ]
    }, 
    "graphene.types.utils": {
      "file": "graphene/types/utils.py", 
      "imports": [
        "collections", 
        "graphene.types", 
        "graphene.types.dynamic", 
        "graphene.types.field", 
        "graphene.types.inputfield", 
        "graphene.types.unmountedtype"
      ]
    }, 
    "graphene.utils": {
      "dir": "graphene/utils"
    }, 
    "graphene.utils.__init__": {
      "file": "graphene/utils/__init__.py", 
      "imports": []
    }, 
    "graphene.utils.is_base_type": {
      "file": "graphene/utils/is_base_type.py", 
      "imports": []
    }, 
    "graphene.utils.orderedtype": {
      "file": "graphene/utils/orderedtype.py", 
      "imports": [
        "functools"
      ]
    }, 
    "graphene.utils.props": {
      "file": "graphene/utils/props.py", 
      "imports": []
    }, 
    "graphene.utils.resolve_only_args": {
      "file": "graphene/utils/resolve_only_args.py", 
      "imports": [
        "functools"
      ]
    }, 
    "graphene.utils.str_converters": {
      "file": "graphene/utils/str_converters.py", 
      "imports": [
        "re"
      ]
    }, 
    "graphql": {
      "dir": "graphql"
    }, 
    "graphql.__init__": {
      "file": "graphql/__init__.py", 
      "imports": [
        "graphql.error", 
        "graphql.error.format_error", 
        "graphql.execution", 
        "graphql.graphql", 
        "graphql.language.base", 
        "graphql.pyutils.version", 
        "graphql.type", 
        "graphql.utils.base", 
        "graphql.validation"
      ]
    }, 
    "graphql.error": {
      "dir": "graphql/error"
    }, 
    "graphql.error.__init__": {
      "file": "graphql/error/__init__.py", 
      "imports": [
        "graphql.error.base", 
        "graphql.error.format_error", 
        "graphql.error.located_error", 
        "graphql.error.syntax_error"
      ]
    }, 
    "graphql.error.base": {
      "file": "graphql/error/base.py", 
      "imports": [
        "graphql.language.location"
      ]
    }, 
    "graphql.error.format_error": {
      "file": "graphql/error/format_error.py", 
      "imports": []
    }, 
    "graphql.error.located_error": {
      "file": "graphql/error/located_error.py", 
      "imports": [
        "graphql.error.base", 
        "sys"
      ]
    }, 
    "graphql.error.syntax_error": {
      "file": "graphql/error/syntax_error.py", 
      "imports": [
        "graphql.error.base", 
        "graphql.language.location"
      ]
    }, 
    "graphql.execution": {
      "dir": "graphql/execution"
    }, 
    "graphql.execution.__init__": {
      "file": "graphql/execution/__init__.py", 
      "imports": [
        "graphql.execution.base", 
        "graphql.execution.executor", 
        "graphql.execution.middleware"
      ]
    }, 
    "graphql.execution.base": {
      "file": "graphql/execution/base.py", 
      "imports": [
        "graphql.error", 
        "graphql.execution.values", 
        "graphql.language.ast", 
        "graphql.pyutils.default_ordered_dict", 
        "graphql.type.definition", 
        "graphql.type.directives", 
        "graphql.type.introspection", 
        "graphql.utils.type_from_ast"
      ]
    }, 
    "graphql.execution.executor": {
      "file": "graphql/execution/executor.py", 
      "imports": [
        "collections", 
        "functools", 
        "graphql.error", 
        "graphql.execution.base", 
        "graphql.execution.executors.sync", 
        "graphql.execution.middleware", 
        "graphql.pyutils.default_ordered_dict", 
        "graphql.pyutils.ordereddict", 
        "graphql.type", 
        "logging", 
        "sys", 
        "promise"
      ]
    }, 
    "graphql.execution.executors": {
      "dir": "graphql/execution/executors"
    }, 
    "graphql.execution.executors.__init__": {
      "file": "graphql/execution/executors/__init__.py", 
      "imports": []
    }, 
    "graphql.execution.executors.asyncio": {
      "file": "graphql/execution/executors/asyncio.py", 
      "imports": [
        "__future__", 
        "asyncio.Future", 
        "asyncio.ensure_future", 
        "asyncio.get_event_loop", 
        "asyncio.iscoroutine", 
        "asyncio.wait", 
        "promise"
      ]
    }, 
    "graphql.execution.executors.gevent": {
      "file": "graphql/execution/executors/gevent.py", 
      "imports": [
        "__future__", 
        "gevent", 
        "graphql.execution.executors.utils", 
        "promise"
      ]
    }, 
    "graphql.execution.executors.process": {
      "file": "graphql/execution/executors/process.py", 
      "imports": [
        "graphql.execution.executors.utils", 
        "multiprocessing.Process", 
        "multiprocessing.Queue", 
        "promise"
      ]
    }, 
    "graphql.execution.executors.sync": {
      "file": "graphql/execution/executors/sync.py", 
      "imports": []
    }, 
    "graphql.execution.executors.thread": {
      "file": "graphql/execution/executors/thread.py", 
      "imports": [
        "graphql.execution.executors.utils", 
        "multiprocessing.pool.ThreadPool", 
        "threading.Thread", 
        "promise"
      ]
    }, 
    "graphql.execution.executors.utils": {
      "file": "graphql/execution/executors/utils.py", 
      "imports": []
    }, 
    "graphql.execution.middleware": {
      "file": "graphql/execution/middleware.py", 
      "imports": [
        "functools", 
        "inspect", 
        "itertools.chain", 
        "promise"
      ]
    }, 
    "graphql.execution.values": {
      "file": "graphql/execution/values.py", 
      "imports": [
        "collections", 
        "graphql.error", 
        "graphql.language.printer", 
        "graphql.type", 
        "graphql.utils.is_valid_value", 
        "graphql.utils.type_from_ast", 
        "graphql.utils.value_from_ast", 
        "json", 
        "six"
      ]
    }, 
    "graphql.graphql": {
      "file": "graphql/graphql.py", 
      "imports": [
        "graphql.execution", 
        "graphql.language.parser", 
        "graphql.language.source", 
        "graphql.validation"
      ]
    }, 
    "graphql.language": {
      "dir": "graphql/language"
    }, 
    "graphql.language.__init__": {
      "file": "graphql/language/__init__.py", 
      "imports": []
    }, 
    "graphql.language.ast": {
      "file": "graphql/language/ast.py", 
      "imports": []
    }, 
    "graphql.language.base": {
      "file": "graphql/language/base.py", 
      "imports": [
        "graphql.language.lexer", 
        "graphql.language.location", 
        "graphql.language.parser", 
        "graphql.language.printer", 
        "graphql.language.source", 
        "graphql.language.visitor"
      ]
    }, 
    "graphql.language.lexer": {
      "file": "graphql/language/lexer.py", 
      "imports": [
        "graphql.error", 
        "json", 
        "six"
      ]
    }, 
    "graphql.language.location": {
      "file": "graphql/language/location.py", 
      "imports": []
    }, 
    "graphql.language.parser": {
      "file": "graphql/language/parser.py", 
      "imports": [
        "graphql.error", 
        "graphql.language.ast", 
        "graphql.language.lexer", 
        "graphql.language.source", 
        "six"
      ]
    }, 
    "graphql.language.printer": {
      "file": "graphql/language/printer.py", 
      "imports": [
        "graphql.language.visitor", 
        "json"
      ]
    }, 
    "graphql.language.source": {
      "file": "graphql/language/source.py", 
      "imports": []
    }, 
    "graphql.language.visitor": {
      "file": "graphql/language/visitor.py", 
      "imports": [
        "copy", 
        "graphql.language.ast", 
        "graphql.language.visitor_meta", 
        "six"
      ]
    }, 
    "graphql.language.visitor_meta": {
      "file": "graphql/language/visitor_meta.py", 
      "imports": [
        "graphql.language.ast"
      ]
    }, 
    "graphql.pyutils": {
      "dir": "graphql/pyutils"
    }, 
    "graphql.pyutils.__init__": {
      "file": "graphql/pyutils/__init__.py", 
      "imports": []
    }, 
    "graphql.pyutils.cached_property": {
      "file": "graphql/pyutils/cached_property.py", 
      "imports": []
    }, 
    "graphql.pyutils.contain_subset": {
      "file": "graphql/pyutils/contain_subset.py", 
      "imports": []
    }, 
    "graphql.pyutils.default_ordered_dict": {
      "file": "graphql/pyutils/default_ordered_dict.py", 
      "imports": [
        "collections", 
        "copy"
      ]
    }, 
    "graphql.pyutils.ordereddict": {
      "file": "graphql/pyutils/ordereddict.py", 
      "imports": [
        "collections", 
        "cyordereddict.OrderedDict"
      ]
    }, 
    "graphql.pyutils.pair_set": {
      "file": "graphql/pyutils/pair_set.py", 
      "imports": []
    }, 
    "graphql.pyutils.version": {
      "file": "graphql/pyutils/version.py", 
      "imports": [
        "__future__", 
        "datetime", 
        "graphql.graphql", 
        "os", 
        "subprocess"
      ]
    }, 
    "graphql.type": {
      "dir": "graphql/type"
    }, 
    "graphql.type.__init__": {
      "file": "graphql/type/__init__.py", 
      "imports": [
        "graphql.type.definition", 
        "graphql.type.directives", 
        "graphql.type.introspection", 
        "graphql.type.scalars", 
        "graphql.type.schema"
      ]
    }, 
    "graphql.type.definition": {
      "file": "graphql/type/definition.py", 
      "imports": [
        "collections", 
        "copy", 
        "graphql.language.ast", 
        "graphql.pyutils.cached_property", 
        "graphql.pyutils.ordereddict", 
        "graphql.utils.assert_valid_name"
      ]
    }, 
    "graphql.type.directives": {
      "file": "graphql/type/directives.py", 
      "imports": [
        "collections", 
        "graphql.pyutils.ordereddict", 
        "graphql.type.definition", 
        "graphql.type.scalars", 
        "graphql.utils.assert_valid_name"
      ]
    }, 
    "graphql.type.introspection": {
      "file": "graphql/type/introspection.py", 
      "imports": [
        "collections", 
        "graphql.language.printer", 
        "graphql.type.definition", 
        "graphql.type.directives", 
        "graphql.type.scalars", 
        "graphql.utils.ast_from_value"
      ]
    }, 
    "graphql.type.scalars": {
      "file": "graphql/type/scalars.py", 
      "imports": [
        "graphql.language.ast", 
        "graphql.type.definition", 
        "six"
      ]
    }, 
    "graphql.type.schema": {
      "file": "graphql/type/schema.py", 
      "imports": [
        "collections", 
        "graphql.type.definition", 
        "graphql.type.directives", 
        "graphql.type.introspection", 
        "graphql.type.typemap"
      ]
    }, 
    "graphql.type.typemap": {
      "file": "graphql/type/typemap.py", 
      "imports": [
        "collections", 
        "functools", 
        "graphql.type.definition", 
        "graphql.utils.type_comparators"
      ]
    }, 
    "graphql.utils": {
      "dir": "graphql/utils"
    }, 
    "graphql.utils.__init__": {
      "file": "graphql/utils/__init__.py", 
      "imports": []
    }, 
    "graphql.utils.assert_valid_name": {
      "file": "graphql/utils/assert_valid_name.py", 
      "imports": [
        "re"
      ]
    }, 
    "graphql.utils.ast_from_value": {
      "file": "graphql/utils/ast_from_value.py", 
      "imports": [
        "graphql.language.ast", 
        "graphql.type.definition", 
        "graphql.type.scalars", 
        "json", 
        "re", 
        "sys", 
        "six"
      ]
    }, 
    "graphql.utils.ast_to_code": {
      "file": "graphql/utils/ast_to_code.py", 
      "imports": [
        "graphql.language.ast", 
        "graphql.language.parser"
      ]
    }, 
    "graphql.utils.ast_to_dict": {
      "file": "graphql/utils/ast_to_dict.py", 
      "imports": [
        "graphql.language.ast"
      ]
    }, 
    "graphql.utils.base": {
      "file": "graphql/utils/base.py", 
      "imports": [
        "graphql.utils.assert_valid_name", 
        "graphql.utils.ast_from_value", 
        "graphql.utils.build_ast_schema", 
        "graphql.utils.build_client_schema", 
        "graphql.utils.concat_ast", 
        "graphql.utils.extend_schema", 
        "graphql.utils.get_operation_ast", 
        "graphql.utils.introspection_query", 
        "graphql.utils.is_valid_literal_value", 
        "graphql.utils.is_valid_value", 
        "graphql.utils.schema_printer", 
        "graphql.utils.type_comparators", 
        "graphql.utils.type_from_ast", 
        "graphql.utils.type_info", 
        "graphql.utils.value_from_ast"
      ]
    }, 
    "graphql.utils.build_ast_schema": {
      "file": "graphql/utils/build_ast_schema.py", 
      "imports": [
        "graphql.execution.values", 
        "graphql.language.ast", 
        "graphql.pyutils.ordereddict", 
        "graphql.type", 
        "graphql.type.introspection", 
        "graphql.utils.value_from_ast"
      ]
    }, 
    "graphql.utils.build_client_schema": {
      "file": "graphql/utils/build_client_schema.py", 
      "imports": [
        "graphql.language.parser", 
        "graphql.pyutils.ordereddict", 
        "graphql.type", 
        "graphql.type.directives", 
        "graphql.type.introspection", 
        "graphql.utils.value_from_ast"
      ]
    }, 
    "graphql.utils.concat_ast": {
      "file": "graphql/utils/concat_ast.py", 
      "imports": [
        "graphql.language.ast", 
        "itertools"
      ]
    }, 
    "graphql.utils.extend_schema": {
      "file": "graphql/utils/extend_schema.py", 
      "imports": [
        "collections", 
        "graphql.error", 
        "graphql.language.ast", 
        "graphql.pyutils.ordereddict", 
        "graphql.type.definition", 
        "graphql.type.introspection", 
        "graphql.type.scalars", 
        "graphql.type.schema", 
        "graphql.utils.value_from_ast"
      ]
    }, 
    "graphql.utils.get_field_def": {
      "file": "graphql/utils/get_field_def.py", 
      "imports": [
        "graphql.type.definition", 
        "graphql.type.introspection"
      ]
    }, 
    "graphql.utils.get_operation_ast": {
      "file": "graphql/utils/get_operation_ast.py", 
      "imports": [
        "graphql.language.ast"
      ]
    }, 
    "graphql.utils.introspection_query": {
      "file": "graphql/utils/introspection_query.py", 
      "imports": []
    }, 
    "graphql.utils.is_valid_literal_value": {
      "file": "graphql/utils/is_valid_literal_value.py", 
      "imports": [
        "graphql.language.ast", 
        "graphql.language.printer", 
        "graphql.type.definition"
      ]
    }, 
    "graphql.utils.is_valid_value": {
      "file": "graphql/utils/is_valid_value.py", 
      "imports": [
        "collections", 
        "graphql.type", 
        "json", 
        "six"
      ]
    }, 
    "graphql.utils.quoted_or_list": {
      "file": "graphql/utils/quoted_or_list.py", 
      "imports": [
        "functools"
      ]
    }, 
    "graphql.utils.schema_printer": {
      "file": "graphql/utils/schema_printer.py", 
      "imports": [
        "graphql.language.printer", 
        "graphql.type.definition", 
        "graphql.type.directives", 
        "graphql.utils.ast_from_value"
      ]
    }, 
    "graphql.utils.suggestion_list": {
      "file": "graphql/utils/suggestion_list.py", 
      "imports": [
        "collections"
      ]
    }, 
    "graphql.utils.type_comparators": {
      "file": "graphql/utils/type_comparators.py", 
      "imports": [
        "graphql.type.definition"
      ]
    }, 
    "graphql.utils.type_from_ast": {
      "file": "graphql/utils/type_from_ast.py", 
      "imports": [
        "graphql.language.ast", 
        "graphql.type.definition"
      ]
    }, 
    "graphql.utils.type_info": {
      "file": "graphql/utils/type_info.py", 
      "imports": [
        "graphql.language.visitor_meta", 
        "graphql.type.definition", 
        "graphql.utils.get_field_def", 
        "graphql.utils.type_from_ast", 
        "six"
      ]
    }, 
    "graphql.utils.value_from_ast": {
      "file": "graphql/utils/value_from_ast.py", 
      "imports": [
        "graphql.language.ast", 
        "graphql.type"
      ]
    }, 
    "graphql.validation": {
      "dir": "graphql/validation"
    }, 
    "graphql.validation.__init__": {
      "file": "graphql/validation/__init__.py", 
      "imports": [
        "graphql.validation.rules", 
        "graphql.validation.validation"
      ]
    }, 
    "graphql.validation.rules": {
      "dir": "graphql/validation/rules"
    }, 
    "graphql.validation.rules.__init__": {
      "file": "graphql/validation/rules/__init__.py", 
      "imports": [
        "graphql.validation.rules.arguments_of_correct_type", 
        "graphql.validation.rules.default_values_of_correct_type", 
        "graphql.validation.rules.fields_on_correct_type", 
        "graphql.validation.rules.fragments_on_composite_types", 
        "graphql.validation.rules.known_argument_names", 
        "graphql.validation.rules.known_directives", 
        "graphql.validation.rules.known_fragment_names", 
        "graphql.validation.rules.known_type_names", 
        "graphql.validation.rules.lone_anonymous_operation", 
        "graphql.validation.rules.no_fragment_cycles", 
        "graphql.validation.rules.no_undefined_variables", 
        "graphql.validation.rules.no_unused_fragments", 
        "graphql.validation.rules.no_unused_variables", 
        "graphql.validation.rules.overlapping_fields_can_be_merged", 
        "graphql.validation.rules.possible_fragment_spreads", 
        "graphql.validation.rules.provided_non_null_arguments", 
        "graphql.validation.rules.scalar_leafs", 
        "graphql.validation.rules.unique_argument_names", 
        "graphql.validation.rules.unique_fragment_names", 
        "graphql.validation.rules.unique_input_field_names", 
        "graphql.validation.rules.unique_operation_names", 
        "graphql.validation.rules.unique_variable_names", 
        "graphql.validation.rules.variables_are_input_types", 
        "graphql.validation.rules.variables_in_allowed_position"
      ]
    }, 
    "graphql.validation.rules.arguments_of_correct_type": {
      "file": "graphql/validation/rules/arguments_of_correct_type.py", 
      "imports": [
        "graphql.error", 
        "graphql.language.printer", 
        "graphql.utils.is_valid_literal_value", 
        "graphql.validation.rules.base"
      ]
    }, 
    "graphql.validation.rules.base": {
      "file": "graphql/validation/rules/base.py", 
      "imports": [
        "graphql.language.visitor"
      ]
    }, 
    "graphql.validation.rules.default_values_of_correct_type": {
      "file": "graphql/validation/rules/default_values_of_correct_type.py", 
      "imports": [
        "graphql.error", 
        "graphql.language.printer", 
        "graphql.type.definition", 
        "graphql.utils.is_valid_literal_value", 
        "graphql.validation.rules.base"
      ]
    }, 
    "graphql.validation.rules.fields_on_correct_type": {
      "file": "graphql/validation/rules/fields_on_correct_type.py", 
      "imports": [
        "collections", 
        "graphql.error", 
        "graphql.pyutils.ordereddict", 
        "graphql.type.definition", 
        "graphql.utils.quoted_or_list", 
        "graphql.utils.suggestion_list", 
        "graphql.validation.rules.base", 
        "itertools.izip"
      ]
    }, 
    "graphql.validation.rules.fragments_on_composite_types": {
      "file": "graphql/validation/rules/fragments_on_composite_types.py", 
      "imports": [
        "graphql.error", 
        "graphql.language.printer", 
        "graphql.type.definition", 
        "graphql.validation.rules.base"
      ]
    }, 
    "graphql.validation.rules.known_argument_names": {
      "file": "graphql/validation/rules/known_argument_names.py", 
      "imports": [
        "graphql.error", 
        "graphql.language.ast", 
        "graphql.utils.quoted_or_list", 
        "graphql.utils.suggestion_list", 
        "graphql.validation.rules.base"
      ]
    }, 
    "graphql.validation.rules.known_directives": {
      "file": "graphql/validation/rules/known_directives.py", 
      "imports": [
        "graphql.error", 
        "graphql.language.ast", 
        "graphql.type.directives", 
        "graphql.validation.rules.base"
      ]
    }, 
    "graphql.validation.rules.known_fragment_names": {
      "file": "graphql/validation/rules/known_fragment_names.py", 
      "imports": [
        "graphql.error", 
        "graphql.validation.rules.base"
      ]
    }, 
    "graphql.validation.rules.known_type_names": {
      "file": "graphql/validation/rules/known_type_names.py", 
      "imports": [
        "graphql.error", 
        "graphql.utils.quoted_or_list", 
        "graphql.utils.suggestion_list", 
        "graphql.validation.rules.base"
      ]
    }, 
    "graphql.validation.rules.lone_anonymous_operation": {
      "file": "graphql/validation/rules/lone_anonymous_operation.py", 
      "imports": [
        "graphql.error", 
        "graphql.language.ast", 
        "graphql.validation.rules.base"
      ]
    }, 
    "graphql.validation.rules.no_fragment_cycles": {
      "file": "graphql/validation/rules/no_fragment_cycles.py", 
      "imports": [
        "graphql.error", 
        "graphql.validation.rules.base"
      ]
    }, 
    "graphql.validation.rules.no_undefined_variables": {
      "file": "graphql/validation/rules/no_undefined_variables.py", 
      "imports": [
        "graphql.error", 
        "graphql.validation.rules.base"
      ]
    }, 
    "graphql.validation.rules.no_unused_fragments": {
      "file": "graphql/validation/rules/no_unused_fragments.py", 
      "imports": [
        "graphql.error", 
        "graphql.validation.rules.base"
      ]
    }, 
    "graphql.validation.rules.no_unused_variables": {
      "file": "graphql/validation/rules/no_unused_variables.py", 
      "imports": [
        "graphql.error", 
        "graphql.validation.rules.base"
      ]
    }, 
    "graphql.validation.rules.overlapping_fields_can_be_merged": {
      "file": "graphql/validation/rules/overlapping_fields_can_be_merged.py", 
      "imports": [
        "collections", 
        "graphql.error", 
        "graphql.language.ast", 
        "graphql.language.printer", 
        "graphql.pyutils.pair_set", 
        "graphql.type.definition", 
        "graphql.utils.type_comparators", 
        "graphql.utils.type_from_ast", 
        "graphql.validation.rules.base", 
        "itertools"
      ]
    }, 
    "graphql.validation.rules.possible_fragment_spreads": {
      "file": "graphql/validation/rules/possible_fragment_spreads.py", 
      "imports": [
        "graphql.error", 
        "graphql.utils.type_comparators", 
        "graphql.utils.type_from_ast", 
        "graphql.validation.rules.base"
      ]
    }, 
    "graphql.validation.rules.provided_non_null_arguments": {
      "file": "graphql/validation/rules/provided_non_null_arguments.py", 
      "imports": [
        "graphql.error", 
        "graphql.type.definition", 
        "graphql.validation.rules.base"
      ]
    }, 
    "graphql.validation.rules.scalar_leafs": {
      "file": "graphql/validation/rules/scalar_leafs.py", 
      "imports": [
        "graphql.error", 
        "graphql.type.definition", 
        "graphql.validation.rules.base"
      ]
    }, 
    "graphql.validation.rules.unique_argument_names": {
      "file": "graphql/validation/rules/unique_argument_names.py", 
      "imports": [
        "graphql.error", 
        "graphql.validation.rules.base"
      ]
    }, 
    "graphql.validation.rules.unique_fragment_names": {
      "file": "graphql/validation/rules/unique_fragment_names.py", 
      "imports": [
        "graphql.error", 
        "graphql.validation.rules.base"
      ]
    }, 
    "graphql.validation.rules.unique_input_field_names": {
      "file": "graphql/validation/rules/unique_input_field_names.py", 
      "imports": [
        "graphql.error", 
        "graphql.validation.rules.base"
      ]
    }, 
    "graphql.validation.rules.unique_operation_names": {
      "file": "graphql/validation/rules/unique_operation_names.py", 
      "imports": [
        "graphql.error", 
        "graphql.validation.rules.base"
      ]
    }, 
    "graphql.validation.rules.unique_variable_names": {
      "file": "graphql/validation/rules/unique_variable_names.py", 
      "imports": [
        "graphql.error", 
        "graphql.validation.rules.base"
      ]
    }, 
    "graphql.validation.rules.variables_are_input_types": {
      "file": "graphql/validation/rules/variables_are_input_types.py", 
      "imports": [
        "graphql.error", 
        "graphql.language.printer", 
        "graphql.type.definition", 
        "graphql.utils.type_from_ast", 
        "graphql.validation.rules.base"
      ]
    }, 
    "graphql.validation.rules.variables_in_allowed_position": {
      "file": "graphql/validation/rules/variables_in_allowed_position.py", 
      "imports": [
        "graphql.error", 
        "graphql.type.definition", 
        "graphql.utils.type_comparators", 
        "graphql.utils.type_from_ast", 
        "graphql.validation.rules.base"
      ]
    }, 
    "graphql.validation.validation": {
      "file": "graphql/validation/validation.py", 
      "imports": [
        "graphql.language.ast", 
        "graphql.language.visitor", 
        "graphql.type", 
        "graphql.utils.type_info", 
        "graphql.validation.rules"
      ]
    }, 
    "graphql_relay": {
      "dir": "graphql_relay"
    }, 
    "graphql_relay.__init__": {
      "file": "graphql_relay/__init__.py", 
      "imports": [
        "graphql_relay.connection.arrayconnection", 
        "graphql_relay.connection.connection", 
        "graphql_relay.mutation.mutation", 
        "graphql_relay.node.node"
      ]
    }, 
    "graphql_relay.connection": {
      "dir": "graphql_relay/connection"
    }, 
    "graphql_relay.connection.__init__": {
      "file": "graphql_relay/connection/__init__.py", 
      "imports": []
    }, 
    "graphql_relay.connection.arrayconnection": {
      "file": "graphql_relay/connection/arrayconnection.py", 
      "imports": [
        "graphql_relay.connection.connectiontypes", 
        "graphql_relay.utils", 
        "promise"
      ]
    }, 
    "graphql_relay.connection.connection": {
      "file": "graphql_relay/connection/connection.py", 
      "imports": [
        "collections", 
        "graphql.type", 
        "graphql_relay.utils"
      ]
    }, 
    "graphql_relay.connection.connectiontypes": {
      "file": "graphql_relay/connection/connectiontypes.py", 
      "imports": []
    }, 
    "graphql_relay.mutation": {
      "dir": "graphql_relay/mutation"
    }, 
    "graphql_relay.mutation.__init__": {
      "file": "graphql_relay/mutation/__init__.py", 
      "imports": []
    }, 
    "graphql_relay.mutation.mutation": {
      "file": "graphql_relay/mutation/mutation.py", 
      "imports": [
        "collections", 
        "graphql.error", 
        "graphql.type", 
        "graphql_relay.utils", 
        "promise"
      ]
    }, 
    "graphql_relay.node": {
      "dir": "graphql_relay/node"
    }, 
    "graphql_relay.node.__init__": {
      "file": "graphql_relay/node/__init__.py", 
      "imports": []
    }, 
    "graphql_relay.node.node": {
      "file": "graphql_relay/node/node.py", 
      "imports": [
        "collections", 
        "graphql.type", 
        "graphql_relay.utils"
      ]
    }, 
    "graphql_relay.node.plural": {
      "file": "graphql_relay/node/plural.py", 
      "imports": [
        "collections", 
        "graphql.type", 
        "promise"
      ]
    }, 
    "graphql_relay.utils": {
      "file": "graphql_relay/utils.py", 
      "imports": [
        "base64"
      ]
    }, 
    "greenlet": {
      "file": "greenlet.py", 
      "imports": [
        "_continuation", 
        "sys", 
        "threading.local"
      ]
    }, 
    "grp": {
      "file": "grp.py", 
      "imports": [
        "__pypy__.builtinify", 
        "_pwdgrp_cffi.ffi", 
        "_pwdgrp_cffi.lib", 
        "_structseq", 
        "os"
      ]
    }, 
    "gzip": {
      "file": "gzip.py", 
      "imports": [
        "__builtin__", 
        "errno", 
        "sys", 
        "time", 
        "zlib", 
        "io", 
        "os", 
        "struct", 
        "warnings"
      ]
    }, 
    "hashlib": {
      "file": "hashlib.py", 
      "imports": [
        "_hashlib", 
        "_hashlib.pbkdf2_hmac", 
        "_md5", 
        "_sha", 
        "binascii", 
        "logging", 
        "struct", 
        "_sha256", 
        "_sha512"
      ]
    }, 
    "heapq": {
      "file": "heapq.py", 
      "imports": [
        "_heapq.*", 
        "doctest", 
        "itertools.chain", 
        "itertools.count", 
        "itertools.imap", 
        "itertools.islice", 
        "itertools.izip", 
        "itertools.tee", 
        "operator.itemgetter"
      ]
    }, 
    "hmac": {
      "file": "hmac.py", 
      "imports": [
        "hashlib", 
        "operator._compare_digest", 
        "warnings"
      ]
    }, 
    "htmlentitydefs": {
      "file": "htmlentitydefs.py", 
      "imports": []
    }, 
    "htmllib": {
      "file": "htmllib.py", 
      "imports": [
        "formatter", 
        "htmlentitydefs", 
        "sys", 
        "sgmllib", 
        "warnings"
      ]
    }, 
    "httplib": {
      "file": "httplib.py", 
      "imports": [
        "array.array", 
        "cStringIO.StringIO", 
        "ssl", 
        "sys.py3kwarning", 
        "mimetools", 
        "os", 
        "socket", 
        "StringIO", 
        "urlparse", 
        "warnings"
      ]
    }, 
    "identity_dict": {
      "file": "identity_dict.py", 
      "imports": [
        "UserDict", 
        "__pypy__.identity_dict"
      ]
    }, 
    "ihooks": {
      "file": "ihooks.py", 
      "imports": [
        "__builtin__", 
        "imp", 
        "imp.C_BUILTIN", 
        "imp.C_EXTENSION", 
        "imp.PKG_DIRECTORY", 
        "imp.PY_COMPILED", 
        "imp.PY_FROZEN", 
        "imp.PY_SOURCE", 
        "marshal", 
        "sys", 
        "os", 
        "warnings"
      ]
    }, 
    "imaplib": {
      "file": "imaplib.py", 
      "imports": [
        "binascii", 
        "errno", 
        "getopt", 
        "getpass", 
        "hmac", 
        "ssl", 
        "subprocess", 
        "sys", 
        "time", 
        "random", 
        "re", 
        "socket"
      ]
    }, 
    "imghdr": {
      "file": "imghdr.py", 
      "imports": [
        "glob", 
        "sys", 
        "os"
      ]
    }, 
    "importlib": {
      "dir": "importlib"
    }, 
    "importlib.__init__": {
      "file": "importlib/__init__.py", 
      "imports": [
        "sys"
      ]
    }, 
    "imputil": {
      "file": "imputil.py", 
      "imports": [
        "__builtin__", 
        "dos.stat", 
        "imp", 
        "marshal", 
        "nt.stat", 
        "os2.stat", 
        "posix.stat", 
        "sys", 
        "struct", 
        "warnings"
      ]
    }, 
    "inspect": {
      "file": "inspect.py", 
      "imports": [
        "collections", 
        "dis", 
        "imp", 
        "operator.attrgetter", 
        "sys", 
        "linecache", 
        "os", 
        "re", 
        "string", 
        "tokenize", 
        "types"
      ]
    }, 
    "io": {
      "file": "io.py", 
      "imports": [
        "_io", 
        "_io.BlockingIOError", 
        "_io.BufferedRWPair", 
        "_io.BufferedRandom", 
        "_io.BufferedReader", 
        "_io.BufferedWriter", 
        "_io.BytesIO", 
        "_io.DEFAULT_BUFFER_SIZE", 
        "_io.FileIO", 
        "_io.IncrementalNewlineDecoder", 
        "_io.StringIO", 
        "_io.TextIOWrapper", 
        "_io.UnsupportedOperation", 
        "_io.open", 
        "abc"
      ]
    }, 
    "json": {
      "dir": "json"
    }, 
    "json.__init__": {
      "file": "json/__init__.py", 
      "imports": [
        "_pypyjson", 
        "json.decoder", 
        "json.encoder"
      ]
    }, 
    "json.decoder": {
      "file": "json/decoder.py", 
      "imports": [
        "_json.scanstring", 
        "json.scanner", 
        "sys", 
        "re", 
        "struct"
      ]
    }, 
    "json.encoder": {
      "file": "json/encoder.py", 
      "imports": [
        "__pypy__.builders.StringBuilder", 
        "__pypy__.builders.UnicodeBuilder", 
        "_pypyjson.raw_encode_basestring_ascii", 
        "re"
      ]
    }, 
    "json.scanner": {
      "file": "json/scanner.py", 
      "imports": [
        "_json.make_scanner", 
        "re"
      ]
    }, 
    "json.tests": {
      "dir": "json/tests"
    }, 
    "json.tests.__init__": {
      "file": "json/tests/__init__.py", 
      "imports": [
        "doctest", 
        "json", 
        "sys", 
        "os", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "json.tests.test_check_circular": {
      "file": "json/tests/test_check_circular.py", 
      "imports": [
        "json.tests"
      ]
    }, 
    "json.tests.test_decode": {
      "file": "json/tests/test_decode.py", 
      "imports": [
        "collections", 
        "decimal", 
        "json.tests", 
        "StringIO"
      ]
    }, 
    "json.tests.test_default": {
      "file": "json/tests/test_default.py", 
      "imports": [
        "json.tests"
      ]
    }, 
    "json.tests.test_dump": {
      "file": "json/tests/test_dump.py", 
      "imports": [
        "cStringIO.StringIO", 
        "json.tests"
      ]
    }, 
    "json.tests.test_encode_basestring_ascii": {
      "file": "json/tests/test_encode_basestring_ascii.py", 
      "imports": [
        "collections", 
        "json.tests"
      ]
    }, 
    "json.tests.test_fail": {
      "file": "json/tests/test_fail.py", 
      "imports": [
        "json.tests"
      ]
    }, 
    "json.tests.test_float": {
      "file": "json/tests/test_float.py", 
      "imports": [
        "json.tests", 
        "math"
      ]
    }, 
    "json.tests.test_indent": {
      "file": "json/tests/test_indent.py", 
      "imports": [
        "json.tests", 
        "StringIO", 
        "textwrap"
      ]
    }, 
    "json.tests.test_pass1": {
      "file": "json/tests/test_pass1.py", 
      "imports": [
        "json.tests"
      ]
    }, 
    "json.tests.test_pass2": {
      "file": "json/tests/test_pass2.py", 
      "imports": [
        "json.tests"
      ]
    }, 
    "json.tests.test_pass3": {
      "file": "json/tests/test_pass3.py", 
      "imports": [
        "json.tests"
      ]
    }, 
    "json.tests.test_recursion": {
      "file": "json/tests/test_recursion.py", 
      "imports": [
        "json.tests"
      ]
    }, 
    "json.tests.test_scanstring": {
      "file": "json/tests/test_scanstring.py", 
      "imports": [
        "json.tests", 
        "sys"
      ]
    }, 
    "json.tests.test_separators": {
      "file": "json/tests/test_separators.py", 
      "imports": [
        "json.tests", 
        "textwrap"
      ]
    }, 
    "json.tests.test_speedups": {
      "file": "json/tests/test_speedups.py", 
      "imports": [
        "json.tests"
      ]
    }, 
    "json.tests.test_tool": {
      "file": "json/tests/test_tool.py", 
      "imports": [
        "subprocess", 
        "sys", 
        "os", 
        "test.test_support", 
        "test.script_helper", 
        "textwrap", 
        "unittest"
      ]
    }, 
    "json.tests.test_unicode": {
      "file": "json/tests/test_unicode.py", 
      "imports": [
        "collections", 
        "json.tests"
      ]
    }, 
    "json.tool": {
      "file": "json/tool.py", 
      "imports": [
        "json", 
        "sys"
      ]
    }, 
    "keyword": {
      "file": "keyword.py", 
      "imports": [
        "sys", 
        "re"
      ]
    }, 
    "lib2to3": {
      "dir": "lib2to3"
    }, 
    "lib2to3.__init__": {
      "file": "lib2to3/__init__.py", 
      "imports": []
    }, 
    "lib2to3.__main__": {
      "file": "lib2to3/__main__.py", 
      "imports": [
        "lib2to3.main", 
        "sys"
      ]
    }, 
    "lib2to3.btm_matcher": {
      "file": "lib2to3/btm_matcher.py", 
      "imports": [
        "collections", 
        "itertools", 
        "lib2to3.btm_utils", 
        "lib2to3.pygram", 
        "lib2to3.pytree", 
        "logging"
      ]
    }, 
    "lib2to3.btm_utils": {
      "file": "lib2to3/btm_utils.py", 
      "imports": [
        "lib2to3.pgen2.grammar", 
        "lib2to3.pgen2.token", 
        "lib2to3.pygram", 
        "lib2to3.pytree"
      ]
    }, 
    "lib2to3.fixer_base": {
      "file": "lib2to3/fixer_base.py", 
      "imports": [
        "itertools", 
        "lib2to3.fixer_util", 
        "lib2to3.patcomp", 
        "lib2to3.pygram", 
        "logging"
      ]
    }, 
    "lib2to3.fixer_util": {
      "file": "lib2to3/fixer_util.py", 
      "imports": [
        "itertools.islice", 
        "lib2to3.patcomp", 
        "lib2to3.pgen2.token", 
        "lib2to3.pygram", 
        "lib2to3.pytree"
      ]
    }, 
    "lib2to3.fixes": {
      "dir": "lib2to3/fixes"
    }, 
    "lib2to3.fixes.__init__": {
      "file": "lib2to3/fixes/__init__.py", 
      "imports": []
    }, 
    "lib2to3.fixes.fix_apply": {
      "file": "lib2to3/fixes/fix_apply.py", 
      "imports": [
        "lib2to3.fixer_base", 
        "lib2to3.fixer_util", 
        "lib2to3.pgen2.token", 
        "lib2to3.pytree"
      ]
    }, 
    "lib2to3.fixes.fix_asserts": {
      "file": "lib2to3/fixes/fix_asserts.py", 
      "imports": [
        "lib2to3.fixer_base", 
        "lib2to3.fixer_util"
      ]
    }, 
    "lib2to3.fixes.fix_basestring": {
      "file": "lib2to3/fixes/fix_basestring.py", 
      "imports": [
        "lib2to3.fixer_base", 
        "lib2to3.fixer_util"
      ]
    }, 
    "lib2to3.fixes.fix_buffer": {
      "file": "lib2to3/fixes/fix_buffer.py", 
      "imports": [
        "lib2to3.fixer_base", 
        "lib2to3.fixer_util"
      ]
    }, 
    "lib2to3.fixes.fix_callable": {
      "file": "lib2to3/fixes/fix_callable.py", 
      "imports": [
        "lib2to3.fixer_base", 
        "lib2to3.fixer_util"
      ]
    }, 
    "lib2to3.fixes.fix_dict": {
      "file": "lib2to3/fixes/fix_dict.py", 
      "imports": [
        "lib2to3.fixer_base", 
        "lib2to3.fixer_util", 
        "lib2to3.patcomp", 
        "lib2to3.pgen2.token", 
        "lib2to3.pytree"
      ]
    }, 
    "lib2to3.fixes.fix_except": {
      "file": "lib2to3/fixes/fix_except.py", 
      "imports": [
        "lib2to3.fixer_base", 
        "lib2to3.fixer_util", 
        "lib2to3.pgen2.token", 
        "lib2to3.pytree"
      ]
    }, 
    "lib2to3.fixes.fix_exec": {
      "file": "lib2to3/fixes/fix_exec.py", 
      "imports": [
        "lib2to3.fixer_base", 
        "lib2to3.fixer_util", 
        "lib2to3.pytree"
      ]
    }, 
    "lib2to3.fixes.fix_execfile": {
      "file": "lib2to3/fixes/fix_execfile.py", 
      "imports": [
        "lib2to3.fixer_base", 
        "lib2to3.fixer_util"
      ]
    }, 
    "lib2to3.fixes.fix_exitfunc": {
      "file": "lib2to3/fixes/fix_exitfunc.py", 
      "imports": [
        "lib2to3.fixer_base", 
        "lib2to3.fixer_util", 
        "lib2to3.pytree"
      ]
    }, 
    "lib2to3.fixes.fix_filter": {
      "file": "lib2to3/fixes/fix_filter.py", 
      "imports": [
        "lib2to3.fixer_base", 
        "lib2to3.fixer_util", 
        "lib2to3.pgen2.token"
      ]
    }, 
    "lib2to3.fixes.fix_funcattrs": {
      "file": "lib2to3/fixes/fix_funcattrs.py", 
      "imports": [
        "lib2to3.fixer_base", 
        "lib2to3.fixer_util"
      ]
    }, 
    "lib2to3.fixes.fix_future": {
      "file": "lib2to3/fixes/fix_future.py", 
      "imports": [
        "lib2to3.fixer_base", 
        "lib2to3.fixer_util"
      ]
    }, 
    "lib2to3.fixes.fix_getcwdu": {
      "file": "lib2to3/fixes/fix_getcwdu.py", 
      "imports": [
        "lib2to3.fixer_base", 
        "lib2to3.fixer_util"
      ]
    }, 
    "lib2to3.fixes.fix_has_key": {
      "file": "lib2to3/fixes/fix_has_key.py", 
      "imports": [
        "lib2to3.fixer_base", 
        "lib2to3.fixer_util", 
        "lib2to3.pgen2.token", 
        "lib2to3.pytree"
      ]
    }, 
    "lib2to3.fixes.fix_idioms": {
      "file": "lib2to3/fixes/fix_idioms.py", 
      "imports": [
        "lib2to3.fixer_base", 
        "lib2to3.fixer_util"
      ]
    }, 
    "lib2to3.fixes.fix_import": {
      "file": "lib2to3/fixes/fix_import.py", 
      "imports": [
        "lib2to3.fixer_base", 
        "lib2to3.fixer_util", 
        "os"
      ]
    }, 
    "lib2to3.fixes.fix_imports": {
      "file": "lib2to3/fixes/fix_imports.py", 
      "imports": [
        "lib2to3.fixer_base", 
        "lib2to3.fixer_util"
      ]
    }, 
    "lib2to3.fixes.fix_imports2": {
      "file": "lib2to3/fixes/fix_imports2.py", 
      "imports": [
        "lib2to3.fixes.fix_imports"
      ]
    }, 
    "lib2to3.fixes.fix_input": {
      "file": "lib2to3/fixes/fix_input.py", 
      "imports": [
        "lib2to3.fixer_base", 
        "lib2to3.fixer_util", 
        "lib2to3.patcomp"
      ]
    }, 
    "lib2to3.fixes.fix_intern": {
      "file": "lib2to3/fixes/fix_intern.py", 
      "imports": [
        "lib2to3.fixer_base", 
        "lib2to3.fixer_util", 
        "lib2to3.pytree"
      ]
    }, 
    "lib2to3.fixes.fix_isinstance": {
      "file": "lib2to3/fixes/fix_isinstance.py", 
      "imports": [
        "lib2to3.fixer_base", 
        "lib2to3.fixer_util"
      ]
    }, 
    "lib2to3.fixes.fix_itertools": {
      "file": "lib2to3/fixes/fix_itertools.py", 
      "imports": [
        "lib2to3.fixer_base", 
        "lib2to3.fixer_util"
      ]
    }, 
    "lib2to3.fixes.fix_itertools_imports": {
      "file": "lib2to3/fixes/fix_itertools_imports.py", 
      "imports": [
        "lib2to3.fixer_base", 
        "lib2to3.fixer_util"
      ]
    }, 
    "lib2to3.fixes.fix_long": {
      "file": "lib2to3/fixes/fix_long.py", 
      "imports": [
        "lib2to3.fixer_base", 
        "lib2to3.fixer_util"
      ]
    }, 
    "lib2to3.fixes.fix_map": {
      "file": "lib2to3/fixes/fix_map.py", 
      "imports": [
        "lib2to3.fixer_base", 
        "lib2to3.fixer_util", 
        "lib2to3.pgen2.token", 
        "lib2to3.pygram"
      ]
    }, 
    "lib2to3.fixes.fix_metaclass": {
      "file": "lib2to3/fixes/fix_metaclass.py", 
      "imports": [
        "lib2to3.fixer_base", 
        "lib2to3.fixer_util", 
        "lib2to3.pygram"
      ]
    }, 
    "lib2to3.fixes.fix_methodattrs": {
      "file": "lib2to3/fixes/fix_methodattrs.py", 
      "imports": [
        "lib2to3.fixer_base", 
        "lib2to3.fixer_util"
      ]
    }, 
    "lib2to3.fixes.fix_ne": {
      "file": "lib2to3/fixes/fix_ne.py", 
      "imports": [
        "lib2to3.fixer_base", 
        "lib2to3.pgen2.token", 
        "lib2to3.pytree"
      ]
    }, 
    "lib2to3.fixes.fix_next": {
      "file": "lib2to3/fixes/fix_next.py", 
      "imports": [
        "lib2to3.fixer_base", 
        "lib2to3.fixer_util", 
        "lib2to3.pgen2.token", 
        "lib2to3.pygram"
      ]
    }, 
    "lib2to3.fixes.fix_nonzero": {
      "file": "lib2to3/fixes/fix_nonzero.py", 
      "imports": [
        "lib2to3.fixer_base", 
        "lib2to3.fixer_util"
      ]
    }, 
    "lib2to3.fixes.fix_numliterals": {
      "file": "lib2to3/fixes/fix_numliterals.py", 
      "imports": [
        "lib2to3.fixer_base", 
        "lib2to3.fixer_util", 
        "lib2to3.pgen2.token"
      ]
    }, 
    "lib2to3.fixes.fix_operator": {
      "file": "lib2to3/fixes/fix_operator.py", 
      "imports": [
        "lib2to3.fixer_base", 
        "lib2to3.fixer_util"
      ]
    }, 
    "lib2to3.fixes.fix_paren": {
      "file": "lib2to3/fixes/fix_paren.py", 
      "imports": [
        "lib2to3.fixer_base", 
        "lib2to3.fixer_util"
      ]
    }, 
    "lib2to3.fixes.fix_print": {
      "file": "lib2to3/fixes/fix_print.py", 
      "imports": [
        "lib2to3.fixer_base", 
        "lib2to3.fixer_util", 
        "lib2to3.patcomp", 
        "lib2to3.pgen2.token", 
        "lib2to3.pytree"
      ]
    }, 
    "lib2to3.fixes.fix_raise": {
      "file": "lib2to3/fixes/fix_raise.py", 
      "imports": [
        "lib2to3.fixer_base", 
        "lib2to3.fixer_util", 
        "lib2to3.pgen2.token", 
        "lib2to3.pytree"
      ]
    }, 
    "lib2to3.fixes.fix_raw_input": {
      "file": "lib2to3/fixes/fix_raw_input.py", 
      "imports": [
        "lib2to3.fixer_base", 
        "lib2to3.fixer_util"
      ]
    }, 
    "lib2to3.fixes.fix_reduce": {
      "file": "lib2to3/fixes/fix_reduce.py", 
      "imports": [
        "lib2to3.fixer_base", 
        "lib2to3.fixer_util"
      ]
    }, 
    "lib2to3.fixes.fix_renames": {
      "file": "lib2to3/fixes/fix_renames.py", 
      "imports": [
        "lib2to3.fixer_base", 
        "lib2to3.fixer_util"
      ]
    }, 
    "lib2to3.fixes.fix_repr": {
      "file": "lib2to3/fixes/fix_repr.py", 
      "imports": [
        "lib2to3.fixer_base", 
        "lib2to3.fixer_util"
      ]
    }, 
    "lib2to3.fixes.fix_set_literal": {
      "file": "lib2to3/fixes/fix_set_literal.py", 
      "imports": [
        "lib2to3.fixer_base", 
        "lib2to3.fixer_util", 
        "lib2to3.pytree"
      ]
    }, 
    "lib2to3.fixes.fix_standarderror": {
      "file": "lib2to3/fixes/fix_standarderror.py", 
      "imports": [
        "lib2to3.fixer_base", 
        "lib2to3.fixer_util"
      ]
    }, 
    "lib2to3.fixes.fix_sys_exc": {
      "file": "lib2to3/fixes/fix_sys_exc.py", 
      "imports": [
        "lib2to3.fixer_base", 
        "lib2to3.fixer_util"
      ]
    }, 
    "lib2to3.fixes.fix_throw": {
      "file": "lib2to3/fixes/fix_throw.py", 
      "imports": [
        "lib2to3.fixer_base", 
        "lib2to3.fixer_util", 
        "lib2to3.pgen2.token", 
        "lib2to3.pytree"
      ]
    }, 
    "lib2to3.fixes.fix_tuple_params": {
      "file": "lib2to3/fixes/fix_tuple_params.py", 
      "imports": [
        "lib2to3.fixer_base", 
        "lib2to3.fixer_util", 
        "lib2to3.pgen2.token", 
        "lib2to3.pytree"
      ]
    }, 
    "lib2to3.fixes.fix_types": {
      "file": "lib2to3/fixes/fix_types.py", 
      "imports": [
        "lib2to3.fixer_base", 
        "lib2to3.fixer_util", 
        "lib2to3.pgen2.token"
      ]
    }, 
    "lib2to3.fixes.fix_unicode": {
      "file": "lib2to3/fixes/fix_unicode.py", 
      "imports": [
        "lib2to3.fixer_base", 
        "lib2to3.pgen2.token"
      ]
    }, 
    "lib2to3.fixes.fix_urllib": {
      "file": "lib2to3/fixes/fix_urllib.py", 
      "imports": [
        "lib2to3.fixer_base", 
        "lib2to3.fixer_util", 
        "lib2to3.fixes.fix_imports"
      ]
    }, 
    "lib2to3.fixes.fix_ws_comma": {
      "file": "lib2to3/fixes/fix_ws_comma.py", 
      "imports": [
        "lib2to3.fixer_base", 
        "lib2to3.pgen2.token", 
        "lib2to3.pytree"
      ]
    }, 
    "lib2to3.fixes.fix_xrange": {
      "file": "lib2to3/fixes/fix_xrange.py", 
      "imports": [
        "lib2to3.fixer_base", 
        "lib2to3.fixer_util", 
        "lib2to3.patcomp"
      ]
    }, 
    "lib2to3.fixes.fix_xreadlines": {
      "file": "lib2to3/fixes/fix_xreadlines.py", 
      "imports": [
        "lib2to3.fixer_base", 
        "lib2to3.fixer_util"
      ]
    }, 
    "lib2to3.fixes.fix_zip": {
      "file": "lib2to3/fixes/fix_zip.py", 
      "imports": [
        "lib2to3.fixer_base", 
        "lib2to3.fixer_util"
      ]
    }, 
    "lib2to3.main": {
      "file": "lib2to3/main.py", 
      "imports": [
        "__future__", 
        "difflib", 
        "lib2to3.refactor", 
        "sys", 
        "logging", 
        "optparse", 
        "os", 
        "shutil"
      ]
    }, 
    "lib2to3.patcomp": {
      "file": "lib2to3/patcomp.py", 
      "imports": [
        "lib2to3.pgen2.driver", 
        "lib2to3.pgen2.grammar", 
        "lib2to3.pgen2.literals", 
        "lib2to3.pgen2.parse", 
        "lib2to3.pgen2.token", 
        "lib2to3.pgen2.tokenize", 
        "lib2to3.pygram", 
        "lib2to3.pytree", 
        "os", 
        "StringIO"
      ]
    }, 
    "lib2to3.pgen2": {
      "dir": "lib2to3/pgen2"
    }, 
    "lib2to3.pgen2.__init__": {
      "file": "lib2to3/pgen2/__init__.py", 
      "imports": []
    }, 
    "lib2to3.pgen2.conv": {
      "file": "lib2to3/pgen2/conv.py", 
      "imports": [
        "lib2to3.pgen2.grammar", 
        "lib2to3.pgen2.token", 
        "re"
      ]
    }, 
    "lib2to3.pgen2.driver": {
      "file": "lib2to3/pgen2/driver.py", 
      "imports": [
        "codecs", 
        "lib2to3.pgen2.grammar", 
        "lib2to3.pgen2.parse", 
        "lib2to3.pgen2.pgen", 
        "lib2to3.pgen2.token", 
        "lib2to3.pgen2.tokenize", 
        "sys", 
        "logging", 
        "os", 
        "StringIO"
      ]
    }, 
    "lib2to3.pgen2.grammar": {
      "file": "lib2to3/pgen2/grammar.py", 
      "imports": [
        "lib2to3.pgen2.token", 
        "lib2to3.pgen2.tokenize", 
        "pickle", 
        "pprint"
      ]
    }, 
    "lib2to3.pgen2.literals": {
      "file": "lib2to3/pgen2/literals.py", 
      "imports": [
        "re"
      ]
    }, 
    "lib2to3.pgen2.parse": {
      "file": "lib2to3/pgen2/parse.py", 
      "imports": [
        "lib2to3.pgen2.token"
      ]
    }, 
    "lib2to3.pgen2.pgen": {
      "file": "lib2to3/pgen2/pgen.py", 
      "imports": [
        "lib2to3.pgen2.grammar", 
        "lib2to3.pgen2.token", 
        "lib2to3.pgen2.tokenize"
      ]
    }, 
    "lib2to3.pgen2.token": {
      "file": "lib2to3/pgen2/token.py", 
      "imports": []
    }, 
    "lib2to3.pgen2.tokenize": {
      "file": "lib2to3/pgen2/tokenize.py", 
      "imports": [
        "codecs", 
        "lib2to3.pgen2.token", 
        "sys", 
        "re", 
        "string"
      ]
    }, 
    "lib2to3.pygram": {
      "file": "lib2to3/pygram.py", 
      "imports": [
        "lib2to3.pgen2.driver", 
        "lib2to3.pgen2.token", 
        "lib2to3.pytree", 
        "os"
      ]
    }, 
    "lib2to3.pytree": {
      "file": "lib2to3/pytree.py", 
      "imports": [
        "lib2to3.pygram", 
        "sys", 
        "StringIO", 
        "warnings"
      ]
    }, 
    "lib2to3.refactor": {
      "file": "lib2to3/refactor.py", 
      "imports": [
        "__future__", 
        "codecs", 
        "collections", 
        "itertools.chain", 
        "lib2to3.btm_matcher", 
        "lib2to3.btm_utils", 
        "lib2to3.fixer_util", 
        "lib2to3.pgen2.driver", 
        "lib2to3.pgen2.token", 
        "lib2to3.pgen2.tokenize", 
        "lib2to3.pygram", 
        "lib2to3.pytree", 
        "multiprocessing", 
        "operator", 
        "sys", 
        "logging", 
        "os", 
        "StringIO"
      ]
    }, 
    "lib2to3.tests": {
      "dir": "lib2to3/tests"
    }, 
    "lib2to3.tests.__init__": {
      "file": "lib2to3/tests/__init__.py", 
      "imports": [
        "lib2to3.tests.support", 
        "os", 
        "types", 
        "unittest"
      ]
    }, 
    "lib2to3.tests.pytree_idempotency": {
      "file": "lib2to3/tests/pytree_idempotency.py", 
      "imports": [
        "lib2to3.pgen2", 
        "lib2to3.pgen2.driver", 
        "lib2to3.pytree", 
        "lib2to3.tests.support", 
        "sys", 
        "logging", 
        "os"
      ]
    }, 
    "lib2to3.tests.support": {
      "file": "lib2to3/tests/support.py", 
      "imports": [
        "lib2to3.pgen2.driver", 
        "lib2to3.pytree", 
        "lib2to3.refactor", 
        "sys", 
        "os", 
        "re", 
        "textwrap", 
        "unittest"
      ]
    }, 
    "lib2to3.tests.test_all_fixers": {
      "file": "lib2to3/tests/test_all_fixers.py", 
      "imports": [
        "lib2to3.refactor", 
        "lib2to3.tests.support", 
        "unittest"
      ]
    }, 
    "lib2to3.tests.test_fixers": {
      "file": "lib2to3/tests/test_fixers.py", 
      "imports": [
        "itertools.chain", 
        "lib2to3.fixer_util", 
        "lib2to3.fixes.fix_import", 
        "lib2to3.fixes.fix_imports", 
        "lib2to3.fixes.fix_imports2", 
        "lib2to3.fixes.fix_urllib", 
        "lib2to3.pygram", 
        "lib2to3.pytree", 
        "lib2to3.refactor", 
        "lib2to3.tests.support", 
        "operator.itemgetter", 
        "os", 
        "unittest"
      ]
    }, 
    "lib2to3.tests.test_main": {
      "file": "lib2to3/tests/test_main.py", 
      "imports": [
        "codecs", 
        "lib2to3.main", 
        "sys", 
        "logging", 
        "os", 
        "re", 
        "shutil", 
        "StringIO", 
        "tempfile", 
        "unittest"
      ]
    }, 
    "lib2to3.tests.test_parser": {
      "file": "lib2to3/tests/test_parser.py", 
      "imports": [
        "__future__", 
        "lib2to3.pgen2.parse", 
        "lib2to3.pgen2.tokenize", 
        "lib2to3.pygram", 
        "lib2to3.tests.support", 
        "sys", 
        "os"
      ]
    }, 
    "lib2to3.tests.test_pytree": {
      "file": "lib2to3/tests/test_pytree.py", 
      "imports": [
        "__future__", 
        "lib2to3.pytree", 
        "lib2to3.tests.support", 
        "sys", 
        "warnings"
      ]
    }, 
    "lib2to3.tests.test_refactor": {
      "file": "lib2to3/tests/test_refactor.py", 
      "imports": [
        "__future__", 
        "codecs", 
        "lib2to3.fixer_base", 
        "lib2to3.pgen2.token", 
        "lib2to3.pygram", 
        "lib2to3.refactor", 
        "lib2to3.tests.support", 
        "myfixes.fix_explicit.FixExplicit", 
        "myfixes.fix_first.FixFirst", 
        "myfixes.fix_last.FixLast", 
        "myfixes.fix_parrot.FixParrot", 
        "myfixes.fix_preorder.FixPreorder", 
        "operator", 
        "sys", 
        "os", 
        "shutil", 
        "StringIO", 
        "tempfile", 
        "unittest", 
        "warnings"
      ]
    }, 
    "lib2to3.tests.test_util": {
      "file": "lib2to3/tests/test_util.py", 
      "imports": [
        "lib2to3.fixer_util", 
        "lib2to3.pgen2.token", 
        "lib2to3.pytree", 
        "lib2to3.tests.support", 
        "os"
      ]
    }, 
    "linecache": {
      "file": "linecache.py", 
      "imports": [
        "sys", 
        "os"
      ]
    }, 
    "locale": {
      "file": "locale.py", 
      "imports": [
        "_locale", 
        "_locale.*", 
        "encodings", 
        "encodings.aliases", 
        "functools", 
        "operator", 
        "sys", 
        "os", 
        "re"
      ]
    }, 
    "logging": {
      "dir": "logging"
    }, 
    "logging.__init__": {
      "file": "logging/__init__.py", 
      "imports": [
        "atexit", 
        "cStringIO", 
        "codecs", 
        "collections", 
        "sys", 
        "thread", 
        "threading", 
        "time", 
        "os", 
        "traceback", 
        "warnings", 
        "weakref"
      ]
    }, 
    "logging.config": {
      "file": "logging/config.py", 
      "imports": [
        "ConfigParser", 
        "cStringIO", 
        "errno", 
        "io", 
        "json", 
        "logging", 
        "logging.handlers", 
        "select", 
        "sys", 
        "thread", 
        "threading", 
        "os", 
        "re", 
        "socket", 
        "SocketServer", 
        "struct", 
        "tempfile", 
        "traceback", 
        "types"
      ]
    }, 
    "logging.handlers": {
      "file": "logging/handlers.py", 
      "imports": [
        "codecs", 
        "email.utils", 
        "errno", 
        "httplib", 
        "logging", 
        "time", 
        "win32evtlog", 
        "win32evtlogutil", 
        "os", 
        "re", 
        "smtplib", 
        "socket", 
        "stat", 
        "struct", 
        "urllib", 
        "cPickle"
      ]
    }, 
    "macurl2path": {
      "file": "macurl2path.py", 
      "imports": [
        "os", 
        "urllib"
      ]
    }, 
    "mailbox": {
      "file": "mailbox.py", 
      "imports": [
        "calendar", 
        "copy", 
        "email", 
        "email.generator", 
        "email.message", 
        "errno", 
        "fcntl", 
        "sys", 
        "time", 
        "os", 
        "re", 
        "rfc822", 
        "socket", 
        "StringIO", 
        "warnings"
      ]
    }, 
    "mailcap": {
      "file": "mailcap.py", 
      "imports": [
        "sys", 
        "os"
      ]
    }, 
    "markupbase": {
      "file": "markupbase.py", 
      "imports": [
        "re"
      ]
    }, 
    "marshal": {
      "file": "marshal.py", 
      "imports": [
        "_marshal"
      ]
    }, 
    "md5": {
      "file": "md5.py", 
      "imports": [
        "hashlib", 
        "warnings"
      ]
    }, 
    "mhlib": {
      "file": "mhlib.py", 
      "imports": [
        "bisect", 
        "cStringIO.StringIO", 
        "sys", 
        "mimetools", 
        "multifile", 
        "os", 
        "re", 
        "shutil", 
        "StringIO", 
        "warnings"
      ]
    }, 
    "mimetools": {
      "file": "mimetools.py", 
      "imports": [
        "base64", 
        "dummy_thread", 
        "sys", 
        "thread", 
        "time", 
        "os", 
        "quopri", 
        "rfc822", 
        "socket", 
        "tempfile", 
        "uu", 
        "warnings"
      ]
    }, 
    "mimetypes": {
      "file": "mimetypes.py", 
      "imports": [
        "_winreg", 
        "getopt", 
        "sys", 
        "os", 
        "posixpath", 
        "urllib"
      ]
    }, 
    "mimify": {
      "file": "mimify.py", 
      "imports": [
        "base64", 
        "getopt", 
        "sys", 
        "os", 
        "re", 
        "warnings"
      ]
    }, 
    "modulefinder": {
      "file": "modulefinder.py", 
      "imports": [
        "__future__", 
        "dis", 
        "getopt", 
        "imp", 
        "marshal", 
        "sys", 
        "os", 
        "struct", 
        "types"
      ]
    }, 
    "multifile": {
      "file": "multifile.py", 
      "imports": [
        "warnings"
      ]
    }, 
    "mutex": {
      "file": "mutex.py", 
      "imports": [
        "collections", 
        "warnings"
      ]
    }, 
    "netrc": {
      "file": "netrc.py", 
      "imports": [
        "os", 
        "shlex", 
        "stat", 
        "pwd"
      ]
    }, 
    "new": {
      "file": "new.py", 
      "imports": [
        "types", 
        "warnings"
      ]
    }, 
    "nntplib": {
      "file": "nntplib.py", 
      "imports": [
        "netrc", 
        "os", 
        "re", 
        "socket"
      ]
    }, 
    "nturl2path": {
      "file": "nturl2path.py", 
      "imports": [
        "string", 
        "urllib"
      ]
    }, 
    "numbers": {
      "file": "numbers.py", 
      "imports": [
        "__future__", 
        "abc"
      ]
    }, 
    "opcode": {
      "file": "opcode.py", 
      "imports": []
    }, 
    "optparse": {
      "file": "optparse.py", 
      "imports": [
        "__builtin__", 
        "gettext", 
        "sys", 
        "os", 
        "textwrap", 
        "types"
      ]
    }, 
    "os": {
      "file": "os.py", 
      "imports": [
        "_emx_link.link", 
        "ce", 
        "ce.*", 
        "ce._exit", 
        "copy_reg", 
        "errno", 
        "nt", 
        "nt.*", 
        "nt._exit", 
        "ntpath", 
        "os", 
        "os2", 
        "os2.*", 
        "os2._exit", 
        "os2emxpath", 
        "posix", 
        "posix.*", 
        "posix._exit", 
        "riscos", 
        "riscos.*", 
        "riscos._exit", 
        "riscosenviron._Environ", 
        "riscospath", 
        "subprocess", 
        "sys", 
        "posixpath", 
        "UserDict", 
        "warnings"
      ]
    }, 
    "pdb": {
      "file": "pdb.py", 
      "imports": [
        "__main__", 
        "bdb", 
        "cmd", 
        "linecache", 
        "os", 
        "pdb", 
        "readline", 
        "sys", 
        "pprint", 
        "re", 
        "repr", 
        "shlex", 
        "traceback"
      ]
    }, 
    "pickle": {
      "file": "pickle.py", 
      "imports": [
        "binascii", 
        "cStringIO.StringIO", 
        "copy_reg", 
        "doctest", 
        "marshal", 
        "org.python.core.PyStringMap", 
        "sys", 
        "re", 
        "StringIO", 
        "struct", 
        "types"
      ]
    }, 
    "pickletools": {
      "file": "pickletools.py", 
      "imports": [
        "cStringIO", 
        "doctest", 
        "pickle", 
        "re", 
        "struct"
      ]
    }, 
    "pipes": {
      "file": "pipes.py", 
      "imports": [
        "os", 
        "re", 
        "string", 
        "tempfile"
      ]
    }, 
    "pkgutil": {
      "file": "pkgutil.py", 
      "imports": [
        "imp", 
        "inspect", 
        "marshal", 
        "os", 
        "sys", 
        "zipimport", 
        "zipimport.zipimporter", 
        "types"
      ]
    }, 
    "platform": {
      "file": "platform.py", 
      "imports": [
        "MacOS", 
        "_winreg", 
        "gestalt", 
        "gestalt.gestalt", 
        "java.lang", 
        "java.lang.System", 
        "os", 
        "subprocess", 
        "sys", 
        "vms_lib", 
        "win32api", 
        "win32api.GetVersionEx", 
        "win32api.RegCloseKey", 
        "win32api.RegOpenKeyEx", 
        "win32api.RegQueryValueEx", 
        "win32con.HKEY_LOCAL_MACHINE", 
        "win32con.VER_NT_WORKSTATION", 
        "win32con.VER_PLATFORM_WIN32_NT", 
        "win32con.VER_PLATFORM_WIN32_WINDOWS", 
        "win32pipe", 
        "plistlib", 
        "re", 
        "socket", 
        "string", 
        "struct", 
        "tempfile"
      ]
    }, 
    "plistlib": {
      "file": "plistlib.py", 
      "imports": [
        "Carbon.File.FSGetResourceForkName", 
        "Carbon.File.FSRef", 
        "Carbon.Files.fsRdPerm", 
        "Carbon.Files.fsRdWrPerm", 
        "Carbon.Res", 
        "binascii", 
        "cStringIO.StringIO", 
        "re", 
        "warnings", 
        "xml.parsers.expat", 
        "datetime"
      ]
    }, 
    "popen2": {
      "file": "popen2.py", 
      "imports": [
        "os", 
        "sys", 
        "warnings"
      ]
    }, 
    "poplib": {
      "file": "poplib.py", 
      "imports": [
        "hashlib", 
        "ssl", 
        "sys", 
        "re", 
        "socket"
      ]
    }, 
    "posixfile": {
      "file": "posixfile.py", 
      "imports": [
        "__builtin__", 
        "fcntl", 
        "os", 
        "posix", 
        "sys", 
        "struct", 
        "types", 
        "warnings"
      ]
    }, 
    "posixpath": {
      "file": "posixpath.py", 
      "imports": [
        "genericpath", 
        "os", 
        "sys", 
        "re", 
        "stat", 
        "warnings", 
        "pwd"
      ]
    }, 
    "pprint": {
      "file": "pprint.py", 
      "imports": [
        "cStringIO.StringIO", 
        "sys", 
        "time", 
        "StringIO", 
        "warnings"
      ]
    }, 
    "profile": {
      "file": "profile.py", 
      "imports": [
        "__main__", 
        "marshal", 
        "optparse", 
        "os", 
        "sys", 
        "time", 
        "pstats", 
        "resource"
      ]
    }, 
    "promise": {
      "dir": "promise"
    }, 
    "promise.__init__": {
      "file": "promise/__init__.py", 
      "imports": [
        "promise.promise"
      ]
    }, 
    "promise.compat": {
      "file": "promise/compat.py", 
      "imports": [
        "asyncio.Future", 
        "asyncio.ensure_future", 
        "asyncio.iscoroutine", 
        "promise.iterate_promise"
      ]
    }, 
    "promise.iterate_promise": {
      "file": "promise/iterate_promise.py", 
      "imports": []
    }, 
    "promise.promise": {
      "file": "promise/promise.py", 
      "imports": [
        "promise.compat", 
        "threading"
      ]
    }, 
    "pstats": {
      "file": "pstats.py", 
      "imports": [
        "cmd", 
        "functools", 
        "marshal", 
        "os", 
        "readline", 
        "sys", 
        "time", 
        "re"
      ]
    }, 
    "pty": {
      "file": "pty.py", 
      "imports": [
        "fcntl.I_PUSH", 
        "fcntl.ioctl", 
        "os", 
        "select.select", 
        "sgi", 
        "tty"
      ]
    }, 
    "pwd": {
      "file": "pwd.py", 
      "imports": [
        "__pypy__.builtinify", 
        "_pwdgrp_cffi.ffi", 
        "_pwdgrp_cffi.lib", 
        "_structseq", 
        "os"
      ]
    }, 
    "py_compile": {
      "file": "py_compile.py", 
      "imports": [
        "__builtin__", 
        "imp", 
        "marshal", 
        "os", 
        "sys", 
        "traceback"
      ]
    }, 
    "pyclbr": {
      "file": "pyclbr.py", 
      "imports": [
        "imp", 
        "operator.itemgetter", 
        "os", 
        "sys", 
        "token.DEDENT", 
        "token.NAME", 
        "token.OP", 
        "tokenize"
      ]
    }, 
    "pydoc": {
      "file": "pydoc.py", 
      "imports": [
        "BaseHTTPServer", 
        "Tkinter", 
        "__builtin__", 
        "collections", 
        "formatter", 
        "getopt", 
        "imp", 
        "inspect", 
        "locale", 
        "mimetools", 
        "nturl2path", 
        "os", 
        "pkgutil", 
        "select", 
        "sys", 
        "threading", 
        "pydoc_data.topics", 
        "re", 
        "repr", 
        "string", 
        "StringIO", 
        "tempfile", 
        "traceback", 
        "tty", 
        "types", 
        "warnings", 
        "webbrowser"
      ]
    }, 
    "pydoc_data": {
      "dir": "pydoc_data"
    }, 
    "pydoc_data.__init__": {
      "file": "pydoc_data/__init__.py", 
      "imports": []
    }, 
    "pydoc_data.topics": {
      "file": "pydoc_data/topics.py", 
      "imports": []
    }, 
    "pyrepl": {
      "dir": "pyrepl"
    }, 
    "pyrepl.__init__": {
      "file": "pyrepl/__init__.py", 
      "imports": []
    }, 
    "pyrepl.cmdrepl": {
      "file": "pyrepl/cmdrepl.py", 
      "imports": [
        "__future__", 
        "cmd", 
        "pyrepl.completer", 
        "pyrepl.completing_reader", 
        "pyrepl.reader"
      ]
    }, 
    "pyrepl.commands": {
      "file": "pyrepl/commands.py", 
      "imports": [
        "os", 
        "pyrepl.input", 
        "signal", 
        "sys"
      ]
    }, 
    "pyrepl.completer": {
      "file": "pyrepl/completer.py", 
      "imports": [
        "__builtin__", 
        "keyword", 
        "re"
      ]
    }, 
    "pyrepl.completing_reader": {
      "file": "pyrepl/completing_reader.py", 
      "imports": [
        "pyrepl.commands", 
        "pyrepl.reader", 
        "re"
      ]
    }, 
    "pyrepl.console": {
      "file": "pyrepl/console.py", 
      "imports": []
    }, 
    "pyrepl.copy_code": {
      "file": "pyrepl/copy_code.py", 
      "imports": [
        "new"
      ]
    }, 
    "pyrepl.curses": {
      "file": "pyrepl/curses.py", 
      "imports": [
        "_curses", 
        "_minimal_curses", 
        "pyrepl.curses", 
        "sys"
      ]
    }, 
    "pyrepl.fancy_termios": {
      "file": "pyrepl/fancy_termios.py", 
      "imports": [
        "termios"
      ]
    }, 
    "pyrepl.historical_reader": {
      "file": "pyrepl/historical_reader.py", 
      "imports": [
        "pyrepl.commands", 
        "pyrepl.input", 
        "pyrepl.reader", 
        "pyrepl.unix_console"
      ]
    }, 
    "pyrepl.input": {
      "file": "pyrepl/input.py", 
      "imports": [
        "pyrepl.keymap", 
        "pyrepl.unicodedata_"
      ]
    }, 
    "pyrepl.keymap": {
      "file": "pyrepl/keymap.py", 
      "imports": []
    }, 
    "pyrepl.keymaps": {
      "file": "pyrepl/keymaps.py", 
      "imports": []
    }, 
    "pyrepl.module_lister": {
      "file": "pyrepl/module_lister.py", 
      "imports": [
        "imp", 
        "os", 
        "sys"
      ]
    }, 
    "pyrepl.pygame_console": {
      "file": "pyrepl/pygame_console.py", 
      "imports": [
        "pygame", 
        "pygame.locals.*", 
        "pyrepl.console", 
        "pyrepl.pygame_keymap", 
        "pyrepl.reader", 
        "types"
      ]
    }, 
    "pyrepl.pygame_keymap": {
      "file": "pyrepl/pygame_keymap.py", 
      "imports": [
        "pygame.locals.*"
      ]
    }, 
    "pyrepl.python_reader": {
      "file": "pyrepl/python_reader.py", 
      "imports": [
        "_tkinter", 
        "atexit", 
        "cPickle", 
        "cocoasupport.CocoaInteracter", 
        "code", 
        "imp", 
        "locale", 
        "new", 
        "os", 
        "pickle", 
        "pyrepl.commands", 
        "pyrepl.completer", 
        "pyrepl.completing_reader", 
        "pyrepl.copy_code", 
        "pyrepl.historical_reader", 
        "pyrepl.module_lister", 
        "pyrepl.pygame_console", 
        "pyrepl.reader", 
        "pyrepl.unix_console", 
        "re", 
        "signal", 
        "sys", 
        "traceback", 
        "twisted.internet.abstract.FileDescriptor", 
        "twisted.internet.reactor", 
        "warnings"
      ]
    }, 
    "pyrepl.reader": {
      "file": "pyrepl/reader.py", 
      "imports": [
        "_pyrepl_utils.disp_str", 
        "_pyrepl_utils.init_unctrl_map", 
        "pyrepl.commands", 
        "pyrepl.input", 
        "pyrepl.unicodedata_", 
        "pyrepl.unix_console", 
        "re", 
        "types"
      ]
    }, 
    "pyrepl.readline": {
      "file": "pyrepl/readline.py", 
      "imports": [
        "__builtin__", 
        "os", 
        "pyrepl.commands", 
        "pyrepl.completing_reader", 
        "pyrepl.historical_reader", 
        "pyrepl.unix_console", 
        "sys", 
        "warnings"
      ]
    }, 
    "pyrepl.simple_interact": {
      "file": "pyrepl/simple_interact.py", 
      "imports": [
        "__main__", 
        "code", 
        "pyrepl.readline", 
        "sys"
      ]
    }, 
    "pyrepl.unicodedata_": {
      "file": "pyrepl/unicodedata_.py", 
      "imports": [
        "unicodedata.*"
      ]
    }, 
    "pyrepl.unix_console": {
      "file": "pyrepl/unix_console.py", 
      "imports": [
        "errno", 
        "fcntl.ioctl", 
        "os", 
        "pyrepl.console", 
        "pyrepl.curses", 
        "pyrepl.fancy_termios", 
        "pyrepl.unix_eventqueue", 
        "re", 
        "select", 
        "signal", 
        "struct", 
        "sys", 
        "termios", 
        "time"
      ]
    }, 
    "pyrepl.unix_eventqueue": {
      "file": "pyrepl/unix_eventqueue.py", 
      "imports": [
        "os", 
        "pyrepl.console", 
        "pyrepl.curses", 
        "pyrepl.keymap", 
        "termios.VERASE", 
        "termios.tcgetattr"
      ]
    }, 
    "quopri": {
      "file": "quopri.py", 
      "imports": [
        "binascii.a2b_qp", 
        "binascii.b2a_qp", 
        "cStringIO.StringIO", 
        "getopt", 
        "sys"
      ]
    }, 
    "random": {
      "file": "random.py", 
      "imports": [
        "__future__", 
        "_random", 
        "binascii.hexlify", 
        "hashlib", 
        "math.acos", 
        "math.ceil", 
        "math.cos", 
        "math.e", 
        "math.exp", 
        "math.log", 
        "math.pi", 
        "math.sin", 
        "math.sqrt", 
        "os", 
        "time", 
        "warnings"
      ]
    }, 
    "re": {
      "file": "re.py", 
      "imports": [
        "_locale", 
        "copy_reg", 
        "sys", 
        "sre_compile", 
        "sre_constants", 
        "sre_parse"
      ]
    }, 
    "repr": {
      "file": "repr.py", 
      "imports": [
        "__builtin__", 
        "itertools.islice"
      ]
    }, 
    "resource": {
      "file": "resource.py", 
      "imports": [
        "__pypy__.builtinify", 
        "_structseq", 
        "ctypes.POINTER", 
        "ctypes.Structure", 
        "ctypes.byref", 
        "ctypes.c_int", 
        "ctypes.c_long", 
        "ctypes_config_cache._resource_cache", 
        "ctypes_support.get_errno", 
        "ctypes_support.standard_c_lib", 
        "errno.EINVAL", 
        "errno.EPERM", 
        "os", 
        "sys"
      ]
    }, 
    "rexec": {
      "file": "rexec.py", 
      "imports": [
        "__builtin__", 
        "code", 
        "getopt", 
        "ihooks", 
        "imp", 
        "os", 
        "readline", 
        "sys", 
        "traceback", 
        "warnings"
      ]
    }, 
    "rfc822": {
      "file": "rfc822.py", 
      "imports": [
        "os", 
        "sys", 
        "time", 
        "warnings"
      ]
    }, 
    "rlcompleter": {
      "file": "rlcompleter.py", 
      "imports": [
        "__builtin__", 
        "__main__", 
        "keyword", 
        "re", 
        "readline"
      ]
    }, 
    "robotparser": {
      "file": "robotparser.py", 
      "imports": [
        "time", 
        "urllib", 
        "urlparse"
      ]
    }, 
    "runpy": {
      "file": "runpy.py", 
      "imports": [
        "imp", 
        "imp.get_loader", 
        "pkgutil", 
        "sys"
      ]
    }, 
    "sched": {
      "file": "sched.py", 
      "imports": [
        "collections", 
        "heapq"
      ]
    }, 
    "sets": {
      "file": "sets.py", 
      "imports": [
        "copy", 
        "itertools.ifilter", 
        "itertools.ifilterfalse", 
        "warnings"
      ]
    }, 
    "sgmllib": {
      "file": "sgmllib.py", 
      "imports": [
        "markupbase", 
        "re", 
        "sys", 
        "warnings"
      ]
    }, 
    "sha": {
      "file": "sha.py", 
      "imports": [
        "hashlib", 
        "warnings"
      ]
    }, 
    "shelve": {
      "file": "shelve.py", 
      "imports": [
        "anydbm", 
        "cStringIO.StringIO", 
        "pickle", 
        "StringIO", 
        "UserDict", 
        "cPickle"
      ]
    }, 
    "shlex": {
      "file": "shlex.py", 
      "imports": [
        "cStringIO.StringIO", 
        "collections", 
        "os", 
        "sys", 
        "StringIO"
      ]
    }, 
    "shutil": {
      "file": "shutil.py", 
      "imports": [
        "collections", 
        "distutils.errors", 
        "distutils.spawn", 
        "errno", 
        "fnmatch", 
        "os", 
        "sys", 
        "stat", 
        "tarfile", 
        "zipfile", 
        "grp", 
        "pwd"
      ]
    }, 
    "site": {
      "file": "site.py", 
      "imports": [
        "__builtin__", 
        "codecs", 
        "distutils.sysconfig", 
        "encodings", 
        "exceptions", 
        "locale", 
        "os", 
        "pydoc", 
        "sitecustomize", 
        "sys", 
        "usercustomize", 
        "zipimport", 
        "sysconfig", 
        "textwrap", 
        "traceback"
      ]
    }, 
    "six": {
      "file": "six.py", 
      "imports": [
        "StringIO", 
        "__future__", 
        "functools", 
        "io", 
        "itertools", 
        "operator", 
        "struct", 
        "sys", 
        "types"
      ]
    }, 
    "smtpd": {
      "file": "smtpd.py", 
      "imports": [
        "Mailman.MailList", 
        "Mailman.Message", 
        "Mailman.Utils", 
        "__main__", 
        "asynchat", 
        "asyncore", 
        "cStringIO.StringIO", 
        "errno", 
        "getopt", 
        "os", 
        "sys", 
        "time", 
        "smtplib", 
        "socket", 
        "pwd"
      ]
    }, 
    "smtplib": {
      "file": "smtplib.py", 
      "imports": [
        "base64", 
        "email.base64mime", 
        "email.utils", 
        "hmac", 
        "re", 
        "ssl", 
        "sys", 
        "sys.stderr", 
        "socket"
      ]
    }, 
    "sndhdr": {
      "file": "sndhdr.py", 
      "imports": [
        "aifc", 
        "glob", 
        "os", 
        "sys"
      ]
    }, 
    "socket": {
      "file": "socket.py", 
      "imports": [
        "_socket", 
        "_socket.*", 
        "_ssl", 
        "_ssl.RAND_add", 
        "_ssl.RAND_egd", 
        "_ssl.RAND_status", 
        "_ssl.SSLError", 
        "_ssl.SSL_ERROR_EOF", 
        "_ssl.SSL_ERROR_INVALID_ERROR_CODE", 
        "_ssl.SSL_ERROR_SSL", 
        "_ssl.SSL_ERROR_SYSCALL", 
        "_ssl.SSL_ERROR_WANT_CONNECT", 
        "_ssl.SSL_ERROR_WANT_READ", 
        "_ssl.SSL_ERROR_WANT_WRITE", 
        "_ssl.SSL_ERROR_WANT_X509_LOOKUP", 
        "_ssl.SSL_ERROR_ZERO_RETURN", 
        "cStringIO.StringIO", 
        "errno", 
        "os", 
        "ssl", 
        "sys", 
        "StringIO", 
        "warnings"
      ]
    }, 
    "sre": {
      "file": "sre.py", 
      "imports": [
        "re", 
        "warnings"
      ]
    }, 
    "sre_compile": {
      "file": "sre_compile.py", 
      "imports": [
        "_sre", 
        "array", 
        "sys", 
        "sre_constants", 
        "sre_parse"
      ]
    }, 
    "sre_constants": {
      "file": "sre_constants.py", 
      "imports": [
        "_sre", 
        "_sre.MAXREPEAT"
      ]
    }, 
    "sre_parse": {
      "file": "sre_parse.py", 
      "imports": [
        "__pypy__.newdict", 
        "sre_constants", 
        "sys"
      ]
    }, 
    "stackless": {
      "file": "stackless.py", 
      "imports": [
        "_continuation", 
        "collections", 
        "operator", 
        "threading.local"
      ]
    }, 
    "stat": {
      "file": "stat.py", 
      "imports": []
    }, 
    "statvfs": {
      "file": "statvfs.py", 
      "imports": [
        "warnings"
      ]
    }, 
    "string": {
      "file": "string.py", 
      "imports": [
        "re", 
        "strop.lowercase", 
        "strop.maketrans", 
        "strop.uppercase", 
        "strop.whitespace"
      ]
    }, 
    "stringold": {
      "file": "stringold.py", 
      "imports": [
        "stringold", 
        "strop.lowercase", 
        "strop.maketrans", 
        "strop.uppercase", 
        "strop.whitespace", 
        "warnings"
      ]
    }, 
    "stringprep": {
      "file": "stringprep.py", 
      "imports": [
        "unicodedata.ucd_3_2_0"
      ]
    }, 
    "struct": {
      "file": "struct.py", 
      "imports": [
        "_struct.*", 
        "_struct.__doc__", 
        "_struct._clearcache"
      ]
    }, 
    "subprocess": {
      "file": "subprocess.py", 
      "imports": []
    }, 
    "symbol": {
      "file": "symbol.py", 
      "imports": [
        "sys", 
        "token"
      ]
    }, 
    "sysconfig": {
      "file": "sysconfig.py", 
      "imports": [
        "_osx_support", 
        "imp", 
        "os", 
        "pprint", 
        "re", 
        "sys"
      ]
    }, 
    "syslog": {
      "file": "syslog.py", 
      "imports": [
        "__pypy__.builtinify", 
        "_syslog_cffi.ffi", 
        "_syslog_cffi.lib", 
        "sys"
      ]
    }, 
    "tabnanny": {
      "file": "tabnanny.py", 
      "imports": [
        "getopt", 
        "os", 
        "sys", 
        "tokenize"
      ]
    }, 
    "tarfile": {
      "file": "tarfile.py", 
      "imports": [
        "StringIO", 
        "bz2", 
        "cStringIO.StringIO", 
        "calendar", 
        "copy", 
        "errno", 
        "gzip", 
        "operator", 
        "os", 
        "re", 
        "shutil", 
        "stat", 
        "struct", 
        "sys", 
        "time", 
        "zlib", 
        "warnings", 
        "grp", 
        "pwd"
      ]
    }, 
    "telnetlib": {
      "file": "telnetlib.py", 
      "imports": [
        "errno", 
        "re", 
        "select", 
        "socket", 
        "sys", 
        "thread", 
        "time.time"
      ]
    }, 
    "tempfile": {
      "file": "tempfile.py", 
      "imports": [
        "StringIO", 
        "cStringIO.StringIO", 
        "dummy_thread", 
        "errno", 
        "fcntl", 
        "io", 
        "os", 
        "random", 
        "thread"
      ]
    }, 
    "test": {
      "dir": "test"
    }, 
    "test.__init__": {
      "file": "test/__init__.py", 
      "imports": []
    }, 
    "test._mock_backport": {
      "file": "test/_mock_backport.py", 
      "imports": [
        "__builtin__", 
        "_io", 
        "functools", 
        "inspect", 
        "java", 
        "pprint", 
        "sys", 
        "types"
      ]
    }, 
    "test.audiotests": {
      "file": "test/audiotests.py", 
      "imports": [
        "array", 
        "base64", 
        "io", 
        "pickle", 
        "sys", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.autotest": {
      "file": "test/autotest.py", 
      "imports": [
        "test.regrtest"
      ]
    }, 
    "test.bad_coding": {
      "file": "test/bad_coding.py", 
      "imports": []
    }, 
    "test.bad_coding2": {
      "file": "test/bad_coding2.py", 
      "imports": []
    }, 
    "test.bad_coding3": {
      "file": "test/bad_coding3.py", 
      "imports": []
    }, 
    "test.badsyntax_future3": {
      "file": "test/badsyntax_future3.py", 
      "imports": [
        "__future__"
      ]
    }, 
    "test.badsyntax_future4": {
      "file": "test/badsyntax_future4.py", 
      "imports": [
        "__future__"
      ]
    }, 
    "test.badsyntax_future5": {
      "file": "test/badsyntax_future5.py", 
      "imports": [
        "__future__", 
        "foo"
      ]
    }, 
    "test.badsyntax_future6": {
      "file": "test/badsyntax_future6.py", 
      "imports": [
        "__future__"
      ]
    }, 
    "test.badsyntax_future7": {
      "file": "test/badsyntax_future7.py", 
      "imports": [
        "__future__", 
        "string"
      ]
    }, 
    "test.badsyntax_future8": {
      "file": "test/badsyntax_future8.py", 
      "imports": [
        "__future__"
      ]
    }, 
    "test.badsyntax_future9": {
      "file": "test/badsyntax_future9.py", 
      "imports": [
        "__future__"
      ]
    }, 
    "test.badsyntax_nocaret": {
      "file": "test/badsyntax_nocaret.py", 
      "imports": []
    }, 
    "test.buffer_tests": {
      "file": "test/buffer_tests.py", 
      "imports": [
        "struct", 
        "sys"
      ]
    }, 
    "test.curses_tests": {
      "file": "test/curses_tests.py", 
      "imports": [
        "curses", 
        "curses.textpad"
      ]
    }, 
    "test.doctest_aliases": {
      "file": "test/doctest_aliases.py", 
      "imports": []
    }, 
    "test.double_const": {
      "file": "test/double_const.py", 
      "imports": [
        "test.test_support"
      ]
    }, 
    "test.fork_wait": {
      "file": "test/fork_wait.py", 
      "imports": [
        "os", 
        "sys", 
        "test.test_support", 
        "time", 
        "unittest"
      ]
    }, 
    "test.gdb_sample": {
      "file": "test/gdb_sample.py", 
      "imports": []
    }, 
    "test.infinite_reload": {
      "file": "test/infinite_reload.py", 
      "imports": [
        "imp", 
        "test.infinite_reload"
      ]
    }, 
    "test.inspect_fodder": {
      "file": "test/inspect_fodder.py", 
      "imports": [
        "inspect", 
        "sys"
      ]
    }, 
    "test.inspect_fodder2": {
      "file": "test/inspect_fodder2.py", 
      "imports": []
    }, 
    "test.leakers": {
      "dir": "test/leakers"
    }, 
    "test.leakers.__init__": {
      "file": "test/leakers/__init__.py", 
      "imports": []
    }, 
    "test.leakers.test_ctypes": {
      "file": "test/leakers/test_ctypes.py", 
      "imports": [
        "ctypes.POINTER", 
        "ctypes.Structure", 
        "ctypes.c_int", 
        "gc"
      ]
    }, 
    "test.leakers.test_dictself": {
      "file": "test/leakers/test_dictself.py", 
      "imports": [
        "gc"
      ]
    }, 
    "test.leakers.test_gestalt": {
      "file": "test/leakers/test_gestalt.py", 
      "imports": [
        "MacOS", 
        "gestalt.gestalt", 
        "sys"
      ]
    }, 
    "test.leakers.test_selftype": {
      "file": "test/leakers/test_selftype.py", 
      "imports": [
        "gc"
      ]
    }, 
    "test.list_tests": {
      "file": "test/list_tests.py", 
      "imports": [
        "os", 
        "sys", 
        "test.seq_tests", 
        "test.test_support"
      ]
    }, 
    "test.lock_tests": {
      "file": "test/lock_tests.py", 
      "imports": [
        "sys", 
        "test.test_support", 
        "thread.get_ident", 
        "thread.start_new_thread", 
        "threading", 
        "time", 
        "unittest"
      ]
    }, 
    "test.make_ssl_certs": {
      "file": "test/make_ssl_certs.py", 
      "imports": [
        "os", 
        "shutil", 
        "subprocess.*", 
        "sys", 
        "tempfile"
      ]
    }, 
    "test.mapping_tests": {
      "file": "test/mapping_tests.py", 
      "imports": [
        "test.test_support", 
        "unittest", 
        "UserDict"
      ]
    }, 
    "test.mp_fork_bomb": {
      "file": "test/mp_fork_bomb.py", 
      "imports": [
        "multiprocessing"
      ]
    }, 
    "test.outstanding_bugs": {
      "file": "test/outstanding_bugs.py", 
      "imports": [
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.pickletester": {
      "file": "test/pickletester.py", 
      "imports": [
        "StringIO", 
        "__main__", 
        "cStringIO", 
        "copy_reg", 
        "locale", 
        "os", 
        "pickle", 
        "pickletools", 
        "sys", 
        "test.test_support", 
        "time", 
        "unittest", 
        "cPickle"
      ]
    }, 
    "test.profilee": {
      "file": "test/profilee.py", 
      "imports": [
        "sys"
      ]
    }, 
    "test.pyclbr_input": {
      "file": "test/pyclbr_input.py", 
      "imports": []
    }, 
    "test.pydoc_mod": {
      "file": "test/pydoc_mod.py", 
      "imports": []
    }, 
    "test.pydocfodder": {
      "file": "test/pydocfodder.py", 
      "imports": [
        "types"
      ]
    }, 
    "test.pystone": {
      "file": "test/pystone.py", 
      "imports": [
        "sys", 
        "time.time"
      ]
    }, 
    "test.re_tests": {
      "file": "test/re_tests.py", 
      "imports": []
    }, 
    "test.regrtest": {
      "file": "test/regrtest.py", 
      "imports": [
        "Queue", 
        "StringIO", 
        "_abcoll", 
        "_pyio", 
        "_strptime", 
        "copy_reg", 
        "ctypes", 
        "distutils.dir_util", 
        "doctest", 
        "filecmp", 
        "gc", 
        "getopt", 
        "imp", 
        "json", 
        "linecache", 
        "mimetypes", 
        "os", 
        "platform", 
        "random", 
        "re", 
        "shutil", 
        "stat", 
        "struct", 
        "subprocess.PIPE", 
        "subprocess.Popen", 
        "sys", 
        "sysconfig", 
        "tempfile", 
        "test.test_support", 
        "test.test_timeout", 
        "threading.Thread", 
        "time", 
        "zipimport", 
        "textwrap", 
        "trace", 
        "traceback", 
        "unittest", 
        "urllib", 
        "urllib2", 
        "urlparse", 
        "warnings", 
        "resource"
      ]
    }, 
    "test.relimport": {
      "file": "test/relimport.py", 
      "imports": [
        "test.test_import"
      ]
    }, 
    "test.reperf": {
      "file": "test/reperf.py", 
      "imports": [
        "re", 
        "time"
      ]
    }, 
    "test.sample_doctest": {
      "file": "test/sample_doctest.py", 
      "imports": [
        "doctest"
      ]
    }, 
    "test.sample_doctest_no_docstrings": {
      "file": "test/sample_doctest_no_docstrings.py", 
      "imports": []
    }, 
    "test.sample_doctest_no_doctests": {
      "file": "test/sample_doctest_no_doctests.py", 
      "imports": []
    }, 
    "test.script_helper": {
      "file": "test/script_helper.py", 
      "imports": [
        "contextlib", 
        "os", 
        "py_compile", 
        "re", 
        "shutil", 
        "subprocess", 
        "sys", 
        "tempfile", 
        "test.test_support", 
        "zipfile"
      ]
    }, 
    "test.seq_tests": {
      "file": "test/seq_tests.py", 
      "imports": [
        "itertools.chain", 
        "itertools.imap", 
        "sys", 
        "unittest"
      ]
    }, 
    "test.sortperf": {
      "file": "test/sortperf.py", 
      "imports": [
        "marshal", 
        "os", 
        "random", 
        "sys", 
        "tempfile", 
        "time"
      ]
    }, 
    "test.ssl_servers": {
      "file": "test/ssl_servers.py", 
      "imports": [
        "BaseHTTPServer", 
        "SimpleHTTPServer", 
        "argparse", 
        "os", 
        "pprint", 
        "ssl", 
        "sys", 
        "test.test_support", 
        "urllib", 
        "urlparse"
      ]
    }, 
    "test.string_tests": {
      "file": "test/string_tests.py", 
      "imports": [
        "string", 
        "struct", 
        "sys", 
        "test.test_support", 
        "zlib", 
        "unittest", 
        "UserList", 
        "_testcapi"
      ]
    }, 
    "test.symlink_support": {
      "file": "test/symlink_support.py", 
      "imports": [
        "ctypes.wintypes", 
        "os", 
        "platform", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_MimeWriter": {
      "file": "test/test_MimeWriter.py", 
      "imports": [
        "MimeWriter", 
        "StringIO", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_SimpleHTTPServer": {
      "file": "test/test_SimpleHTTPServer.py", 
      "imports": [
        "SimpleHTTPServer", 
        "os", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_StringIO": {
      "file": "test/test_StringIO.py", 
      "imports": [
        "StringIO", 
        "array", 
        "cStringIO", 
        "sys", 
        "test.test_support", 
        "types", 
        "unittest"
      ]
    }, 
    "test.test___all__": {
      "file": "test/test___all__.py", 
      "imports": [
        "__future__", 
        "_socket", 
        "locale", 
        "os", 
        "rlcompleter", 
        "sys", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test___future__": {
      "file": "test/test___future__.py", 
      "imports": [
        "__future__", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test__locale": {
      "file": "test/test__locale.py", 
      "imports": [
        "_locale.Error", 
        "_locale.LC_NUMERIC", 
        "_locale.RADIXCHAR", 
        "_locale.THOUSEP", 
        "_locale.localeconv", 
        "_locale.nl_langinfo", 
        "_locale.setlocale", 
        "platform", 
        "sys", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test__osx_support": {
      "file": "test/test__osx_support.py", 
      "imports": [
        "_osx_support", 
        "os", 
        "platform", 
        "shutil", 
        "stat", 
        "sys", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_abc": {
      "file": "test/test_abc.py", 
      "imports": [
        "abc", 
        "inspect", 
        "test.test_support", 
        "unittest", 
        "weakref"
      ]
    }, 
    "test.test_abstract_numbers": {
      "file": "test/test_abstract_numbers.py", 
      "imports": [
        "math", 
        "numbers", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_aepack": {
      "file": "test/test_aepack.py", 
      "imports": [
        "Carbon.File", 
        "os", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_aifc": {
      "file": "test/test_aifc.py", 
      "imports": [
        "aifc", 
        "io", 
        "os", 
        "struct", 
        "sys", 
        "test.audiotests", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_al": {
      "file": "test/test_al.py", 
      "imports": [
        "test.test_support"
      ]
    }, 
    "test.test_anydbm": {
      "file": "test/test_anydbm.py", 
      "imports": [
        "glob", 
        "os", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_applesingle": {
      "file": "test/test_applesingle.py", 
      "imports": [
        "applesingle", 
        "os", 
        "struct", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_argparse": {
      "file": "test/test_argparse.py", 
      "imports": [
        "StringIO", 
        "argparse", 
        "codecs", 
        "gc", 
        "inspect", 
        "os", 
        "shutil", 
        "stat", 
        "sys", 
        "tempfile", 
        "test.test_support", 
        "textwrap", 
        "unittest"
      ]
    }, 
    "test.test_array": {
      "file": "test/test_array.py", 
      "imports": [
        "array", 
        "cStringIO", 
        "copy", 
        "gc", 
        "sys", 
        "sys.maxsize", 
        "test.test_support", 
        "unittest", 
        "warnings", 
        "weakref", 
        "cPickle"
      ]
    }, 
    "test.test_ascii_formatd": {
      "file": "test/test_ascii_formatd.py", 
      "imports": [
        "ctypes.byref", 
        "ctypes.c_double", 
        "ctypes.create_string_buffer", 
        "ctypes.pythonapi", 
        "ctypes.sizeof", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_ast": {
      "file": "test/test_ast.py", 
      "imports": [
        "ast", 
        "itertools", 
        "pickle", 
        "sys", 
        "test.test_support", 
        "unittest", 
        "cPickle"
      ]
    }, 
    "test.test_asynchat": {
      "file": "test/test_asynchat.py", 
      "imports": [
        "asynchat", 
        "asyncore", 
        "errno", 
        "socket", 
        "sys", 
        "test.test_support", 
        "threading", 
        "time", 
        "unittest"
      ]
    }, 
    "test.test_asyncore": {
      "file": "test/test_asyncore.py", 
      "imports": [
        "StringIO", 
        "asyncore", 
        "errno", 
        "os", 
        "select", 
        "socket", 
        "struct", 
        "sys", 
        "test.test_support", 
        "threading", 
        "time", 
        "unittest", 
        "warnings"
      ]
    }, 
    "test.test_atexit": {
      "file": "test/test_atexit.py", 
      "imports": [
        "StringIO", 
        "atexit", 
        "imp.reload", 
        "sys", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_audioop": {
      "file": "test/test_audioop.py", 
      "imports": [
        "audioop", 
        "struct", 
        "sys", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_augassign": {
      "file": "test/test_augassign.py", 
      "imports": [
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_base64": {
      "file": "test/test_base64.py", 
      "imports": [
        "base64", 
        "cStringIO.StringIO", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_bastion": {
      "file": "test/test_bastion.py", 
      "imports": []
    }, 
    "test.test_bigaddrspace": {
      "file": "test/test_bigaddrspace.py", 
      "imports": [
        "operator", 
        "sys", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_bigmem": {
      "file": "test/test_bigmem.py", 
      "imports": [
        "operator", 
        "string", 
        "sys", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_binascii": {
      "file": "test/test_binascii.py", 
      "imports": [
        "array", 
        "binascii", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_binhex": {
      "file": "test/test_binhex.py", 
      "imports": [
        "binhex", 
        "os", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_binop": {
      "file": "test/test_binop.py", 
      "imports": [
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_bisect": {
      "file": "test/test_bisect.py", 
      "imports": [
        "bisect", 
        "gc", 
        "random", 
        "sys", 
        "test.test_bisect", 
        "test.test_support", 
        "unittest", 
        "UserList"
      ]
    }, 
    "test.test_bool": {
      "file": "test/test_bool.py", 
      "imports": [
        "marshal", 
        "operator", 
        "os", 
        "pickle", 
        "test.test_support", 
        "unittest", 
        "cPickle"
      ]
    }, 
    "test.test_bsddb": {
      "file": "test/test_bsddb.py", 
      "imports": [
        "os", 
        "sys", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_bsddb185": {
      "file": "test/test_bsddb185.py", 
      "imports": [
        "anydbm", 
        "os", 
        "shutil", 
        "tempfile", 
        "test.test_support", 
        "unittest", 
        "whichdb"
      ]
    }, 
    "test.test_bsddb3": {
      "file": "test/test_bsddb3.py", 
      "imports": [
        "bsddb.db", 
        "bsddb.test.test_all", 
        "os", 
        "sys", 
        "tempfile", 
        "test.test_support", 
        "time", 
        "unittest"
      ]
    }, 
    "test.test_buffer": {
      "file": "test/test_buffer.py", 
      "imports": [
        "sys", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_bufio": {
      "file": "test/test_bufio.py", 
      "imports": [
        "_pyio", 
        "io", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_builtin": {
      "file": "test/test_builtin.py", 
      "imports": [
        "cStringIO", 
        "gc", 
        "marshal", 
        "math.sqrt", 
        "operator.neg", 
        "os", 
        "platform", 
        "random", 
        "string", 
        "sys", 
        "test.test_support", 
        "types", 
        "unittest", 
        "UserDict", 
        "UserList", 
        "warnings"
      ]
    }, 
    "test.test_bytes": {
      "file": "test/test_bytes.py", 
      "imports": [
        "copy", 
        "functools", 
        "os", 
        "pickle", 
        "re", 
        "sys", 
        "tempfile", 
        "test.buffer_tests", 
        "test.string_tests", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_bz2": {
      "file": "test/test_bz2.py", 
      "imports": [
        "bz2.BZ2Compressor", 
        "bz2.BZ2Decompressor", 
        "bz2.BZ2File", 
        "cStringIO.StringIO", 
        "os", 
        "subprocess", 
        "sys", 
        "test.test_support", 
        "threading", 
        "unittest"
      ]
    }, 
    "test.test_calendar": {
      "file": "test/test_calendar.py", 
      "imports": [
        "calendar", 
        "locale", 
        "test.test_support", 
        "unittest", 
        "datetime"
      ]
    }, 
    "test.test_call": {
      "file": "test/test_call.py", 
      "imports": [
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_capi": {
      "file": "test/test_capi.py", 
      "imports": [
        "__future__", 
        "random", 
        "sys", 
        "test.test_support", 
        "thread", 
        "threading", 
        "time", 
        "unittest"
      ]
    }, 
    "test.test_cd": {
      "file": "test/test_cd.py", 
      "imports": [
        "test.test_support"
      ]
    }, 
    "test.test_cfgparser": {
      "file": "test/test_cfgparser.py", 
      "imports": [
        "ConfigParser", 
        "StringIO", 
        "os", 
        "pickle", 
        "test.test_support", 
        "unittest", 
        "UserDict"
      ]
    }, 
    "test.test_cgi": {
      "file": "test/test_cgi.py", 
      "imports": [
        "StringIO", 
        "cStringIO.StringIO", 
        "cgi", 
        "collections", 
        "os", 
        "sys", 
        "tempfile", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_charmapcodec": {
      "file": "test/test_charmapcodec.py", 
      "imports": [
        "codecs", 
        "test.test_support", 
        "test.testcodec", 
        "unittest"
      ]
    }, 
    "test.test_cl": {
      "file": "test/test_cl.py", 
      "imports": [
        "test.test_support"
      ]
    }, 
    "test.test_class": {
      "file": "test/test_class.py", 
      "imports": [
        "gc", 
        "test.test_support", 
        "unittest", 
        "_testcapi"
      ]
    }, 
    "test.test_cmath": {
      "file": "test/test_cmath.py", 
      "imports": [
        "cmath", 
        "cmath.phase", 
        "cmath.pi", 
        "cmath.polar", 
        "cmath.rect", 
        "math", 
        "test.test_math", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_cmd": {
      "file": "test/test_cmd.py", 
      "imports": [
        "StringIO", 
        "cmd", 
        "re", 
        "sys", 
        "test.test_cmd", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_cmd_line": {
      "file": "test/test_cmd_line.py", 
      "imports": [
        "sys", 
        "test.script_helper", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_cmd_line_script": {
      "file": "test/test_cmd_line_script.py", 
      "imports": [
        "os", 
        "test.script_helper", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_code": {
      "file": "test/test_code.py", 
      "imports": [
        "test.test_code", 
        "test.test_support", 
        "unittest", 
        "weakref", 
        "_testcapi"
      ]
    }, 
    "test.test_codeccallbacks": {
      "file": "test/test_codeccallbacks.py", 
      "imports": [
        "codecs", 
        "htmlentitydefs", 
        "sys", 
        "test.test_support", 
        "unicodedata", 
        "unittest"
      ]
    }, 
    "test.test_codecencodings_cn": {
      "file": "test/test_codecencodings_cn.py", 
      "imports": [
        "test.test_multibytecodec_support", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_codecencodings_hk": {
      "file": "test/test_codecencodings_hk.py", 
      "imports": [
        "test.test_multibytecodec_support", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_codecencodings_iso2022": {
      "file": "test/test_codecencodings_iso2022.py", 
      "imports": [
        "test.test_multibytecodec_support", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_codecencodings_jp": {
      "file": "test/test_codecencodings_jp.py", 
      "imports": [
        "test.test_multibytecodec_support", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_codecencodings_kr": {
      "file": "test/test_codecencodings_kr.py", 
      "imports": [
        "test.test_multibytecodec_support", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_codecencodings_tw": {
      "file": "test/test_codecencodings_tw.py", 
      "imports": [
        "test.test_multibytecodec_support", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_codecmaps_cn": {
      "file": "test/test_codecmaps_cn.py", 
      "imports": [
        "test.test_multibytecodec_support", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_codecmaps_hk": {
      "file": "test/test_codecmaps_hk.py", 
      "imports": [
        "test.test_multibytecodec_support", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_codecmaps_jp": {
      "file": "test/test_codecmaps_jp.py", 
      "imports": [
        "test.test_multibytecodec_support", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_codecmaps_kr": {
      "file": "test/test_codecmaps_kr.py", 
      "imports": [
        "test.test_multibytecodec_support", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_codecmaps_tw": {
      "file": "test/test_codecmaps_tw.py", 
      "imports": [
        "test.test_multibytecodec_support", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_codecs": {
      "file": "test/test_codecs.py", 
      "imports": [
        "StringIO", 
        "array", 
        "bz2", 
        "codecs", 
        "encodings.cp1140", 
        "encodings.idna", 
        "locale", 
        "sys", 
        "test.test_support", 
        "zlib", 
        "unittest", 
        "_testcapi"
      ]
    }, 
    "test.test_codeop": {
      "file": "test/test_codeop.py", 
      "imports": [
        "cStringIO", 
        "codeop", 
        "sys", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_coding": {
      "file": "test/test_coding.py", 
      "imports": [
        "os", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_coercion": {
      "file": "test/test_coercion.py", 
      "imports": [
        "copy", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_collections": {
      "file": "test/test_collections.py", 
      "imports": [
        "collections", 
        "copy", 
        "doctest", 
        "inspect", 
        "keyword", 
        "operator", 
        "pickle", 
        "random", 
        "re", 
        "string", 
        "sys", 
        "test.mapping_tests", 
        "test.test_support", 
        "unittest", 
        "cPickle"
      ]
    }, 
    "test.test_colorsys": {
      "file": "test/test_colorsys.py", 
      "imports": [
        "colorsys", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_commands": {
      "file": "test/test_commands.py", 
      "imports": [
        "os", 
        "re", 
        "tempfile", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_compare": {
      "file": "test/test_compare.py", 
      "imports": [
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_compile": {
      "file": "test/test_compile.py", 
      "imports": [
        "__builtin__", 
        "__mangled_mod", 
        "__package__.module", 
        "_ast", 
        "math", 
        "sys", 
        "test.test_support", 
        "textwrap", 
        "unittest"
      ]
    }, 
    "test.test_compileall": {
      "file": "test/test_compileall.py", 
      "imports": [
        "compileall", 
        "imp", 
        "os", 
        "py_compile", 
        "shutil", 
        "struct", 
        "tempfile", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_compiler": {
      "file": "test/test_compiler.py", 
      "imports": [
        "StringIO", 
        "compiler.ast", 
        "math.*", 
        "os", 
        "random", 
        "sys", 
        "test.test_support", 
        "time", 
        "unittest"
      ]
    }, 
    "test.test_complex": {
      "file": "test/test_complex.py", 
      "imports": [
        "math.atan2", 
        "math.copysign", 
        "math.isnan", 
        "random", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_complex_args": {
      "file": "test/test_complex_args.py", 
      "imports": [
        "test.test_support", 
        "textwrap", 
        "unittest"
      ]
    }, 
    "test.test_contains": {
      "file": "test/test_contains.py", 
      "imports": [
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_contextlib": {
      "file": "test/test_contextlib.py", 
      "imports": [
        "contextlib", 
        "sys", 
        "tempfile", 
        "test.test_support", 
        "threading", 
        "unittest"
      ]
    }, 
    "test.test_cookie": {
      "file": "test/test_cookie.py", 
      "imports": [
        "Cookie", 
        "pickle", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_cookielib": {
      "file": "test/test_cookielib.py", 
      "imports": [
        "StringIO", 
        "cookielib", 
        "mimetools", 
        "os", 
        "re", 
        "test.test_support", 
        "time", 
        "traceback", 
        "unittest", 
        "urllib2"
      ]
    }, 
    "test.test_copy": {
      "file": "test/test_copy.py", 
      "imports": [
        "copy", 
        "copy_reg", 
        "test.test_support", 
        "unittest", 
        "weakref"
      ]
    }, 
    "test.test_copy_reg": {
      "file": "test/test_copy_reg.py", 
      "imports": [
        "copy", 
        "copy_reg", 
        "test.pickletester", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_cpickle": {
      "file": "test/test_cpickle.py", 
      "imports": [
        "cStringIO", 
        "io", 
        "test.pickletester", 
        "test.test_support", 
        "unittest", 
        "cPickle"
      ]
    }, 
    "test.test_cprofile": {
      "file": "test/test_cprofile.py", 
      "imports": [
        "_lsprof", 
        "cProfile", 
        "sys", 
        "test.test_profile", 
        "test.test_support"
      ]
    }, 
    "test.test_crypt": {
      "file": "test/test_crypt.py", 
      "imports": [
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_csv": {
      "file": "test/test_csv.py", 
      "imports": [
        "StringIO", 
        "array", 
        "csv", 
        "gc", 
        "io", 
        "itertools", 
        "os", 
        "string", 
        "sys", 
        "tempfile", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_ctypes": {
      "file": "test/test_ctypes.py", 
      "imports": [
        "ctypes.test", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_curses": {
      "file": "test/test_curses.py", 
      "imports": [
        "curses.ascii", 
        "os", 
        "sys", 
        "tempfile", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_datetime": {
      "file": "test/test_datetime.py", 
      "imports": [
        "__future__", 
        "_strptime", 
        "pickle", 
        "random", 
        "sys", 
        "test.test_support", 
        "time", 
        "unittest", 
        "cPickle", 
        "datetime"
      ]
    }, 
    "test.test_dbm": {
      "file": "test/test_dbm.py", 
      "imports": [
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_decimal": {
      "file": "test/test_decimal.py", 
      "imports": [
        "copy", 
        "decimal", 
        "locale", 
        "math", 
        "numbers", 
        "operator", 
        "optparse", 
        "os", 
        "pickle", 
        "random", 
        "sys", 
        "test.test_support", 
        "threading", 
        "unittest"
      ]
    }, 
    "test.test_decorators": {
      "file": "test/test_decorators.py", 
      "imports": [
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_defaultdict": {
      "file": "test/test_defaultdict.py", 
      "imports": [
        "collections", 
        "copy", 
        "os", 
        "tempfile", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_deque": {
      "file": "test/test_deque.py", 
      "imports": [
        "collections", 
        "copy", 
        "gc", 
        "random", 
        "struct", 
        "sys", 
        "test.seq_tests", 
        "test.test_deque", 
        "test.test_support", 
        "unittest", 
        "weakref", 
        "cPickle"
      ]
    }, 
    "test.test_descr": {
      "file": "test/test_descr.py", 
      "imports": [
        "__builtin__", 
        "abc", 
        "binascii", 
        "cStringIO", 
        "copy", 
        "gc", 
        "operator", 
        "pickle", 
        "popen2", 
        "sys", 
        "test.test_support", 
        "xxsubtype", 
        "types", 
        "unittest", 
        "weakref", 
        "_testcapi", 
        "cPickle"
      ]
    }, 
    "test.test_descrtut": {
      "file": "test/test_descrtut.py", 
      "imports": [
        "pprint", 
        "test.test_descrtut", 
        "test.test_support"
      ]
    }, 
    "test.test_dict": {
      "file": "test/test_dict.py", 
      "imports": [
        "gc", 
        "random", 
        "string", 
        "test.mapping_tests", 
        "test.test_support", 
        "unittest", 
        "UserDict", 
        "weakref"
      ]
    }, 
    "test.test_dictcomps": {
      "file": "test/test_dictcomps.py", 
      "imports": [
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_dictviews": {
      "file": "test/test_dictviews.py", 
      "imports": [
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_difflib": {
      "file": "test/test_difflib.py", 
      "imports": [
        "difflib", 
        "doctest", 
        "sys", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_dircache": {
      "file": "test/test_dircache.py", 
      "imports": [
        "os", 
        "sys", 
        "tempfile", 
        "test.test_support", 
        "time", 
        "unittest"
      ]
    }, 
    "test.test_dis": {
      "file": "test/test_dis.py", 
      "imports": [
        "StringIO", 
        "difflib", 
        "dis", 
        "sys", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_distutils": {
      "file": "test/test_distutils.py", 
      "imports": [
        "distutils.tests", 
        "test.test_support"
      ]
    }, 
    "test.test_dl": {
      "file": "test/test_dl.py", 
      "imports": [
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_doctest": {
      "file": "test/test_doctest.py", 
      "imports": [
        "doctest", 
        "sys", 
        "test.test_doctest", 
        "test.test_support"
      ]
    }, 
    "test.test_doctest2": {
      "file": "test/test_doctest2.py", 
      "imports": [
        "doctest", 
        "sys", 
        "test.test_doctest2", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_docxmlrpc": {
      "file": "test/test_docxmlrpc.py", 
      "imports": [
        "DocXMLRPCServer", 
        "httplib", 
        "socket", 
        "sys", 
        "test.test_support", 
        "time", 
        "unittest"
      ]
    }, 
    "test.test_dumbdbm": {
      "file": "test/test_dumbdbm.py", 
      "imports": [
        "dumbdbm", 
        "os", 
        "random", 
        "stat", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_dummy_thread": {
      "file": "test/test_dummy_thread.py", 
      "imports": [
        "Queue", 
        "dummy_thread", 
        "random", 
        "test.test_support", 
        "time", 
        "unittest"
      ]
    }, 
    "test.test_dummy_threading": {
      "file": "test/test_dummy_threading.py", 
      "imports": [
        "dummy_threading", 
        "test.test_support", 
        "time", 
        "unittest"
      ]
    }, 
    "test.test_email": {
      "file": "test/test_email.py", 
      "imports": [
        "email.test.test_email", 
        "email.test.test_email_renamed", 
        "test.test_support"
      ]
    }, 
    "test.test_email_codecs": {
      "file": "test/test_email_codecs.py", 
      "imports": [
        "email.test.test_email_codecs", 
        "email.test.test_email_codecs_renamed", 
        "test.test_support"
      ]
    }, 
    "test.test_email_renamed": {
      "file": "test/test_email_renamed.py", 
      "imports": [
        "email.test.test_email_renamed", 
        "test.test_support"
      ]
    }, 
    "test.test_ensurepip": {
      "file": "test/test_ensurepip.py", 
      "imports": [
        "contextlib", 
        "ensurepip", 
        "ensurepip._uninstall", 
        "os", 
        "ssl", 
        "sys", 
        "test._mock_backport", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_enumerate": {
      "file": "test/test_enumerate.py", 
      "imports": [
        "sys", 
        "test.test_iterlen", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_eof": {
      "file": "test/test_eof.py", 
      "imports": [
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_epoll": {
      "file": "test/test_epoll.py", 
      "imports": [
        "errno", 
        "select", 
        "socket", 
        "test.test_support", 
        "time", 
        "unittest"
      ]
    }, 
    "test.test_errno": {
      "file": "test/test_errno.py", 
      "imports": [
        "errno", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_exception_variations": {
      "file": "test/test_exception_variations.py", 
      "imports": [
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_exceptions": {
      "file": "test/test_exceptions.py", 
      "imports": [
        "exceptions", 
        "imp.reload", 
        "os", 
        "pickle", 
        "sys", 
        "test.test_pep352", 
        "test.test_support", 
        "unittest", 
        "_testcapi", 
        "cPickle"
      ]
    }, 
    "test.test_extcall": {
      "file": "test/test_extcall.py", 
      "imports": [
        "sys", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_fcntl": {
      "file": "test/test_fcntl.py", 
      "imports": [
        "os", 
        "struct", 
        "sys", 
        "test.test_support", 
        "unittest", 
        "_testcapi"
      ]
    }, 
    "test.test_file": {
      "file": "test/test_file.py", 
      "imports": [
        "__future__", 
        "_pyio", 
        "array.array", 
        "io", 
        "os", 
        "sys", 
        "test.test_support", 
        "unittest", 
        "UserList", 
        "weakref"
      ]
    }, 
    "test.test_file2k": {
      "file": "test/test_file2k.py", 
      "imports": [
        "array.array", 
        "itertools", 
        "os", 
        "select", 
        "signal", 
        "subprocess", 
        "sys", 
        "test.test_support", 
        "threading", 
        "time", 
        "unittest", 
        "UserList", 
        "weakref"
      ]
    }, 
    "test.test_file_eintr": {
      "file": "test/test_file_eintr.py", 
      "imports": [
        "_io.FileIO", 
        "os", 
        "select", 
        "signal", 
        "subprocess", 
        "sys", 
        "test.test_support", 
        "time", 
        "unittest"
      ]
    }, 
    "test.test_filecmp": {
      "file": "test/test_filecmp.py", 
      "imports": [
        "filecmp", 
        "os", 
        "shutil", 
        "tempfile", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_fileinput": {
      "file": "test/test_fileinput.py", 
      "imports": [
        "StringIO", 
        "fileinput", 
        "re", 
        "sys", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_fileio": {
      "file": "test/test_fileio.py", 
      "imports": [
        "__future__", 
        "_io.FileIO", 
        "array.array", 
        "errno", 
        "functools", 
        "msvcrt", 
        "os", 
        "sys", 
        "test.script_helper", 
        "test.test_support", 
        "unittest", 
        "UserList", 
        "weakref", 
        "_testcapi"
      ]
    }, 
    "test.test_float": {
      "file": "test/test_float.py", 
      "imports": [
        "fractions", 
        "locale", 
        "math", 
        "math.copysign", 
        "math.isinf", 
        "math.isnan", 
        "math.ldexp", 
        "operator", 
        "os", 
        "random", 
        "struct", 
        "sys", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_fnmatch": {
      "file": "test/test_fnmatch.py", 
      "imports": [
        "fnmatch", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_fork1": {
      "file": "test/test_fork1.py", 
      "imports": [
        "imp", 
        "os", 
        "signal", 
        "sys", 
        "test.fork_wait", 
        "test.test_support", 
        "time"
      ]
    }, 
    "test.test_format": {
      "file": "test/test_format.py", 
      "imports": [
        "sys", 
        "test.test_support", 
        "unittest", 
        "_testcapi"
      ]
    }, 
    "test.test_fpformat": {
      "file": "test/test_fpformat.py", 
      "imports": [
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_fractions": {
      "file": "test/test_fractions.py", 
      "imports": [
        "copy", 
        "decimal", 
        "fractions", 
        "math", 
        "numbers", 
        "operator", 
        "sys", 
        "test.test_support", 
        "unittest", 
        "cPickle"
      ]
    }, 
    "test.test_frozen": {
      "file": "test/test_frozen.py", 
      "imports": [
        "__hello__", 
        "__phello__", 
        "__phello__.foo", 
        "__phello__.spam", 
        "sys", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_ftplib": {
      "file": "test/test_ftplib.py", 
      "imports": [
        "StringIO", 
        "asynchat", 
        "asyncore", 
        "errno", 
        "ftplib", 
        "os", 
        "socket", 
        "ssl", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_funcattrs": {
      "file": "test/test_funcattrs.py", 
      "imports": [
        "test.test_support", 
        "types", 
        "unittest", 
        "UserDict"
      ]
    }, 
    "test.test_functools": {
      "file": "test/test_functools.py", 
      "imports": [
        "functools", 
        "gc", 
        "pickle", 
        "sys", 
        "test.test_support", 
        "unittest", 
        "weakref"
      ]
    }, 
    "test.test_future": {
      "file": "test/test_future.py", 
      "imports": [
        "re", 
        "test.badsyntax_future3", 
        "test.badsyntax_future4", 
        "test.badsyntax_future5", 
        "test.badsyntax_future6", 
        "test.badsyntax_future7", 
        "test.badsyntax_future8", 
        "test.badsyntax_future9", 
        "test.test_future1", 
        "test.test_future2", 
        "test.test_future3", 
        "test.test_future5", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_future1": {
      "file": "test/test_future1.py", 
      "imports": [
        "__future__"
      ]
    }, 
    "test.test_future2": {
      "file": "test/test_future2.py", 
      "imports": [
        "__future__", 
        "string"
      ]
    }, 
    "test.test_future3": {
      "file": "test/test_future3.py", 
      "imports": [
        "__future__", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_future4": {
      "file": "test/test_future4.py", 
      "imports": [
        "__future__", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_future5": {
      "file": "test/test_future5.py", 
      "imports": [
        "__future__", 
        "sys", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_future_builtins": {
      "file": "test/test_future_builtins.py", 
      "imports": [
        "itertools.ifilter", 
        "itertools.imap", 
        "itertools.izip", 
        "test.test_support", 
        "unittest", 
        "future_builtins"
      ]
    }, 
    "test.test_gc": {
      "file": "test/test_gc.py", 
      "imports": [
        "gc", 
        "sys", 
        "test.test_support", 
        "threading", 
        "time", 
        "unittest", 
        "weakref"
      ]
    }, 
    "test.test_gdb": {
      "file": "test/test_gdb.py", 
      "imports": [
        "os", 
        "re", 
        "subprocess", 
        "sys", 
        "sysconfig", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_gdbm": {
      "file": "test/test_gdbm.py", 
      "imports": [
        "os", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_generators": {
      "file": "test/test_generators.py", 
      "imports": [
        "test.test_generators", 
        "test.test_support"
      ]
    }, 
    "test.test_genericpath": {
      "file": "test/test_genericpath.py", 
      "imports": [
        "genericpath", 
        "os", 
        "sys", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_genexps": {
      "file": "test/test_genexps.py", 
      "imports": [
        "gc", 
        "sys", 
        "test.test_genexps", 
        "test.test_support"
      ]
    }, 
    "test.test_getargs": {
      "file": "test/test_getargs.py", 
      "imports": [
        "marshal", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_getargs2": {
      "file": "test/test_getargs2.py", 
      "imports": [
        "test.test_support", 
        "unittest", 
        "warnings", 
        "_testcapi"
      ]
    }, 
    "test.test_getopt": {
      "file": "test/test_getopt.py", 
      "imports": [
        "getopt", 
        "test.test_support", 
        "types", 
        "unittest"
      ]
    }, 
    "test.test_gettext": {
      "file": "test/test_gettext.py", 
      "imports": [
        "__builtin__", 
        "base64", 
        "gettext", 
        "os", 
        "shutil", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_gl": {
      "file": "test/test_gl.py", 
      "imports": [
        "os", 
        "test.test_support", 
        "time", 
        "unittest"
      ]
    }, 
    "test.test_glob": {
      "file": "test/test_glob.py", 
      "imports": [
        "glob", 
        "os", 
        "shutil", 
        "sys", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_global": {
      "file": "test/test_global.py", 
      "imports": [
        "test.test_support", 
        "unittest", 
        "warnings"
      ]
    }, 
    "test.test_grammar": {
      "file": "test/test_grammar.py", 
      "imports": [
        "StringIO", 
        "sys", 
        "sys.*", 
        "sys.argv", 
        "sys.maxint", 
        "sys.path", 
        "test.test_support", 
        "time", 
        "time.time", 
        "types", 
        "unittest"
      ]
    }, 
    "test.test_grp": {
      "file": "test/test_grp.py", 
      "imports": [
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_gzip": {
      "file": "test/test_gzip.py", 
      "imports": [
        "io", 
        "os", 
        "struct", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_hash": {
      "file": "test/test_hash.py", 
      "imports": [
        "collections", 
        "os", 
        "struct", 
        "subprocess", 
        "sys", 
        "test.test_support", 
        "unittest", 
        "datetime"
      ]
    }, 
    "test.test_hashlib": {
      "file": "test/test_hashlib.py", 
      "imports": [
        "_md5", 
        "array", 
        "binascii.unhexlify", 
        "hashlib", 
        "itertools", 
        "string", 
        "sys", 
        "test.test_support", 
        "threading", 
        "unittest", 
        "warnings"
      ]
    }, 
    "test.test_heapq": {
      "file": "test/test_heapq.py", 
      "imports": [
        "gc", 
        "itertools.chain", 
        "itertools.imap", 
        "random", 
        "sys", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_hmac": {
      "file": "test/test_hmac.py", 
      "imports": [
        "hashlib", 
        "hmac", 
        "test.test_support", 
        "unittest", 
        "warnings"
      ]
    }, 
    "test.test_hotshot": {
      "file": "test/test_hotshot.py", 
      "imports": [
        "_hotshot", 
        "gc", 
        "hotshot.log.ENTER", 
        "hotshot.log.EXIT", 
        "hotshot.log.LINE", 
        "hotshot.stats", 
        "os", 
        "pprint", 
        "sys", 
        "tempfile", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_htmllib": {
      "file": "test/test_htmllib.py", 
      "imports": [
        "formatter", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_htmlparser": {
      "file": "test/test_htmlparser.py", 
      "imports": [
        "HTMLParser", 
        "pprint", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_httplib": {
      "file": "test/test_httplib.py", 
      "imports": [
        "StringIO", 
        "array", 
        "errno", 
        "httplib", 
        "os", 
        "socket", 
        "ssl", 
        "test.ssl_servers", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_httpservers": {
      "file": "test/test_httpservers.py", 
      "imports": [
        "BaseHTTPServer", 
        "CGIHTTPServer", 
        "SimpleHTTPServer", 
        "StringIO", 
        "base64", 
        "httplib", 
        "os", 
        "re", 
        "shutil", 
        "sys", 
        "tempfile", 
        "test.test_support", 
        "unittest", 
        "urllib"
      ]
    }, 
    "test.test_idle": {
      "file": "test/test_idle.py", 
      "imports": [
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_imageop": {
      "file": "test/test_imageop.py", 
      "imports": [
        "imgfile", 
        "os", 
        "sys", 
        "test.test_support", 
        "unittest", 
        "uu"
      ]
    }, 
    "test.test_imaplib": {
      "file": "test/test_imaplib.py", 
      "imports": [
        "SocketServer", 
        "contextlib", 
        "imaplib", 
        "os", 
        "ssl", 
        "test.test_support", 
        "time", 
        "unittest"
      ]
    }, 
    "test.test_imgfile": {
      "file": "test/test_imgfile.py", 
      "imports": [
        "os", 
        "sys", 
        "test.test_support", 
        "uu"
      ]
    }, 
    "test.test_imghdr": {
      "file": "test/test_imghdr.py", 
      "imports": [
        "imghdr", 
        "io", 
        "sys", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_imp": {
      "file": "test/test_imp.py", 
      "imports": [
        "imp", 
        "marshal", 
        "os", 
        "test.test_support", 
        "thread", 
        "time", 
        "unittest"
      ]
    }, 
    "test.test_import": {
      "file": "test/test_import.py", 
      "imports": [
        "RAnDoM", 
        "errno", 
        "imp", 
        "marshal", 
        "os", 
        "py_compile", 
        "random", 
        "shutil", 
        "socket", 
        "stat", 
        "struct", 
        "sys", 
        "test", 
        "test.double_const", 
        "test.infinite_reload", 
        "test.relimport", 
        "test.script_helper", 
        "test.symlink_support", 
        "test.test_import", 
        "test.test_support", 
        "textwrap", 
        "unittest"
      ]
    }, 
    "test.test_importhooks": {
      "file": "test/test_importhooks.py", 
      "imports": [
        "hooktestmodule", 
        "hooktestpackage", 
        "hooktestpackage.futrel", 
        "hooktestpackage.newabs", 
        "hooktestpackage.newrel", 
        "hooktestpackage.oldabs", 
        "hooktestpackage.sub", 
        "hooktestpackage.sub.subber", 
        "hooktestpackage.sub.subber.subest", 
        "imp", 
        "os", 
        "reloadmodule", 
        "sub", 
        "sys", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_importlib": {
      "file": "test/test_importlib.py", 
      "imports": [
        "contextlib", 
        "imp", 
        "importlib", 
        "sys", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_index": {
      "file": "test/test_index.py", 
      "imports": [
        "operator", 
        "sys.maxint", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_inspect": {
      "file": "test/test_inspect.py", 
      "imports": [
        "__builtin__", 
        "abc", 
        "inspect", 
        "linecache", 
        "re", 
        "sys", 
        "test.inspect_fodder", 
        "test.inspect_fodder2", 
        "test.test_support", 
        "unicodedata", 
        "types", 
        "unittest", 
        "UserDict", 
        "UserList"
      ]
    }, 
    "test.test_int": {
      "file": "test/test_int.py", 
      "imports": [
        "math", 
        "sys", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_int_literal": {
      "file": "test/test_int_literal.py", 
      "imports": [
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_io": {
      "file": "test/test_io.py", 
      "imports": [
        "__future__", 
        "_pyio", 
        "abc", 
        "array", 
        "codecs", 
        "collections", 
        "contextlib", 
        "errno", 
        "fcntl", 
        "io", 
        "itertools.count", 
        "itertools.cycle", 
        "os", 
        "random", 
        "signal", 
        "sys", 
        "test.test_support", 
        "threading", 
        "time", 
        "unittest", 
        "UserList", 
        "warnings", 
        "weakref"
      ]
    }, 
    "test.test_ioctl": {
      "file": "test/test_ioctl.py", 
      "imports": [
        "array", 
        "os", 
        "pty", 
        "struct", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_isinstance": {
      "file": "test/test_isinstance.py", 
      "imports": [
        "sys", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_iter": {
      "file": "test/test_iter.py", 
      "imports": [
        "operator.add", 
        "operator.countOf", 
        "operator.indexOf", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_iterlen": {
      "file": "test/test_iterlen.py", 
      "imports": [
        "__builtin__.len", 
        "collections", 
        "itertools.repeat", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_itertools": {
      "file": "test/test_itertools.py", 
      "imports": [
        "copy", 
        "decimal", 
        "fractions", 
        "functools", 
        "gc", 
        "itertools.*", 
        "operator", 
        "pickle", 
        "random", 
        "sys", 
        "test.test_iterlen", 
        "test.test_support", 
        "unittest", 
        "weakref"
      ]
    }, 
    "test.test_json": {
      "file": "test/test_json.py", 
      "imports": [
        "json.tests", 
        "test.test_support"
      ]
    }, 
    "test.test_kqueue": {
      "file": "test/test_kqueue.py", 
      "imports": [
        "errno", 
        "select", 
        "socket", 
        "sys", 
        "test.test_support", 
        "time", 
        "unittest"
      ]
    }, 
    "test.test_largefile": {
      "file": "test/test_largefile.py", 
      "imports": [
        "__future__", 
        "_pyio", 
        "io", 
        "os", 
        "signal", 
        "stat", 
        "sys", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_lib2to3": {
      "file": "test/test_lib2to3.py", 
      "imports": [
        "lib2to3.tests.test_fixers", 
        "lib2to3.tests.test_main", 
        "lib2to3.tests.test_parser", 
        "lib2to3.tests.test_pytree", 
        "lib2to3.tests.test_refactor", 
        "lib2to3.tests.test_util", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_linecache": {
      "file": "test/test_linecache.py", 
      "imports": [
        "linecache", 
        "os", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_linuxaudiodev": {
      "file": "test/test_linuxaudiodev.py", 
      "imports": [
        "audioop", 
        "errno", 
        "sys", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_list": {
      "file": "test/test_list.py", 
      "imports": [
        "gc", 
        "sys", 
        "test.list_tests", 
        "test.test_support"
      ]
    }, 
    "test.test_locale": {
      "file": "test/test_locale.py", 
      "imports": [
        "codecs", 
        "locale", 
        "os", 
        "sys", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_logging": {
      "file": "test/test_logging.py", 
      "imports": [
        "SocketServer", 
        "cStringIO", 
        "codecs", 
        "gc", 
        "json", 
        "logging", 
        "logging.config", 
        "logging.handlers", 
        "os", 
        "random", 
        "re", 
        "select", 
        "socket", 
        "struct", 
        "sys", 
        "tempfile", 
        "test.test_support", 
        "threading", 
        "time", 
        "textwrap", 
        "unittest", 
        "warnings", 
        "weakref", 
        "cPickle"
      ]
    }, 
    "test.test_long": {
      "file": "test/test_long.py", 
      "imports": [
        "math", 
        "random", 
        "sys", 
        "test.test_int", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_long_future": {
      "file": "test/test_long_future.py", 
      "imports": [
        "__future__", 
        "math", 
        "random", 
        "sys", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_longexp": {
      "file": "test/test_longexp.py", 
      "imports": [
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_macos": {
      "file": "test/test_macos.py", 
      "imports": [
        "os", 
        "subprocess", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_macostools": {
      "file": "test/test_macostools.py", 
      "imports": [
        "Carbon.File", 
        "macostools", 
        "os", 
        "sys", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_macpath": {
      "file": "test/test_macpath.py", 
      "imports": [
        "macpath", 
        "test.test_genericpath", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_macurl2path": {
      "file": "test/test_macurl2path.py", 
      "imports": [
        "macurl2path", 
        "unittest"
      ]
    }, 
    "test.test_mailbox": {
      "file": "test/test_mailbox.py", 
      "imports": [
        "StringIO", 
        "email", 
        "email.message", 
        "email.parser", 
        "fcntl", 
        "glob", 
        "mailbox", 
        "os", 
        "re", 
        "shutil", 
        "socket", 
        "stat", 
        "sys", 
        "tempfile", 
        "test.test_support", 
        "time", 
        "unittest"
      ]
    }, 
    "test.test_marshal": {
      "file": "test/test_marshal.py", 
      "imports": [
        "marshal", 
        "os", 
        "sys", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_math": {
      "file": "test/test_math.py", 
      "imports": [
        "doctest", 
        "math", 
        "os", 
        "random", 
        "struct", 
        "sys", 
        "sys.float_info", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_md5": {
      "file": "test/test_md5.py", 
      "imports": [
        "md5", 
        "string", 
        "test.test_support", 
        "unittest", 
        "warnings"
      ]
    }, 
    "test.test_memoryio": {
      "file": "test/test_memoryio.py", 
      "imports": [
        "__future__", 
        "__main__", 
        "_pyio", 
        "array", 
        "io", 
        "pickle", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_memoryview": {
      "file": "test/test_memoryview.py", 
      "imports": [
        "array", 
        "gc", 
        "io", 
        "sys", 
        "test.test_support", 
        "unittest", 
        "weakref"
      ]
    }, 
    "test.test_mhlib": {
      "file": "test/test_mhlib.py", 
      "imports": [
        "StringIO", 
        "os", 
        "sys", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_mimetools": {
      "file": "test/test_mimetools.py", 
      "imports": [
        "StringIO", 
        "string", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_mimetypes": {
      "file": "test/test_mimetypes.py", 
      "imports": [
        "StringIO", 
        "_winreg", 
        "mimetypes", 
        "sys", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_minidom": {
      "file": "test/test_minidom.py", 
      "imports": [
        "StringIO", 
        "pickle", 
        "test.test_support", 
        "unittest", 
        "xml.parsers.expat", 
        "xml.dom.pulldom", 
        "xml.dom.minidom", 
        "xml.dom"
      ]
    }, 
    "test.test_mmap": {
      "file": "test/test_mmap.py", 
      "imports": [
        "itertools", 
        "os", 
        "re", 
        "socket", 
        "sys", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_module": {
      "file": "test/test_module.py", 
      "imports": [
        "sys", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_modulefinder": {
      "file": "test/test_modulefinder.py", 
      "imports": [
        "__future__", 
        "distutils.dir_util", 
        "modulefinder", 
        "os", 
        "sets", 
        "tempfile", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_msilib": {
      "file": "test/test_msilib.py", 
      "imports": [
        "os", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_multibytecodec": {
      "file": "test/test_multibytecodec.py", 
      "imports": [
        "StringIO", 
        "_multibytecodec", 
        "codecs", 
        "os", 
        "sys", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_multibytecodec_support": {
      "file": "test/test_multibytecodec_support.py", 
      "imports": [
        "StringIO", 
        "codecs", 
        "htmlentitydefs", 
        "httplib", 
        "os", 
        "re", 
        "sys", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_multifile": {
      "file": "test/test_multifile.py", 
      "imports": [
        "cStringIO", 
        "test.test_support"
      ]
    }, 
    "test.test_multiprocessing": {
      "file": "test/test_multiprocessing.py", 
      "imports": [
        "Queue", 
        "StringIO", 
        "array", 
        "ctypes.Structure", 
        "ctypes.c_double", 
        "ctypes.c_int", 
        "errno", 
        "gc", 
        "json", 
        "logging", 
        "msvcrt", 
        "multiprocessing.connection", 
        "multiprocessing.dummy", 
        "multiprocessing.forking", 
        "multiprocessing.heap", 
        "multiprocessing.managers", 
        "multiprocessing.managers.BaseManager", 
        "multiprocessing.managers.BaseProxy", 
        "multiprocessing.managers.RemoteError", 
        "multiprocessing.pool", 
        "multiprocessing.pool.MaybeEncodingError", 
        "multiprocessing.reduction", 
        "multiprocessing.sharedctypes.Value", 
        "multiprocessing.sharedctypes.copy", 
        "multiprocessing.util", 
        "os", 
        "random", 
        "signal", 
        "socket", 
        "subprocess", 
        "sys", 
        "test.script_helper", 
        "test.test_support", 
        "threading", 
        "time", 
        "unittest"
      ]
    }, 
    "test.test_mutants": {
      "file": "test/test_mutants.py", 
      "imports": [
        "os", 
        "random", 
        "test.test_support"
      ]
    }, 
    "test.test_mutex": {
      "file": "test/test_mutex.py", 
      "imports": [
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_netrc": {
      "file": "test/test_netrc.py", 
      "imports": [
        "netrc", 
        "os", 
        "sys", 
        "test.test_support", 
        "textwrap", 
        "unittest"
      ]
    }, 
    "test.test_new": {
      "file": "test/test_new.py", 
      "imports": [
        "Spam", 
        "__builtin__", 
        "sys", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_nis": {
      "file": "test/test_nis.py", 
      "imports": [
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_nntplib": {
      "file": "test/test_nntplib.py", 
      "imports": [
        "nntplib", 
        "socket", 
        "test.test_support", 
        "threading", 
        "time", 
        "unittest"
      ]
    }, 
    "test.test_normalization": {
      "file": "test/test_normalization.py", 
      "imports": [
        "httplib", 
        "os", 
        "sys", 
        "test.test_support", 
        "unicodedata.normalize", 
        "unicodedata.unidata_version", 
        "unittest"
      ]
    }, 
    "test.test_ntpath": {
      "file": "test/test_ntpath.py", 
      "imports": [
        "nt", 
        "ntpath", 
        "os", 
        "sys", 
        "test.test_genericpath", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_old_mailbox": {
      "file": "test/test_old_mailbox.py", 
      "imports": [
        "email.parser", 
        "mailbox", 
        "os", 
        "test.test_support", 
        "time", 
        "unittest"
      ]
    }, 
    "test.test_opcodes": {
      "file": "test/test_opcodes.py", 
      "imports": [
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_openpty": {
      "file": "test/test_openpty.py", 
      "imports": [
        "os", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_operator": {
      "file": "test/test_operator.py", 
      "imports": [
        "gc", 
        "operator", 
        "sys", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_optparse": {
      "file": "test/test_optparse.py", 
      "imports": [
        "StringIO", 
        "copy", 
        "optparse", 
        "os", 
        "re", 
        "sys", 
        "test.test_support", 
        "types", 
        "unittest"
      ]
    }, 
    "test.test_os": {
      "file": "test/test_os.py", 
      "imports": [
        "ctypes", 
        "ctypes.wintypes", 
        "errno", 
        "mmap", 
        "msvcrt", 
        "os", 
        "signal", 
        "stat", 
        "subprocess", 
        "sys", 
        "test.mapping_tests", 
        "test.script_helper", 
        "test.test_support", 
        "time", 
        "unittest", 
        "uuid", 
        "warnings", 
        "resource"
      ]
    }, 
    "test.test_ossaudiodev": {
      "file": "test/test_ossaudiodev.py", 
      "imports": [
        "audioop", 
        "errno", 
        "ossaudiodev.AFMT_S16_NE", 
        "sunau", 
        "sys", 
        "test.test_support", 
        "time", 
        "unittest"
      ]
    }, 
    "test.test_parser": {
      "file": "test/test_parser.py", 
      "imports": [
        "parser", 
        "struct", 
        "sys", 
        "test.script_helper", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_pdb": {
      "file": "test/test_pdb.py", 
      "imports": [
        "imp", 
        "os", 
        "subprocess", 
        "sys", 
        "test.test_doctest", 
        "test.test_pdb", 
        "test.test_support", 
        "textwrap", 
        "unittest"
      ]
    }, 
    "test.test_peepholer": {
      "file": "test/test_peepholer.py", 
      "imports": [
        "cStringIO.StringIO", 
        "dis", 
        "gc", 
        "sys", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_pep247": {
      "file": "test/test_pep247.py", 
      "imports": [
        "hmac", 
        "md5", 
        "sha", 
        "test.test_support", 
        "unittest", 
        "warnings"
      ]
    }, 
    "test.test_pep263": {
      "file": "test/test_pep263.py", 
      "imports": [
        "test.bad_coding3", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_pep277": {
      "file": "test/test_pep277.py", 
      "imports": [
        "os", 
        "sys", 
        "test.test_support", 
        "unicodedata.normalize", 
        "unittest"
      ]
    }, 
    "test.test_pep292": {
      "file": "test/test_pep292.py", 
      "imports": [
        "string", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_pep352": {
      "file": "test/test_pep352.py", 
      "imports": [
        "__builtin__", 
        "exceptions", 
        "os", 
        "platform", 
        "sys", 
        "test.test_support", 
        "unittest", 
        "warnings"
      ]
    }, 
    "test.test_pickle": {
      "file": "test/test_pickle.py", 
      "imports": [
        "cStringIO.StringIO", 
        "pickle", 
        "test.pickletester", 
        "test.test_support"
      ]
    }, 
    "test.test_pickletools": {
      "file": "test/test_pickletools.py", 
      "imports": [
        "pickle", 
        "pickletools", 
        "test.pickletester", 
        "test.test_support"
      ]
    }, 
    "test.test_pipes": {
      "file": "test/test_pipes.py", 
      "imports": [
        "os", 
        "pipes", 
        "string", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_pkg": {
      "file": "test/test_pkg.py", 
      "imports": [
        "os", 
        "sys", 
        "t1", 
        "t2.sub", 
        "t2.sub.subsub", 
        "t2.sub.subsub.spam", 
        "t3.sub.subsub", 
        "t5", 
        "t6", 
        "t7", 
        "t7.sub", 
        "t7.sub.subsub", 
        "t7.sub.subsub.spam", 
        "t8", 
        "tempfile", 
        "test.test_support", 
        "textwrap", 
        "unittest"
      ]
    }, 
    "test.test_pkgimport": {
      "file": "test/test_pkgimport.py", 
      "imports": [
        "os", 
        "random", 
        "string", 
        "sys", 
        "tempfile", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_pkgutil": {
      "file": "test/test_pkgutil.py", 
      "imports": [
        "foo", 
        "imp", 
        "os", 
        "pkgutil", 
        "shutil", 
        "sys", 
        "tempfile", 
        "test.test_support", 
        "zipimport", 
        "unittest", 
        "zipfile"
      ]
    }, 
    "test.test_platform": {
      "file": "test/test_platform.py", 
      "imports": [
        "gestalt", 
        "os", 
        "platform", 
        "subprocess", 
        "sys", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_plistlib": {
      "file": "test/test_plistlib.py", 
      "imports": [
        "StringIO", 
        "cStringIO.StringIO", 
        "os", 
        "plistlib", 
        "test.test_support", 
        "unittest", 
        "datetime"
      ]
    }, 
    "test.test_poll": {
      "file": "test/test_poll.py", 
      "imports": [
        "os", 
        "random", 
        "select", 
        "test.test_support", 
        "threading", 
        "time", 
        "unittest", 
        "_testcapi"
      ]
    }, 
    "test.test_popen": {
      "file": "test/test_popen.py", 
      "imports": [
        "os", 
        "sys", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_popen2": {
      "file": "test/test_popen2.py", 
      "imports": [
        "os", 
        "popen2", 
        "subprocess", 
        "sys", 
        "test.test_support", 
        "unittest", 
        "warnings"
      ]
    }, 
    "test.test_poplib": {
      "file": "test/test_poplib.py", 
      "imports": [
        "asynchat", 
        "asyncore", 
        "errno", 
        "os", 
        "poplib", 
        "socket", 
        "ssl", 
        "test.test_support", 
        "time", 
        "unittest"
      ]
    }, 
    "test.test_posix": {
      "file": "test/test_posix.py", 
      "imports": [
        "errno", 
        "os", 
        "platform", 
        "shutil", 
        "stat", 
        "sys", 
        "sysconfig", 
        "tempfile", 
        "test.test_support", 
        "time", 
        "unittest", 
        "warnings", 
        "pwd"
      ]
    }, 
    "test.test_posixpath": {
      "file": "test/test_posixpath.py", 
      "imports": [
        "os", 
        "posixpath", 
        "test.test_genericpath", 
        "test.test_support", 
        "unittest", 
        "pwd"
      ]
    }, 
    "test.test_pow": {
      "file": "test/test_pow.py", 
      "imports": [
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_pprint": {
      "file": "test/test_pprint.py", 
      "imports": [
        "pprint", 
        "test.test_set", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_print": {
      "file": "test/test_print.py", 
      "imports": [
        "StringIO", 
        "__future__", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_profile": {
      "file": "test/test_profile.py", 
      "imports": [
        "StringIO", 
        "profile", 
        "pstats", 
        "sys", 
        "test.profilee", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_property": {
      "file": "test/test_property.py", 
      "imports": [
        "sys", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_pstats": {
      "file": "test/test_pstats.py", 
      "imports": [
        "pstats", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_pty": {
      "file": "test/test_pty.py", 
      "imports": [
        "errno", 
        "os", 
        "pty", 
        "select", 
        "signal", 
        "socket", 
        "sys", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_pwd": {
      "file": "test/test_pwd.py", 
      "imports": [
        "sys", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_py3kwarn": {
      "file": "test/test_py3kwarn.py", 
      "imports": [
        "operator.add", 
        "operator.isCallable", 
        "operator.sequenceIncludes", 
        "sys", 
        "test.test_support", 
        "unittest", 
        "UserString", 
        "warnings"
      ]
    }, 
    "test.test_py_compile": {
      "file": "test/test_py_compile.py", 
      "imports": [
        "imp", 
        "os", 
        "py_compile", 
        "shutil", 
        "tempfile", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_pyclbr": {
      "file": "test/test_pyclbr.py", 
      "imports": [
        "commands", 
        "pyclbr", 
        "sys", 
        "test.test_support", 
        "types", 
        "unittest"
      ]
    }, 
    "test.test_pydoc": {
      "file": "test/test_pydoc.py", 
      "imports": [
        "__builtin__", 
        "collections", 
        "contextlib", 
        "difflib", 
        "inspect", 
        "keyword", 
        "nturl2path", 
        "os", 
        "pkgutil", 
        "pydoc", 
        "re", 
        "sys", 
        "test.pydoc_mod", 
        "test.pydocfodder", 
        "test.script_helper", 
        "test.test_support", 
        "types", 
        "unittest", 
        "xml.etree"
      ]
    }, 
    "test.test_pyexpat": {
      "file": "test/test_pyexpat.py", 
      "imports": [
        "StringIO", 
        "sys", 
        "test.test_support", 
        "unittest", 
        "xml.parsers.expat"
      ]
    }, 
    "test.test_queue": {
      "file": "test/test_queue.py", 
      "imports": [
        "Queue", 
        "test.test_support", 
        "time", 
        "unittest"
      ]
    }, 
    "test.test_quopri": {
      "file": "test/test_quopri.py", 
      "imports": [
        "cStringIO", 
        "quopri", 
        "subprocess", 
        "sys", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_random": {
      "file": "test/test_random.py", 
      "imports": [
        "functools", 
        "math.exp", 
        "math.fsum", 
        "math.ldexp", 
        "math.log", 
        "math.pi", 
        "math.sin", 
        "pickle", 
        "random", 
        "sys", 
        "test.test_support", 
        "time", 
        "unittest", 
        "warnings"
      ]
    }, 
    "test.test_re": {
      "file": "test/test_re.py", 
      "imports": [
        "_sre", 
        "_sre.MAXREPEAT", 
        "array", 
        "locale", 
        "pickle", 
        "re", 
        "sre", 
        "sre_constants", 
        "string", 
        "sys", 
        "test.re_tests", 
        "test.test_support", 
        "traceback", 
        "unittest", 
        "weakref", 
        "cPickle"
      ]
    }, 
    "test.test_readline": {
      "file": "test/test_readline.py", 
      "imports": [
        "os", 
        "test.script_helper", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_repr": {
      "file": "test/test_repr.py", 
      "imports": [
        "areallylongpackageandmodulenametotestreprtruncation.areallylongpackageandmodulenametotestreprtruncation.areallylongpackageandmodulenametotestreprtruncation", 
        "areallylongpackageandmodulenametotestreprtruncation.areallylongpackageandmodulenametotestreprtruncation.bar", 
        "areallylongpackageandmodulenametotestreprtruncation.areallylongpackageandmodulenametotestreprtruncation.baz", 
        "areallylongpackageandmodulenametotestreprtruncation.areallylongpackageandmodulenametotestreprtruncation.foo", 
        "areallylongpackageandmodulenametotestreprtruncation.areallylongpackageandmodulenametotestreprtruncation.qux", 
        "array.array", 
        "collections", 
        "os", 
        "repr", 
        "shutil", 
        "sys", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_resource": {
      "file": "test/test_resource.py", 
      "imports": [
        "test.test_support", 
        "time", 
        "unittest"
      ]
    }, 
    "test.test_rfc822": {
      "file": "test/test_rfc822.py", 
      "imports": [
        "StringIO", 
        "cStringIO.StringIO", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_richcmp": {
      "file": "test/test_richcmp.py", 
      "imports": [
        "operator", 
        "random", 
        "test.test_support", 
        "unittest", 
        "UserList"
      ]
    }, 
    "test.test_rlcompleter": {
      "file": "test/test_rlcompleter.py", 
      "imports": [
        "__builtin__", 
        "rlcompleter", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_robotparser": {
      "file": "test/test_robotparser.py", 
      "imports": [
        "StringIO", 
        "robotparser", 
        "test.test_support", 
        "unittest", 
        "urllib2"
      ]
    }, 
    "test.test_runpy": {
      "file": "test/test_runpy.py", 
      "imports": [
        "os", 
        "re", 
        "runpy", 
        "sys", 
        "tempfile", 
        "test.script_helper", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_sax": {
      "file": "test/test_sax.py", 
      "imports": [
        "cStringIO.StringIO", 
        "io", 
        "os", 
        "shutil", 
        "sys", 
        "test.test_support", 
        "unittest", 
        "xml.sax.xmlreader", 
        "xml.sax.saxutils", 
        "xml.sax.handler", 
        "xml.sax.expatreader", 
        "xml.sax"
      ]
    }, 
    "test.test_scope": {
      "file": "test/test_scope.py", 
      "imports": [
        "sys", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_scriptpackages": {
      "file": "test/test_scriptpackages.py", 
      "imports": [
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_select": {
      "file": "test/test_select.py", 
      "imports": [
        "os", 
        "select", 
        "sys", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_set": {
      "file": "test/test_set.py", 
      "imports": [
        "collections", 
        "copy", 
        "gc", 
        "itertools.chain", 
        "itertools.imap", 
        "operator", 
        "pickle", 
        "random", 
        "sys", 
        "test.test_support", 
        "unittest", 
        "weakref"
      ]
    }, 
    "test.test_setcomps": {
      "file": "test/test_setcomps.py", 
      "imports": [
        "gc", 
        "sys", 
        "test.test_setcomps", 
        "test.test_support"
      ]
    }, 
    "test.test_sets": {
      "file": "test/test_sets.py", 
      "imports": [
        "copy", 
        "doctest", 
        "operator", 
        "pickle", 
        "random", 
        "sets", 
        "test.test_sets", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_sgmllib": {
      "file": "test/test_sgmllib.py", 
      "imports": [
        "pprint", 
        "re", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_sha": {
      "file": "test/test_sha.py", 
      "imports": [
        "sha", 
        "test.test_support", 
        "unittest", 
        "warnings"
      ]
    }, 
    "test.test_shelve": {
      "file": "test/test_shelve.py", 
      "imports": [
        "glob", 
        "os", 
        "shelve", 
        "test.mapping_tests", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_shlex": {
      "file": "test/test_shlex.py", 
      "imports": [
        "StringIO", 
        "cStringIO.StringIO", 
        "shlex", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_shutil": {
      "file": "test/test_shutil.py", 
      "imports": [
        "distutils.spawn", 
        "errno", 
        "os", 
        "shutil", 
        "stat", 
        "sys", 
        "tarfile", 
        "tempfile", 
        "test.test_support", 
        "zlib", 
        "unittest", 
        "warnings", 
        "zipfile", 
        "grp", 
        "pwd"
      ]
    }, 
    "test.test_signal": {
      "file": "test/test_signal.py", 
      "imports": [
        "contextlib", 
        "errno", 
        "fcntl", 
        "gc", 
        "os", 
        "pickle", 
        "select", 
        "signal", 
        "subprocess", 
        "sys", 
        "test.test_support", 
        "time", 
        "traceback", 
        "unittest"
      ]
    }, 
    "test.test_site": {
      "file": "test/test_site.py", 
      "imports": [
        "__builtin__", 
        "copy", 
        "encodings", 
        "locale", 
        "os", 
        "re", 
        "site", 
        "sitecustomize", 
        "subprocess", 
        "sys", 
        "sysconfig", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_slice": {
      "file": "test/test_slice.py", 
      "imports": [
        "sys", 
        "test.test_support", 
        "unittest", 
        "cPickle"
      ]
    }, 
    "test.test_smtplib": {
      "file": "test/test_smtplib.py", 
      "imports": [
        "StringIO", 
        "asyncore", 
        "email.utils", 
        "select", 
        "smtpd", 
        "smtplib", 
        "socket", 
        "sys", 
        "test.test_support", 
        "threading", 
        "time", 
        "unittest"
      ]
    }, 
    "test.test_smtpnet": {
      "file": "test/test_smtpnet.py", 
      "imports": [
        "smtplib", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_socket": {
      "file": "test/test_socket.py", 
      "imports": [
        "Queue", 
        "_socket", 
        "array", 
        "contextlib", 
        "errno", 
        "itertools", 
        "math", 
        "os", 
        "select", 
        "signal", 
        "socket", 
        "sys", 
        "test.test_support", 
        "thread", 
        "threading", 
        "time", 
        "traceback", 
        "unittest", 
        "weakref", 
        "_testcapi"
      ]
    }, 
    "test.test_socketserver": {
      "file": "test/test_socketserver.py", 
      "imports": [
        "SocketServer", 
        "contextlib", 
        "errno", 
        "imp", 
        "os", 
        "select", 
        "signal", 
        "socket", 
        "tempfile", 
        "test.test_support", 
        "threading", 
        "unittest"
      ]
    }, 
    "test.test_softspace": {
      "file": "test/test_softspace.py", 
      "imports": [
        "StringIO", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_sort": {
      "file": "test/test_sort.py", 
      "imports": [
        "gc", 
        "random", 
        "sys", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_spwd": {
      "file": "test/test_spwd.py", 
      "imports": [
        "os", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_sqlite": {
      "file": "test/test_sqlite.py", 
      "imports": [
        "sqlite3.test.dbapi", 
        "sqlite3.test.dump", 
        "sqlite3.test.factory", 
        "sqlite3.test.hooks", 
        "sqlite3.test.py25tests", 
        "sqlite3.test.regression", 
        "sqlite3.test.transactions", 
        "sqlite3.test.types", 
        "sqlite3.test.userfunctions", 
        "test.test_support"
      ]
    }, 
    "test.test_ssl": {
      "file": "test/test_ssl.py", 
      "imports": [
        "asyncore", 
        "contextlib", 
        "errno", 
        "functools", 
        "gc", 
        "os", 
        "platform", 
        "pprint", 
        "select", 
        "socket", 
        "sys", 
        "tempfile", 
        "test.ssl_servers", 
        "test.test_support", 
        "threading", 
        "time", 
        "traceback", 
        "unittest", 
        "urllib2", 
        "weakref", 
        "datetime"
      ]
    }, 
    "test.test_startfile": {
      "file": "test/test_startfile.py", 
      "imports": [
        "os", 
        "test.test_support", 
        "time.sleep", 
        "unittest"
      ]
    }, 
    "test.test_stat": {
      "file": "test/test_stat.py", 
      "imports": [
        "os", 
        "stat", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_str": {
      "file": "test/test_str.py", 
      "imports": [
        "struct", 
        "sys", 
        "test.string_tests", 
        "test.test_support", 
        "unittest", 
        "_testcapi", 
        "datetime"
      ]
    }, 
    "test.test_strftime": {
      "file": "test/test_strftime.py", 
      "imports": [
        "calendar", 
        "java", 
        "locale", 
        "re", 
        "sys", 
        "test.test_support", 
        "time", 
        "unittest"
      ]
    }, 
    "test.test_string": {
      "file": "test/test_string.py", 
      "imports": [
        "string", 
        "test.string_tests", 
        "test.test_support", 
        "unittest", 
        "UserList"
      ]
    }, 
    "test.test_stringprep": {
      "file": "test/test_stringprep.py", 
      "imports": [
        "stringprep", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_strop": {
      "file": "test/test_strop.py", 
      "imports": [
        "strop", 
        "sys", 
        "test.test_support", 
        "unittest", 
        "warnings"
      ]
    }, 
    "test.test_strptime": {
      "file": "test/test_strptime.py", 
      "imports": [
        "_strptime", 
        "locale", 
        "re", 
        "sys", 
        "test.test_support", 
        "time", 
        "unittest", 
        "datetime"
      ]
    }, 
    "test.test_strtod": {
      "file": "test/test_strtod.py", 
      "imports": [
        "random", 
        "re", 
        "struct", 
        "sys", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_struct": {
      "file": "test/test_struct.py", 
      "imports": [
        "array", 
        "binascii", 
        "inspect", 
        "math", 
        "os", 
        "random", 
        "struct", 
        "sys", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_structmembers": {
      "file": "test/test_structmembers.py", 
      "imports": [
        "test.test_support", 
        "unittest", 
        "_testcapi"
      ]
    }, 
    "test.test_structseq": {
      "file": "test/test_structseq.py", 
      "imports": [
        "test.test_support", 
        "time", 
        "unittest"
      ]
    }, 
    "test.test_subprocess": {
      "file": "test/test_subprocess.py", 
      "imports": [
        "errno", 
        "os", 
        "re", 
        "signal", 
        "subprocess", 
        "sys", 
        "sysconfig", 
        "tempfile", 
        "test.test_support", 
        "threading", 
        "time", 
        "unittest", 
        "resource"
      ]
    }, 
    "test.test_sunau": {
      "file": "test/test_sunau.py", 
      "imports": [
        "sunau", 
        "sys", 
        "test.audiotests", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_sunaudiodev": {
      "file": "test/test_sunaudiodev.py", 
      "imports": [
        "os", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_sundry": {
      "file": "test/test_sundry.py", 
      "imports": [
        "CGIHTTPServer", 
        "audiodev", 
        "bdb", 
        "cgitb", 
        "code", 
        "compileall", 
        "distutils.bcppcompiler", 
        "distutils.ccompiler", 
        "distutils.command.bdist", 
        "distutils.command.bdist_dumb", 
        "distutils.command.bdist_msi", 
        "distutils.command.bdist_rpm", 
        "distutils.command.bdist_wininst", 
        "distutils.command.build", 
        "distutils.command.build_clib", 
        "distutils.command.build_ext", 
        "distutils.command.clean", 
        "distutils.command.config", 
        "distutils.command.install_data", 
        "distutils.command.install_egg_info", 
        "distutils.command.install_headers", 
        "distutils.command.install_lib", 
        "distutils.command.register", 
        "distutils.command.sdist", 
        "distutils.command.upload", 
        "distutils.cygwinccompiler", 
        "distutils.emxccompiler", 
        "distutils.filelist", 
        "distutils.msvccompiler", 
        "distutils.text_file", 
        "distutils.unixccompiler", 
        "encodings", 
        "formatter", 
        "getpass", 
        "htmlentitydefs", 
        "ihooks", 
        "imputil", 
        "keyword", 
        "linecache", 
        "mailcap", 
        "mimify", 
        "nntplib", 
        "nturl2path", 
        "opcode", 
        "os2emxpath", 
        "pdb", 
        "posixfile", 
        "pstats", 
        "py_compile", 
        "rexec", 
        "sched", 
        "sndhdr", 
        "statvfs", 
        "stringold", 
        "sunau", 
        "sunaudio", 
        "symbol", 
        "sys", 
        "tabnanny", 
        "test.test_support", 
        "token", 
        "timeit", 
        "toaiff", 
        "tty", 
        "unittest", 
        "webbrowser", 
        "xml"
      ]
    }, 
    "test.test_support": {
      "file": "test/test_support.py", 
      "imports": [
        "StringIO", 
        "Tkinter.Tk", 
        "contextlib", 
        "ctypes", 
        "ctypes.Structure", 
        "ctypes.c_int", 
        "ctypes.cdll", 
        "ctypes.pointer", 
        "ctypes.util.find_library", 
        "ctypes.wintypes", 
        "doctest", 
        "errno", 
        "functools", 
        "gc", 
        "importlib", 
        "locale", 
        "os", 
        "pdb", 
        "platform", 
        "re", 
        "shutil", 
        "socket", 
        "struct", 
        "subprocess", 
        "sys", 
        "sysconfig", 
        "test", 
        "thread", 
        "time", 
        "traceback", 
        "unittest", 
        "urllib2", 
        "urlparse", 
        "UserDict", 
        "warnings", 
        "_testcapi"
      ]
    }, 
    "test.test_symtable": {
      "file": "test/test_symtable.py", 
      "imports": [
        "symtable", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_syntax": {
      "file": "test/test_syntax.py", 
      "imports": [
        "re", 
        "test.test_support", 
        "test.test_syntax", 
        "unittest", 
        "warnings"
      ]
    }, 
    "test.test_sys": {
      "file": "test/test_sys.py", 
      "imports": [
        "__builtin__", 
        "_ast", 
        "cStringIO", 
        "codecs", 
        "encodings.iso8859_3", 
        "imp", 
        "inspect", 
        "operator", 
        "os", 
        "re", 
        "struct", 
        "subprocess", 
        "sys", 
        "test.script_helper", 
        "test.test_support", 
        "thread", 
        "threading", 
        "traceback", 
        "types", 
        "unittest", 
        "weakref", 
        "_testcapi", 
        "datetime"
      ]
    }, 
    "test.test_sys_setprofile": {
      "file": "test/test_sys_setprofile.py", 
      "imports": [
        "gc", 
        "pprint", 
        "sys", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_sys_settrace": {
      "file": "test/test_sys_settrace.py", 
      "imports": [
        "difflib", 
        "gc", 
        "sys", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_sysconfig": {
      "file": "test/test_sysconfig.py", 
      "imports": [
        "_osx_support", 
        "copy", 
        "os", 
        "shutil", 
        "subprocess", 
        "sys", 
        "sysconfig", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_tarfile": {
      "file": "test/test_tarfile.py", 
      "imports": [
        "StringIO", 
        "bz2", 
        "errno", 
        "gzip", 
        "hashlib", 
        "os", 
        "shutil", 
        "sys", 
        "tarfile", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_tcl": {
      "file": "test/test_tcl.py", 
      "imports": [
        "Tkinter.Tcl", 
        "_tkinter.TclError", 
        "os", 
        "subprocess.PIPE", 
        "subprocess.Popen", 
        "sys", 
        "test.test_support", 
        "unittest", 
        "_testcapi"
      ]
    }, 
    "test.test_telnetlib": {
      "file": "test/test_telnetlib.py", 
      "imports": [
        "Queue", 
        "socket", 
        "telnetlib", 
        "test.test_support", 
        "time", 
        "unittest"
      ]
    }, 
    "test.test_tempfile": {
      "file": "test/test_tempfile.py", 
      "imports": [
        "contextlib", 
        "errno", 
        "io", 
        "os", 
        "re", 
        "shutil", 
        "signal", 
        "stat", 
        "sys", 
        "tempfile", 
        "test.test_support", 
        "unittest", 
        "warnings"
      ]
    }, 
    "test.test_textwrap": {
      "file": "test/test_textwrap.py", 
      "imports": [
        "test.test_support", 
        "textwrap", 
        "unittest"
      ]
    }, 
    "test.test_thread": {
      "file": "test/test_thread.py", 
      "imports": [
        "os", 
        "random", 
        "sys", 
        "test.lock_tests", 
        "test.test_support", 
        "time", 
        "unittest", 
        "weakref"
      ]
    }, 
    "test.test_threaded_import": {
      "file": "test/test_threaded_import.py", 
      "imports": [
        "imp", 
        "random", 
        "sys", 
        "test.test_support", 
        "test.threaded_import_hangers", 
        "unittest"
      ]
    }, 
    "test.test_threadedtempfile": {
      "file": "test/test_threadedtempfile.py", 
      "imports": [
        "StringIO", 
        "tempfile", 
        "test.test_support", 
        "traceback", 
        "unittest"
      ]
    }, 
    "test.test_threading": {
      "file": "test/test_threading.py", 
      "imports": [
        "ctypes", 
        "os", 
        "random", 
        "re", 
        "subprocess", 
        "sys", 
        "test.lock_tests", 
        "test.script_helper", 
        "test.test_support", 
        "time", 
        "unittest", 
        "weakref", 
        "_testcapi"
      ]
    }, 
    "test.test_threading_local": {
      "file": "test/test_threading_local.py", 
      "imports": [
        "_threading_local", 
        "doctest", 
        "gc", 
        "test.test_support", 
        "thread._local", 
        "time", 
        "unittest", 
        "weakref"
      ]
    }, 
    "test.test_threadsignals": {
      "file": "test/test_threadsignals.py", 
      "imports": [
        "os", 
        "signal", 
        "sys", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_time": {
      "file": "test/test_time.py", 
      "imports": [
        "os", 
        "sys", 
        "test.test_support", 
        "time", 
        "unittest"
      ]
    }, 
    "test.test_timeout": {
      "file": "test/test_timeout.py", 
      "imports": [
        "socket", 
        "test.test_support", 
        "time", 
        "unittest"
      ]
    }, 
    "test.test_tk": {
      "file": "test/test_tk.py", 
      "imports": [
        "os", 
        "runtktests", 
        "test.test_support"
      ]
    }, 
    "test.test_tokenize": {
      "file": "test/test_tokenize.py", 
      "imports": [
        "StringIO", 
        "os", 
        "test.test_support", 
        "test.test_tokenize", 
        "tokenize", 
        "unittest"
      ]
    }, 
    "test.test_tools": {
      "file": "test/test_tools.py", 
      "imports": [
        "os", 
        "shutil", 
        "subprocess", 
        "sys", 
        "sysconfig", 
        "tempfile", 
        "test.script_helper", 
        "test.test_support", 
        "textwrap", 
        "unittest"
      ]
    }, 
    "test.test_trace": {
      "file": "test/test_trace.py", 
      "imports": [
        "os", 
        "sys", 
        "test.test_support", 
        "test.tracedmodules.testmod", 
        "trace", 
        "unittest"
      ]
    }, 
    "test.test_traceback": {
      "file": "test/test_traceback.py", 
      "imports": [
        "StringIO", 
        "imp.reload", 
        "os", 
        "sys", 
        "tempfile", 
        "test.badsyntax_nocaret", 
        "test.test_support", 
        "test_bug737473", 
        "time", 
        "traceback", 
        "unittest", 
        "_testcapi"
      ]
    }, 
    "test.test_transformer": {
      "file": "test/test_transformer.py", 
      "imports": [
        "compiler", 
        "compiler.ast", 
        "compiler.transformer", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_ttk_guionly": {
      "file": "test/test_ttk_guionly.py", 
      "imports": [
        "Tkinter", 
        "_tkinter.TclError", 
        "os", 
        "runtktests", 
        "test.test_support", 
        "ttk", 
        "unittest"
      ]
    }, 
    "test.test_ttk_textonly": {
      "file": "test/test_ttk_textonly.py", 
      "imports": [
        "os", 
        "runtktests", 
        "test.test_support"
      ]
    }, 
    "test.test_tuple": {
      "file": "test/test_tuple.py", 
      "imports": [
        "gc", 
        "test.seq_tests", 
        "test.test_support"
      ]
    }, 
    "test.test_typechecks": {
      "file": "test/test_typechecks.py", 
      "imports": [
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_types": {
      "file": "test/test_types.py", 
      "imports": [
        "array", 
        "locale", 
        "sys", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_ucn": {
      "file": "test/test_ucn.py", 
      "imports": [
        "sys", 
        "test.test_support", 
        "unicodedata", 
        "unittest", 
        "_testcapi"
      ]
    }, 
    "test.test_unary": {
      "file": "test/test_unary.py", 
      "imports": [
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_undocumented_details": {
      "file": "test/test_undocumented_details.py", 
      "imports": [
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_unicode": {
      "file": "test/test_unicode.py", 
      "imports": [
        "codecs", 
        "ctypes.c_int", 
        "ctypes.c_long", 
        "ctypes.c_longlong", 
        "ctypes.c_size_t", 
        "ctypes.c_ssize_t", 
        "ctypes.c_uint", 
        "ctypes.c_ulong", 
        "ctypes.c_ulonglong", 
        "ctypes.c_void_p", 
        "ctypes.py_object", 
        "ctypes.pythonapi", 
        "ctypes.sizeof", 
        "imp", 
        "struct", 
        "sys", 
        "test.string_tests", 
        "test.test_support", 
        "unittest", 
        "_testcapi", 
        "datetime"
      ]
    }, 
    "test.test_unicode_file": {
      "file": "test/test_unicode_file.py", 
      "imports": [
        "glob", 
        "os", 
        "shutil", 
        "sys", 
        "test.test_support", 
        "time", 
        "unicodedata", 
        "unittest"
      ]
    }, 
    "test.test_unicodedata": {
      "file": "test/test_unicodedata.py", 
      "imports": [
        "hashlib", 
        "subprocess", 
        "sys", 
        "test.test_support", 
        "unicodedata", 
        "unittest"
      ]
    }, 
    "test.test_unittest": {
      "file": "test/test_unittest.py", 
      "imports": [
        "test.test_support", 
        "unittest.test"
      ]
    }, 
    "test.test_univnewlines": {
      "file": "test/test_univnewlines.py", 
      "imports": [
        "__future__", 
        "_pyio", 
        "io", 
        "os", 
        "sys", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_univnewlines2k": {
      "file": "test/test_univnewlines2k.py", 
      "imports": [
        "os", 
        "sys", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_unpack": {
      "file": "test/test_unpack.py", 
      "imports": [
        "test.test_support", 
        "test.test_unpack"
      ]
    }, 
    "test.test_urllib": {
      "file": "test/test_urllib.py", 
      "imports": [
        "StringIO", 
        "base64", 
        "httplib", 
        "mimetools", 
        "os", 
        "sys", 
        "tempfile", 
        "test.test_support", 
        "unittest", 
        "urllib", 
        "warnings"
      ]
    }, 
    "test.test_urllib2": {
      "file": "test/test_urllib2.py", 
      "imports": [
        "StringIO", 
        "base64", 
        "cookielib", 
        "copy", 
        "ftplib", 
        "httplib", 
        "mimetools", 
        "os", 
        "rfc822", 
        "socket", 
        "ssl", 
        "string", 
        "test.test_cookielib", 
        "test.test_support", 
        "test.test_urllib2", 
        "unittest", 
        "urllib", 
        "urllib2"
      ]
    }, 
    "test.test_urllib2_localnet": {
      "file": "test/test_urllib2_localnet.py", 
      "imports": [
        "BaseHTTPServer", 
        "base64", 
        "hashlib", 
        "os", 
        "ssl", 
        "test.ssl_servers", 
        "test.test_support", 
        "unittest", 
        "urllib2", 
        "urlparse"
      ]
    }, 
    "test.test_urllib2net": {
      "file": "test/test_urllib2net.py", 
      "imports": [
        "httplib", 
        "logging", 
        "os", 
        "socket", 
        "sys", 
        "test.test_support", 
        "test.test_urllib2", 
        "time", 
        "unittest", 
        "urllib2"
      ]
    }, 
    "test.test_urllibnet": {
      "file": "test/test_urllibnet.py", 
      "imports": [
        "os", 
        "socket", 
        "ssl", 
        "sys", 
        "test.test_support", 
        "time", 
        "unittest", 
        "urllib"
      ]
    }, 
    "test.test_urlparse": {
      "file": "test/test_urlparse.py", 
      "imports": [
        "test.test_support", 
        "unittest", 
        "urlparse"
      ]
    }, 
    "test.test_userdict": {
      "file": "test/test_userdict.py", 
      "imports": [
        "test.mapping_tests", 
        "test.test_support", 
        "UserDict"
      ]
    }, 
    "test.test_userlist": {
      "file": "test/test_userlist.py", 
      "imports": [
        "test.list_tests", 
        "test.test_support", 
        "UserList"
      ]
    }, 
    "test.test_userstring": {
      "file": "test/test_userstring.py", 
      "imports": [
        "string", 
        "test.string_tests", 
        "test.test_support", 
        "UserString", 
        "warnings"
      ]
    }, 
    "test.test_uu": {
      "file": "test/test_uu.py", 
      "imports": [
        "cStringIO", 
        "codecs", 
        "os", 
        "sys", 
        "test.test_support", 
        "unittest", 
        "uu"
      ]
    }, 
    "test.test_uuid": {
      "file": "test/test_uuid.py", 
      "imports": [
        "io", 
        "os", 
        "test.test_support", 
        "unittest", 
        "uuid"
      ]
    }, 
    "test.test_wait3": {
      "file": "test/test_wait3.py", 
      "imports": [
        "os", 
        "test.fork_wait", 
        "test.test_support", 
        "time", 
        "unittest"
      ]
    }, 
    "test.test_wait4": {
      "file": "test/test_wait4.py", 
      "imports": [
        "os", 
        "sys", 
        "test.fork_wait", 
        "test.test_support", 
        "time"
      ]
    }, 
    "test.test_warnings": {
      "file": "test/test_warnings.py", 
      "imports": [
        "StringIO", 
        "contextlib", 
        "linecache", 
        "os", 
        "subprocess", 
        "sys", 
        "test.script_helper", 
        "test.test_support", 
        "test.warning_tests", 
        "unittest", 
        "warnings"
      ]
    }, 
    "test.test_wave": {
      "file": "test/test_wave.py", 
      "imports": [
        "sys", 
        "test.audiotests", 
        "test.test_support", 
        "wave", 
        "unittest"
      ]
    }, 
    "test.test_weakref": {
      "file": "test/test_weakref.py", 
      "imports": [
        "contextlib", 
        "copy", 
        "gc", 
        "operator", 
        "sys", 
        "test.mapping_tests", 
        "test.test_support", 
        "unittest", 
        "UserList", 
        "weakref"
      ]
    }, 
    "test.test_weakset": {
      "file": "test/test_weakset.py", 
      "imports": [
        "collections", 
        "contextlib", 
        "copy", 
        "gc", 
        "operator", 
        "os", 
        "random", 
        "string", 
        "sys", 
        "test.test_support", 
        "unittest", 
        "warnings", 
        "weakref"
      ]
    }, 
    "test.test_whichdb": {
      "file": "test/test_whichdb.py", 
      "imports": [
        "glob", 
        "os", 
        "test.test_support", 
        "unittest", 
        "whichdb"
      ]
    }, 
    "test.test_winreg": {
      "file": "test/test_winreg.py", 
      "imports": [
        "_winreg.*", 
        "errno", 
        "os", 
        "platform", 
        "sys", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_winsound": {
      "file": "test/test_winsound.py", 
      "imports": [
        "_winreg", 
        "os", 
        "subprocess", 
        "test.test_support", 
        "time", 
        "unittest"
      ]
    }, 
    "test.test_with": {
      "file": "test/test_with.py", 
      "imports": [
        "collections", 
        "contextlib", 
        "sys", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_wsgiref": {
      "file": "test/test_wsgiref.py", 
      "imports": [
        "SocketServer", 
        "StringIO", 
        "__future__", 
        "os", 
        "re", 
        "sys", 
        "test.test_support", 
        "unittest", 
        "wsgiref.validate", 
        "wsgiref.util", 
        "wsgiref.simple_server", 
        "wsgiref.headers", 
        "wsgiref.handlers"
      ]
    }, 
    "test.test_xdrlib": {
      "file": "test/test_xdrlib.py", 
      "imports": [
        "test.test_support", 
        "unittest", 
        "xdrlib"
      ]
    }, 
    "test.test_xml_etree": {
      "file": "test/test_xml_etree.py", 
      "imports": [
        "StringIO", 
        "cgi", 
        "sys", 
        "test.test_support", 
        "test.test_xml_etree", 
        "xml.etree.ElementTree", 
        "xml.etree.ElementPath"
      ]
    }, 
    "test.test_xml_etree_c": {
      "file": "test/test_xml_etree_c.py", 
      "imports": [
        "test.test_support", 
        "test.test_xml_etree", 
        "test.test_xml_etree_c", 
        "unittest"
      ]
    }, 
    "test.test_xmllib": {
      "file": "test/test_xmllib.py", 
      "imports": [
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_xmlrpc": {
      "file": "test/test_xmlrpc.py", 
      "imports": [
        "SimpleXMLRPCServer", 
        "StringIO", 
        "base64", 
        "gzip", 
        "httplib", 
        "mimetools", 
        "os", 
        "re", 
        "socket", 
        "sys", 
        "test.test_support", 
        "threading", 
        "time", 
        "unittest", 
        "xmlrpclib", 
        "datetime"
      ]
    }, 
    "test.test_xpickle": {
      "file": "test/test_xpickle.py", 
      "imports": [
        "os", 
        "pickle", 
        "subprocess", 
        "sys", 
        "test.test_support", 
        "types", 
        "unittest", 
        "cPickle"
      ]
    }, 
    "test.test_xrange": {
      "file": "test/test_xrange.py", 
      "imports": [
        "itertools", 
        "pickle", 
        "sys", 
        "test.test_support", 
        "unittest", 
        "warnings"
      ]
    }, 
    "test.test_zipfile": {
      "file": "test/test_zipfile.py", 
      "imports": [
        "StringIO", 
        "email", 
        "io", 
        "os", 
        "random", 
        "struct", 
        "sys", 
        "tempfile", 
        "test.test_support", 
        "time", 
        "zlib", 
        "unittest", 
        "zipfile"
      ]
    }, 
    "test.test_zipfile64": {
      "file": "test/test_zipfile64.py", 
      "imports": [
        "os", 
        "sys", 
        "tempfile", 
        "test.test_support", 
        "time", 
        "zlib", 
        "unittest", 
        "zipfile"
      ]
    }, 
    "test.test_zipimport": {
      "file": "test/test_zipimport.py", 
      "imports": [
        "StringIO", 
        "doctest", 
        "imp", 
        "inspect", 
        "linecache", 
        "marshal", 
        "os", 
        "struct", 
        "sys", 
        "test.test_importhooks", 
        "test.test_support", 
        "time", 
        "zipimport", 
        "zlib", 
        "traceback", 
        "unittest", 
        "zipfile"
      ]
    }, 
    "test.test_zipimport_support": {
      "file": "test/test_zipimport_support.py", 
      "imports": [
        "doctest", 
        "inspect", 
        "linecache", 
        "os", 
        "pdb", 
        "sys", 
        "test.sample_doctest", 
        "test.sample_doctest_no_docstrings", 
        "test.sample_doctest_no_doctests", 
        "test.script_helper", 
        "test.test_doctest", 
        "test.test_importhooks", 
        "test.test_support", 
        "test_zipped_doctest", 
        "zip_pkg", 
        "zipimport", 
        "textwrap", 
        "warnings", 
        "zipfile"
      ]
    }, 
    "test.test_zlib": {
      "file": "test/test_zlib.py", 
      "imports": [
        "binascii", 
        "mmap", 
        "os", 
        "random", 
        "sys", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.testall": {
      "file": "test/testall.py", 
      "imports": [
        "sys", 
        "test.regrtest", 
        "warnings"
      ]
    }, 
    "test.testcodec": {
      "file": "test/testcodec.py", 
      "imports": [
        "codecs"
      ]
    }, 
    "test.tf_inherit_check": {
      "file": "test/tf_inherit_check.py", 
      "imports": [
        "os", 
        "sys"
      ]
    }, 
    "test.threaded_import_hangers": {
      "file": "test/threaded_import_hangers.py", 
      "imports": [
        "os", 
        "tempfile", 
        "threading"
      ]
    }, 
    "test.time_hashlib": {
      "file": "test/time_hashlib.py", 
      "imports": [
        "_hashlib", 
        "hashlib", 
        "sys", 
        "time"
      ]
    }, 
    "test.tracedmodules": {
      "dir": "test/tracedmodules"
    }, 
    "test.tracedmodules.__init__": {
      "file": "test/tracedmodules/__init__.py", 
      "imports": []
    }, 
    "test.tracedmodules.testmod": {
      "file": "test/tracedmodules/testmod.py", 
      "imports": []
    }, 
    "test.warning_tests": {
      "file": "test/warning_tests.py", 
      "imports": [
        "warnings"
      ]
    }, 
    "test.win_console_handler": {
      "file": "test/win_console_handler.py", 
      "imports": [
        "ctypes", 
        "ctypes.WINFUNCTYPE", 
        "ctypes.wintypes", 
        "mmap", 
        "signal", 
        "sys"
      ]
    }, 
    "test.xmltests": {
      "file": "test/xmltests.py", 
      "imports": [
        "sys", 
        "test.test_support"
      ]
    }, 
    "textwrap": {
      "file": "textwrap.py", 
      "imports": [
        "re", 
        "string"
      ]
    }, 
    "this": {
      "file": "this.py", 
      "imports": []
    }, 
    "threading": {
      "file": "threading.py", 
      "imports": []
    }, 
    "timeit": {
      "file": "timeit.py", 
      "imports": [
        "gc", 
        "getopt", 
        "linecache", 
        "os", 
        "sys", 
        "time", 
        "traceback"
      ]
    }, 
    "toaiff": {
      "file": "toaiff.py", 
      "imports": [
        "os", 
        "pipes", 
        "sndhdr", 
        "tempfile", 
        "warnings"
      ]
    }, 
    "token": {
      "file": "token.py", 
      "imports": [
        "re", 
        "sys"
      ]
    }, 
    "tokenize": {
      "file": "tokenize.py", 
      "imports": [
        "itertools.chain", 
        "re", 
        "string", 
        "sys", 
        "token"
      ]
    }, 
    "tputil": {
      "file": "tputil.py", 
      "imports": [
        "__pypy__.tproxy", 
        "types"
      ]
    }, 
    "trace": {
      "file": "trace.py", 
      "imports": [
        "__main__", 
        "dis", 
        "gc", 
        "getopt", 
        "inspect", 
        "linecache", 
        "os", 
        "pickle", 
        "re", 
        "sys", 
        "threading", 
        "time", 
        "token", 
        "tokenize", 
        "cPickle"
      ]
    }, 
    "traceback": {
      "file": "traceback.py", 
      "imports": [
        "linecache", 
        "sys", 
        "types"
      ]
    }, 
    "tty": {
      "file": "tty.py", 
      "imports": [
        "termios.*"
      ]
    }, 
    "types": {
      "file": "types.py", 
      "imports": [
        "sys"
      ]
    }, 
    "urllib": {
      "file": "urllib.py", 
      "imports": [
        "StringIO", 
        "_winreg", 
        "base64", 
        "cStringIO.StringIO", 
        "email.utils", 
        "fnmatch", 
        "ftplib", 
        "getpass", 
        "httplib", 
        "mimetools", 
        "mimetypes", 
        "nturl2path", 
        "os", 
        "re", 
        "rourl2path.pathname2url", 
        "rourl2path.url2pathname", 
        "socket", 
        "ssl", 
        "string", 
        "sys", 
        "tempfile", 
        "time", 
        "urlparse", 
        "warnings", 
        "_scproxy"
      ]
    }, 
    "urllib2": {
      "file": "urllib2.py", 
      "imports": [
        "StringIO", 
        "base64", 
        "bisect", 
        "cStringIO.StringIO", 
        "cookielib", 
        "email.utils", 
        "ftplib", 
        "hashlib", 
        "httplib", 
        "mimetools", 
        "mimetypes", 
        "os", 
        "posixpath", 
        "random", 
        "re", 
        "socket", 
        "ssl", 
        "sys", 
        "time", 
        "types", 
        "urllib", 
        "urlparse", 
        "warnings"
      ]
    }, 
    "urlparse": {
      "file": "urlparse.py", 
      "imports": [
        "collections", 
        "re"
      ]
    }, 
    "user": {
      "file": "user.py", 
      "imports": [
        "os", 
        "warnings"
      ]
    }, 
    "uu": {
      "file": "uu.py", 
      "imports": [
        "binascii", 
        "optparse", 
        "os", 
        "sys"
      ]
    }, 
    "uuid": {
      "file": "uuid.py", 
      "imports": [
        "ctypes", 
        "ctypes.util", 
        "hashlib", 
        "netbios", 
        "os", 
        "random", 
        "re", 
        "socket", 
        "struct", 
        "sys", 
        "time", 
        "win32wnet"
      ]
    }, 
    "warnings": {
      "file": "warnings.py", 
      "imports": [
        "_warnings.default_action", 
        "_warnings.filters", 
        "_warnings.once_registry", 
        "_warnings.warn", 
        "_warnings.warn_explicit", 
        "linecache", 
        "re", 
        "sys", 
        "types"
      ]
    }, 
    "weakref": {
      "file": "weakref.py", 
      "imports": [
        "UserDict", 
        "_weakref.CallableProxyType", 
        "_weakref.ProxyType", 
        "_weakref.ReferenceType", 
        "_weakref.getweakrefcount", 
        "_weakref.getweakrefs", 
        "_weakref.proxy", 
        "_weakref.ref", 
        "_weakrefset", 
        "copy", 
        "exceptions.ReferenceError"
      ]
    }, 
    "webbrowser": {
      "file": "webbrowser.py", 
      "imports": [
        "copy", 
        "getopt", 
        "glob", 
        "os", 
        "shlex", 
        "socket", 
        "stat", 
        "subprocess", 
        "sys", 
        "tempfile", 
        "time", 
        "pwd"
      ]
    }, 
    "whichdb": {
      "file": "whichdb.py", 
      "imports": [
        "os", 
        "struct", 
        "sys", 
        "dbm"
      ]
    }, 
    "wsgiref": {
      "dir": "wsgiref"
    }, 
    "wsgiref.__init__": {
      "file": "wsgiref/__init__.py", 
      "imports": []
    }, 
    "wsgiref.handlers": {
      "file": "wsgiref/handlers.py", 
      "imports": [
        "os", 
        "sys", 
        "time", 
        "traceback", 
        "types", 
        "wsgiref.headers", 
        "wsgiref.util"
      ]
    }, 
    "wsgiref.headers": {
      "file": "wsgiref/headers.py", 
      "imports": [
        "re", 
        "types"
      ]
    }, 
    "wsgiref.simple_server": {
      "file": "wsgiref/simple_server.py", 
      "imports": [
        "BaseHTTPServer", 
        "StringIO", 
        "sys", 
        "urllib", 
        "webbrowser", 
        "wsgiref.handlers"
      ]
    }, 
    "wsgiref.util": {
      "file": "wsgiref/util.py", 
      "imports": [
        "StringIO", 
        "posixpath", 
        "urllib"
      ]
    }, 
    "wsgiref.validate": {
      "file": "wsgiref/validate.py", 
      "imports": [
        "re", 
        "sys", 
        "types", 
        "warnings"
      ]
    }, 
    "xdrlib": {
      "file": "xdrlib.py", 
      "imports": [
        "StringIO", 
        "cStringIO.StringIO", 
        "functools", 
        "struct"
      ]
    }, 
    "xml": {
      "dir": "xml"
    }, 
    "xml.__init__": {
      "file": "xml/__init__.py", 
      "imports": [
        "_xmlplus", 
        "sys"
      ]
    }, 
    "xml.dom": {
      "dir": "xml/dom"
    }, 
    "xml.dom.NodeFilter": {
      "file": "xml/dom/NodeFilter.py", 
      "imports": []
    }, 
    "xml.dom.__init__": {
      "file": "xml/dom/__init__.py", 
      "imports": [
        "xml.dom.domreg"
      ]
    }, 
    "xml.dom.domreg": {
      "file": "xml/dom/domreg.py", 
      "imports": [
        "os", 
        "xml.dom.minicompat"
      ]
    }, 
    "xml.dom.expatbuilder": {
      "file": "xml/dom/expatbuilder.py", 
      "imports": [
        "xml.dom", 
        "xml.dom.NodeFilter", 
        "xml.dom.minicompat", 
        "xml.dom.minidom", 
        "xml.dom.xmlbuilder", 
        "xml.parsers.expat"
      ]
    }, 
    "xml.dom.minicompat": {
      "file": "xml/dom/minicompat.py", 
      "imports": [
        "xml.dom"
      ]
    }, 
    "xml.dom.minidom": {
      "file": "xml/dom/minidom.py", 
      "imports": [
        "StringIO", 
        "codecs", 
        "xml.dom", 
        "xml.dom.domreg", 
        "xml.dom.expatbuilder", 
        "xml.dom.minicompat", 
        "xml.dom.pulldom", 
        "xml.dom.xmlbuilder"
      ]
    }, 
    "xml.dom.pulldom": {
      "file": "xml/dom/pulldom.py", 
      "imports": [
        "StringIO", 
        "cStringIO.StringIO", 
        "types", 
        "xml.dom", 
        "xml.dom.minidom", 
        "xml.sax", 
        "xml.sax.handler"
      ]
    }, 
    "xml.dom.xmlbuilder": {
      "file": "xml/dom/xmlbuilder.py", 
      "imports": [
        "copy", 
        "posixpath", 
        "urllib2", 
        "urlparse", 
        "xml.dom", 
        "xml.dom.NodeFilter", 
        "xml.dom.expatbuilder"
      ]
    }, 
    "xml.etree": {
      "dir": "xml/etree"
    }, 
    "xml.etree.ElementInclude": {
      "file": "xml/etree/ElementInclude.py", 
      "imports": [
        "copy", 
        "xml.etree.ElementTree"
      ]
    }, 
    "xml.etree.ElementPath": {
      "file": "xml/etree/ElementPath.py", 
      "imports": [
        "re"
      ]
    }, 
    "xml.etree.ElementTree": {
      "file": "xml/etree/ElementTree.py", 
      "imports": [
        "ElementC14N._serialize_c14n", 
        "pyexpat", 
        "re", 
        "sys", 
        "warnings", 
        "xml.etree.ElementPath", 
        "xml.parsers.expat"
      ]
    }, 
    "xml.etree.__init__": {
      "file": "xml/etree/__init__.py", 
      "imports": []
    }, 
    "xml.etree.cElementTree": {
      "file": "xml/etree/cElementTree.py", 
      "imports": [
        "_elementtree"
      ]
    }, 
    "xml.parsers": {
      "dir": "xml/parsers"
    }, 
    "xml.parsers.__init__": {
      "file": "xml/parsers/__init__.py", 
      "imports": []
    }, 
    "xml.parsers.expat": {
      "file": "xml/parsers/expat.py", 
      "imports": [
        "pyexpat.*"
      ]
    }, 
    "xml.sax": {
      "dir": "xml/sax"
    }, 
    "xml.sax.__init__": {
      "file": "xml/sax/__init__.py", 
      "imports": [
        "StringIO", 
        "cStringIO.StringIO", 
        "org.python.core.imp", 
        "os", 
        "sys", 
        "xml.sax._exceptions", 
        "xml.sax.expatreader", 
        "xml.sax.handler", 
        "xml.sax.xmlreader"
      ]
    }, 
    "xml.sax._exceptions": {
      "file": "xml/sax/_exceptions.py", 
      "imports": [
        "java.lang.Exception", 
        "sys"
      ]
    }, 
    "xml.sax.expatreader": {
      "file": "xml/sax/expatreader.py", 
      "imports": [
        "_weakref", 
        "sys", 
        "weakref", 
        "xml.parsers.expat", 
        "xml.sax._exceptions", 
        "xml.sax.handler", 
        "xml.sax.saxutils", 
        "xml.sax.xmlreader"
      ]
    }, 
    "xml.sax.handler": {
      "file": "xml/sax/handler.py", 
      "imports": []
    }, 
    "xml.sax.saxutils": {
      "file": "xml/sax/saxutils.py", 
      "imports": [
        "io", 
        "os", 
        "sys", 
        "types", 
        "urllib", 
        "urlparse", 
        "xml.sax.handler", 
        "xml.sax.xmlreader"
      ]
    }, 
    "xml.sax.xmlreader": {
      "file": "xml/sax/xmlreader.py", 
      "imports": [
        "xml.sax._exceptions", 
        "xml.sax.handler", 
        "xml.sax.saxutils"
      ]
    }, 
    "xmllib": {
      "file": "xmllib.py", 
      "imports": [
        "getopt", 
        "re", 
        "string", 
        "sys", 
        "time.time", 
        "warnings"
      ]
    }, 
    "xmlrpclib": {
      "file": "xmlrpclib.py", 
      "imports": [
        "StringIO", 
        "_xmlrpclib", 
        "base64", 
        "cStringIO", 
        "errno", 
        "gzip", 
        "httplib", 
        "operator", 
        "re", 
        "socket", 
        "string", 
        "sys.modules", 
        "time", 
        "types", 
        "urllib", 
        "xml.parsers.expat", 
        "xmllib", 
        "datetime"
      ]
    }, 
    "zipfile": {
      "file": "zipfile.py", 
      "imports": [
        "binascii", 
        "cStringIO", 
        "io", 
        "os", 
        "py_compile", 
        "re", 
        "shutil", 
        "stat", 
        "string", 
        "struct", 
        "sys", 
        "textwrap", 
        "time", 
        "warnings", 
        "zlib"
      ]
    }
  }, 
  "preload": {
    "StringIO": "r\"\"\"File-like objects that read from or write to a string buffer.\n\nThis implements (nearly) all stdio methods.\n\nf = StringIO()      # ready for writing\nf = StringIO(buf)   # ready for reading\nf.close()           # explicitly release resources held\nflag = f.isatty()   # always false\npos = f.tell()      # get current position\nf.seek(pos)         # set current position\nf.seek(pos, mode)   # mode 0: absolute; 1: relative; 2: relative to EOF\nbuf = f.read()      # read until EOF\nbuf = f.read(n)     # read up to n bytes\nbuf = f.readline()  # read until end of line ('\\n') or EOF\nlist = f.readlines()# list of f.readline() results until EOF\nf.truncate([size])  # truncate file at to at most size (default: current pos)\nf.write(buf)        # write at current position\nf.writelines(list)  # for line in list: f.write(line)\nf.getvalue()        # return whole file's contents as a string\n\nNotes:\n- Using a real file is often faster (but less convenient).\n- There's also a much faster implementation in C, called cStringIO, but\n  it's not subclassable.\n- fileno() is left unimplemented so that code which uses it triggers\n  an exception early.\n- Seeking far beyond EOF and then writing will insert real null\n  bytes that occupy space in the buffer.\n- There's a simple test set (see end of this file).\n\"\"\"\ntry:\n    from errno import EINVAL\nexcept ImportError:\n    EINVAL = 22\n\n__all__ = [\"StringIO\"]\n\ndef _complain_ifclosed(closed):\n    if closed:\n        raise ValueError, \"I/O operation on closed file\"\n\nclass StringIO:\n    \"\"\"class StringIO([buffer])\n\n    When a StringIO object is created, it can be initialized to an existing\n    string by passing the string to the constructor. If no string is given,\n    the StringIO will start empty.\n\n    The StringIO object can accept either Unicode or 8-bit strings, but\n    mixing the two may take some care. If both are used, 8-bit strings that\n    cannot be interpreted as 7-bit ASCII (that use the 8th bit) will cause\n    a UnicodeError to be raised when getvalue() is called.\n    \"\"\"\n    def __init__(self, buf = ''):\n        # Force self.buf to be a string or unicode\n        if not isinstance(buf, basestring):\n            buf = str(buf)\n        self.buf = buf\n        self.len = len(buf)\n        self.buflist = []\n        self.pos = 0\n        self.closed = False\n        self.softspace = 0\n\n    def __iter__(self):\n        return self\n\n    def next(self):\n        \"\"\"A file object is its own iterator, for example iter(f) returns f\n        (unless f is closed). When a file is used as an iterator, typically\n        in a for loop (for example, for line in f: print line), the next()\n        method is called repeatedly. This method returns the next input line,\n        or raises StopIteration when EOF is hit.\n        \"\"\"\n        _complain_ifclosed(self.closed)\n        r = self.readline()\n        if not r:\n            raise StopIteration\n        return r\n\n    def close(self):\n        \"\"\"Free the memory buffer.\n        \"\"\"\n        if not self.closed:\n            self.closed = True\n            del self.buf, self.pos\n\n    def isatty(self):\n        \"\"\"Returns False because StringIO objects are not connected to a\n        tty-like device.\n        \"\"\"\n        _complain_ifclosed(self.closed)\n        return False\n\n    def seek(self, pos, mode = 0):\n        \"\"\"Set the file's current position.\n\n        The mode argument is optional and defaults to 0 (absolute file\n        positioning); other values are 1 (seek relative to the current\n        position) and 2 (seek relative to the file's end).\n\n        There is no return value.\n        \"\"\"\n        _complain_ifclosed(self.closed)\n        if self.buflist:\n            self.buf += ''.join(self.buflist)\n            self.buflist = []\n        if mode == 1:\n            pos += self.pos\n        elif mode == 2:\n            pos += self.len\n        self.pos = max(0, pos)\n\n    def tell(self):\n        \"\"\"Return the file's current position.\"\"\"\n        _complain_ifclosed(self.closed)\n        return self.pos\n\n    def read(self, n = -1):\n        \"\"\"Read at most size bytes from the file\n        (less if the read hits EOF before obtaining size bytes).\n\n        If the size argument is negative or omitted, read all data until EOF\n        is reached. The bytes are returned as a string object. An empty\n        string is returned when EOF is encountered immediately.\n        \"\"\"\n        _complain_ifclosed(self.closed)\n        if self.buflist:\n            self.buf += ''.join(self.buflist)\n            self.buflist = []\n        if n is None or n < 0:\n            newpos = self.len\n        else:\n            newpos = min(self.pos+n, self.len)\n        r = self.buf[self.pos:newpos]\n        self.pos = newpos\n        return r\n\n    def readline(self, length=None):\n        r\"\"\"Read one entire line from the file.\n\n        A trailing newline character is kept in the string (but may be absent\n        when a file ends with an incomplete line). If the size argument is\n        present and non-negative, it is a maximum byte count (including the\n        trailing newline) and an incomplete line may be returned.\n\n        An empty string is returned only when EOF is encountered immediately.\n\n        Note: Unlike stdio's fgets(), the returned string contains null\n        characters ('\\0') if they occurred in the input.\n        \"\"\"\n        _complain_ifclosed(self.closed)\n        if self.buflist:\n            self.buf += ''.join(self.buflist)\n            self.buflist = []\n        i = self.buf.find('\\n', self.pos)\n        if i < 0:\n            newpos = self.len\n        else:\n            newpos = i+1\n        if length is not None and length >= 0:\n            if self.pos + length < newpos:\n                newpos = self.pos + length\n        r = self.buf[self.pos:newpos]\n        self.pos = newpos\n        return r\n\n    def readlines(self, sizehint = 0):\n        \"\"\"Read until EOF using readline() and return a list containing the\n        lines thus read.\n\n        If the optional sizehint argument is present, instead of reading up\n        to EOF, whole lines totalling approximately sizehint bytes (or more\n        to accommodate a final whole line).\n        \"\"\"\n        total = 0\n        lines = []\n        line = self.readline()\n        while line:\n            lines.append(line)\n            total += len(line)\n            if 0 < sizehint <= total:\n                break\n            line = self.readline()\n        return lines\n\n    def truncate(self, size=None):\n        \"\"\"Truncate the file's size.\n\n        If the optional size argument is present, the file is truncated to\n        (at most) that size. The size defaults to the current position.\n        The current file position is not changed unless the position\n        is beyond the new file size.\n\n        If the specified size exceeds the file's current size, the\n        file remains unchanged.\n        \"\"\"\n        _complain_ifclosed(self.closed)\n        if size is None:\n            size = self.pos\n        elif size < 0:\n            raise IOError(EINVAL, \"Negative size not allowed\")\n        elif size < self.pos:\n            self.pos = size\n        self.buf = self.getvalue()[:size]\n        self.len = size\n\n    def write(self, s):\n        \"\"\"Write a string to the file.\n\n        There is no return value.\n        \"\"\"\n        _complain_ifclosed(self.closed)\n        if not s: return\n        # Force s to be a string or unicode\n        if not isinstance(s, basestring):\n            s = str(s)\n        spos = self.pos\n        slen = self.len\n        if spos == slen:\n            self.buflist.append(s)\n            self.len = self.pos = spos + len(s)\n            return\n        if spos > slen:\n            self.buflist.append('\\0'*(spos - slen))\n            slen = spos\n        newpos = spos + len(s)\n        if spos < slen:\n            if self.buflist:\n                self.buf += ''.join(self.buflist)\n            self.buflist = [self.buf[:spos], s, self.buf[newpos:]]\n            self.buf = ''\n            if newpos > slen:\n                slen = newpos\n        else:\n            self.buflist.append(s)\n            slen = newpos\n        self.len = slen\n        self.pos = newpos\n\n    def writelines(self, iterable):\n        \"\"\"Write a sequence of strings to the file. The sequence can be any\n        iterable object producing strings, typically a list of strings. There\n        is no return value.\n\n        (The name is intended to match readlines(); writelines() does not add\n        line separators.)\n        \"\"\"\n        write = self.write\n        for line in iterable:\n            write(line)\n\n    def flush(self):\n        \"\"\"Flush the internal buffer\n        \"\"\"\n        _complain_ifclosed(self.closed)\n\n    def getvalue(self):\n        \"\"\"\n        Retrieve the entire contents of the \"file\" at any time before\n        the StringIO object's close() method is called.\n\n        The StringIO object can accept either Unicode or 8-bit strings,\n        but mixing the two may take some care. If both are used, 8-bit\n        strings that cannot be interpreted as 7-bit ASCII (that use the\n        8th bit) will cause a UnicodeError to be raised when getvalue()\n        is called.\n        \"\"\"\n        _complain_ifclosed(self.closed)\n        if self.buflist:\n            self.buf += ''.join(self.buflist)\n            self.buflist = []\n        return self.buf\n\n\n# A little test suite\n\ndef test():\n    import sys\n    if sys.argv[1:]:\n        file = sys.argv[1]\n    else:\n        file = '/etc/passwd'\n    lines = open(file, 'r').readlines()\n    text = open(file, 'r').read()\n    f = StringIO()\n    for line in lines[:-2]:\n        f.write(line)\n    f.writelines(lines[-2:])\n    if f.getvalue() != text:\n        raise RuntimeError, 'write failed'\n    length = f.tell()\n    print 'File length =', length\n    f.seek(len(lines[0]))\n    f.write(lines[1])\n    f.seek(0)\n    print 'First line =', repr(f.readline())\n    print 'Position =', f.tell()\n    line = f.readline()\n    print 'Second line =', repr(line)\n    f.seek(-len(line), 1)\n    line2 = f.read(len(line))\n    if line != line2:\n        raise RuntimeError, 'bad result after seek back'\n    f.seek(len(line2), 1)\n    list = f.readlines()\n    line = list[-1]\n    f.seek(f.tell() - len(line))\n    line2 = f.read()\n    if line != line2:\n        raise RuntimeError, 'bad result after seek back from EOF'\n    print 'Read', len(list), 'more lines'\n    print 'File length =', f.tell()\n    if f.tell() != length:\n        raise RuntimeError, 'bad length'\n    f.truncate(length/2)\n    f.seek(0, 2)\n    print 'Truncated length =', f.tell()\n    if f.tell() != length/2:\n        raise RuntimeError, 'truncate did not adjust length'\n    f.close()\n\nif __name__ == '__main__':\n    test()\n", 
    "UserDict": "\"\"\"A more or less complete user-defined wrapper around dictionary objects.\"\"\"\n\nclass UserDict:\n    def __init__(self, dict=None, **kwargs):\n        self.data = {}\n        if dict is not None:\n            self.update(dict)\n        if len(kwargs):\n            self.update(kwargs)\n    def __repr__(self): return repr(self.data)\n    def __cmp__(self, dict):\n        if isinstance(dict, UserDict):\n            return cmp(self.data, dict.data)\n        else:\n            return cmp(self.data, dict)\n    __hash__ = None # Avoid Py3k warning\n    def __len__(self): return len(self.data)\n    def __getitem__(self, key):\n        if key in self.data:\n            return self.data[key]\n        if hasattr(self.__class__, \"__missing__\"):\n            return self.__class__.__missing__(self, key)\n        raise KeyError(key)\n    def __setitem__(self, key, item): self.data[key] = item\n    def __delitem__(self, key): del self.data[key]\n    def clear(self): self.data.clear()\n    def copy(self):\n        if self.__class__ is UserDict:\n            return UserDict(self.data.copy())\n        import copy\n        data = self.data\n        try:\n            self.data = {}\n            c = copy.copy(self)\n        finally:\n            self.data = data\n        c.update(self)\n        return c\n    def keys(self): return self.data.keys()\n    def items(self): return self.data.items()\n    def iteritems(self): return self.data.iteritems()\n    def iterkeys(self): return self.data.iterkeys()\n    def itervalues(self): return self.data.itervalues()\n    def values(self): return self.data.values()\n    def has_key(self, key): return key in self.data\n    def update(self, dict=None, **kwargs):\n        if dict is None:\n            pass\n        elif isinstance(dict, UserDict):\n            self.data.update(dict.data)\n        elif isinstance(dict, type({})) or not hasattr(dict, 'items'):\n            self.data.update(dict)\n        else:\n            for k, v in dict.items():\n                self[k] = v\n        if len(kwargs):\n            self.data.update(kwargs)\n    def get(self, key, failobj=None):\n        if key not in self:\n            return failobj\n        return self[key]\n    def setdefault(self, key, failobj=None):\n        if key not in self:\n            self[key] = failobj\n        return self[key]\n    def pop(self, key, *args):\n        return self.data.pop(key, *args)\n    def popitem(self):\n        return self.data.popitem()\n    def __contains__(self, key):\n        return key in self.data\n    @classmethod\n    def fromkeys(cls, iterable, value=None):\n        d = cls()\n        for key in iterable:\n            d[key] = value\n        return d\n\nclass IterableUserDict(UserDict):\n    def __iter__(self):\n        return iter(self.data)\n\ntry:\n    import _abcoll\nexcept ImportError:\n    pass    # e.g. no '_weakref' module on this pypy\nelse:\n    _abcoll.MutableMapping.register(IterableUserDict)\n\n\nclass DictMixin:\n    # Mixin defining all dictionary methods for classes that already have\n    # a minimum dictionary interface including getitem, setitem, delitem,\n    # and keys. Without knowledge of the subclass constructor, the mixin\n    # does not define __init__() or copy().  In addition to the four base\n    # methods, progressively more efficiency comes with defining\n    # __contains__(), __iter__(), and iteritems().\n\n    # second level definitions support higher levels\n    def __iter__(self):\n        for k in self.keys():\n            yield k\n    def has_key(self, key):\n        try:\n            self[key]\n        except KeyError:\n            return False\n        return True\n    def __contains__(self, key):\n        return self.has_key(key)\n\n    # third level takes advantage of second level definitions\n    def iteritems(self):\n        for k in self:\n            yield (k, self[k])\n    def iterkeys(self):\n        return self.__iter__()\n\n    # fourth level uses definitions from lower levels\n    def itervalues(self):\n        for _, v in self.iteritems():\n            yield v\n    def values(self):\n        return [v for _, v in self.iteritems()]\n    def items(self):\n        return list(self.iteritems())\n    def clear(self):\n        for key in self.keys():\n            del self[key]\n    def setdefault(self, key, default=None):\n        try:\n            return self[key]\n        except KeyError:\n            self[key] = default\n        return default\n    def pop(self, key, *args):\n        if len(args) > 1:\n            raise TypeError, \"pop expected at most 2 arguments, got \"\\\n                              + repr(1 + len(args))\n        try:\n            value = self[key]\n        except KeyError:\n            if args:\n                return args[0]\n            raise\n        del self[key]\n        return value\n    def popitem(self):\n        try:\n            k, v = self.iteritems().next()\n        except StopIteration:\n            raise KeyError, 'container is empty'\n        del self[k]\n        return (k, v)\n    def update(self, other=None, **kwargs):\n        # Make progressively weaker assumptions about \"other\"\n        if other is None:\n            pass\n        elif hasattr(other, 'iteritems'):  # iteritems saves memory and lookups\n            for k, v in other.iteritems():\n                self[k] = v\n        elif hasattr(other, 'keys'):\n            for k in other.keys():\n                self[k] = other[k]\n        else:\n            for k, v in other:\n                self[k] = v\n        if kwargs:\n            self.update(kwargs)\n    def get(self, key, default=None):\n        try:\n            return self[key]\n        except KeyError:\n            return default\n    def __repr__(self):\n        return repr(dict(self.iteritems()))\n    def __cmp__(self, other):\n        if other is None:\n            return 1\n        if isinstance(other, DictMixin):\n            other = dict(other.iteritems())\n        return cmp(dict(self.iteritems()), other)\n    def __len__(self):\n        return len(self.keys())\n", 
    "__future__": "\"\"\"Record of phased-in incompatible language changes.\n\nEach line is of the form:\n\n    FeatureName = \"_Feature(\" OptionalRelease \",\" MandatoryRelease \",\"\n                              CompilerFlag \")\"\n\nwhere, normally, OptionalRelease < MandatoryRelease, and both are 5-tuples\nof the same form as sys.version_info:\n\n    (PY_MAJOR_VERSION, # the 2 in 2.1.0a3; an int\n     PY_MINOR_VERSION, # the 1; an int\n     PY_MICRO_VERSION, # the 0; an int\n     PY_RELEASE_LEVEL, # \"alpha\", \"beta\", \"candidate\" or \"final\"; string\n     PY_RELEASE_SERIAL # the 3; an int\n    )\n\nOptionalRelease records the first release in which\n\n    from __future__ import FeatureName\n\nwas accepted.\n\nIn the case of MandatoryReleases that have not yet occurred,\nMandatoryRelease predicts the release in which the feature will become part\nof the language.\n\nElse MandatoryRelease records when the feature became part of the language;\nin releases at or after that, modules no longer need\n\n    from __future__ import FeatureName\n\nto use the feature in question, but may continue to use such imports.\n\nMandatoryRelease may also be None, meaning that a planned feature got\ndropped.\n\nInstances of class _Feature have two corresponding methods,\n.getOptionalRelease() and .getMandatoryRelease().\n\nCompilerFlag is the (bitfield) flag that should be passed in the fourth\nargument to the builtin function compile() to enable the feature in\ndynamically compiled code.  This flag is stored in the .compiler_flag\nattribute on _Future instances.  These values must match the appropriate\n#defines of CO_xxx flags in Include/compile.h.\n\nNo feature line is ever to be deleted from this file.\n\"\"\"\n\nall_feature_names = [\n    \"nested_scopes\",\n    \"generators\",\n    \"division\",\n    \"absolute_import\",\n    \"with_statement\",\n    \"print_function\",\n    \"unicode_literals\",\n]\n\n__all__ = [\"all_feature_names\"] + all_feature_names\n\n# The CO_xxx symbols are defined here under the same names used by\n# compile.h, so that an editor search will find them here.  However,\n# they're not exported in __all__, because they don't really belong to\n# this module.\nCO_NESTED            = 0x0010   # nested_scopes\nCO_GENERATOR_ALLOWED = 0        # generators (obsolete, was 0x1000)\nCO_FUTURE_DIVISION   = 0x2000   # division\nCO_FUTURE_ABSOLUTE_IMPORT = 0x4000 # perform absolute imports by default\nCO_FUTURE_WITH_STATEMENT  = 0x8000   # with statement\nCO_FUTURE_PRINT_FUNCTION  = 0x10000   # print function\nCO_FUTURE_UNICODE_LITERALS = 0x20000 # unicode string literals\n\nclass _Feature:\n    def __init__(self, optionalRelease, mandatoryRelease, compiler_flag):\n        self.optional = optionalRelease\n        self.mandatory = mandatoryRelease\n        self.compiler_flag = compiler_flag\n\n    def getOptionalRelease(self):\n        \"\"\"Return first release in which this feature was recognized.\n\n        This is a 5-tuple, of the same form as sys.version_info.\n        \"\"\"\n\n        return self.optional\n\n    def getMandatoryRelease(self):\n        \"\"\"Return release in which this feature will become mandatory.\n\n        This is a 5-tuple, of the same form as sys.version_info, or, if\n        the feature was dropped, is None.\n        \"\"\"\n\n        return self.mandatory\n\n    def __repr__(self):\n        return \"_Feature\" + repr((self.optional,\n                                  self.mandatory,\n                                  self.compiler_flag))\n\nnested_scopes = _Feature((2, 1, 0, \"beta\",  1),\n                         (2, 2, 0, \"alpha\", 0),\n                         CO_NESTED)\n\ngenerators = _Feature((2, 2, 0, \"alpha\", 1),\n                      (2, 3, 0, \"final\", 0),\n                      CO_GENERATOR_ALLOWED)\n\ndivision = _Feature((2, 2, 0, \"alpha\", 2),\n                    (3, 0, 0, \"alpha\", 0),\n                    CO_FUTURE_DIVISION)\n\nabsolute_import = _Feature((2, 5, 0, \"alpha\", 1),\n                           (3, 0, 0, \"alpha\", 0),\n                           CO_FUTURE_ABSOLUTE_IMPORT)\n\nwith_statement = _Feature((2, 5, 0, \"alpha\", 1),\n                          (2, 6, 0, \"alpha\", 0),\n                          CO_FUTURE_WITH_STATEMENT)\n\nprint_function = _Feature((2, 6, 0, \"alpha\", 2),\n                          (3, 0, 0, \"alpha\", 0),\n                          CO_FUTURE_PRINT_FUNCTION)\n\nunicode_literals = _Feature((2, 6, 0, \"alpha\", 2),\n                            (3, 0, 0, \"alpha\", 0),\n                            CO_FUTURE_UNICODE_LITERALS)\n", 
    "_abcoll": "# Copyright 2007 Google, Inc. All Rights Reserved.\n# Licensed to PSF under a Contributor Agreement.\n\n\"\"\"Abstract Base Classes (ABCs) for collections, according to PEP 3119.\n\nDON'T USE THIS MODULE DIRECTLY!  The classes here should be imported\nvia collections; they are defined here only to alleviate certain\nbootstrapping issues.  Unit tests are in test_collections.\n\"\"\"\n\nfrom abc import ABCMeta, abstractmethod\nimport sys\n\n__all__ = [\"Hashable\", \"Iterable\", \"Iterator\",\n           \"Sized\", \"Container\", \"Callable\",\n           \"Set\", \"MutableSet\",\n           \"Mapping\", \"MutableMapping\",\n           \"MappingView\", \"KeysView\", \"ItemsView\", \"ValuesView\",\n           \"Sequence\", \"MutableSequence\",\n           ]\n\n### ONE-TRICK PONIES ###\n\ndef _hasattr(C, attr):\n    try:\n        return any(attr in B.__dict__ for B in C.__mro__)\n    except AttributeError:\n        # Old-style class\n        return hasattr(C, attr)\n\n\nclass Hashable:\n    __metaclass__ = ABCMeta\n\n    @abstractmethod\n    def __hash__(self):\n        return 0\n\n    @classmethod\n    def __subclasshook__(cls, C):\n        if cls is Hashable:\n            try:\n                for B in C.__mro__:\n                    if \"__hash__\" in B.__dict__:\n                        if B.__dict__[\"__hash__\"]:\n                            return True\n                        break\n            except AttributeError:\n                # Old-style class\n                if getattr(C, \"__hash__\", None):\n                    return True\n        return NotImplemented\n\n\nclass Iterable:\n    __metaclass__ = ABCMeta\n\n    @abstractmethod\n    def __iter__(self):\n        while False:\n            yield None\n\n    @classmethod\n    def __subclasshook__(cls, C):\n        if cls is Iterable:\n            if _hasattr(C, \"__iter__\"):\n                return True\n        return NotImplemented\n\nIterable.register(str)\n\n\nclass Iterator(Iterable):\n\n    @abstractmethod\n    def next(self):\n        'Return the next item from the iterator. When exhausted, raise StopIteration'\n        raise StopIteration\n\n    def __iter__(self):\n        return self\n\n    @classmethod\n    def __subclasshook__(cls, C):\n        if cls is Iterator:\n            if _hasattr(C, \"next\") and _hasattr(C, \"__iter__\"):\n                return True\n        return NotImplemented\n\n\nclass Sized:\n    __metaclass__ = ABCMeta\n\n    @abstractmethod\n    def __len__(self):\n        return 0\n\n    @classmethod\n    def __subclasshook__(cls, C):\n        if cls is Sized:\n            if _hasattr(C, \"__len__\"):\n                return True\n        return NotImplemented\n\n\nclass Container:\n    __metaclass__ = ABCMeta\n\n    @abstractmethod\n    def __contains__(self, x):\n        return False\n\n    @classmethod\n    def __subclasshook__(cls, C):\n        if cls is Container:\n            if _hasattr(C, \"__contains__\"):\n                return True\n        return NotImplemented\n\n\nclass Callable:\n    __metaclass__ = ABCMeta\n\n    @abstractmethod\n    def __call__(self, *args, **kwds):\n        return False\n\n    @classmethod\n    def __subclasshook__(cls, C):\n        if cls is Callable:\n            if _hasattr(C, \"__call__\"):\n                return True\n        return NotImplemented\n\n\n### SETS ###\n\n\nclass Set(Sized, Iterable, Container):\n    \"\"\"A set is a finite, iterable container.\n\n    This class provides concrete generic implementations of all\n    methods except for __contains__, __iter__ and __len__.\n\n    To override the comparisons (presumably for speed, as the\n    semantics are fixed), redefine __le__ and __ge__,\n    then the other operations will automatically follow suit.\n    \"\"\"\n\n    def __le__(self, other):\n        if not isinstance(other, Set):\n            return NotImplemented\n        if len(self) > len(other):\n            return False\n        for elem in self:\n            if elem not in other:\n                return False\n        return True\n\n    def __lt__(self, other):\n        if not isinstance(other, Set):\n            return NotImplemented\n        return len(self) < len(other) and self.__le__(other)\n\n    def __gt__(self, other):\n        if not isinstance(other, Set):\n            return NotImplemented\n        return len(self) > len(other) and self.__ge__(other)\n\n    def __ge__(self, other):\n        if not isinstance(other, Set):\n            return NotImplemented\n        if len(self) < len(other):\n            return False\n        for elem in other:\n            if elem not in self:\n                return False\n        return True\n\n    def __eq__(self, other):\n        if not isinstance(other, Set):\n            return NotImplemented\n        return len(self) == len(other) and self.__le__(other)\n\n    def __ne__(self, other):\n        return not (self == other)\n\n    @classmethod\n    def _from_iterable(cls, it):\n        '''Construct an instance of the class from any iterable input.\n\n        Must override this method if the class constructor signature\n        does not accept an iterable for an input.\n        '''\n        return cls(it)\n\n    def __and__(self, other):\n        if not isinstance(other, Iterable):\n            return NotImplemented\n        return self._from_iterable(value for value in other if value in self)\n\n    __rand__ = __and__\n\n    def isdisjoint(self, other):\n        'Return True if two sets have a null intersection.'\n        for value in other:\n            if value in self:\n                return False\n        return True\n\n    def __or__(self, other):\n        if not isinstance(other, Iterable):\n            return NotImplemented\n        chain = (e for s in (self, other) for e in s)\n        return self._from_iterable(chain)\n\n    __ror__ = __or__\n\n    def __sub__(self, other):\n        if not isinstance(other, Set):\n            if not isinstance(other, Iterable):\n                return NotImplemented\n            other = self._from_iterable(other)\n        return self._from_iterable(value for value in self\n                                   if value not in other)\n\n    def __rsub__(self, other):\n        if not isinstance(other, Set):\n            if not isinstance(other, Iterable):\n                return NotImplemented\n            other = self._from_iterable(other)\n        return self._from_iterable(value for value in other\n                                   if value not in self)\n\n    def __xor__(self, other):\n        if not isinstance(other, Set):\n            if not isinstance(other, Iterable):\n                return NotImplemented\n            other = self._from_iterable(other)\n        return (self - other) | (other - self)\n\n    __rxor__ = __xor__\n\n    # Sets are not hashable by default, but subclasses can change this\n    __hash__ = None\n\n    def _hash(self):\n        \"\"\"Compute the hash value of a set.\n\n        Note that we don't define __hash__: not all sets are hashable.\n        But if you define a hashable set type, its __hash__ should\n        call this function.\n\n        This must be compatible __eq__.\n\n        All sets ought to compare equal if they contain the same\n        elements, regardless of how they are implemented, and\n        regardless of the order of the elements; so there's not much\n        freedom for __eq__ or __hash__.  We match the algorithm used\n        by the built-in frozenset type.\n        \"\"\"\n        MAX = sys.maxint\n        MASK = 2 * MAX + 1\n        n = len(self)\n        h = 1927868237 * (n + 1)\n        h &= MASK\n        for x in self:\n            hx = hash(x)\n            h ^= (hx ^ (hx << 16) ^ 89869747)  * 3644798167\n            h &= MASK\n        h = h * 69069 + 907133923\n        h &= MASK\n        if h > MAX:\n            h -= MASK + 1\n        if h == -1:\n            h = 590923713\n        return h\n\nSet.register(frozenset)\n\n\nclass MutableSet(Set):\n    \"\"\"A mutable set is a finite, iterable container.\n\n    This class provides concrete generic implementations of all\n    methods except for __contains__, __iter__, __len__,\n    add(), and discard().\n\n    To override the comparisons (presumably for speed, as the\n    semantics are fixed), all you have to do is redefine __le__ and\n    then the other operations will automatically follow suit.\n    \"\"\"\n\n    @abstractmethod\n    def add(self, value):\n        \"\"\"Add an element.\"\"\"\n        raise NotImplementedError\n\n    @abstractmethod\n    def discard(self, value):\n        \"\"\"Remove an element.  Do not raise an exception if absent.\"\"\"\n        raise NotImplementedError\n\n    def remove(self, value):\n        \"\"\"Remove an element. If not a member, raise a KeyError.\"\"\"\n        if value not in self:\n            raise KeyError(value)\n        self.discard(value)\n\n    def pop(self):\n        \"\"\"Return the popped value.  Raise KeyError if empty.\"\"\"\n        it = iter(self)\n        try:\n            value = next(it)\n        except StopIteration:\n            raise KeyError\n        self.discard(value)\n        return value\n\n    def clear(self):\n        \"\"\"This is slow (creates N new iterators!) but effective.\"\"\"\n        try:\n            while True:\n                self.pop()\n        except KeyError:\n            pass\n\n    def __ior__(self, it):\n        for value in it:\n            self.add(value)\n        return self\n\n    def __iand__(self, it):\n        for value in (self - it):\n            self.discard(value)\n        return self\n\n    def __ixor__(self, it):\n        if it is self:\n            self.clear()\n        else:\n            if not isinstance(it, Set):\n                it = self._from_iterable(it)\n            for value in it:\n                if value in self:\n                    self.discard(value)\n                else:\n                    self.add(value)\n        return self\n\n    def __isub__(self, it):\n        if it is self:\n            self.clear()\n        else:\n            for value in it:\n                self.discard(value)\n        return self\n\nMutableSet.register(set)\n\n\n### MAPPINGS ###\n\n\nclass Mapping(Sized, Iterable, Container):\n\n    \"\"\"A Mapping is a generic container for associating key/value\n    pairs.\n\n    This class provides concrete generic implementations of all\n    methods except for __getitem__, __iter__, and __len__.\n\n    \"\"\"\n\n    @abstractmethod\n    def __getitem__(self, key):\n        raise KeyError\n\n    def get(self, key, default=None):\n        'D.get(k[,d]) -> D[k] if k in D, else d.  d defaults to None.'\n        try:\n            return self[key]\n        except KeyError:\n            return default\n\n    def __contains__(self, key):\n        try:\n            self[key]\n        except KeyError:\n            return False\n        else:\n            return True\n\n    def iterkeys(self):\n        'D.iterkeys() -> an iterator over the keys of D'\n        return iter(self)\n\n    def itervalues(self):\n        'D.itervalues() -> an iterator over the values of D'\n        for key in self:\n            yield self[key]\n\n    def iteritems(self):\n        'D.iteritems() -> an iterator over the (key, value) items of D'\n        for key in self:\n            yield (key, self[key])\n\n    def keys(self):\n        \"D.keys() -> list of D's keys\"\n        return list(self)\n\n    def items(self):\n        \"D.items() -> list of D's (key, value) pairs, as 2-tuples\"\n        return [(key, self[key]) for key in self]\n\n    def values(self):\n        \"D.values() -> list of D's values\"\n        return [self[key] for key in self]\n\n    # Mappings are not hashable by default, but subclasses can change this\n    __hash__ = None\n\n    def __eq__(self, other):\n        if not isinstance(other, Mapping):\n            return NotImplemented\n        return dict(self.items()) == dict(other.items())\n\n    def __ne__(self, other):\n        return not (self == other)\n\nclass MappingView(Sized):\n\n    def __init__(self, mapping):\n        self._mapping = mapping\n\n    def __len__(self):\n        return len(self._mapping)\n\n    def __repr__(self):\n        return '{0.__class__.__name__}({0._mapping!r})'.format(self)\n\n\nclass KeysView(MappingView, Set):\n\n    @classmethod\n    def _from_iterable(self, it):\n        return set(it)\n\n    def __contains__(self, key):\n        return key in self._mapping\n\n    def __iter__(self):\n        for key in self._mapping:\n            yield key\n\n\nclass ItemsView(MappingView, Set):\n\n    @classmethod\n    def _from_iterable(self, it):\n        return set(it)\n\n    def __contains__(self, item):\n        key, value = item\n        try:\n            v = self._mapping[key]\n        except KeyError:\n            return False\n        else:\n            return v == value\n\n    def __iter__(self):\n        for key in self._mapping:\n            yield (key, self._mapping[key])\n\n\nclass ValuesView(MappingView):\n\n    def __contains__(self, value):\n        for key in self._mapping:\n            if value == self._mapping[key]:\n                return True\n        return False\n\n    def __iter__(self):\n        for key in self._mapping:\n            yield self._mapping[key]\n\n\nclass MutableMapping(Mapping):\n\n    \"\"\"A MutableMapping is a generic container for associating\n    key/value pairs.\n\n    This class provides concrete generic implementations of all\n    methods except for __getitem__, __setitem__, __delitem__,\n    __iter__, and __len__.\n\n    \"\"\"\n\n    @abstractmethod\n    def __setitem__(self, key, value):\n        raise KeyError\n\n    @abstractmethod\n    def __delitem__(self, key):\n        raise KeyError\n\n    __marker = object()\n\n    def pop(self, key, default=__marker):\n        '''D.pop(k[,d]) -> v, remove specified key and return the corresponding value.\n          If key is not found, d is returned if given, otherwise KeyError is raised.\n        '''\n        try:\n            value = self[key]\n        except KeyError:\n            if default is self.__marker:\n                raise\n            return default\n        else:\n            del self[key]\n            return value\n\n    def popitem(self):\n        '''D.popitem() -> (k, v), remove and return some (key, value) pair\n           as a 2-tuple; but raise KeyError if D is empty.\n        '''\n        try:\n            key = next(iter(self))\n        except StopIteration:\n            raise KeyError\n        value = self[key]\n        del self[key]\n        return key, value\n\n    def clear(self):\n        'D.clear() -> None.  Remove all items from D.'\n        try:\n            while True:\n                self.popitem()\n        except KeyError:\n            pass\n\n    def update(*args, **kwds):\n        ''' D.update([E, ]**F) -> None.  Update D from mapping/iterable E and F.\n            If E present and has a .keys() method, does:     for k in E: D[k] = E[k]\n            If E present and lacks .keys() method, does:     for (k, v) in E: D[k] = v\n            In either case, this is followed by: for k, v in F.items(): D[k] = v\n        '''\n        if len(args) > 2:\n            raise TypeError(\"update() takes at most 2 positional \"\n                            \"arguments ({} given)\".format(len(args)))\n        elif not args:\n            raise TypeError(\"update() takes at least 1 argument (0 given)\")\n        self = args[0]\n        other = args[1] if len(args) >= 2 else ()\n\n        if isinstance(other, Mapping):\n            for key in other:\n                self[key] = other[key]\n        elif hasattr(other, \"keys\"):\n            for key in other.keys():\n                self[key] = other[key]\n        else:\n            for key, value in other:\n                self[key] = value\n        for key, value in kwds.items():\n            self[key] = value\n\n    def setdefault(self, key, default=None):\n        'D.setdefault(k[,d]) -> D.get(k,d), also set D[k]=d if k not in D'\n        try:\n            return self[key]\n        except KeyError:\n            self[key] = default\n        return default\n\nMutableMapping.register(dict)\n\n\n### SEQUENCES ###\n\n\nclass Sequence(Sized, Iterable, Container):\n    \"\"\"All the operations on a read-only sequence.\n\n    Concrete subclasses must override __new__ or __init__,\n    __getitem__, and __len__.\n    \"\"\"\n\n    @abstractmethod\n    def __getitem__(self, index):\n        raise IndexError\n\n    def __iter__(self):\n        i = 0\n        try:\n            while True:\n                v = self[i]\n                yield v\n                i += 1\n        except IndexError:\n            return\n\n    def __contains__(self, value):\n        for v in self:\n            if v == value:\n                return True\n        return False\n\n    def __reversed__(self):\n        for i in reversed(range(len(self))):\n            yield self[i]\n\n    def index(self, value):\n        '''S.index(value) -> integer -- return first index of value.\n           Raises ValueError if the value is not present.\n        '''\n        for i, v in enumerate(self):\n            if v == value:\n                return i\n        raise ValueError\n\n    def count(self, value):\n        'S.count(value) -> integer -- return number of occurrences of value'\n        return sum(1 for v in self if v == value)\n\nSequence.register(tuple)\nSequence.register(basestring)\nSequence.register(buffer)\nSequence.register(xrange)\n\n\nclass MutableSequence(Sequence):\n\n    \"\"\"All the operations on a read-only sequence.\n\n    Concrete subclasses must provide __new__ or __init__,\n    __getitem__, __setitem__, __delitem__, __len__, and insert().\n\n    \"\"\"\n\n    @abstractmethod\n    def __setitem__(self, index, value):\n        raise IndexError\n\n    @abstractmethod\n    def __delitem__(self, index):\n        raise IndexError\n\n    @abstractmethod\n    def insert(self, index, value):\n        'S.insert(index, object) -- insert object before index'\n        raise IndexError\n\n    def append(self, value):\n        'S.append(object) -- append object to the end of the sequence'\n        self.insert(len(self), value)\n\n    def reverse(self):\n        'S.reverse() -- reverse *IN PLACE*'\n        n = len(self)\n        for i in range(n//2):\n            self[i], self[n-i-1] = self[n-i-1], self[i]\n\n    def extend(self, values):\n        'S.extend(iterable) -- extend sequence by appending elements from the iterable'\n        for v in values:\n            self.append(v)\n\n    def pop(self, index=-1):\n        '''S.pop([index]) -> item -- remove and return item at index (default last).\n           Raise IndexError if list is empty or index is out of range.\n        '''\n        v = self[index]\n        del self[index]\n        return v\n\n    def remove(self, value):\n        '''S.remove(value) -- remove first occurrence of value.\n           Raise ValueError if the value is not present.\n        '''\n        del self[self.index(value)]\n\n    def __iadd__(self, values):\n        self.extend(values)\n        return self\n\nMutableSequence.register(list)\n", 
    "_functools": "\"\"\" Supplies the internal functions for functools.py in the standard library \"\"\"\n\n# reduce() has moved to _functools in Python 2.6+.\nreduce = reduce\n\nclass partial(object):\n    \"\"\"\n    partial(func, *args, **keywords) - new function with partial application\n    of the given arguments and keywords.\n    \"\"\"\n    def __init__(*args, **keywords):\n        if len(args) < 2:\n            raise TypeError('__init__() takes at least 2 arguments (%d given)'\n                            % len(args))\n        self, func, args = args[0], args[1], args[2:]\n        if not callable(func):\n            raise TypeError(\"the first argument must be callable\")\n        self._func = func\n        self._args = args\n        self._keywords = keywords\n\n    def __delattr__(self, key):\n        if key == '__dict__':\n            raise TypeError(\"a partial object's dictionary may not be deleted\")\n        object.__delattr__(self, key)\n\n    @property\n    def func(self):\n        return self._func\n\n    @property\n    def args(self):\n        return self._args\n\n    @property\n    def keywords(self):\n        return self._keywords\n\n    def __call__(self, *fargs, **fkeywords):\n        if self._keywords:\n            fkeywords = dict(self._keywords, **fkeywords)\n        return self._func(*(self._args + fargs), **fkeywords)\n\n    def __reduce__(self):\n        d = dict((k, v) for k, v in self.__dict__.iteritems() if k not in\n                ('_func', '_args', '_keywords'))\n        if len(d) == 0:\n            d = None\n        return (type(self), (self._func,),\n                (self._func, self._args, self._keywords, d))\n\n    def __setstate__(self, state):\n        func, args, keywords, d = state\n        if d is not None:\n            self.__dict__.update(d)\n        self._func = func\n        self._args = args\n        self._keywords = keywords\n", 
    "_marshal": "\"\"\"Internal Python object serialization\n\nThis module contains functions that can read and write Python values in a binary format. The format is specific to Python, but independent of machine architecture issues (e.g., you can write a Python value to a file on a PC, transport the file to a Sun, and read it back there). Details of the format may change between Python versions.\n\"\"\"\n\n# NOTE: This module is used in the Python3 interpreter, but also by\n# the \"sandboxed\" process.  It must work for Python2 as well.\n\nimport types\n\ntry:\n    intern\nexcept NameError:\n    from sys import intern\n\ntry: from __pypy__ import builtinify\nexcept ImportError: builtinify = lambda f: f\n\n\nTYPE_NULL     = '0'\nTYPE_NONE     = 'N'\nTYPE_FALSE    = 'F'\nTYPE_TRUE     = 'T'\nTYPE_STOPITER = 'S'\nTYPE_ELLIPSIS = '.'\nTYPE_INT      = 'i'\nTYPE_INT64    = 'I'\nTYPE_FLOAT    = 'f'\nTYPE_COMPLEX  = 'x'\nTYPE_LONG     = 'l'\nTYPE_STRING   = 's'\nTYPE_INTERNED = 't'\nTYPE_STRINGREF= 'R'\nTYPE_TUPLE    = '('\nTYPE_LIST     = '['\nTYPE_DICT     = '{'\nTYPE_CODE     = 'c'\nTYPE_UNICODE  = 'u'\nTYPE_UNKNOWN  = '?'\nTYPE_SET      = '<'\nTYPE_FROZENSET= '>'\n\nclass _Marshaller:\n\n    dispatch = {}\n\n    def __init__(self, writefunc):\n        self._write = writefunc\n\n    def dump(self, x):\n        try:\n            self.dispatch[type(x)](self, x)\n        except KeyError:\n            for tp in type(x).mro():\n                func = self.dispatch.get(tp)\n                if func:\n                    break\n            else:\n                raise ValueError(\"unmarshallable object\")\n            func(self, x)\n\n    def w_long64(self, x):\n        self.w_long(x)\n        self.w_long(x>>32)\n\n    def w_long(self, x):\n        a = chr(x & 0xff)\n        x >>= 8\n        b = chr(x & 0xff)\n        x >>= 8\n        c = chr(x & 0xff)\n        x >>= 8\n        d = chr(x & 0xff)\n        self._write(a + b + c + d)\n\n    def w_short(self, x):\n        self._write(chr((x)     & 0xff))\n        self._write(chr((x>> 8) & 0xff))\n\n    def dump_none(self, x):\n        self._write(TYPE_NONE)\n    dispatch[type(None)] = dump_none\n\n    def dump_bool(self, x):\n        if x:\n            self._write(TYPE_TRUE)\n        else:\n            self._write(TYPE_FALSE)\n    dispatch[bool] = dump_bool\n\n    def dump_stopiter(self, x):\n        if x is not StopIteration:\n            raise ValueError(\"unmarshallable object\")\n        self._write(TYPE_STOPITER)\n    dispatch[type(StopIteration)] = dump_stopiter\n\n    def dump_ellipsis(self, x):\n        self._write(TYPE_ELLIPSIS)\n    \n    try:\n        dispatch[type(Ellipsis)] = dump_ellipsis\n    except NameError:\n        pass\n\n    # In Python3, this function is not used; see dump_long() below.\n    def dump_int(self, x):\n        y = x>>31\n        if y and y != -1:\n            self._write(TYPE_INT64)\n            self.w_long64(x)\n        else:\n            self._write(TYPE_INT)\n            self.w_long(x)\n    dispatch[int] = dump_int\n\n    def dump_long(self, x):\n        self._write(TYPE_LONG)\n        sign = 1\n        if x < 0:\n            sign = -1\n            x = -x\n        digits = []\n        while x:\n            digits.append(x & 0x7FFF)\n            x = x>>15\n        self.w_long(len(digits) * sign)\n        for d in digits:\n            self.w_short(d)\n    try:\n        long\n    except NameError:\n        dispatch[int] = dump_long\n    else:\n        dispatch[long] = dump_long\n\n    def dump_float(self, x):\n        write = self._write\n        write(TYPE_FLOAT)\n        s = repr(x)\n        write(chr(len(s)))\n        write(s)\n    dispatch[float] = dump_float\n\n    def dump_complex(self, x):\n        write = self._write\n        write(TYPE_COMPLEX)\n        s = repr(x.real)\n        write(chr(len(s)))\n        write(s)\n        s = repr(x.imag)\n        write(chr(len(s)))\n        write(s)\n    try:\n        dispatch[complex] = dump_complex\n    except NameError:\n        pass\n\n    def dump_string(self, x):\n        # XXX we can't check for interned strings, yet,\n        # so we (for now) never create TYPE_INTERNED or TYPE_STRINGREF\n        self._write(TYPE_STRING)\n        self.w_long(len(x))\n        self._write(x)\n    dispatch[bytes] = dump_string\n\n    def dump_unicode(self, x):\n        self._write(TYPE_UNICODE)\n        s = x.encode('utf8')\n        self.w_long(len(s))\n        self._write(s)\n    try:\n        unicode\n    except NameError:\n        dispatch[str] = dump_unicode\n    else:\n        dispatch[unicode] = dump_unicode\n\n    def dump_tuple(self, x):\n        self._write(TYPE_TUPLE)\n        self.w_long(len(x))\n        for item in x:\n            self.dump(item)\n    dispatch[tuple] = dump_tuple\n\n    def dump_list(self, x):\n        self._write(TYPE_LIST)\n        self.w_long(len(x))\n        for item in x:\n            self.dump(item)\n    dispatch[list] = dump_list\n\n    def dump_dict(self, x):\n        self._write(TYPE_DICT)\n        for key, value in x.items():\n            self.dump(key)\n            self.dump(value)\n        self._write(TYPE_NULL)\n    dispatch[dict] = dump_dict\n\n    def dump_code(self, x):\n        self._write(TYPE_CODE)\n        self.w_long(x.co_argcount)\n        self.w_long(x.co_nlocals)\n        self.w_long(x.co_stacksize)\n        self.w_long(x.co_flags)\n        self.dump(x.co_code)\n        self.dump(x.co_consts)\n        self.dump(x.co_names)\n        self.dump(x.co_varnames)\n        self.dump(x.co_freevars)\n        self.dump(x.co_cellvars)\n        self.dump(x.co_filename)\n        self.dump(x.co_name)\n        self.w_long(x.co_firstlineno)\n        self.dump(x.co_lnotab)\n    try:\n        dispatch[types.CodeType] = dump_code\n    except NameError:\n        pass\n\n    def dump_set(self, x):\n        self._write(TYPE_SET)\n        self.w_long(len(x))\n        for each in x:\n            self.dump(each)\n    try:\n        dispatch[set] = dump_set\n    except NameError:\n        pass\n\n    def dump_frozenset(self, x):\n        self._write(TYPE_FROZENSET)\n        self.w_long(len(x))\n        for each in x:\n            self.dump(each)\n    try:\n        dispatch[frozenset] = dump_frozenset\n    except NameError:\n        pass\n\nclass _NULL:\n    pass\n\nclass _StringBuffer:\n    def __init__(self, value):\n        self.bufstr = value\n        self.bufpos = 0\n\n    def read(self, n):\n        pos = self.bufpos\n        newpos = pos + n\n        ret = self.bufstr[pos : newpos]\n        self.bufpos = newpos\n        return ret\n\n\nclass _Unmarshaller:\n\n    dispatch = {}\n\n    def __init__(self, readfunc):\n        self._read = readfunc\n        self._stringtable = []\n\n    def load(self):\n        c = self._read(1)\n        if not c:\n            raise EOFError\n        try:\n            return self.dispatch[c](self)\n        except KeyError:\n            raise ValueError(\"bad marshal code: %c (%d)\" % (c, ord(c)))\n\n    def r_short(self):\n        lo = ord(self._read(1))\n        hi = ord(self._read(1))\n        x = lo | (hi<<8)\n        if x & 0x8000:\n            x = x - 0x10000\n        return x\n\n    def r_long(self):\n        s = self._read(4)\n        a = ord(s[0])\n        b = ord(s[1])\n        c = ord(s[2])\n        d = ord(s[3])\n        x = a | (b<<8) | (c<<16) | (d<<24)\n        if d & 0x80 and x > 0:\n            x = -((1<<32) - x)\n            return int(x)\n        else:\n            return x\n\n    def r_long64(self):\n        a = ord(self._read(1))\n        b = ord(self._read(1))\n        c = ord(self._read(1))\n        d = ord(self._read(1))\n        e = ord(self._read(1))\n        f = ord(self._read(1))\n        g = ord(self._read(1))\n        h = ord(self._read(1))\n        x = a | (b<<8) | (c<<16) | (d<<24)\n        x = x | (e<<32) | (f<<40) | (g<<48) | (h<<56)\n        if h & 0x80 and x > 0:\n            x = -((1<<64) - x)\n        return x\n\n    def load_null(self):\n        return _NULL\n    dispatch[TYPE_NULL] = load_null\n\n    def load_none(self):\n        return None\n    dispatch[TYPE_NONE] = load_none\n\n    def load_true(self):\n        return True\n    dispatch[TYPE_TRUE] = load_true\n\n    def load_false(self):\n        return False\n    dispatch[TYPE_FALSE] = load_false\n\n    def load_stopiter(self):\n        return StopIteration\n    dispatch[TYPE_STOPITER] = load_stopiter\n\n    def load_ellipsis(self):\n        return Ellipsis\n    dispatch[TYPE_ELLIPSIS] = load_ellipsis\n\n    dispatch[TYPE_INT] = r_long\n\n    dispatch[TYPE_INT64] = r_long64\n\n    def load_long(self):\n        size = self.r_long()\n        sign = 1\n        if size < 0:\n            sign = -1\n            size = -size\n        x = 0\n        for i in range(size):\n            d = self.r_short()\n            x = x | (d<<(i*15))\n        return x * sign\n    dispatch[TYPE_LONG] = load_long\n\n    def load_float(self):\n        n = ord(self._read(1))\n        s = self._read(n)\n        return float(s)\n    dispatch[TYPE_FLOAT] = load_float\n\n    def load_complex(self):\n        n = ord(self._read(1))\n        s = self._read(n)\n        real = float(s)\n        n = ord(self._read(1))\n        s = self._read(n)\n        imag = float(s)\n        return complex(real, imag)\n    dispatch[TYPE_COMPLEX] = load_complex\n\n    def load_string(self):\n        n = self.r_long()\n        return self._read(n)\n    dispatch[TYPE_STRING] = load_string\n\n    def load_interned(self):\n        n = self.r_long()\n        ret = intern(self._read(n))\n        self._stringtable.append(ret)\n        return ret\n    dispatch[TYPE_INTERNED] = load_interned\n\n    def load_stringref(self):\n        n = self.r_long()\n        return self._stringtable[n]\n    dispatch[TYPE_STRINGREF] = load_stringref\n\n    def load_unicode(self):\n        n = self.r_long()\n        s = self._read(n)\n        ret = s.decode('utf8')\n        return ret\n    dispatch[TYPE_UNICODE] = load_unicode\n\n    def load_tuple(self):\n        return tuple(self.load_list())\n    dispatch[TYPE_TUPLE] = load_tuple\n\n    def load_list(self):\n        n = self.r_long()\n        list = [self.load() for i in range(n)]\n        return list\n    dispatch[TYPE_LIST] = load_list\n\n    def load_dict(self):\n        d = {}\n        while 1:\n            key = self.load()\n            if key is _NULL:\n                break\n            value = self.load()\n            d[key] = value\n        return d\n    dispatch[TYPE_DICT] = load_dict\n\n    def load_code(self):\n        argcount = self.r_long()\n        nlocals = self.r_long()\n        stacksize = self.r_long()\n        flags = self.r_long()\n        code = self.load()\n        consts = self.load()\n        names = self.load()\n        varnames = self.load()\n        freevars = self.load()\n        cellvars = self.load()\n        filename = self.load()\n        name = self.load()\n        firstlineno = self.r_long()\n        lnotab = self.load()\n        return types.CodeType(argcount, nlocals, stacksize, flags, code, consts,\n                              names, varnames, filename, name, firstlineno,\n                              lnotab, freevars, cellvars)\n    dispatch[TYPE_CODE] = load_code\n\n    def load_set(self):\n        n = self.r_long()\n        args = [self.load() for i in range(n)]\n        return set(args)\n    dispatch[TYPE_SET] = load_set\n\n    def load_frozenset(self):\n        n = self.r_long()\n        args = [self.load() for i in range(n)]\n        return frozenset(args)\n    dispatch[TYPE_FROZENSET] = load_frozenset\n\n# ________________________________________________________________\n\ndef _read(self, n):\n    pos = self.bufpos\n    newpos = pos + n\n    if newpos > len(self.bufstr): raise EOFError\n    ret = self.bufstr[pos : newpos]\n    self.bufpos = newpos\n    return ret\n\ndef _read1(self):\n    ret = self.bufstr[self.bufpos]\n    self.bufpos += 1\n    return ret\n\ndef _r_short(self):\n    lo = ord(_read1(self))\n    hi = ord(_read1(self))\n    x = lo | (hi<<8)\n    if x & 0x8000:\n        x = x - 0x10000\n    return x\n\ndef _r_long(self):\n    # inlined this most common case\n    p = self.bufpos\n    s = self.bufstr\n    a = ord(s[p])\n    b = ord(s[p+1])\n    c = ord(s[p+2])\n    d = ord(s[p+3])\n    self.bufpos += 4\n    x = a | (b<<8) | (c<<16) | (d<<24)\n    if d & 0x80 and x > 0:\n        x = -((1<<32) - x)\n        return int(x)\n    else:\n        return x\n\ndef _r_long64(self):\n    a = ord(_read1(self))\n    b = ord(_read1(self))\n    c = ord(_read1(self))\n    d = ord(_read1(self))\n    e = ord(_read1(self))\n    f = ord(_read1(self))\n    g = ord(_read1(self))\n    h = ord(_read1(self))\n    x = a | (b<<8) | (c<<16) | (d<<24)\n    x = x | (e<<32) | (f<<40) | (g<<48) | (h<<56)\n    if h & 0x80 and x > 0:\n        x = -((1<<64) - x)\n    return x\n\n_load_dispatch = {}\n\nclass _FastUnmarshaller:\n\n    dispatch = {}\n\n    def __init__(self, buffer):\n        self.bufstr = buffer\n        self.bufpos = 0\n        self._stringtable = []\n\n    def load(self):\n        # make flow space happy\n        c = '?'\n        try:\n            c = self.bufstr[self.bufpos]\n            self.bufpos += 1\n            return _load_dispatch[c](self)\n        except KeyError:\n            raise ValueError(\"bad marshal code: %c (%d)\" % (c, ord(c)))\n        except IndexError:\n            raise EOFError\n\n    def load_null(self):\n        return _NULL\n    dispatch[TYPE_NULL] = load_null\n\n    def load_none(self):\n        return None\n    dispatch[TYPE_NONE] = load_none\n\n    def load_true(self):\n        return True\n    dispatch[TYPE_TRUE] = load_true\n\n    def load_false(self):\n        return False\n    dispatch[TYPE_FALSE] = load_false\n\n    def load_stopiter(self):\n        return StopIteration\n    dispatch[TYPE_STOPITER] = load_stopiter\n\n    def load_ellipsis(self):\n        return Ellipsis\n    dispatch[TYPE_ELLIPSIS] = load_ellipsis\n\n    def load_int(self):\n        return _r_long(self)\n    dispatch[TYPE_INT] = load_int\n\n    def load_int64(self):\n        return _r_long64(self)\n    dispatch[TYPE_INT64] = load_int64\n\n    def load_long(self):\n        size = _r_long(self)\n        sign = 1\n        if size < 0:\n            sign = -1\n            size = -size\n        x = 0\n        for i in range(size):\n            d = _r_short(self)\n            x = x | (d<<(i*15))\n        return x * sign\n    dispatch[TYPE_LONG] = load_long\n\n    def load_float(self):\n        n = ord(_read1(self))\n        s = _read(self, n)\n        return float(s)\n    dispatch[TYPE_FLOAT] = load_float\n\n    def load_complex(self):\n        n = ord(_read1(self))\n        s = _read(self, n)\n        real = float(s)\n        n = ord(_read1(self))\n        s = _read(self, n)\n        imag = float(s)\n        return complex(real, imag)\n    dispatch[TYPE_COMPLEX] = load_complex\n\n    def load_string(self):\n        n = _r_long(self)\n        return _read(self, n)\n    dispatch[TYPE_STRING] = load_string\n\n    def load_interned(self):\n        n = _r_long(self)\n        ret = intern(_read(self, n))\n        self._stringtable.append(ret)\n        return ret\n    dispatch[TYPE_INTERNED] = load_interned\n\n    def load_stringref(self):\n        n = _r_long(self)\n        return self._stringtable[n]\n    dispatch[TYPE_STRINGREF] = load_stringref\n\n    def load_unicode(self):\n        n = _r_long(self)\n        s = _read(self, n)\n        ret = s.decode('utf8')\n        return ret\n    dispatch[TYPE_UNICODE] = load_unicode\n\n    def load_tuple(self):\n        return tuple(self.load_list())\n    dispatch[TYPE_TUPLE] = load_tuple\n\n    def load_list(self):\n        n = _r_long(self)\n        list = []\n        for i in range(n):\n            list.append(self.load())\n        return list\n    dispatch[TYPE_LIST] = load_list\n\n    def load_dict(self):\n        d = {}\n        while 1:\n            key = self.load()\n            if key is _NULL:\n                break\n            value = self.load()\n            d[key] = value\n        return d\n    dispatch[TYPE_DICT] = load_dict\n\n    def load_code(self):\n        argcount = _r_long(self)\n        nlocals = _r_long(self)\n        stacksize = _r_long(self)\n        flags = _r_long(self)\n        code = self.load()\n        consts = self.load()\n        names = self.load()\n        varnames = self.load()\n        freevars = self.load()\n        cellvars = self.load()\n        filename = self.load()\n        name = self.load()\n        firstlineno = _r_long(self)\n        lnotab = self.load()\n        return types.CodeType(argcount, nlocals, stacksize, flags, code, consts,\n                              names, varnames, filename, name, firstlineno,\n                              lnotab, freevars, cellvars)\n    dispatch[TYPE_CODE] = load_code\n\n    def load_set(self):\n        n = _r_long(self)\n        args = [self.load() for i in range(n)]\n        return set(args)\n    dispatch[TYPE_SET] = load_set\n\n    def load_frozenset(self):\n        n = _r_long(self)\n        args = [self.load() for i in range(n)]\n        return frozenset(args)\n    dispatch[TYPE_FROZENSET] = load_frozenset\n\n_load_dispatch = _FastUnmarshaller.dispatch\n\n# _________________________________________________________________\n#\n# user interface\n\nversion = 1\n\n@builtinify\ndef dump(x, f, version=version):\n    # XXX 'version' is ignored, we always dump in a version-0-compatible format\n    m = _Marshaller(f.write)\n    m.dump(x)\n\n@builtinify\ndef load(f):\n    um = _Unmarshaller(f.read)\n    return um.load()\n\n@builtinify\ndef dumps(x, version=version):\n    # XXX 'version' is ignored, we always dump in a version-0-compatible format\n    buffer = []\n    m = _Marshaller(buffer.append)\n    m.dump(x)\n    return ''.join(buffer)\n\n@builtinify\ndef loads(s):\n    um = _FastUnmarshaller(s)\n    return um.load()\n", 
    "_md5": "#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n\n# Note that PyPy contains also a built-in module 'md5' which will hide\n# this one if compiled in.\n\n\"\"\"A sample implementation of MD5 in pure Python.\n\nThis is an implementation of the MD5 hash function, as specified by\nRFC 1321, in pure Python. It was implemented using Bruce Schneier's\nexcellent book \"Applied Cryptography\", 2nd ed., 1996.\n\nSurely this is not meant to compete with the existing implementation\nof the Python standard library (written in C). Rather, it should be\nseen as a Python complement that is more readable than C and can be\nused more conveniently for learning and experimenting purposes in\nthe field of cryptography.\n\nThis module tries very hard to follow the API of the existing Python\nstandard library's \"md5\" module, but although it seems to work fine,\nit has not been extensively tested! (But note that there is a test\nmodule, test_md5py.py, that compares this Python implementation with\nthe C one of the Python standard library.\n\nBEWARE: this comes with no guarantee whatsoever about fitness and/or\nother properties! Specifically, do not use this in any production\ncode! License is Python License!\n\nSpecial thanks to Aurelian Coman who fixed some nasty bugs!\n\nDinu C. Gherman\n\"\"\"\n\n\n__date__    = '2004-11-17'\n__version__ = 0.91 # Modernised by J. Hall\u00e9n and L. Creighton for Pypy\n\n__metaclass__ = type # or genrpy won't work\n\nimport struct, copy\n\n\n# ======================================================================\n# Bit-Manipulation helpers\n# ======================================================================\n\ndef _bytelist2long(list):\n    \"Transform a list of characters into a list of longs.\"\n\n    imax = len(list) // 4\n    hl = [0] * imax\n\n    j = 0\n    i = 0\n    while i < imax:\n        b0 = ord(list[j])\n        b1 = ord(list[j+1]) << 8\n        b2 = ord(list[j+2]) << 16\n        b3 = ord(list[j+3]) << 24\n        hl[i] = b0 | b1 |b2 | b3\n        i = i+1\n        j = j+4\n\n    return hl\n\n\ndef _rotateLeft(x, n):\n    \"Rotate x (32 bit) left n bits circularly.\"\n\n    return (x << n) | (x >> (32-n))\n\n\n# ======================================================================\n# The real MD5 meat...\n#\n#   Implemented after \"Applied Cryptography\", 2nd ed., 1996,\n#   pp. 436-441 by Bruce Schneier.\n# ======================================================================\n\n# F, G, H and I are basic MD5 functions.\n\ndef F(x, y, z):\n    return (x & y) | ((~x) & z)\n\ndef G(x, y, z):\n    return (x & z) | (y & (~z))\n\ndef H(x, y, z):\n    return x ^ y ^ z\n\ndef I(x, y, z):\n    return y ^ (x | (~z))\n\n\ndef XX(func, a, b, c, d, x, s, ac):\n    \"\"\"Wrapper for call distribution to functions F, G, H and I.\n\n    This replaces functions FF, GG, HH and II from \"Appl. Crypto.\"\n    Rotation is separate from addition to prevent recomputation\n    (now summed-up in one function).\n    \"\"\"\n\n    res = 0\n    res = res + a + func(b, c, d)\n    res = res + x \n    res = res + ac\n    res = res & 0xffffffff\n    res = _rotateLeft(res, s)\n    res = res & 0xffffffff\n    res = res + b\n\n    return res & 0xffffffff\n\n\nclass MD5Type:\n    \"An implementation of the MD5 hash function in pure Python.\"\n\n    digest_size = digestsize = 16\n    block_size = 64\n\n    def __init__(self):\n        \"Initialisation.\"\n        \n        # Initial message length in bits(!).\n        self.length = 0\n        self.count = [0, 0]\n\n        # Initial empty message as a sequence of bytes (8 bit characters).\n        self.input = []\n\n        # Call a separate init function, that can be used repeatedly\n        # to start from scratch on the same object.\n        self.init()\n\n\n    def init(self):\n        \"Initialize the message-digest and set all fields to zero.\"\n\n        self.length = 0\n        self.count = [0, 0]\n        self.input = []\n\n        # Load magic initialization constants.\n        self.A = 0x67452301\n        self.B = 0xefcdab89\n        self.C = 0x98badcfe\n        self.D = 0x10325476\n\n\n    def _transform(self, inp):\n        \"\"\"Basic MD5 step transforming the digest based on the input.\n\n        Note that if the Mysterious Constants are arranged backwards\n        in little-endian order and decrypted with the DES they produce\n        OCCULT MESSAGES!\n        \"\"\"\n\n        a, b, c, d = A, B, C, D = self.A, self.B, self.C, self.D\n\n        # Round 1.\n\n        S11, S12, S13, S14 = 7, 12, 17, 22\n\n        a = XX(F, a, b, c, d, inp[ 0], S11, 0xD76AA478) # 1 \n        d = XX(F, d, a, b, c, inp[ 1], S12, 0xE8C7B756) # 2 \n        c = XX(F, c, d, a, b, inp[ 2], S13, 0x242070DB) # 3 \n        b = XX(F, b, c, d, a, inp[ 3], S14, 0xC1BDCEEE) # 4 \n        a = XX(F, a, b, c, d, inp[ 4], S11, 0xF57C0FAF) # 5 \n        d = XX(F, d, a, b, c, inp[ 5], S12, 0x4787C62A) # 6 \n        c = XX(F, c, d, a, b, inp[ 6], S13, 0xA8304613) # 7 \n        b = XX(F, b, c, d, a, inp[ 7], S14, 0xFD469501) # 8 \n        a = XX(F, a, b, c, d, inp[ 8], S11, 0x698098D8) # 9 \n        d = XX(F, d, a, b, c, inp[ 9], S12, 0x8B44F7AF) # 10 \n        c = XX(F, c, d, a, b, inp[10], S13, 0xFFFF5BB1) # 11 \n        b = XX(F, b, c, d, a, inp[11], S14, 0x895CD7BE) # 12 \n        a = XX(F, a, b, c, d, inp[12], S11, 0x6B901122) # 13 \n        d = XX(F, d, a, b, c, inp[13], S12, 0xFD987193) # 14 \n        c = XX(F, c, d, a, b, inp[14], S13, 0xA679438E) # 15 \n        b = XX(F, b, c, d, a, inp[15], S14, 0x49B40821) # 16 \n\n        # Round 2.\n\n        S21, S22, S23, S24 = 5, 9, 14, 20\n\n        a = XX(G, a, b, c, d, inp[ 1], S21, 0xF61E2562) # 17 \n        d = XX(G, d, a, b, c, inp[ 6], S22, 0xC040B340) # 18 \n        c = XX(G, c, d, a, b, inp[11], S23, 0x265E5A51) # 19 \n        b = XX(G, b, c, d, a, inp[ 0], S24, 0xE9B6C7AA) # 20 \n        a = XX(G, a, b, c, d, inp[ 5], S21, 0xD62F105D) # 21 \n        d = XX(G, d, a, b, c, inp[10], S22, 0x02441453) # 22 \n        c = XX(G, c, d, a, b, inp[15], S23, 0xD8A1E681) # 23 \n        b = XX(G, b, c, d, a, inp[ 4], S24, 0xE7D3FBC8) # 24 \n        a = XX(G, a, b, c, d, inp[ 9], S21, 0x21E1CDE6) # 25 \n        d = XX(G, d, a, b, c, inp[14], S22, 0xC33707D6) # 26 \n        c = XX(G, c, d, a, b, inp[ 3], S23, 0xF4D50D87) # 27 \n        b = XX(G, b, c, d, a, inp[ 8], S24, 0x455A14ED) # 28 \n        a = XX(G, a, b, c, d, inp[13], S21, 0xA9E3E905) # 29 \n        d = XX(G, d, a, b, c, inp[ 2], S22, 0xFCEFA3F8) # 30 \n        c = XX(G, c, d, a, b, inp[ 7], S23, 0x676F02D9) # 31 \n        b = XX(G, b, c, d, a, inp[12], S24, 0x8D2A4C8A) # 32 \n\n        # Round 3.\n\n        S31, S32, S33, S34 = 4, 11, 16, 23\n\n        a = XX(H, a, b, c, d, inp[ 5], S31, 0xFFFA3942) # 33 \n        d = XX(H, d, a, b, c, inp[ 8], S32, 0x8771F681) # 34 \n        c = XX(H, c, d, a, b, inp[11], S33, 0x6D9D6122) # 35 \n        b = XX(H, b, c, d, a, inp[14], S34, 0xFDE5380C) # 36 \n        a = XX(H, a, b, c, d, inp[ 1], S31, 0xA4BEEA44) # 37 \n        d = XX(H, d, a, b, c, inp[ 4], S32, 0x4BDECFA9) # 38 \n        c = XX(H, c, d, a, b, inp[ 7], S33, 0xF6BB4B60) # 39 \n        b = XX(H, b, c, d, a, inp[10], S34, 0xBEBFBC70) # 40 \n        a = XX(H, a, b, c, d, inp[13], S31, 0x289B7EC6) # 41 \n        d = XX(H, d, a, b, c, inp[ 0], S32, 0xEAA127FA) # 42 \n        c = XX(H, c, d, a, b, inp[ 3], S33, 0xD4EF3085) # 43 \n        b = XX(H, b, c, d, a, inp[ 6], S34, 0x04881D05) # 44 \n        a = XX(H, a, b, c, d, inp[ 9], S31, 0xD9D4D039) # 45 \n        d = XX(H, d, a, b, c, inp[12], S32, 0xE6DB99E5) # 46 \n        c = XX(H, c, d, a, b, inp[15], S33, 0x1FA27CF8) # 47 \n        b = XX(H, b, c, d, a, inp[ 2], S34, 0xC4AC5665) # 48 \n\n        # Round 4.\n\n        S41, S42, S43, S44 = 6, 10, 15, 21\n\n        a = XX(I, a, b, c, d, inp[ 0], S41, 0xF4292244) # 49 \n        d = XX(I, d, a, b, c, inp[ 7], S42, 0x432AFF97) # 50 \n        c = XX(I, c, d, a, b, inp[14], S43, 0xAB9423A7) # 51 \n        b = XX(I, b, c, d, a, inp[ 5], S44, 0xFC93A039) # 52 \n        a = XX(I, a, b, c, d, inp[12], S41, 0x655B59C3) # 53 \n        d = XX(I, d, a, b, c, inp[ 3], S42, 0x8F0CCC92) # 54 \n        c = XX(I, c, d, a, b, inp[10], S43, 0xFFEFF47D) # 55 \n        b = XX(I, b, c, d, a, inp[ 1], S44, 0x85845DD1) # 56 \n        a = XX(I, a, b, c, d, inp[ 8], S41, 0x6FA87E4F) # 57 \n        d = XX(I, d, a, b, c, inp[15], S42, 0xFE2CE6E0) # 58 \n        c = XX(I, c, d, a, b, inp[ 6], S43, 0xA3014314) # 59 \n        b = XX(I, b, c, d, a, inp[13], S44, 0x4E0811A1) # 60 \n        a = XX(I, a, b, c, d, inp[ 4], S41, 0xF7537E82) # 61 \n        d = XX(I, d, a, b, c, inp[11], S42, 0xBD3AF235) # 62 \n        c = XX(I, c, d, a, b, inp[ 2], S43, 0x2AD7D2BB) # 63 \n        b = XX(I, b, c, d, a, inp[ 9], S44, 0xEB86D391) # 64 \n\n        A = (A + a) & 0xffffffff\n        B = (B + b) & 0xffffffff\n        C = (C + c) & 0xffffffff\n        D = (D + d) & 0xffffffff\n\n        self.A, self.B, self.C, self.D = A, B, C, D\n\n\n    # Down from here all methods follow the Python Standard Library\n    # API of the md5 module.\n\n    def update(self, inBuf):\n        \"\"\"Add to the current message.\n\n        Update the md5 object with the string arg. Repeated calls\n        are equivalent to a single call with the concatenation of all\n        the arguments, i.e. m.update(a); m.update(b) is equivalent\n        to m.update(a+b).\n\n        The hash is immediately calculated for all full blocks. The final\n        calculation is made in digest(). This allows us to keep an\n        intermediate value for the hash, so that we only need to make\n        minimal recalculation if we call update() to add moredata to\n        the hashed string.\n        \"\"\"\n\n        leninBuf = len(inBuf)\n\n        # Compute number of bytes mod 64.\n        index = (self.count[0] >> 3) & 0x3F\n\n        # Update number of bits.\n        self.count[0] = self.count[0] + (leninBuf << 3)\n        if self.count[0] < (leninBuf << 3):\n            self.count[1] = self.count[1] + 1\n        self.count[1] = self.count[1] + (leninBuf >> 29)\n\n        partLen = 64 - index\n\n        if leninBuf >= partLen:\n            self.input[index:] = list(inBuf[:partLen])\n            self._transform(_bytelist2long(self.input))\n            i = partLen\n            while i + 63 < leninBuf:\n                self._transform(_bytelist2long(list(inBuf[i:i+64])))\n                i = i + 64\n            else:\n                self.input = list(inBuf[i:leninBuf])\n        else:\n            i = 0\n            self.input = self.input + list(inBuf)\n\n\n    def digest(self):\n        \"\"\"Terminate the message-digest computation and return digest.\n\n        Return the digest of the strings passed to the update()\n        method so far. This is a 16-byte string which may contain\n        non-ASCII characters, including null bytes.\n        \"\"\"\n\n        A = self.A\n        B = self.B\n        C = self.C\n        D = self.D\n        input = [] + self.input\n        count = [] + self.count\n\n        index = (self.count[0] >> 3) & 0x3f\n\n        if index < 56:\n            padLen = 56 - index\n        else:\n            padLen = 120 - index\n\n        padding = [b'\\200'] + [b'\\000'] * 63\n        self.update(padding[:padLen])\n\n        # Append length (before padding).\n        bits = _bytelist2long(self.input[:56]) + count\n\n        self._transform(bits)\n\n        # Store state in digest.\n        digest = struct.pack(\"<IIII\", self.A, self.B, self.C, self.D)\n\n        self.A = A \n        self.B = B\n        self.C = C\n        self.D = D\n        self.input = input \n        self.count = count \n\n        return digest\n\n\n    def hexdigest(self):\n        \"\"\"Terminate and return digest in HEX form.\n\n        Like digest() except the digest is returned as a string of\n        length 32, containing only hexadecimal digits. This may be\n        used to exchange the value safely in email or other non-\n        binary environments.\n        \"\"\"\n\n        return ''.join(['%02x' % ord(c) for c in self.digest()])\n\n    def copy(self):\n        \"\"\"Return a clone object.\n\n        Return a copy ('clone') of the md5 object. This can be used\n        to efficiently compute the digests of strings that share\n        a common initial substring.\n        \"\"\"\n        if 0: # set this to 1 to make the flow space crash\n            return copy.deepcopy(self)\n        clone = self.__class__()\n        clone.length = self.length\n        clone.count  = [] + self.count[:]\n        clone.input  = [] + self.input\n        clone.A = self.A\n        clone.B = self.B\n        clone.C = self.C\n        clone.D = self.D\n        return clone\n\n\n# ======================================================================\n# Mimic Python top-level functions from standard library API\n# for consistency with the _md5 module of the standard library.\n# ======================================================================\n\ndigest_size = 16\n\ndef new(arg=None):\n    \"\"\"Return a new md5 crypto object.\n    If arg is present, the method call update(arg) is made.\n    \"\"\"\n\n    crypto = MD5Type()\n    if arg:\n        crypto.update(arg)\n\n    return crypto\n\n", 
    "_sha": "#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n\n# Note that PyPy contains also a built-in module 'sha' which will hide\n# this one if compiled in.\n\n\"\"\"A sample implementation of SHA-1 in pure Python.\n\n   Framework adapted from Dinu Gherman's MD5 implementation by\n   J. Hall\u00e9n and L. Creighton. SHA-1 implementation based directly on\n   the text of the NIST standard FIPS PUB 180-1.\n\"\"\"\n\n\n__date__    = '2004-11-17'\n__version__ = 0.91 # Modernised by J. Hall\u00e9n and L. Creighton for Pypy\n\n\nimport struct, copy\n\n\n# ======================================================================\n# Bit-Manipulation helpers\n#\n#   _long2bytes() was contributed by Barry Warsaw\n#   and is reused here with tiny modifications.\n# ======================================================================\n\ndef _long2bytesBigEndian(n, blocksize=0):\n    \"\"\"Convert a long integer to a byte string.\n\n    If optional blocksize is given and greater than zero, pad the front\n    of the byte string with binary zeros so that the length is a multiple\n    of blocksize.\n    \"\"\"\n\n    # After much testing, this algorithm was deemed to be the fastest.\n    s = b''\n    pack = struct.pack\n    while n > 0:\n        s = pack('>I', n & 0xffffffff) + s\n        n = n >> 32\n\n    # Strip off leading zeros.\n    for i in range(len(s)):\n        if s[i] != '\\000':\n            break\n    else:\n        # Only happens when n == 0.\n        s = '\\000'\n        i = 0\n\n    s = s[i:]\n\n    # Add back some pad bytes. This could be done more efficiently\n    # w.r.t. the de-padding being done above, but sigh...\n    if blocksize > 0 and len(s) % blocksize:\n        s = (blocksize - len(s) % blocksize) * '\\000' + s\n\n    return s\n\n\ndef _bytelist2longBigEndian(list):\n    \"Transform a list of characters into a list of longs.\"\n\n    imax = len(list) // 4\n    hl = [0] * imax\n\n    j = 0\n    i = 0\n    while i < imax:\n        b0 = ord(list[j]) << 24\n        b1 = ord(list[j+1]) << 16\n        b2 = ord(list[j+2]) << 8\n        b3 = ord(list[j+3])\n        hl[i] = b0 | b1 | b2 | b3\n        i = i+1\n        j = j+4\n\n    return hl\n\n\ndef _rotateLeft(x, n):\n    \"Rotate x (32 bit) left n bits circularly.\"\n\n    return (x << n) | (x >> (32-n))\n\n\n# ======================================================================\n# The SHA transformation functions\n#\n# ======================================================================\n\ndef f0_19(B, C, D):\n    return (B & C) | ((~ B) & D)\n\ndef f20_39(B, C, D):\n    return B ^ C ^ D\n\ndef f40_59(B, C, D):\n    return (B & C) | (B & D) | (C & D)\n\ndef f60_79(B, C, D):\n    return B ^ C ^ D\n\n\nf = [f0_19, f20_39, f40_59, f60_79]\n\n# Constants to be used\nK = [\n    0x5A827999, # ( 0 <= t <= 19)\n    0x6ED9EBA1, # (20 <= t <= 39)\n    0x8F1BBCDC, # (40 <= t <= 59)\n    0xCA62C1D6  # (60 <= t <= 79)\n    ]\n\nclass sha:\n    \"An implementation of the SHA hash function in pure Python.\"\n\n    digest_size = digestsize = 20\n    block_size = 512 // 8\n\n    def __init__(self):\n        \"Initialisation.\"\n\n        # Initial message length in bits(!).\n        self.length = 0\n        self.count = [0, 0]\n\n        # Initial empty message as a sequence of bytes (8 bit characters).\n        self.input = []\n\n        # Call a separate init function, that can be used repeatedly\n        # to start from scratch on the same object.\n        self.init()\n\n\n    def init(self):\n        \"Initialize the message-digest and set all fields to zero.\"\n\n        self.length = 0\n        self.input = []\n\n        # Initial 160 bit message digest (5 times 32 bit).\n        self.H0 = 0x67452301\n        self.H1 = 0xEFCDAB89\n        self.H2 = 0x98BADCFE\n        self.H3 = 0x10325476\n        self.H4 = 0xC3D2E1F0\n\n    def _transform(self, W):\n\n        for t in range(16, 80):\n            W.append(_rotateLeft(\n                W[t-3] ^ W[t-8] ^ W[t-14] ^ W[t-16], 1) & 0xffffffff)\n\n        A = self.H0\n        B = self.H1\n        C = self.H2\n        D = self.H3\n        E = self.H4\n\n        \"\"\"\n        This loop was unrolled to gain about 10% in speed\n        for t in range(0, 80):\n            TEMP = _rotateLeft(A, 5) + f[t/20] + E + W[t] + K[t/20]\n            E = D\n            D = C\n            C = _rotateLeft(B, 30) & 0xffffffff\n            B = A\n            A = TEMP & 0xffffffff\n        \"\"\"\n\n        for t in range(0, 20):\n            TEMP = _rotateLeft(A, 5) + ((B & C) | ((~ B) & D)) + E + W[t] + K[0]\n            E = D\n            D = C\n            C = _rotateLeft(B, 30) & 0xffffffff\n            B = A\n            A = TEMP & 0xffffffff\n\n        for t in range(20, 40):\n            TEMP = _rotateLeft(A, 5) + (B ^ C ^ D) + E + W[t] + K[1]\n            E = D\n            D = C\n            C = _rotateLeft(B, 30) & 0xffffffff\n            B = A\n            A = TEMP & 0xffffffff\n\n        for t in range(40, 60):\n            TEMP = _rotateLeft(A, 5) + ((B & C) | (B & D) | (C & D)) + E + W[t] + K[2]\n            E = D\n            D = C\n            C = _rotateLeft(B, 30) & 0xffffffff\n            B = A\n            A = TEMP & 0xffffffff\n\n        for t in range(60, 80):\n            TEMP = _rotateLeft(A, 5) + (B ^ C ^ D)  + E + W[t] + K[3]\n            E = D\n            D = C\n            C = _rotateLeft(B, 30) & 0xffffffff\n            B = A\n            A = TEMP & 0xffffffff\n\n\n        self.H0 = (self.H0 + A) & 0xffffffff\n        self.H1 = (self.H1 + B) & 0xffffffff\n        self.H2 = (self.H2 + C) & 0xffffffff\n        self.H3 = (self.H3 + D) & 0xffffffff\n        self.H4 = (self.H4 + E) & 0xffffffff\n\n\n    # Down from here all methods follow the Python Standard Library\n    # API of the sha module.\n\n    def update(self, inBuf):\n        \"\"\"Add to the current message.\n\n        Update the md5 object with the string arg. Repeated calls\n        are equivalent to a single call with the concatenation of all\n        the arguments, i.e. m.update(a); m.update(b) is equivalent\n        to m.update(a+b).\n\n        The hash is immediately calculated for all full blocks. The final\n        calculation is made in digest(). It will calculate 1-2 blocks,\n        depending on how much padding we have to add. This allows us to\n        keep an intermediate value for the hash, so that we only need to\n        make minimal recalculation if we call update() to add more data\n        to the hashed string.\n        \"\"\"\n\n        leninBuf = len(inBuf)\n\n        # Compute number of bytes mod 64.\n        index = (self.count[1] >> 3) & 0x3F\n\n        # Update number of bits.\n        self.count[1] = self.count[1] + (leninBuf << 3)\n        if self.count[1] < (leninBuf << 3):\n            self.count[0] = self.count[0] + 1\n        self.count[0] = self.count[0] + (leninBuf >> 29)\n\n        partLen = 64 - index\n\n        if leninBuf >= partLen:\n            self.input[index:] = list(inBuf[:partLen])\n            self._transform(_bytelist2longBigEndian(self.input))\n            i = partLen\n            while i + 63 < leninBuf:\n                self._transform(_bytelist2longBigEndian(list(inBuf[i:i+64])))\n                i = i + 64\n            else:\n                self.input = list(inBuf[i:leninBuf])\n        else:\n            i = 0\n            self.input = self.input + list(inBuf)\n\n\n    def digest(self):\n        \"\"\"Terminate the message-digest computation and return digest.\n\n        Return the digest of the strings passed to the update()\n        method so far. This is a 16-byte string which may contain\n        non-ASCII characters, including null bytes.\n        \"\"\"\n\n        H0 = self.H0\n        H1 = self.H1\n        H2 = self.H2\n        H3 = self.H3\n        H4 = self.H4\n        input = [] + self.input\n        count = [] + self.count\n\n        index = (self.count[1] >> 3) & 0x3f\n\n        if index < 56:\n            padLen = 56 - index\n        else:\n            padLen = 120 - index\n\n        padding = ['\\200'] + ['\\000'] * 63\n        self.update(padding[:padLen])\n\n        # Append length (before padding).\n        bits = _bytelist2longBigEndian(self.input[:56]) + count\n\n        self._transform(bits)\n\n        # Store state in digest.\n        digest = _long2bytesBigEndian(self.H0, 4) + \\\n                 _long2bytesBigEndian(self.H1, 4) + \\\n                 _long2bytesBigEndian(self.H2, 4) + \\\n                 _long2bytesBigEndian(self.H3, 4) + \\\n                 _long2bytesBigEndian(self.H4, 4)\n\n        self.H0 = H0\n        self.H1 = H1\n        self.H2 = H2\n        self.H3 = H3\n        self.H4 = H4\n        self.input = input\n        self.count = count\n\n        return digest\n\n\n    def hexdigest(self):\n        \"\"\"Terminate and return digest in HEX form.\n\n        Like digest() except the digest is returned as a string of\n        length 32, containing only hexadecimal digits. This may be\n        used to exchange the value safely in email or other non-\n        binary environments.\n        \"\"\"\n        return ''.join(['%02x' % ord(c) for c in self.digest()])\n\n    def copy(self):\n        \"\"\"Return a clone object.\n\n        Return a copy ('clone') of the md5 object. This can be used\n        to efficiently compute the digests of strings that share\n        a common initial substring.\n        \"\"\"\n\n        return copy.deepcopy(self)\n\n\n# ======================================================================\n# Mimic Python top-level functions from standard library API\n# for consistency with the _sha module of the standard library.\n# ======================================================================\n\n# These are mandatory variables in the module. They have constant values\n# in the SHA standard.\n\ndigest_size = 20\ndigestsize = 20\nblocksize = 1\n\ndef new(arg=None):\n    \"\"\"Return a new sha crypto object.\n\n    If arg is present, the method call update(arg) is made.\n    \"\"\"\n\n    crypto = sha()\n    if arg:\n        crypto.update(arg)\n\n    return crypto\n", 
    "_sha256": "import struct\n\nSHA_BLOCKSIZE = 64\nSHA_DIGESTSIZE = 32\n\n\ndef new_shaobject():\n    return {\n        'digest': [0]*8,\n        'count_lo': 0,\n        'count_hi': 0,\n        'data': [0]* SHA_BLOCKSIZE,\n        'local': 0,\n        'digestsize': 0\n    }\n\nROR = lambda x, y: (((x & 0xffffffff) >> (y & 31)) | (x << (32 - (y & 31)))) & 0xffffffff\nCh = lambda x, y, z: (z ^ (x & (y ^ z)))\nMaj = lambda x, y, z: (((x | y) & z) | (x & y))\nS = lambda x, n: ROR(x, n)\nR = lambda x, n: (x & 0xffffffff) >> n\nSigma0 = lambda x: (S(x, 2) ^ S(x, 13) ^ S(x, 22))\nSigma1 = lambda x: (S(x, 6) ^ S(x, 11) ^ S(x, 25))\nGamma0 = lambda x: (S(x, 7) ^ S(x, 18) ^ R(x, 3))\nGamma1 = lambda x: (S(x, 17) ^ S(x, 19) ^ R(x, 10))\n\ndef sha_transform(sha_info):\n    W = []\n    \n    d = sha_info['data']\n    for i in xrange(0,16):\n        W.append( (d[4*i]<<24) + (d[4*i+1]<<16) + (d[4*i+2]<<8) + d[4*i+3])\n    \n    for i in xrange(16,64):\n        W.append( (Gamma1(W[i - 2]) + W[i - 7] + Gamma0(W[i - 15]) + W[i - 16]) & 0xffffffff )\n    \n    ss = sha_info['digest'][:]\n    \n    def RND(a,b,c,d,e,f,g,h,i,ki):\n        t0 = h + Sigma1(e) + Ch(e, f, g) + ki + W[i];\n        t1 = Sigma0(a) + Maj(a, b, c);\n        d += t0;\n        h  = t0 + t1;\n        return d & 0xffffffff, h & 0xffffffff\n    \n    ss[3], ss[7] = RND(ss[0],ss[1],ss[2],ss[3],ss[4],ss[5],ss[6],ss[7],0,0x428a2f98);\n    ss[2], ss[6] = RND(ss[7],ss[0],ss[1],ss[2],ss[3],ss[4],ss[5],ss[6],1,0x71374491);\n    ss[1], ss[5] = RND(ss[6],ss[7],ss[0],ss[1],ss[2],ss[3],ss[4],ss[5],2,0xb5c0fbcf);\n    ss[0], ss[4] = RND(ss[5],ss[6],ss[7],ss[0],ss[1],ss[2],ss[3],ss[4],3,0xe9b5dba5);\n    ss[7], ss[3] = RND(ss[4],ss[5],ss[6],ss[7],ss[0],ss[1],ss[2],ss[3],4,0x3956c25b);\n    ss[6], ss[2] = RND(ss[3],ss[4],ss[5],ss[6],ss[7],ss[0],ss[1],ss[2],5,0x59f111f1);\n    ss[5], ss[1] = RND(ss[2],ss[3],ss[4],ss[5],ss[6],ss[7],ss[0],ss[1],6,0x923f82a4);\n    ss[4], ss[0] = RND(ss[1],ss[2],ss[3],ss[4],ss[5],ss[6],ss[7],ss[0],7,0xab1c5ed5);\n    ss[3], ss[7] = RND(ss[0],ss[1],ss[2],ss[3],ss[4],ss[5],ss[6],ss[7],8,0xd807aa98);\n    ss[2], ss[6] = RND(ss[7],ss[0],ss[1],ss[2],ss[3],ss[4],ss[5],ss[6],9,0x12835b01);\n    ss[1], ss[5] = RND(ss[6],ss[7],ss[0],ss[1],ss[2],ss[3],ss[4],ss[5],10,0x243185be);\n    ss[0], ss[4] = RND(ss[5],ss[6],ss[7],ss[0],ss[1],ss[2],ss[3],ss[4],11,0x550c7dc3);\n    ss[7], ss[3] = RND(ss[4],ss[5],ss[6],ss[7],ss[0],ss[1],ss[2],ss[3],12,0x72be5d74);\n    ss[6], ss[2] = RND(ss[3],ss[4],ss[5],ss[6],ss[7],ss[0],ss[1],ss[2],13,0x80deb1fe);\n    ss[5], ss[1] = RND(ss[2],ss[3],ss[4],ss[5],ss[6],ss[7],ss[0],ss[1],14,0x9bdc06a7);\n    ss[4], ss[0] = RND(ss[1],ss[2],ss[3],ss[4],ss[5],ss[6],ss[7],ss[0],15,0xc19bf174);\n    ss[3], ss[7] = RND(ss[0],ss[1],ss[2],ss[3],ss[4],ss[5],ss[6],ss[7],16,0xe49b69c1);\n    ss[2], ss[6] = RND(ss[7],ss[0],ss[1],ss[2],ss[3],ss[4],ss[5],ss[6],17,0xefbe4786);\n    ss[1], ss[5] = RND(ss[6],ss[7],ss[0],ss[1],ss[2],ss[3],ss[4],ss[5],18,0x0fc19dc6);\n    ss[0], ss[4] = RND(ss[5],ss[6],ss[7],ss[0],ss[1],ss[2],ss[3],ss[4],19,0x240ca1cc);\n    ss[7], ss[3] = RND(ss[4],ss[5],ss[6],ss[7],ss[0],ss[1],ss[2],ss[3],20,0x2de92c6f);\n    ss[6], ss[2] = RND(ss[3],ss[4],ss[5],ss[6],ss[7],ss[0],ss[1],ss[2],21,0x4a7484aa);\n    ss[5], ss[1] = RND(ss[2],ss[3],ss[4],ss[5],ss[6],ss[7],ss[0],ss[1],22,0x5cb0a9dc);\n    ss[4], ss[0] = RND(ss[1],ss[2],ss[3],ss[4],ss[5],ss[6],ss[7],ss[0],23,0x76f988da);\n    ss[3], ss[7] = RND(ss[0],ss[1],ss[2],ss[3],ss[4],ss[5],ss[6],ss[7],24,0x983e5152);\n    ss[2], ss[6] = RND(ss[7],ss[0],ss[1],ss[2],ss[3],ss[4],ss[5],ss[6],25,0xa831c66d);\n    ss[1], ss[5] = RND(ss[6],ss[7],ss[0],ss[1],ss[2],ss[3],ss[4],ss[5],26,0xb00327c8);\n    ss[0], ss[4] = RND(ss[5],ss[6],ss[7],ss[0],ss[1],ss[2],ss[3],ss[4],27,0xbf597fc7);\n    ss[7], ss[3] = RND(ss[4],ss[5],ss[6],ss[7],ss[0],ss[1],ss[2],ss[3],28,0xc6e00bf3);\n    ss[6], ss[2] = RND(ss[3],ss[4],ss[5],ss[6],ss[7],ss[0],ss[1],ss[2],29,0xd5a79147);\n    ss[5], ss[1] = RND(ss[2],ss[3],ss[4],ss[5],ss[6],ss[7],ss[0],ss[1],30,0x06ca6351);\n    ss[4], ss[0] = RND(ss[1],ss[2],ss[3],ss[4],ss[5],ss[6],ss[7],ss[0],31,0x14292967);\n    ss[3], ss[7] = RND(ss[0],ss[1],ss[2],ss[3],ss[4],ss[5],ss[6],ss[7],32,0x27b70a85);\n    ss[2], ss[6] = RND(ss[7],ss[0],ss[1],ss[2],ss[3],ss[4],ss[5],ss[6],33,0x2e1b2138);\n    ss[1], ss[5] = RND(ss[6],ss[7],ss[0],ss[1],ss[2],ss[3],ss[4],ss[5],34,0x4d2c6dfc);\n    ss[0], ss[4] = RND(ss[5],ss[6],ss[7],ss[0],ss[1],ss[2],ss[3],ss[4],35,0x53380d13);\n    ss[7], ss[3] = RND(ss[4],ss[5],ss[6],ss[7],ss[0],ss[1],ss[2],ss[3],36,0x650a7354);\n    ss[6], ss[2] = RND(ss[3],ss[4],ss[5],ss[6],ss[7],ss[0],ss[1],ss[2],37,0x766a0abb);\n    ss[5], ss[1] = RND(ss[2],ss[3],ss[4],ss[5],ss[6],ss[7],ss[0],ss[1],38,0x81c2c92e);\n    ss[4], ss[0] = RND(ss[1],ss[2],ss[3],ss[4],ss[5],ss[6],ss[7],ss[0],39,0x92722c85);\n    ss[3], ss[7] = RND(ss[0],ss[1],ss[2],ss[3],ss[4],ss[5],ss[6],ss[7],40,0xa2bfe8a1);\n    ss[2], ss[6] = RND(ss[7],ss[0],ss[1],ss[2],ss[3],ss[4],ss[5],ss[6],41,0xa81a664b);\n    ss[1], ss[5] = RND(ss[6],ss[7],ss[0],ss[1],ss[2],ss[3],ss[4],ss[5],42,0xc24b8b70);\n    ss[0], ss[4] = RND(ss[5],ss[6],ss[7],ss[0],ss[1],ss[2],ss[3],ss[4],43,0xc76c51a3);\n    ss[7], ss[3] = RND(ss[4],ss[5],ss[6],ss[7],ss[0],ss[1],ss[2],ss[3],44,0xd192e819);\n    ss[6], ss[2] = RND(ss[3],ss[4],ss[5],ss[6],ss[7],ss[0],ss[1],ss[2],45,0xd6990624);\n    ss[5], ss[1] = RND(ss[2],ss[3],ss[4],ss[5],ss[6],ss[7],ss[0],ss[1],46,0xf40e3585);\n    ss[4], ss[0] = RND(ss[1],ss[2],ss[3],ss[4],ss[5],ss[6],ss[7],ss[0],47,0x106aa070);\n    ss[3], ss[7] = RND(ss[0],ss[1],ss[2],ss[3],ss[4],ss[5],ss[6],ss[7],48,0x19a4c116);\n    ss[2], ss[6] = RND(ss[7],ss[0],ss[1],ss[2],ss[3],ss[4],ss[5],ss[6],49,0x1e376c08);\n    ss[1], ss[5] = RND(ss[6],ss[7],ss[0],ss[1],ss[2],ss[3],ss[4],ss[5],50,0x2748774c);\n    ss[0], ss[4] = RND(ss[5],ss[6],ss[7],ss[0],ss[1],ss[2],ss[3],ss[4],51,0x34b0bcb5);\n    ss[7], ss[3] = RND(ss[4],ss[5],ss[6],ss[7],ss[0],ss[1],ss[2],ss[3],52,0x391c0cb3);\n    ss[6], ss[2] = RND(ss[3],ss[4],ss[5],ss[6],ss[7],ss[0],ss[1],ss[2],53,0x4ed8aa4a);\n    ss[5], ss[1] = RND(ss[2],ss[3],ss[4],ss[5],ss[6],ss[7],ss[0],ss[1],54,0x5b9cca4f);\n    ss[4], ss[0] = RND(ss[1],ss[2],ss[3],ss[4],ss[5],ss[6],ss[7],ss[0],55,0x682e6ff3);\n    ss[3], ss[7] = RND(ss[0],ss[1],ss[2],ss[3],ss[4],ss[5],ss[6],ss[7],56,0x748f82ee);\n    ss[2], ss[6] = RND(ss[7],ss[0],ss[1],ss[2],ss[3],ss[4],ss[5],ss[6],57,0x78a5636f);\n    ss[1], ss[5] = RND(ss[6],ss[7],ss[0],ss[1],ss[2],ss[3],ss[4],ss[5],58,0x84c87814);\n    ss[0], ss[4] = RND(ss[5],ss[6],ss[7],ss[0],ss[1],ss[2],ss[3],ss[4],59,0x8cc70208);\n    ss[7], ss[3] = RND(ss[4],ss[5],ss[6],ss[7],ss[0],ss[1],ss[2],ss[3],60,0x90befffa);\n    ss[6], ss[2] = RND(ss[3],ss[4],ss[5],ss[6],ss[7],ss[0],ss[1],ss[2],61,0xa4506ceb);\n    ss[5], ss[1] = RND(ss[2],ss[3],ss[4],ss[5],ss[6],ss[7],ss[0],ss[1],62,0xbef9a3f7);\n    ss[4], ss[0] = RND(ss[1],ss[2],ss[3],ss[4],ss[5],ss[6],ss[7],ss[0],63,0xc67178f2);\n    \n    dig = []\n    for i, x in enumerate(sha_info['digest']):\n        dig.append( (x + ss[i]) & 0xffffffff )\n    sha_info['digest'] = dig\n\ndef sha_init():\n    sha_info = new_shaobject()\n    sha_info['digest'] = [0x6A09E667, 0xBB67AE85, 0x3C6EF372, 0xA54FF53A, 0x510E527F, 0x9B05688C, 0x1F83D9AB, 0x5BE0CD19]\n    sha_info['count_lo'] = 0\n    sha_info['count_hi'] = 0\n    sha_info['local'] = 0\n    sha_info['digestsize'] = 32\n    return sha_info\n\ndef sha224_init():\n    sha_info = new_shaobject()\n    sha_info['digest'] = [0xc1059ed8, 0x367cd507, 0x3070dd17, 0xf70e5939, 0xffc00b31, 0x68581511, 0x64f98fa7, 0xbefa4fa4]\n    sha_info['count_lo'] = 0\n    sha_info['count_hi'] = 0\n    sha_info['local'] = 0\n    sha_info['digestsize'] = 28\n    return sha_info\n\ndef getbuf(s):\n    if isinstance(s, str):\n        return s\n    elif isinstance(s, unicode):\n        return str(s)\n    else:\n        return buffer(s)\n\ndef sha_update(sha_info, buffer):\n    count = len(buffer)\n    buffer_idx = 0\n    clo = (sha_info['count_lo'] + (count << 3)) & 0xffffffff\n    if clo < sha_info['count_lo']:\n        sha_info['count_hi'] += 1\n    sha_info['count_lo'] = clo\n    \n    sha_info['count_hi'] += (count >> 29)\n    \n    if sha_info['local']:\n        i = SHA_BLOCKSIZE - sha_info['local']\n        if i > count:\n            i = count\n        \n        # copy buffer\n        for x in enumerate(buffer[buffer_idx:buffer_idx+i]):\n            sha_info['data'][sha_info['local']+x[0]] = struct.unpack('B', x[1])[0]\n        \n        count -= i\n        buffer_idx += i\n        \n        sha_info['local'] += i\n        if sha_info['local'] == SHA_BLOCKSIZE:\n            sha_transform(sha_info)\n            sha_info['local'] = 0\n        else:\n            return\n    \n    while count >= SHA_BLOCKSIZE:\n        # copy buffer\n        sha_info['data'] = [struct.unpack('B',c)[0] for c in buffer[buffer_idx:buffer_idx + SHA_BLOCKSIZE]]\n        count -= SHA_BLOCKSIZE\n        buffer_idx += SHA_BLOCKSIZE\n        sha_transform(sha_info)\n        \n    \n    # copy buffer\n    pos = sha_info['local']\n    sha_info['data'][pos:pos+count] = [struct.unpack('B',c)[0] for c in buffer[buffer_idx:buffer_idx + count]]\n    sha_info['local'] = count\n\ndef sha_final(sha_info):\n    lo_bit_count = sha_info['count_lo']\n    hi_bit_count = sha_info['count_hi']\n    count = (lo_bit_count >> 3) & 0x3f\n    sha_info['data'][count] = 0x80;\n    count += 1\n    if count > SHA_BLOCKSIZE - 8:\n        # zero the bytes in data after the count\n        sha_info['data'] = sha_info['data'][:count] + ([0] * (SHA_BLOCKSIZE - count))\n        sha_transform(sha_info)\n        # zero bytes in data\n        sha_info['data'] = [0] * SHA_BLOCKSIZE\n    else:\n        sha_info['data'] = sha_info['data'][:count] + ([0] * (SHA_BLOCKSIZE - count))\n    \n    sha_info['data'][56] = (hi_bit_count >> 24) & 0xff\n    sha_info['data'][57] = (hi_bit_count >> 16) & 0xff\n    sha_info['data'][58] = (hi_bit_count >>  8) & 0xff\n    sha_info['data'][59] = (hi_bit_count >>  0) & 0xff\n    sha_info['data'][60] = (lo_bit_count >> 24) & 0xff\n    sha_info['data'][61] = (lo_bit_count >> 16) & 0xff\n    sha_info['data'][62] = (lo_bit_count >>  8) & 0xff\n    sha_info['data'][63] = (lo_bit_count >>  0) & 0xff\n    \n    sha_transform(sha_info)\n    \n    dig = []\n    for i in sha_info['digest']:\n        dig.extend([ ((i>>24) & 0xff), ((i>>16) & 0xff), ((i>>8) & 0xff), (i & 0xff) ])\n    return ''.join([chr(i) for i in dig])\n\nclass sha256(object):\n    digest_size = digestsize = SHA_DIGESTSIZE\n    block_size = SHA_BLOCKSIZE\n\n    def __init__(self, s=None):\n        self._sha = sha_init()\n        if s:\n            sha_update(self._sha, getbuf(s))\n    \n    def update(self, s):\n        sha_update(self._sha, getbuf(s))\n    \n    def digest(self):\n        return sha_final(self._sha.copy())[:self._sha['digestsize']]\n    \n    def hexdigest(self):\n        return ''.join(['%.2x' % ord(i) for i in self.digest()])\n\n    def copy(self):\n        new = sha256.__new__(sha256)\n        new._sha = self._sha.copy()\n        return new\n\nclass sha224(sha256):\n    digest_size = digestsize = 28\n\n    def __init__(self, s=None):\n        self._sha = sha224_init()\n        if s:\n            sha_update(self._sha, getbuf(s))\n\n    def copy(self):\n        new = sha224.__new__(sha224)\n        new._sha = self._sha.copy()\n        return new\n\ndef test():\n    a_str = \"just a test string\"\n    \n    assert 'e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855' == sha256().hexdigest()\n    assert 'd7b553c6f09ac85d142415f857c5310f3bbbe7cdd787cce4b985acedd585266f' == sha256(a_str).hexdigest()\n    assert '8113ebf33c97daa9998762aacafe750c7cefc2b2f173c90c59663a57fe626f21' == sha256(a_str*7).hexdigest()\n    \n    s = sha256(a_str)\n    s.update(a_str)\n    assert '03d9963e05a094593190b6fc794cb1a3e1ac7d7883f0b5855268afeccc70d461' == s.hexdigest()\n\nif __name__ == \"__main__\":\n    test()\n\n\n", 
    "_sha512": "\"\"\"\nThis code was Ported from CPython's sha512module.c\n\"\"\"\n\nimport struct\n\nSHA_BLOCKSIZE = 128\nSHA_DIGESTSIZE = 64\n\n\ndef new_shaobject():\n    return {\n        'digest': [0]*8,\n        'count_lo': 0,\n        'count_hi': 0,\n        'data': [0]* SHA_BLOCKSIZE,\n        'local': 0,\n        'digestsize': 0\n    }\n\nROR64 = lambda x, y: (((x & 0xffffffffffffffff) >> (y & 63)) | (x << (64 - (y & 63)))) & 0xffffffffffffffff\nCh = lambda x, y, z: (z ^ (x & (y ^ z)))\nMaj = lambda x, y, z: (((x | y) & z) | (x & y))\nS = lambda x, n: ROR64(x, n)\nR = lambda x, n: (x & 0xffffffffffffffff) >> n\nSigma0 = lambda x: (S(x, 28) ^ S(x, 34) ^ S(x, 39))\nSigma1 = lambda x: (S(x, 14) ^ S(x, 18) ^ S(x, 41))\nGamma0 = lambda x: (S(x, 1) ^ S(x, 8) ^ R(x, 7))\nGamma1 = lambda x: (S(x, 19) ^ S(x, 61) ^ R(x, 6))\n\ndef sha_transform(sha_info):\n    W = []\n\n    d = sha_info['data']\n    for i in xrange(0,16):\n        W.append( (d[8*i]<<56) + (d[8*i+1]<<48) + (d[8*i+2]<<40) + (d[8*i+3]<<32) + (d[8*i+4]<<24) + (d[8*i+5]<<16) + (d[8*i+6]<<8) + d[8*i+7])\n\n    for i in xrange(16,80):\n        W.append( (Gamma1(W[i - 2]) + W[i - 7] + Gamma0(W[i - 15]) + W[i - 16]) & 0xffffffffffffffff )\n\n    ss = sha_info['digest'][:]\n\n    def RND(a,b,c,d,e,f,g,h,i,ki):\n        t0 = (h + Sigma1(e) + Ch(e, f, g) + ki + W[i]) & 0xffffffffffffffff\n        t1 = (Sigma0(a) + Maj(a, b, c)) & 0xffffffffffffffff\n        d = (d + t0) & 0xffffffffffffffff\n        h = (t0 + t1) & 0xffffffffffffffff\n        return d & 0xffffffffffffffff, h & 0xffffffffffffffff\n\n    ss[3], ss[7] = RND(ss[0],ss[1],ss[2],ss[3],ss[4],ss[5],ss[6],ss[7],0,0x428a2f98d728ae22)\n    ss[2], ss[6] = RND(ss[7],ss[0],ss[1],ss[2],ss[3],ss[4],ss[5],ss[6],1,0x7137449123ef65cd)\n    ss[1], ss[5] = RND(ss[6],ss[7],ss[0],ss[1],ss[2],ss[3],ss[4],ss[5],2,0xb5c0fbcfec4d3b2f)\n    ss[0], ss[4] = RND(ss[5],ss[6],ss[7],ss[0],ss[1],ss[2],ss[3],ss[4],3,0xe9b5dba58189dbbc)\n    ss[7], ss[3] = RND(ss[4],ss[5],ss[6],ss[7],ss[0],ss[1],ss[2],ss[3],4,0x3956c25bf348b538)\n    ss[6], ss[2] = RND(ss[3],ss[4],ss[5],ss[6],ss[7],ss[0],ss[1],ss[2],5,0x59f111f1b605d019)\n    ss[5], ss[1] = RND(ss[2],ss[3],ss[4],ss[5],ss[6],ss[7],ss[0],ss[1],6,0x923f82a4af194f9b)\n    ss[4], ss[0] = RND(ss[1],ss[2],ss[3],ss[4],ss[5],ss[6],ss[7],ss[0],7,0xab1c5ed5da6d8118)\n    ss[3], ss[7] = RND(ss[0],ss[1],ss[2],ss[3],ss[4],ss[5],ss[6],ss[7],8,0xd807aa98a3030242)\n    ss[2], ss[6] = RND(ss[7],ss[0],ss[1],ss[2],ss[3],ss[4],ss[5],ss[6],9,0x12835b0145706fbe)\n    ss[1], ss[5] = RND(ss[6],ss[7],ss[0],ss[1],ss[2],ss[3],ss[4],ss[5],10,0x243185be4ee4b28c)\n    ss[0], ss[4] = RND(ss[5],ss[6],ss[7],ss[0],ss[1],ss[2],ss[3],ss[4],11,0x550c7dc3d5ffb4e2)\n    ss[7], ss[3] = RND(ss[4],ss[5],ss[6],ss[7],ss[0],ss[1],ss[2],ss[3],12,0x72be5d74f27b896f)\n    ss[6], ss[2] = RND(ss[3],ss[4],ss[5],ss[6],ss[7],ss[0],ss[1],ss[2],13,0x80deb1fe3b1696b1)\n    ss[5], ss[1] = RND(ss[2],ss[3],ss[4],ss[5],ss[6],ss[7],ss[0],ss[1],14,0x9bdc06a725c71235)\n    ss[4], ss[0] = RND(ss[1],ss[2],ss[3],ss[4],ss[5],ss[6],ss[7],ss[0],15,0xc19bf174cf692694)\n    ss[3], ss[7] = RND(ss[0],ss[1],ss[2],ss[3],ss[4],ss[5],ss[6],ss[7],16,0xe49b69c19ef14ad2)\n    ss[2], ss[6] = RND(ss[7],ss[0],ss[1],ss[2],ss[3],ss[4],ss[5],ss[6],17,0xefbe4786384f25e3)\n    ss[1], ss[5] = RND(ss[6],ss[7],ss[0],ss[1],ss[2],ss[3],ss[4],ss[5],18,0x0fc19dc68b8cd5b5)\n    ss[0], ss[4] = RND(ss[5],ss[6],ss[7],ss[0],ss[1],ss[2],ss[3],ss[4],19,0x240ca1cc77ac9c65)\n    ss[7], ss[3] = RND(ss[4],ss[5],ss[6],ss[7],ss[0],ss[1],ss[2],ss[3],20,0x2de92c6f592b0275)\n    ss[6], ss[2] = RND(ss[3],ss[4],ss[5],ss[6],ss[7],ss[0],ss[1],ss[2],21,0x4a7484aa6ea6e483)\n    ss[5], ss[1] = RND(ss[2],ss[3],ss[4],ss[5],ss[6],ss[7],ss[0],ss[1],22,0x5cb0a9dcbd41fbd4)\n    ss[4], ss[0] = RND(ss[1],ss[2],ss[3],ss[4],ss[5],ss[6],ss[7],ss[0],23,0x76f988da831153b5)\n    ss[3], ss[7] = RND(ss[0],ss[1],ss[2],ss[3],ss[4],ss[5],ss[6],ss[7],24,0x983e5152ee66dfab)\n    ss[2], ss[6] = RND(ss[7],ss[0],ss[1],ss[2],ss[3],ss[4],ss[5],ss[6],25,0xa831c66d2db43210)\n    ss[1], ss[5] = RND(ss[6],ss[7],ss[0],ss[1],ss[2],ss[3],ss[4],ss[5],26,0xb00327c898fb213f)\n    ss[0], ss[4] = RND(ss[5],ss[6],ss[7],ss[0],ss[1],ss[2],ss[3],ss[4],27,0xbf597fc7beef0ee4)\n    ss[7], ss[3] = RND(ss[4],ss[5],ss[6],ss[7],ss[0],ss[1],ss[2],ss[3],28,0xc6e00bf33da88fc2)\n    ss[6], ss[2] = RND(ss[3],ss[4],ss[5],ss[6],ss[7],ss[0],ss[1],ss[2],29,0xd5a79147930aa725)\n    ss[5], ss[1] = RND(ss[2],ss[3],ss[4],ss[5],ss[6],ss[7],ss[0],ss[1],30,0x06ca6351e003826f)\n    ss[4], ss[0] = RND(ss[1],ss[2],ss[3],ss[4],ss[5],ss[6],ss[7],ss[0],31,0x142929670a0e6e70)\n    ss[3], ss[7] = RND(ss[0],ss[1],ss[2],ss[3],ss[4],ss[5],ss[6],ss[7],32,0x27b70a8546d22ffc)\n    ss[2], ss[6] = RND(ss[7],ss[0],ss[1],ss[2],ss[3],ss[4],ss[5],ss[6],33,0x2e1b21385c26c926)\n    ss[1], ss[5] = RND(ss[6],ss[7],ss[0],ss[1],ss[2],ss[3],ss[4],ss[5],34,0x4d2c6dfc5ac42aed)\n    ss[0], ss[4] = RND(ss[5],ss[6],ss[7],ss[0],ss[1],ss[2],ss[3],ss[4],35,0x53380d139d95b3df)\n    ss[7], ss[3] = RND(ss[4],ss[5],ss[6],ss[7],ss[0],ss[1],ss[2],ss[3],36,0x650a73548baf63de)\n    ss[6], ss[2] = RND(ss[3],ss[4],ss[5],ss[6],ss[7],ss[0],ss[1],ss[2],37,0x766a0abb3c77b2a8)\n    ss[5], ss[1] = RND(ss[2],ss[3],ss[4],ss[5],ss[6],ss[7],ss[0],ss[1],38,0x81c2c92e47edaee6)\n    ss[4], ss[0] = RND(ss[1],ss[2],ss[3],ss[4],ss[5],ss[6],ss[7],ss[0],39,0x92722c851482353b)\n    ss[3], ss[7] = RND(ss[0],ss[1],ss[2],ss[3],ss[4],ss[5],ss[6],ss[7],40,0xa2bfe8a14cf10364)\n    ss[2], ss[6] = RND(ss[7],ss[0],ss[1],ss[2],ss[3],ss[4],ss[5],ss[6],41,0xa81a664bbc423001)\n    ss[1], ss[5] = RND(ss[6],ss[7],ss[0],ss[1],ss[2],ss[3],ss[4],ss[5],42,0xc24b8b70d0f89791)\n    ss[0], ss[4] = RND(ss[5],ss[6],ss[7],ss[0],ss[1],ss[2],ss[3],ss[4],43,0xc76c51a30654be30)\n    ss[7], ss[3] = RND(ss[4],ss[5],ss[6],ss[7],ss[0],ss[1],ss[2],ss[3],44,0xd192e819d6ef5218)\n    ss[6], ss[2] = RND(ss[3],ss[4],ss[5],ss[6],ss[7],ss[0],ss[1],ss[2],45,0xd69906245565a910)\n    ss[5], ss[1] = RND(ss[2],ss[3],ss[4],ss[5],ss[6],ss[7],ss[0],ss[1],46,0xf40e35855771202a)\n    ss[4], ss[0] = RND(ss[1],ss[2],ss[3],ss[4],ss[5],ss[6],ss[7],ss[0],47,0x106aa07032bbd1b8)\n    ss[3], ss[7] = RND(ss[0],ss[1],ss[2],ss[3],ss[4],ss[5],ss[6],ss[7],48,0x19a4c116b8d2d0c8)\n    ss[2], ss[6] = RND(ss[7],ss[0],ss[1],ss[2],ss[3],ss[4],ss[5],ss[6],49,0x1e376c085141ab53)\n    ss[1], ss[5] = RND(ss[6],ss[7],ss[0],ss[1],ss[2],ss[3],ss[4],ss[5],50,0x2748774cdf8eeb99)\n    ss[0], ss[4] = RND(ss[5],ss[6],ss[7],ss[0],ss[1],ss[2],ss[3],ss[4],51,0x34b0bcb5e19b48a8)\n    ss[7], ss[3] = RND(ss[4],ss[5],ss[6],ss[7],ss[0],ss[1],ss[2],ss[3],52,0x391c0cb3c5c95a63)\n    ss[6], ss[2] = RND(ss[3],ss[4],ss[5],ss[6],ss[7],ss[0],ss[1],ss[2],53,0x4ed8aa4ae3418acb)\n    ss[5], ss[1] = RND(ss[2],ss[3],ss[4],ss[5],ss[6],ss[7],ss[0],ss[1],54,0x5b9cca4f7763e373)\n    ss[4], ss[0] = RND(ss[1],ss[2],ss[3],ss[4],ss[5],ss[6],ss[7],ss[0],55,0x682e6ff3d6b2b8a3)\n    ss[3], ss[7] = RND(ss[0],ss[1],ss[2],ss[3],ss[4],ss[5],ss[6],ss[7],56,0x748f82ee5defb2fc)\n    ss[2], ss[6] = RND(ss[7],ss[0],ss[1],ss[2],ss[3],ss[4],ss[5],ss[6],57,0x78a5636f43172f60)\n    ss[1], ss[5] = RND(ss[6],ss[7],ss[0],ss[1],ss[2],ss[3],ss[4],ss[5],58,0x84c87814a1f0ab72)\n    ss[0], ss[4] = RND(ss[5],ss[6],ss[7],ss[0],ss[1],ss[2],ss[3],ss[4],59,0x8cc702081a6439ec)\n    ss[7], ss[3] = RND(ss[4],ss[5],ss[6],ss[7],ss[0],ss[1],ss[2],ss[3],60,0x90befffa23631e28)\n    ss[6], ss[2] = RND(ss[3],ss[4],ss[5],ss[6],ss[7],ss[0],ss[1],ss[2],61,0xa4506cebde82bde9)\n    ss[5], ss[1] = RND(ss[2],ss[3],ss[4],ss[5],ss[6],ss[7],ss[0],ss[1],62,0xbef9a3f7b2c67915)\n    ss[4], ss[0] = RND(ss[1],ss[2],ss[3],ss[4],ss[5],ss[6],ss[7],ss[0],63,0xc67178f2e372532b)\n    ss[3], ss[7] = RND(ss[0],ss[1],ss[2],ss[3],ss[4],ss[5],ss[6],ss[7],64,0xca273eceea26619c)\n    ss[2], ss[6] = RND(ss[7],ss[0],ss[1],ss[2],ss[3],ss[4],ss[5],ss[6],65,0xd186b8c721c0c207)\n    ss[1], ss[5] = RND(ss[6],ss[7],ss[0],ss[1],ss[2],ss[3],ss[4],ss[5],66,0xeada7dd6cde0eb1e)\n    ss[0], ss[4] = RND(ss[5],ss[6],ss[7],ss[0],ss[1],ss[2],ss[3],ss[4],67,0xf57d4f7fee6ed178)\n    ss[7], ss[3] = RND(ss[4],ss[5],ss[6],ss[7],ss[0],ss[1],ss[2],ss[3],68,0x06f067aa72176fba)\n    ss[6], ss[2] = RND(ss[3],ss[4],ss[5],ss[6],ss[7],ss[0],ss[1],ss[2],69,0x0a637dc5a2c898a6)\n    ss[5], ss[1] = RND(ss[2],ss[3],ss[4],ss[5],ss[6],ss[7],ss[0],ss[1],70,0x113f9804bef90dae)\n    ss[4], ss[0] = RND(ss[1],ss[2],ss[3],ss[4],ss[5],ss[6],ss[7],ss[0],71,0x1b710b35131c471b)\n    ss[3], ss[7] = RND(ss[0],ss[1],ss[2],ss[3],ss[4],ss[5],ss[6],ss[7],72,0x28db77f523047d84)\n    ss[2], ss[6] = RND(ss[7],ss[0],ss[1],ss[2],ss[3],ss[4],ss[5],ss[6],73,0x32caab7b40c72493)\n    ss[1], ss[5] = RND(ss[6],ss[7],ss[0],ss[1],ss[2],ss[3],ss[4],ss[5],74,0x3c9ebe0a15c9bebc)\n    ss[0], ss[4] = RND(ss[5],ss[6],ss[7],ss[0],ss[1],ss[2],ss[3],ss[4],75,0x431d67c49c100d4c)\n    ss[7], ss[3] = RND(ss[4],ss[5],ss[6],ss[7],ss[0],ss[1],ss[2],ss[3],76,0x4cc5d4becb3e42b6)\n    ss[6], ss[2] = RND(ss[3],ss[4],ss[5],ss[6],ss[7],ss[0],ss[1],ss[2],77,0x597f299cfc657e2a)\n    ss[5], ss[1] = RND(ss[2],ss[3],ss[4],ss[5],ss[6],ss[7],ss[0],ss[1],78,0x5fcb6fab3ad6faec)\n    ss[4], ss[0] = RND(ss[1],ss[2],ss[3],ss[4],ss[5],ss[6],ss[7],ss[0],79,0x6c44198c4a475817)\n\n    dig = []\n    for i, x in enumerate(sha_info['digest']):\n        dig.append( (x + ss[i]) & 0xffffffffffffffff )\n    sha_info['digest'] = dig\n\ndef sha_init():\n    sha_info = new_shaobject()\n    sha_info['digest'] = [ 0x6a09e667f3bcc908, 0xbb67ae8584caa73b, 0x3c6ef372fe94f82b, 0xa54ff53a5f1d36f1, 0x510e527fade682d1, 0x9b05688c2b3e6c1f, 0x1f83d9abfb41bd6b, 0x5be0cd19137e2179]\n    sha_info['count_lo'] = 0\n    sha_info['count_hi'] = 0\n    sha_info['local'] = 0\n    sha_info['digestsize'] = 64\n    return sha_info\n\ndef sha384_init():\n    sha_info = new_shaobject()\n    sha_info['digest'] = [ 0xcbbb9d5dc1059ed8, 0x629a292a367cd507, 0x9159015a3070dd17, 0x152fecd8f70e5939, 0x67332667ffc00b31, 0x8eb44a8768581511, 0xdb0c2e0d64f98fa7, 0x47b5481dbefa4fa4]\n    sha_info['count_lo'] = 0\n    sha_info['count_hi'] = 0\n    sha_info['local'] = 0\n    sha_info['digestsize'] = 48\n    return sha_info\n\ndef getbuf(s):\n    if isinstance(s, str):\n        return s\n    elif isinstance(s, unicode):\n        return str(s)\n    else:\n        return buffer(s)\n\ndef sha_update(sha_info, buffer):\n    count = len(buffer)\n    buffer_idx = 0\n    clo = (sha_info['count_lo'] + (count << 3)) & 0xffffffff\n    if clo < sha_info['count_lo']:\n        sha_info['count_hi'] += 1\n    sha_info['count_lo'] = clo\n\n    sha_info['count_hi'] += (count >> 29)\n\n    if sha_info['local']:\n        i = SHA_BLOCKSIZE - sha_info['local']\n        if i > count:\n            i = count\n\n        # copy buffer\n        for x in enumerate(buffer[buffer_idx:buffer_idx+i]):\n            sha_info['data'][sha_info['local']+x[0]] = struct.unpack('B', x[1])[0]\n\n        count -= i\n        buffer_idx += i\n\n        sha_info['local'] += i\n        if sha_info['local'] == SHA_BLOCKSIZE:\n            sha_transform(sha_info)\n            sha_info['local'] = 0\n        else:\n            return\n\n    while count >= SHA_BLOCKSIZE:\n        # copy buffer\n        sha_info['data'] = [struct.unpack('B',c)[0] for c in buffer[buffer_idx:buffer_idx + SHA_BLOCKSIZE]]\n        count -= SHA_BLOCKSIZE\n        buffer_idx += SHA_BLOCKSIZE\n        sha_transform(sha_info)\n\n    # copy buffer\n    pos = sha_info['local']\n    sha_info['data'][pos:pos+count] = [struct.unpack('B',c)[0] for c in buffer[buffer_idx:buffer_idx + count]]\n    sha_info['local'] = count\n\ndef sha_final(sha_info):\n    lo_bit_count = sha_info['count_lo']\n    hi_bit_count = sha_info['count_hi']\n    count = (lo_bit_count >> 3) & 0x7f\n    sha_info['data'][count] = 0x80;\n    count += 1\n    if count > SHA_BLOCKSIZE - 16:\n        # zero the bytes in data after the count\n        sha_info['data'] = sha_info['data'][:count] + ([0] * (SHA_BLOCKSIZE - count))\n        sha_transform(sha_info)\n        # zero bytes in data\n        sha_info['data'] = [0] * SHA_BLOCKSIZE\n    else:\n        sha_info['data'] = sha_info['data'][:count] + ([0] * (SHA_BLOCKSIZE - count))\n\n    sha_info['data'][112] = 0;\n    sha_info['data'][113] = 0;\n    sha_info['data'][114] = 0;\n    sha_info['data'][115] = 0;\n    sha_info['data'][116] = 0;\n    sha_info['data'][117] = 0;\n    sha_info['data'][118] = 0;\n    sha_info['data'][119] = 0;\n\n    sha_info['data'][120] = (hi_bit_count >> 24) & 0xff\n    sha_info['data'][121] = (hi_bit_count >> 16) & 0xff\n    sha_info['data'][122] = (hi_bit_count >>  8) & 0xff\n    sha_info['data'][123] = (hi_bit_count >>  0) & 0xff\n    sha_info['data'][124] = (lo_bit_count >> 24) & 0xff\n    sha_info['data'][125] = (lo_bit_count >> 16) & 0xff\n    sha_info['data'][126] = (lo_bit_count >>  8) & 0xff\n    sha_info['data'][127] = (lo_bit_count >>  0) & 0xff\n\n    sha_transform(sha_info)\n\n    dig = []\n    for i in sha_info['digest']:\n        dig.extend([ ((i>>56) & 0xff), ((i>>48) & 0xff), ((i>>40) & 0xff), ((i>>32) & 0xff), ((i>>24) & 0xff), ((i>>16) & 0xff), ((i>>8) & 0xff), (i & 0xff) ])\n    return ''.join([chr(i) for i in dig])\n\nclass sha512(object):\n    digest_size = digestsize = SHA_DIGESTSIZE\n    block_size = SHA_BLOCKSIZE\n\n    def __init__(self, s=None):\n        self._sha = sha_init()\n        if s:\n            sha_update(self._sha, getbuf(s))\n\n    def update(self, s):\n        sha_update(self._sha, getbuf(s))\n\n    def digest(self):\n        return sha_final(self._sha.copy())[:self._sha['digestsize']]\n\n    def hexdigest(self):\n        return ''.join(['%.2x' % ord(i) for i in self.digest()])\n\n    def copy(self):\n        new = sha512.__new__(sha512)\n        new._sha = self._sha.copy()\n        return new\n\nclass sha384(sha512):\n    digest_size = digestsize = 48\n\n    def __init__(self, s=None):\n        self._sha = sha384_init()\n        if s:\n            sha_update(self._sha, getbuf(s))\n\n    def copy(self):\n        new = sha384.__new__(sha384)\n        new._sha = self._sha.copy()\n        return new\n\ndef test():\n    import _sha512\n\n    a_str = \"just a test string\"\n\n    assert _sha512.sha512().hexdigest() == sha512().hexdigest()\n    assert _sha512.sha512(a_str).hexdigest() == sha512(a_str).hexdigest()\n    assert _sha512.sha512(a_str*7).hexdigest() == sha512(a_str*7).hexdigest()\n\n    s = sha512(a_str)\n    s.update(a_str)\n    assert _sha512.sha512(a_str+a_str).hexdigest() == s.hexdigest()\n\nif __name__ == \"__main__\":\n    test()\n", 
    "_strptime": "\"\"\"Strptime-related classes and functions.\n\nCLASSES:\n    LocaleTime -- Discovers and stores locale-specific time information\n    TimeRE -- Creates regexes for pattern matching a string of text containing\n                time information\n\nFUNCTIONS:\n    _getlang -- Figure out what language is being used for the locale\n    strptime -- Calculates the time struct represented by the passed-in string\n\n\"\"\"\nimport time\nimport locale\nimport calendar\nfrom re import compile as re_compile\nfrom re import IGNORECASE\nfrom re import escape as re_escape\nfrom datetime import date as datetime_date\ntry:\n    from thread import allocate_lock as _thread_allocate_lock\nexcept:\n    from dummy_thread import allocate_lock as _thread_allocate_lock\n\n__all__ = []\n\ndef _getlang():\n    # Figure out what the current language is set to.\n    return locale.getlocale(locale.LC_TIME)\n\nclass LocaleTime(object):\n    \"\"\"Stores and handles locale-specific information related to time.\n\n    ATTRIBUTES:\n        f_weekday -- full weekday names (7-item list)\n        a_weekday -- abbreviated weekday names (7-item list)\n        f_month -- full month names (13-item list; dummy value in [0], which\n                    is added by code)\n        a_month -- abbreviated month names (13-item list, dummy value in\n                    [0], which is added by code)\n        am_pm -- AM/PM representation (2-item list)\n        LC_date_time -- format string for date/time representation (string)\n        LC_date -- format string for date representation (string)\n        LC_time -- format string for time representation (string)\n        timezone -- daylight- and non-daylight-savings timezone representation\n                    (2-item list of sets)\n        lang -- Language used by instance (2-item tuple)\n    \"\"\"\n\n    def __init__(self):\n        \"\"\"Set all attributes.\n\n        Order of methods called matters for dependency reasons.\n\n        The locale language is set at the offset and then checked again before\n        exiting.  This is to make sure that the attributes were not set with a\n        mix of information from more than one locale.  This would most likely\n        happen when using threads where one thread calls a locale-dependent\n        function while another thread changes the locale while the function in\n        the other thread is still running.  Proper coding would call for\n        locks to prevent changing the locale while locale-dependent code is\n        running.  The check here is done in case someone does not think about\n        doing this.\n\n        Only other possible issue is if someone changed the timezone and did\n        not call tz.tzset .  That is an issue for the programmer, though,\n        since changing the timezone is worthless without that call.\n\n        \"\"\"\n        self.lang = _getlang()\n        self.__calc_weekday()\n        self.__calc_month()\n        self.__calc_am_pm()\n        self.__calc_timezone()\n        self.__calc_date_time()\n        if _getlang() != self.lang:\n            raise ValueError(\"locale changed during initialization\")\n\n    def __pad(self, seq, front):\n        # Add '' to seq to either the front (is True), else the back.\n        seq = list(seq)\n        if front:\n            seq.insert(0, '')\n        else:\n            seq.append('')\n        return seq\n\n    def __calc_weekday(self):\n        # Set self.a_weekday and self.f_weekday using the calendar\n        # module.\n        a_weekday = [calendar.day_abbr[i].lower() for i in range(7)]\n        f_weekday = [calendar.day_name[i].lower() for i in range(7)]\n        self.a_weekday = a_weekday\n        self.f_weekday = f_weekday\n\n    def __calc_month(self):\n        # Set self.f_month and self.a_month using the calendar module.\n        a_month = [calendar.month_abbr[i].lower() for i in range(13)]\n        f_month = [calendar.month_name[i].lower() for i in range(13)]\n        self.a_month = a_month\n        self.f_month = f_month\n\n    def __calc_am_pm(self):\n        # Set self.am_pm by using time.strftime().\n\n        # The magic date (1999,3,17,hour,44,55,2,76,0) is not really that\n        # magical; just happened to have used it everywhere else where a\n        # static date was needed.\n        am_pm = []\n        for hour in (01,22):\n            time_tuple = time.struct_time((1999,3,17,hour,44,55,2,76,0))\n            am_pm.append(time.strftime(\"%p\", time_tuple).lower())\n        self.am_pm = am_pm\n\n    def __calc_date_time(self):\n        # Set self.date_time, self.date, & self.time by using\n        # time.strftime().\n\n        # Use (1999,3,17,22,44,55,2,76,0) for magic date because the amount of\n        # overloaded numbers is minimized.  The order in which searches for\n        # values within the format string is very important; it eliminates\n        # possible ambiguity for what something represents.\n        time_tuple = time.struct_time((1999,3,17,22,44,55,2,76,0))\n        date_time = [None, None, None]\n        date_time[0] = time.strftime(\"%c\", time_tuple).lower()\n        date_time[1] = time.strftime(\"%x\", time_tuple).lower()\n        date_time[2] = time.strftime(\"%X\", time_tuple).lower()\n        replacement_pairs = [('%', '%%'), (self.f_weekday[2], '%A'),\n                    (self.f_month[3], '%B'), (self.a_weekday[2], '%a'),\n                    (self.a_month[3], '%b'), (self.am_pm[1], '%p'),\n                    ('1999', '%Y'), ('99', '%y'), ('22', '%H'),\n                    ('44', '%M'), ('55', '%S'), ('76', '%j'),\n                    ('17', '%d'), ('03', '%m'), ('3', '%m'),\n                    # '3' needed for when no leading zero.\n                    ('2', '%w'), ('10', '%I')]\n        replacement_pairs.extend([(tz, \"%Z\") for tz_values in self.timezone\n                                                for tz in tz_values])\n        for offset,directive in ((0,'%c'), (1,'%x'), (2,'%X')):\n            current_format = date_time[offset]\n            for old, new in replacement_pairs:\n                # Must deal with possible lack of locale info\n                # manifesting itself as the empty string (e.g., Swedish's\n                # lack of AM/PM info) or a platform returning a tuple of empty\n                # strings (e.g., MacOS 9 having timezone as ('','')).\n                if old:\n                    current_format = current_format.replace(old, new)\n            # If %W is used, then Sunday, 2005-01-03 will fall on week 0 since\n            # 2005-01-03 occurs before the first Monday of the year.  Otherwise\n            # %U is used.\n            time_tuple = time.struct_time((1999,1,3,1,1,1,6,3,0))\n            if '00' in time.strftime(directive, time_tuple):\n                U_W = '%W'\n            else:\n                U_W = '%U'\n            date_time[offset] = current_format.replace('11', U_W)\n        self.LC_date_time = date_time[0]\n        self.LC_date = date_time[1]\n        self.LC_time = date_time[2]\n\n    def __calc_timezone(self):\n        # Set self.timezone by using time.tzname.\n        # Do not worry about possibility of time.tzname[0] == timetzname[1]\n        # and time.daylight; handle that in strptime .\n        try:\n            time.tzset()\n        except AttributeError:\n            pass\n        no_saving = frozenset([\"utc\", \"gmt\", time.tzname[0].lower()])\n        if time.daylight:\n            has_saving = frozenset([time.tzname[1].lower()])\n        else:\n            has_saving = frozenset()\n        self.timezone = (no_saving, has_saving)\n\n\nclass TimeRE(dict):\n    \"\"\"Handle conversion from format directives to regexes.\"\"\"\n\n    def __init__(self, locale_time=None):\n        \"\"\"Create keys/values.\n\n        Order of execution is important for dependency reasons.\n\n        \"\"\"\n        if locale_time:\n            self.locale_time = locale_time\n        else:\n            self.locale_time = LocaleTime()\n        base = super(TimeRE, self)\n        base.__init__({\n            # The \" \\d\" part of the regex is to make %c from ANSI C work\n            'd': r\"(?P<d>3[0-1]|[1-2]\\d|0[1-9]|[1-9]| [1-9])\",\n            'f': r\"(?P<f>[0-9]{1,6})\",\n            'H': r\"(?P<H>2[0-3]|[0-1]\\d|\\d)\",\n            'I': r\"(?P<I>1[0-2]|0[1-9]|[1-9])\",\n            'j': r\"(?P<j>36[0-6]|3[0-5]\\d|[1-2]\\d\\d|0[1-9]\\d|00[1-9]|[1-9]\\d|0[1-9]|[1-9])\",\n            'm': r\"(?P<m>1[0-2]|0[1-9]|[1-9])\",\n            'M': r\"(?P<M>[0-5]\\d|\\d)\",\n            'S': r\"(?P<S>6[0-1]|[0-5]\\d|\\d)\",\n            'U': r\"(?P<U>5[0-3]|[0-4]\\d|\\d)\",\n            'w': r\"(?P<w>[0-6])\",\n            # W is set below by using 'U'\n            'y': r\"(?P<y>\\d\\d)\",\n            #XXX: Does 'Y' need to worry about having less or more than\n            #     4 digits?\n            'Y': r\"(?P<Y>\\d\\d\\d\\d)\",\n            'A': self.__seqToRE(self.locale_time.f_weekday, 'A'),\n            'a': self.__seqToRE(self.locale_time.a_weekday, 'a'),\n            'B': self.__seqToRE(self.locale_time.f_month[1:], 'B'),\n            'b': self.__seqToRE(self.locale_time.a_month[1:], 'b'),\n            'p': self.__seqToRE(self.locale_time.am_pm, 'p'),\n            'Z': self.__seqToRE((tz for tz_names in self.locale_time.timezone\n                                        for tz in tz_names),\n                                'Z'),\n            '%': '%'})\n        base.__setitem__('W', base.__getitem__('U').replace('U', 'W'))\n        base.__setitem__('c', self.pattern(self.locale_time.LC_date_time))\n        base.__setitem__('x', self.pattern(self.locale_time.LC_date))\n        base.__setitem__('X', self.pattern(self.locale_time.LC_time))\n\n    def __seqToRE(self, to_convert, directive):\n        \"\"\"Convert a list to a regex string for matching a directive.\n\n        Want possible matching values to be from longest to shortest.  This\n        prevents the possibility of a match occurring for a value that also\n        a substring of a larger value that should have matched (e.g., 'abc'\n        matching when 'abcdef' should have been the match).\n\n        \"\"\"\n        to_convert = sorted(to_convert, key=len, reverse=True)\n        for value in to_convert:\n            if value != '':\n                break\n        else:\n            return ''\n        regex = '|'.join(re_escape(stuff) for stuff in to_convert)\n        regex = '(?P<%s>%s' % (directive, regex)\n        return '%s)' % regex\n\n    def pattern(self, format):\n        \"\"\"Return regex pattern for the format string.\n\n        Need to make sure that any characters that might be interpreted as\n        regex syntax are escaped.\n\n        \"\"\"\n        processed_format = ''\n        # The sub() call escapes all characters that might be misconstrued\n        # as regex syntax.  Cannot use re.escape since we have to deal with\n        # format directives (%m, etc.).\n        regex_chars = re_compile(r\"([\\\\.^$*+?\\(\\){}\\[\\]|])\")\n        format = regex_chars.sub(r\"\\\\\\1\", format)\n        whitespace_replacement = re_compile('\\s+')\n        format = whitespace_replacement.sub('\\s+', format)\n        while '%' in format:\n            directive_index = format.index('%')+1\n            processed_format = \"%s%s%s\" % (processed_format,\n                                           format[:directive_index-1],\n                                           self[format[directive_index]])\n            format = format[directive_index+1:]\n        return \"%s%s\" % (processed_format, format)\n\n    def compile(self, format):\n        \"\"\"Return a compiled re object for the format string.\"\"\"\n        return re_compile(self.pattern(format), IGNORECASE)\n\n_cache_lock = _thread_allocate_lock()\n# DO NOT modify _TimeRE_cache or _regex_cache without acquiring the cache lock\n# first!\n_TimeRE_cache = TimeRE()\n_CACHE_MAX_SIZE = 5 # Max number of regexes stored in _regex_cache\n_regex_cache = {}\n\ndef _calc_julian_from_U_or_W(year, week_of_year, day_of_week, week_starts_Mon):\n    \"\"\"Calculate the Julian day based on the year, week of the year, and day of\n    the week, with week_start_day representing whether the week of the year\n    assumes the week starts on Sunday or Monday (6 or 0).\"\"\"\n    first_weekday = datetime_date(year, 1, 1).weekday()\n    # If we are dealing with the %U directive (week starts on Sunday), it's\n    # easier to just shift the view to Sunday being the first day of the\n    # week.\n    if not week_starts_Mon:\n        first_weekday = (first_weekday + 1) % 7\n        day_of_week = (day_of_week + 1) % 7\n    # Need to watch out for a week 0 (when the first day of the year is not\n    # the same as that specified by %U or %W).\n    week_0_length = (7 - first_weekday) % 7\n    if week_of_year == 0:\n        return 1 + day_of_week - first_weekday\n    else:\n        days_to_week = week_0_length + (7 * (week_of_year - 1))\n        return 1 + days_to_week + day_of_week\n\n\ndef _strptime(data_string, format=\"%a %b %d %H:%M:%S %Y\"):\n    \"\"\"Return a time struct based on the input string and the format string.\"\"\"\n    global _TimeRE_cache, _regex_cache\n    with _cache_lock:\n        if _getlang() != _TimeRE_cache.locale_time.lang:\n            _TimeRE_cache = TimeRE()\n            _regex_cache.clear()\n        if len(_regex_cache) > _CACHE_MAX_SIZE:\n            _regex_cache.clear()\n        locale_time = _TimeRE_cache.locale_time\n        format_regex = _regex_cache.get(format)\n        if not format_regex:\n            try:\n                format_regex = _TimeRE_cache.compile(format)\n            # KeyError raised when a bad format is found; can be specified as\n            # \\\\, in which case it was a stray % but with a space after it\n            except KeyError, err:\n                bad_directive = err.args[0]\n                if bad_directive == \"\\\\\":\n                    bad_directive = \"%\"\n                del err\n                raise ValueError(\"'%s' is a bad directive in format '%s'\" %\n                                    (bad_directive, format))\n            # IndexError only occurs when the format string is \"%\"\n            except IndexError:\n                raise ValueError(\"stray %% in format '%s'\" % format)\n            _regex_cache[format] = format_regex\n    found = format_regex.match(data_string)\n    if not found:\n        raise ValueError(\"time data %r does not match format %r\" %\n                         (data_string, format))\n    if len(data_string) != found.end():\n        raise ValueError(\"unconverted data remains: %s\" %\n                          data_string[found.end():])\n\n    year = None\n    month = day = 1\n    hour = minute = second = fraction = 0\n    tz = -1\n    # Default to -1 to signify that values not known; not critical to have,\n    # though\n    week_of_year = -1\n    week_of_year_start = -1\n    # weekday and julian defaulted to -1 so as to signal need to calculate\n    # values\n    weekday = julian = -1\n    found_dict = found.groupdict()\n    for group_key in found_dict.iterkeys():\n        # Directives not explicitly handled below:\n        #   c, x, X\n        #      handled by making out of other directives\n        #   U, W\n        #      worthless without day of the week\n        if group_key == 'y':\n            year = int(found_dict['y'])\n            # Open Group specification for strptime() states that a %y\n            #value in the range of [00, 68] is in the century 2000, while\n            #[69,99] is in the century 1900\n            if year <= 68:\n                year += 2000\n            else:\n                year += 1900\n        elif group_key == 'Y':\n            year = int(found_dict['Y'])\n        elif group_key == 'm':\n            month = int(found_dict['m'])\n        elif group_key == 'B':\n            month = locale_time.f_month.index(found_dict['B'].lower())\n        elif group_key == 'b':\n            month = locale_time.a_month.index(found_dict['b'].lower())\n        elif group_key == 'd':\n            day = int(found_dict['d'])\n        elif group_key == 'H':\n            hour = int(found_dict['H'])\n        elif group_key == 'I':\n            hour = int(found_dict['I'])\n            ampm = found_dict.get('p', '').lower()\n            # If there was no AM/PM indicator, we'll treat this like AM\n            if ampm in ('', locale_time.am_pm[0]):\n                # We're in AM so the hour is correct unless we're\n                # looking at 12 midnight.\n                # 12 midnight == 12 AM == hour 0\n                if hour == 12:\n                    hour = 0\n            elif ampm == locale_time.am_pm[1]:\n                # We're in PM so we need to add 12 to the hour unless\n                # we're looking at 12 noon.\n                # 12 noon == 12 PM == hour 12\n                if hour != 12:\n                    hour += 12\n        elif group_key == 'M':\n            minute = int(found_dict['M'])\n        elif group_key == 'S':\n            second = int(found_dict['S'])\n        elif group_key == 'f':\n            s = found_dict['f']\n            # Pad to always return microseconds.\n            s += \"0\" * (6 - len(s))\n            fraction = int(s)\n        elif group_key == 'A':\n            weekday = locale_time.f_weekday.index(found_dict['A'].lower())\n        elif group_key == 'a':\n            weekday = locale_time.a_weekday.index(found_dict['a'].lower())\n        elif group_key == 'w':\n            weekday = int(found_dict['w'])\n            if weekday == 0:\n                weekday = 6\n            else:\n                weekday -= 1\n        elif group_key == 'j':\n            julian = int(found_dict['j'])\n        elif group_key in ('U', 'W'):\n            week_of_year = int(found_dict[group_key])\n            if group_key == 'U':\n                # U starts week on Sunday.\n                week_of_year_start = 6\n            else:\n                # W starts week on Monday.\n                week_of_year_start = 0\n        elif group_key == 'Z':\n            # Since -1 is default value only need to worry about setting tz if\n            # it can be something other than -1.\n            found_zone = found_dict['Z'].lower()\n            for value, tz_values in enumerate(locale_time.timezone):\n                if found_zone in tz_values:\n                    # Deal with bad locale setup where timezone names are the\n                    # same and yet time.daylight is true; too ambiguous to\n                    # be able to tell what timezone has daylight savings\n                    if (time.tzname[0] == time.tzname[1] and\n                       time.daylight and found_zone not in (\"utc\", \"gmt\")):\n                        break\n                    else:\n                        tz = value\n                        break\n    leap_year_fix = False\n    if year is None and month == 2 and day == 29:\n        year = 1904  # 1904 is first leap year of 20th century\n        leap_year_fix = True\n    elif year is None:\n        year = 1900\n    # If we know the week of the year and what day of that week, we can figure\n    # out the Julian day of the year.\n    if julian == -1 and week_of_year != -1 and weekday != -1:\n        week_starts_Mon = True if week_of_year_start == 0 else False\n        julian = _calc_julian_from_U_or_W(year, week_of_year, weekday,\n                                            week_starts_Mon)\n    # Cannot pre-calculate datetime_date() since can change in Julian\n    # calculation and thus could have different value for the day of the week\n    # calculation.\n    if julian == -1:\n        # Need to add 1 to result since first day of the year is 1, not 0.\n        julian = datetime_date(year, month, day).toordinal() - \\\n                  datetime_date(year, 1, 1).toordinal() + 1\n    else:  # Assume that if they bothered to include Julian day it will\n           # be accurate.\n        datetime_result = datetime_date.fromordinal((julian - 1) + datetime_date(year, 1, 1).toordinal())\n        year = datetime_result.year\n        month = datetime_result.month\n        day = datetime_result.day\n    if weekday == -1:\n        weekday = datetime_date(year, month, day).weekday()\n    if leap_year_fix:\n        # the caller didn't supply a year but asked for Feb 29th. We couldn't\n        # use the default of 1900 for computations. We set it back to ensure\n        # that February 29th is smaller than March 1st.\n        year = 1900\n\n    return (time.struct_time((year, month, day,\n                              hour, minute, second,\n                              weekday, julian, tz)), fraction)\n\ndef _strptime_time(data_string, format=\"%a %b %d %H:%M:%S %Y\"):\n    return _strptime(data_string, format)[0]\n", 
    "_structseq": "\"\"\"\nImplementation helper: a struct that looks like a tuple.  See timemodule\nand posixmodule for example uses.\n\"\"\"\n\nclass structseqfield(object):\n    \"\"\"Definition of field of a structseq.  The 'index' is for positional\n    tuple-like indexing.  Fields whose index is after a gap in the numbers\n    cannot be accessed like this, but only by name.\n    \"\"\"\n    def __init__(self, index, doc=None, default=lambda self: None):\n        self.__name__ = '?'\n        self.index    = index    # patched to None if not positional\n        self._index   = index\n        self.__doc__  = doc\n        self._default = default\n\n    def __repr__(self):\n        return '<field %s (%s)>' % (self.__name__,\n                                    self.__doc__ or 'undocumented')\n\n    def __get__(self, obj, typ=None):\n        if obj is None:\n            return self\n        if self.index is None:\n            return obj.__dict__[self.__name__]\n        else:\n            return obj[self.index]\n\n    def __set__(self, obj, value):\n        raise TypeError(\"readonly attribute\")\n\n\nclass structseqtype(type):\n\n    def __new__(metacls, classname, bases, dict):\n        assert not bases\n        fields_by_index = {}\n        for name, field in dict.items():\n            if isinstance(field, structseqfield):\n                assert field._index not in fields_by_index\n                fields_by_index[field._index] = field\n                field.__name__ = name\n        dict['n_fields'] = len(fields_by_index)\n\n        extra_fields = sorted(fields_by_index.iteritems())\n        n_sequence_fields = 0\n        while extra_fields and extra_fields[0][0] == n_sequence_fields:\n            extra_fields.pop(0)\n            n_sequence_fields += 1\n        dict['n_sequence_fields'] = n_sequence_fields\n        dict['n_unnamed_fields'] = 0     # no fully anonymous fields in PyPy\n\n        extra_fields = [field for index, field in extra_fields]\n        for field in extra_fields:\n            field.index = None     # no longer relevant\n\n        assert '__new__' not in dict\n        dict['_extra_fields'] = tuple(extra_fields)\n        dict['__new__'] = structseq_new\n        dict['__reduce__'] = structseq_reduce\n        dict['__setattr__'] = structseq_setattr\n        dict['__repr__'] = structseq_repr\n        dict['_name'] = dict.get('name', '')\n        return type.__new__(metacls, classname, (tuple,), dict)\n\n\nbuiltin_dict = dict\n\ndef structseq_new(cls, sequence, dict={}):\n    sequence = tuple(sequence)\n    dict = builtin_dict(dict)\n    N = cls.n_sequence_fields\n    if len(sequence) < N:\n        if N < cls.n_fields:\n            msg = \"at least\"\n        else:\n            msg = \"exactly\"\n        raise TypeError(\"expected a sequence with %s %d items\" % (\n            msg, N))\n    if len(sequence) > N:\n        if len(sequence) > cls.n_fields:\n            if N < cls.n_fields:\n                msg = \"at most\"\n            else:\n                msg = \"exactly\"\n            raise TypeError(\"expected a sequence with %s %d items\" % (\n                msg, cls.n_fields))\n        for field, value in zip(cls._extra_fields, sequence[N:]):\n            name = field.__name__\n            if name in dict:\n                raise TypeError(\"duplicate value for %r\" % (name,))\n            dict[name] = value\n        sequence = sequence[:N]\n    result = tuple.__new__(cls, sequence)\n    object.__setattr__(result, '__dict__', dict)\n    for field in cls._extra_fields:\n        name = field.__name__\n        if name not in dict:\n            dict[name] = field._default(result)\n    return result\n\ndef structseq_reduce(self):\n    return type(self), (tuple(self), self.__dict__)\n\ndef structseq_setattr(self, attr, value):\n    raise AttributeError(\"%r object has no attribute %r\" % (\n        self.__class__.__name__, attr))\n\ndef structseq_repr(self):\n    fields = {}\n    for field in type(self).__dict__.values():\n        if isinstance(field, structseqfield):\n            fields[field._index] = field\n    parts = [\"%s=%r\" % (fields[index].__name__, value)\n             for index, value in enumerate(self)]\n    return \"%s(%s)\" % (self._name, \", \".join(parts))\n", 
    "_weakrefset": "# Access WeakSet through the weakref module.\n# This code is separated-out because it is needed\n# by abc.py to load everything else at startup.\n\nfrom _weakref import ref\n\n__all__ = ['WeakSet']\n\n\nclass _IterationGuard(object):\n    # This context manager registers itself in the current iterators of the\n    # weak container, such as to delay all removals until the context manager\n    # exits.\n    # This technique should be relatively thread-safe (since sets are).\n\n    def __init__(self, weakcontainer):\n        # Don't create cycles\n        self.weakcontainer = ref(weakcontainer)\n\n    def __enter__(self):\n        w = self.weakcontainer()\n        if w is not None:\n            w._iterating.add(self)\n        return self\n\n    def __exit__(self, e, t, b):\n        w = self.weakcontainer()\n        if w is not None:\n            s = w._iterating\n            s.remove(self)\n            if not s:\n                w._commit_removals()\n\n\nclass WeakSet(object):\n    def __init__(self, data=None):\n        self.data = set()\n        def _remove(item, selfref=ref(self)):\n            self = selfref()\n            if self is not None:\n                if self._iterating:\n                    self._pending_removals.append(item)\n                else:\n                    self.data.discard(item)\n        self._remove = _remove\n        # A list of keys to be removed\n        self._pending_removals = []\n        self._iterating = set()\n        if data is not None:\n            self.update(data)\n\n    def _commit_removals(self):\n        l = self._pending_removals\n        discard = self.data.discard\n        while l:\n            discard(l.pop())\n\n    def __iter__(self):\n        with _IterationGuard(self):\n            for itemref in self.data:\n                item = itemref()\n                if item is not None:\n                    # Caveat: the iterator will keep a strong reference to\n                    # `item` until it is resumed or closed.\n                    yield item\n\n    def __len__(self):\n        return len(self.data) - len(self._pending_removals)\n\n    def __contains__(self, item):\n        try:\n            wr = ref(item)\n        except TypeError:\n            return False\n        return wr in self.data\n\n    def __reduce__(self):\n        return (self.__class__, (list(self),),\n                getattr(self, '__dict__', None))\n\n    __hash__ = None\n\n    def add(self, item):\n        if self._pending_removals:\n            self._commit_removals()\n        self.data.add(ref(item, self._remove))\n\n    def clear(self):\n        if self._pending_removals:\n            self._commit_removals()\n        self.data.clear()\n\n    def copy(self):\n        return self.__class__(self)\n\n    def pop(self):\n        if self._pending_removals:\n            self._commit_removals()\n        while True:\n            try:\n                itemref = self.data.pop()\n            except KeyError:\n                raise KeyError('pop from empty WeakSet')\n            item = itemref()\n            if item is not None:\n                return item\n\n    def remove(self, item):\n        if self._pending_removals:\n            self._commit_removals()\n        self.data.remove(ref(item))\n\n    def discard(self, item):\n        if self._pending_removals:\n            self._commit_removals()\n        self.data.discard(ref(item))\n\n    def update(self, other):\n        if self._pending_removals:\n            self._commit_removals()\n        for element in other:\n            self.add(element)\n\n    def __ior__(self, other):\n        self.update(other)\n        return self\n\n    def difference(self, other):\n        newset = self.copy()\n        newset.difference_update(other)\n        return newset\n    __sub__ = difference\n\n    def difference_update(self, other):\n        self.__isub__(other)\n    def __isub__(self, other):\n        if self._pending_removals:\n            self._commit_removals()\n        if self is other:\n            self.data.clear()\n        else:\n            self.data.difference_update(ref(item) for item in other)\n        return self\n\n    def intersection(self, other):\n        return self.__class__(item for item in other if item in self)\n    __and__ = intersection\n\n    def intersection_update(self, other):\n        self.__iand__(other)\n    def __iand__(self, other):\n        if self._pending_removals:\n            self._commit_removals()\n        self.data.intersection_update(ref(item) for item in other)\n        return self\n\n    def issubset(self, other):\n        return self.data.issubset(ref(item) for item in other)\n    __le__ = issubset\n\n    def __lt__(self, other):\n        return self.data < set(ref(item) for item in other)\n\n    def issuperset(self, other):\n        return self.data.issuperset(ref(item) for item in other)\n    __ge__ = issuperset\n\n    def __gt__(self, other):\n        return self.data > set(ref(item) for item in other)\n\n    def __eq__(self, other):\n        if not isinstance(other, self.__class__):\n            return NotImplemented\n        return self.data == set(ref(item) for item in other)\n\n    def __ne__(self, other):\n        opposite = self.__eq__(other)\n        if opposite is NotImplemented:\n            return NotImplemented\n        return not opposite\n\n    def symmetric_difference(self, other):\n        newset = self.copy()\n        newset.symmetric_difference_update(other)\n        return newset\n    __xor__ = symmetric_difference\n\n    def symmetric_difference_update(self, other):\n        self.__ixor__(other)\n    def __ixor__(self, other):\n        if self._pending_removals:\n            self._commit_removals()\n        if self is other:\n            self.data.clear()\n        else:\n            self.data.symmetric_difference_update(ref(item, self._remove) for item in other)\n        return self\n\n    def union(self, other):\n        return self.__class__(e for s in (self, other) for e in s)\n    __or__ = union\n\n    def isdisjoint(self, other):\n        return len(self.intersection(other)) == 0\n", 
    "abc": "# Copyright 2007 Google, Inc. All Rights Reserved.\n# Licensed to PSF under a Contributor Agreement.\n\n\"\"\"Abstract Base Classes (ABCs) according to PEP 3119.\"\"\"\n\nimport types\n\nfrom _weakrefset import WeakSet\n\n# Instance of old-style class\nclass _C: pass\n_InstanceType = type(_C())\n\n\ndef abstractmethod(funcobj):\n    \"\"\"A decorator indicating abstract methods.\n\n    Requires that the metaclass is ABCMeta or derived from it.  A\n    class that has a metaclass derived from ABCMeta cannot be\n    instantiated unless all of its abstract methods are overridden.\n    The abstract methods can be called using any of the normal\n    'super' call mechanisms.\n\n    Usage:\n\n        class C:\n            __metaclass__ = ABCMeta\n            @abstractmethod\n            def my_abstract_method(self, ...):\n                ...\n    \"\"\"\n    funcobj.__isabstractmethod__ = True\n    return funcobj\n\n\nclass abstractproperty(property):\n    \"\"\"A decorator indicating abstract properties.\n\n    Requires that the metaclass is ABCMeta or derived from it.  A\n    class that has a metaclass derived from ABCMeta cannot be\n    instantiated unless all of its abstract properties are overridden.\n    The abstract properties can be called using any of the normal\n    'super' call mechanisms.\n\n    Usage:\n\n        class C:\n            __metaclass__ = ABCMeta\n            @abstractproperty\n            def my_abstract_property(self):\n                ...\n\n    This defines a read-only property; you can also define a read-write\n    abstract property using the 'long' form of property declaration:\n\n        class C:\n            __metaclass__ = ABCMeta\n            def getx(self): ...\n            def setx(self, value): ...\n            x = abstractproperty(getx, setx)\n    \"\"\"\n    __isabstractmethod__ = True\n\n\nclass ABCMeta(type):\n\n    \"\"\"Metaclass for defining Abstract Base Classes (ABCs).\n\n    Use this metaclass to create an ABC.  An ABC can be subclassed\n    directly, and then acts as a mix-in class.  You can also register\n    unrelated concrete classes (even built-in classes) and unrelated\n    ABCs as 'virtual subclasses' -- these and their descendants will\n    be considered subclasses of the registering ABC by the built-in\n    issubclass() function, but the registering ABC won't show up in\n    their MRO (Method Resolution Order) nor will method\n    implementations defined by the registering ABC be callable (not\n    even via super()).\n\n    \"\"\"\n\n    # A global counter that is incremented each time a class is\n    # registered as a virtual subclass of anything.  It forces the\n    # negative cache to be cleared before its next use.\n    _abc_invalidation_counter = 0\n\n    def __new__(mcls, name, bases, namespace):\n        cls = super(ABCMeta, mcls).__new__(mcls, name, bases, namespace)\n        # Compute set of abstract method names\n        abstracts = set(name\n                     for name, value in namespace.items()\n                     if getattr(value, \"__isabstractmethod__\", False))\n        for base in bases:\n            for name in getattr(base, \"__abstractmethods__\", set()):\n                value = getattr(cls, name, None)\n                if getattr(value, \"__isabstractmethod__\", False):\n                    abstracts.add(name)\n        cls.__abstractmethods__ = frozenset(abstracts)\n        # Set up inheritance registry\n        cls._abc_registry = WeakSet()\n        cls._abc_cache = WeakSet()\n        cls._abc_negative_cache = WeakSet()\n        cls._abc_negative_cache_version = ABCMeta._abc_invalidation_counter\n        return cls\n\n    def register(cls, subclass):\n        \"\"\"Register a virtual subclass of an ABC.\"\"\"\n        if not isinstance(subclass, (type, types.ClassType)):\n            raise TypeError(\"Can only register classes\")\n        if issubclass(subclass, cls):\n            return  # Already a subclass\n        # Subtle: test for cycles *after* testing for \"already a subclass\";\n        # this means we allow X.register(X) and interpret it as a no-op.\n        if issubclass(cls, subclass):\n            # This would create a cycle, which is bad for the algorithm below\n            raise RuntimeError(\"Refusing to create an inheritance cycle\")\n        cls._abc_registry.add(subclass)\n        ABCMeta._abc_invalidation_counter += 1  # Invalidate negative cache\n\n    def _dump_registry(cls, file=None):\n        \"\"\"Debug helper to print the ABC registry.\"\"\"\n        print >> file, \"Class: %s.%s\" % (cls.__module__, cls.__name__)\n        print >> file, \"Inv.counter: %s\" % ABCMeta._abc_invalidation_counter\n        for name in sorted(cls.__dict__.keys()):\n            if name.startswith(\"_abc_\"):\n                value = getattr(cls, name)\n                print >> file, \"%s: %r\" % (name, value)\n\n    def __instancecheck__(cls, instance):\n        \"\"\"Override for isinstance(instance, cls).\"\"\"\n        # Inline the cache checking when it's simple.\n        subclass = getattr(instance, '__class__', None)\n        if subclass is not None and subclass in cls._abc_cache:\n            return True\n        subtype = type(instance)\n        # Old-style instances\n        if subtype is _InstanceType:\n            subtype = subclass\n        if subtype is subclass or subclass is None:\n            if (cls._abc_negative_cache_version ==\n                ABCMeta._abc_invalidation_counter and\n                subtype in cls._abc_negative_cache):\n                return False\n            # Fall back to the subclass check.\n            return cls.__subclasscheck__(subtype)\n        return (cls.__subclasscheck__(subclass) or\n                cls.__subclasscheck__(subtype))\n\n    def __subclasscheck__(cls, subclass):\n        \"\"\"Override for issubclass(subclass, cls).\"\"\"\n        # Check cache\n        if subclass in cls._abc_cache:\n            return True\n        # Check negative cache; may have to invalidate\n        if cls._abc_negative_cache_version < ABCMeta._abc_invalidation_counter:\n            # Invalidate the negative cache\n            cls._abc_negative_cache = WeakSet()\n            cls._abc_negative_cache_version = ABCMeta._abc_invalidation_counter\n        elif subclass in cls._abc_negative_cache:\n            return False\n        # Check the subclass hook\n        ok = cls.__subclasshook__(subclass)\n        if ok is not NotImplemented:\n            assert isinstance(ok, bool)\n            if ok:\n                cls._abc_cache.add(subclass)\n            else:\n                cls._abc_negative_cache.add(subclass)\n            return ok\n        # Check if it's a direct subclass\n        if cls in getattr(subclass, '__mro__', ()):\n            cls._abc_cache.add(subclass)\n            return True\n        # Check if it's a subclass of a registered class (recursive)\n        for rcls in cls._abc_registry:\n            if issubclass(subclass, rcls):\n                cls._abc_cache.add(subclass)\n                return True\n        # Check if it's a subclass of a subclass (recursive)\n        for scls in cls.__subclasses__():\n            if issubclass(subclass, scls):\n                cls._abc_cache.add(subclass)\n                return True\n        # No dice; update negative cache\n        cls._abc_negative_cache.add(subclass)\n        return False\n", 
    "atexit": "\"\"\"\natexit.py - allow programmer to define multiple exit functions to be executed\nupon normal program termination.\n\nOne public function, register, is defined.\n\"\"\"\n\n__all__ = [\"register\"]\n\nimport sys\n\n_exithandlers = []\ndef _run_exitfuncs():\n    \"\"\"run any registered exit functions\n\n    _exithandlers is traversed in reverse order so functions are executed\n    last in, first out.\n    \"\"\"\n\n    exc_info = None\n    while _exithandlers:\n        func, targs, kargs = _exithandlers.pop()\n        try:\n            func(*targs, **kargs)\n        except SystemExit:\n            exc_info = sys.exc_info()\n        except:\n            import traceback\n            print >> sys.stderr, \"Error in atexit._run_exitfuncs:\"\n            traceback.print_exc()\n            exc_info = sys.exc_info()\n\n    if exc_info is not None:\n        raise exc_info[0], exc_info[1], exc_info[2]\n\n\ndef register(func, *targs, **kargs):\n    \"\"\"register a function to be executed upon normal program termination\n\n    func - function to be called at exit\n    targs - optional arguments to pass to func\n    kargs - optional keyword arguments to pass to func\n\n    func is returned to facilitate usage as a decorator.\n    \"\"\"\n    _exithandlers.append((func, targs, kargs))\n    return func\n\nif hasattr(sys, \"exitfunc\"):\n    # Assume it's another registered exit function - append it to our list\n    register(sys.exitfunc)\nsys.exitfunc = _run_exitfuncs\n\nif __name__ == \"__main__\":\n    def x1():\n        print \"running x1\"\n    def x2(n):\n        print \"running x2(%r)\" % (n,)\n    def x3(n, kwd=None):\n        print \"running x3(%r, kwd=%r)\" % (n, kwd)\n\n    register(x1)\n    register(x2, 12)\n    register(x3, 5, \"bar\")\n    register(x3, \"no kwd args\")\n", 
    "base64": "#! /usr/bin/env python\n\n\"\"\"RFC 3548: Base16, Base32, Base64 Data Encodings\"\"\"\n\n# Modified 04-Oct-1995 by Jack Jansen to use binascii module\n# Modified 30-Dec-2003 by Barry Warsaw to add full RFC 3548 support\n\nimport re\nimport struct\nimport binascii\n\n\n__all__ = [\n    # Legacy interface exports traditional RFC 1521 Base64 encodings\n    'encode', 'decode', 'encodestring', 'decodestring',\n    # Generalized interface for other encodings\n    'b64encode', 'b64decode', 'b32encode', 'b32decode',\n    'b16encode', 'b16decode',\n    # Standard Base64 encoding\n    'standard_b64encode', 'standard_b64decode',\n    # Some common Base64 alternatives.  As referenced by RFC 3458, see thread\n    # starting at:\n    #\n    # http://zgp.org/pipermail/p2p-hackers/2001-September/000316.html\n    'urlsafe_b64encode', 'urlsafe_b64decode',\n    ]\n\n_translation = [chr(_x) for _x in range(256)]\nEMPTYSTRING = ''\n\n\ndef _translate(s, altchars):\n    translation = _translation[:]\n    for k, v in altchars.items():\n        translation[ord(k)] = v\n    return s.translate(''.join(translation))\n\n\n\f\n# Base64 encoding/decoding uses binascii\n\ndef b64encode(s, altchars=None):\n    \"\"\"Encode a string using Base64.\n\n    s is the string to encode.  Optional altchars must be a string of at least\n    length 2 (additional characters are ignored) which specifies an\n    alternative alphabet for the '+' and '/' characters.  This allows an\n    application to e.g. generate url or filesystem safe Base64 strings.\n\n    The encoded string is returned.\n    \"\"\"\n    # Strip off the trailing newline\n    encoded = binascii.b2a_base64(s)[:-1]\n    if altchars is not None:\n        return _translate(encoded, {'+': altchars[0], '/': altchars[1]})\n    return encoded\n\n\ndef b64decode(s, altchars=None):\n    \"\"\"Decode a Base64 encoded string.\n\n    s is the string to decode.  Optional altchars must be a string of at least\n    length 2 (additional characters are ignored) which specifies the\n    alternative alphabet used instead of the '+' and '/' characters.\n\n    The decoded string is returned.  A TypeError is raised if s were\n    incorrectly padded or if there are non-alphabet characters present in the\n    string.\n    \"\"\"\n    if altchars is not None:\n        s = _translate(s, {altchars[0]: '+', altchars[1]: '/'})\n    try:\n        return binascii.a2b_base64(s)\n    except binascii.Error, msg:\n        # Transform this exception for consistency\n        raise TypeError(msg)\n\n\ndef standard_b64encode(s):\n    \"\"\"Encode a string using the standard Base64 alphabet.\n\n    s is the string to encode.  The encoded string is returned.\n    \"\"\"\n    return b64encode(s)\n\ndef standard_b64decode(s):\n    \"\"\"Decode a string encoded with the standard Base64 alphabet.\n\n    s is the string to decode.  The decoded string is returned.  A TypeError\n    is raised if the string is incorrectly padded or if there are non-alphabet\n    characters present in the string.\n    \"\"\"\n    return b64decode(s)\n\ndef urlsafe_b64encode(s):\n    \"\"\"Encode a string using a url-safe Base64 alphabet.\n\n    s is the string to encode.  The encoded string is returned.  The alphabet\n    uses '-' instead of '+' and '_' instead of '/'.\n    \"\"\"\n    return b64encode(s, '-_')\n\ndef urlsafe_b64decode(s):\n    \"\"\"Decode a string encoded with the standard Base64 alphabet.\n\n    s is the string to decode.  The decoded string is returned.  A TypeError\n    is raised if the string is incorrectly padded or if there are non-alphabet\n    characters present in the string.\n\n    The alphabet uses '-' instead of '+' and '_' instead of '/'.\n    \"\"\"\n    return b64decode(s, '-_')\n\n\n\f\n# Base32 encoding/decoding must be done in Python\n_b32alphabet = {\n    0: 'A',  9: 'J', 18: 'S', 27: '3',\n    1: 'B', 10: 'K', 19: 'T', 28: '4',\n    2: 'C', 11: 'L', 20: 'U', 29: '5',\n    3: 'D', 12: 'M', 21: 'V', 30: '6',\n    4: 'E', 13: 'N', 22: 'W', 31: '7',\n    5: 'F', 14: 'O', 23: 'X',\n    6: 'G', 15: 'P', 24: 'Y',\n    7: 'H', 16: 'Q', 25: 'Z',\n    8: 'I', 17: 'R', 26: '2',\n    }\n\n_b32tab = _b32alphabet.items()\n_b32tab.sort()\n_b32tab = [v for k, v in _b32tab]\n_b32rev = dict([(v, long(k)) for k, v in _b32alphabet.items()])\n\n\ndef b32encode(s):\n    \"\"\"Encode a string using Base32.\n\n    s is the string to encode.  The encoded string is returned.\n    \"\"\"\n    parts = []\n    quanta, leftover = divmod(len(s), 5)\n    # Pad the last quantum with zero bits if necessary\n    if leftover:\n        s += ('\\0' * (5 - leftover))\n        quanta += 1\n    for i in range(quanta):\n        # c1 and c2 are 16 bits wide, c3 is 8 bits wide.  The intent of this\n        # code is to process the 40 bits in units of 5 bits.  So we take the 1\n        # leftover bit of c1 and tack it onto c2.  Then we take the 2 leftover\n        # bits of c2 and tack them onto c3.  The shifts and masks are intended\n        # to give us values of exactly 5 bits in width.\n        c1, c2, c3 = struct.unpack('!HHB', s[i*5:(i+1)*5])\n        c2 += (c1 & 1) << 16 # 17 bits wide\n        c3 += (c2 & 3) << 8  # 10 bits wide\n        parts.extend([_b32tab[c1 >> 11],         # bits 1 - 5\n                      _b32tab[(c1 >> 6) & 0x1f], # bits 6 - 10\n                      _b32tab[(c1 >> 1) & 0x1f], # bits 11 - 15\n                      _b32tab[c2 >> 12],         # bits 16 - 20 (1 - 5)\n                      _b32tab[(c2 >> 7) & 0x1f], # bits 21 - 25 (6 - 10)\n                      _b32tab[(c2 >> 2) & 0x1f], # bits 26 - 30 (11 - 15)\n                      _b32tab[c3 >> 5],          # bits 31 - 35 (1 - 5)\n                      _b32tab[c3 & 0x1f],        # bits 36 - 40 (1 - 5)\n                      ])\n    encoded = EMPTYSTRING.join(parts)\n    # Adjust for any leftover partial quanta\n    if leftover == 1:\n        return encoded[:-6] + '======'\n    elif leftover == 2:\n        return encoded[:-4] + '===='\n    elif leftover == 3:\n        return encoded[:-3] + '==='\n    elif leftover == 4:\n        return encoded[:-1] + '='\n    return encoded\n\n\ndef b32decode(s, casefold=False, map01=None):\n    \"\"\"Decode a Base32 encoded string.\n\n    s is the string to decode.  Optional casefold is a flag specifying whether\n    a lowercase alphabet is acceptable as input.  For security purposes, the\n    default is False.\n\n    RFC 3548 allows for optional mapping of the digit 0 (zero) to the letter O\n    (oh), and for optional mapping of the digit 1 (one) to either the letter I\n    (eye) or letter L (el).  The optional argument map01 when not None,\n    specifies which letter the digit 1 should be mapped to (when map01 is not\n    None, the digit 0 is always mapped to the letter O).  For security\n    purposes the default is None, so that 0 and 1 are not allowed in the\n    input.\n\n    The decoded string is returned.  A TypeError is raised if s were\n    incorrectly padded or if there are non-alphabet characters present in the\n    string.\n    \"\"\"\n    quanta, leftover = divmod(len(s), 8)\n    if leftover:\n        raise TypeError('Incorrect padding')\n    # Handle section 2.4 zero and one mapping.  The flag map01 will be either\n    # False, or the character to map the digit 1 (one) to.  It should be\n    # either L (el) or I (eye).\n    if map01:\n        s = _translate(s, {'0': 'O', '1': map01})\n    if casefold:\n        s = s.upper()\n    # Strip off pad characters from the right.  We need to count the pad\n    # characters because this will tell us how many null bytes to remove from\n    # the end of the decoded string.\n    padchars = 0\n    mo = re.search('(?P<pad>[=]*)$', s)\n    if mo:\n        padchars = len(mo.group('pad'))\n        if padchars > 0:\n            s = s[:-padchars]\n    # Now decode the full quanta\n    parts = []\n    acc = 0\n    shift = 35\n    for c in s:\n        val = _b32rev.get(c)\n        if val is None:\n            raise TypeError('Non-base32 digit found')\n        acc += _b32rev[c] << shift\n        shift -= 5\n        if shift < 0:\n            parts.append(binascii.unhexlify('%010x' % acc))\n            acc = 0\n            shift = 35\n    # Process the last, partial quanta\n    last = binascii.unhexlify('%010x' % acc)\n    if padchars == 0:\n        last = ''                       # No characters\n    elif padchars == 1:\n        last = last[:-1]\n    elif padchars == 3:\n        last = last[:-2]\n    elif padchars == 4:\n        last = last[:-3]\n    elif padchars == 6:\n        last = last[:-4]\n    else:\n        raise TypeError('Incorrect padding')\n    parts.append(last)\n    return EMPTYSTRING.join(parts)\n\n\n\f\n# RFC 3548, Base 16 Alphabet specifies uppercase, but hexlify() returns\n# lowercase.  The RFC also recommends against accepting input case\n# insensitively.\ndef b16encode(s):\n    \"\"\"Encode a string using Base16.\n\n    s is the string to encode.  The encoded string is returned.\n    \"\"\"\n    return binascii.hexlify(s).upper()\n\n\ndef b16decode(s, casefold=False):\n    \"\"\"Decode a Base16 encoded string.\n\n    s is the string to decode.  Optional casefold is a flag specifying whether\n    a lowercase alphabet is acceptable as input.  For security purposes, the\n    default is False.\n\n    The decoded string is returned.  A TypeError is raised if s were\n    incorrectly padded or if there are non-alphabet characters present in the\n    string.\n    \"\"\"\n    if casefold:\n        s = s.upper()\n    if re.search('[^0-9A-F]', s):\n        raise TypeError('Non-base16 digit found')\n    return binascii.unhexlify(s)\n\n\n\f\n# Legacy interface.  This code could be cleaned up since I don't believe\n# binascii has any line length limitations.  It just doesn't seem worth it\n# though.\n\nMAXLINESIZE = 76 # Excluding the CRLF\nMAXBINSIZE = (MAXLINESIZE//4)*3\n\ndef encode(input, output):\n    \"\"\"Encode a file.\"\"\"\n    while True:\n        s = input.read(MAXBINSIZE)\n        if not s:\n            break\n        while len(s) < MAXBINSIZE:\n            ns = input.read(MAXBINSIZE-len(s))\n            if not ns:\n                break\n            s += ns\n        line = binascii.b2a_base64(s)\n        output.write(line)\n\n\ndef decode(input, output):\n    \"\"\"Decode a file.\"\"\"\n    while True:\n        line = input.readline()\n        if not line:\n            break\n        s = binascii.a2b_base64(line)\n        output.write(s)\n\n\ndef encodestring(s):\n    \"\"\"Encode a string into multiple lines of base-64 data.\"\"\"\n    pieces = []\n    for i in range(0, len(s), MAXBINSIZE):\n        chunk = s[i : i + MAXBINSIZE]\n        pieces.append(binascii.b2a_base64(chunk))\n    return \"\".join(pieces)\n\n\ndef decodestring(s):\n    \"\"\"Decode a string.\"\"\"\n    return binascii.a2b_base64(s)\n\n\n\f\n# Useable as a script...\ndef test():\n    \"\"\"Small test program\"\"\"\n    import sys, getopt\n    try:\n        opts, args = getopt.getopt(sys.argv[1:], 'deut')\n    except getopt.error, msg:\n        sys.stdout = sys.stderr\n        print msg\n        print \"\"\"usage: %s [-d|-e|-u|-t] [file|-]\n        -d, -u: decode\n        -e: encode (default)\n        -t: encode and decode string 'Aladdin:open sesame'\"\"\"%sys.argv[0]\n        sys.exit(2)\n    func = encode\n    for o, a in opts:\n        if o == '-e': func = encode\n        if o == '-d': func = decode\n        if o == '-u': func = decode\n        if o == '-t': test1(); return\n    if args and args[0] != '-':\n        with open(args[0], 'rb') as f:\n            func(f, sys.stdout)\n    else:\n        func(sys.stdin, sys.stdout)\n\n\ndef test1():\n    s0 = \"Aladdin:open sesame\"\n    s1 = encodestring(s0)\n    s2 = decodestring(s1)\n    print s0, repr(s1), s2\n\n\nif __name__ == '__main__':\n    test()\n", 
    "bdb": "\"\"\"Debugger basics\"\"\"\n\nimport fnmatch\nimport sys\nimport os\nimport types\n\n__all__ = [\"BdbQuit\",\"Bdb\",\"Breakpoint\"]\n\nclass BdbQuit(Exception):\n    \"\"\"Exception to give up completely\"\"\"\n\n\nclass Bdb:\n\n    \"\"\"Generic Python debugger base class.\n\n    This class takes care of details of the trace facility;\n    a derived class should implement user interaction.\n    The standard debugger class (pdb.Pdb) is an example.\n    \"\"\"\n\n    def __init__(self, skip=None):\n        self.skip = set(skip) if skip else None\n        self.breaks = {}\n        self.fncache = {}\n        self.frame_returning = None\n\n    def canonic(self, filename):\n        if filename == \"<\" + filename[1:-1] + \">\":\n            return filename\n        canonic = self.fncache.get(filename)\n        if not canonic:\n            canonic = os.path.abspath(filename)\n            canonic = os.path.normcase(canonic)\n            self.fncache[filename] = canonic\n        return canonic\n\n    def reset(self):\n        import linecache\n        linecache.checkcache()\n        self.botframe = None\n        self._set_stopinfo(None, None)\n\n    def trace_dispatch(self, frame, event, arg):\n        if self.quitting:\n            return # None\n        if event == 'line':\n            return self.dispatch_line(frame)\n        if event == 'call':\n            return self.dispatch_call(frame, arg)\n        if event == 'return':\n            return self.dispatch_return(frame, arg)\n        if event == 'exception':\n            return self.dispatch_exception(frame, arg)\n        if event == 'c_call':\n            return self.trace_dispatch\n        if event == 'c_exception':\n            return self.trace_dispatch\n        if event == 'c_return':\n            return self.trace_dispatch\n        print 'bdb.Bdb.dispatch: unknown debugging event:', repr(event)\n        return self.trace_dispatch\n\n    def dispatch_line(self, frame):\n        if self.stop_here(frame) or self.break_here(frame):\n            self.user_line(frame)\n            if self.quitting: raise BdbQuit\n        return self.trace_dispatch\n\n    def dispatch_call(self, frame, arg):\n        # XXX 'arg' is no longer used\n        if self.botframe is None:\n            # First call of dispatch since reset()\n            self.botframe = frame.f_back # (CT) Note that this may also be None!\n            return self.trace_dispatch\n        if not (self.stop_here(frame) or self.break_anywhere(frame)):\n            # No need to trace this function\n            return # None\n        self.user_call(frame, arg)\n        if self.quitting: raise BdbQuit\n        return self.trace_dispatch\n\n    def dispatch_return(self, frame, arg):\n        if self.stop_here(frame) or frame == self.returnframe:\n            try:\n                self.frame_returning = frame\n                self.user_return(frame, arg)\n            finally:\n                self.frame_returning = None\n            if self.quitting: raise BdbQuit\n        return self.trace_dispatch\n\n    def dispatch_exception(self, frame, arg):\n        if self.stop_here(frame):\n            self.user_exception(frame, arg)\n            if self.quitting: raise BdbQuit\n        return self.trace_dispatch\n\n    # Normally derived classes don't override the following\n    # methods, but they may if they want to redefine the\n    # definition of stopping and breakpoints.\n\n    def is_skipped_module(self, module_name):\n        for pattern in self.skip:\n            if fnmatch.fnmatch(module_name, pattern):\n                return True\n        return False\n\n    def stop_here(self, frame):\n        # (CT) stopframe may now also be None, see dispatch_call.\n        # (CT) the former test for None is therefore removed from here.\n        if self.skip and \\\n               self.is_skipped_module(frame.f_globals.get('__name__')):\n            return False\n        if frame is self.stopframe:\n            if self.stoplineno == -1:\n                return False\n            return frame.f_lineno >= self.stoplineno\n        while frame is not None and frame is not self.stopframe:\n            if frame is self.botframe:\n                return True\n            frame = frame.f_back\n        return False\n\n    def break_here(self, frame):\n        filename = self.canonic(frame.f_code.co_filename)\n        if not filename in self.breaks:\n            return False\n        lineno = frame.f_lineno\n        if not lineno in self.breaks[filename]:\n            # The line itself has no breakpoint, but maybe the line is the\n            # first line of a function with breakpoint set by function name.\n            lineno = frame.f_code.co_firstlineno\n            if not lineno in self.breaks[filename]:\n                return False\n\n        # flag says ok to delete temp. bp\n        (bp, flag) = effective(filename, lineno, frame)\n        if bp:\n            self.currentbp = bp.number\n            if (flag and bp.temporary):\n                self.do_clear(str(bp.number))\n            return True\n        else:\n            return False\n\n    def do_clear(self, arg):\n        raise NotImplementedError, \"subclass of bdb must implement do_clear()\"\n\n    def break_anywhere(self, frame):\n        return self.canonic(frame.f_code.co_filename) in self.breaks\n\n    # Derived classes should override the user_* methods\n    # to gain control.\n\n    def user_call(self, frame, argument_list):\n        \"\"\"This method is called when there is the remote possibility\n        that we ever need to stop in this function.\"\"\"\n        pass\n\n    def user_line(self, frame):\n        \"\"\"This method is called when we stop or break at this line.\"\"\"\n        pass\n\n    def user_return(self, frame, return_value):\n        \"\"\"This method is called when a return trap is set here.\"\"\"\n        pass\n\n    def user_exception(self, frame, exc_info):\n        exc_type, exc_value, exc_traceback = exc_info\n        \"\"\"This method is called if an exception occurs,\n        but only if we are to stop at or just below this level.\"\"\"\n        pass\n\n    def _set_stopinfo(self, stopframe, returnframe, stoplineno=0):\n        self.stopframe = stopframe\n        self.returnframe = returnframe\n        self.quitting = 0\n        # stoplineno >= 0 means: stop at line >= the stoplineno\n        # stoplineno -1 means: don't stop at all\n        self.stoplineno = stoplineno\n\n    # Derived classes and clients can call the following methods\n    # to affect the stepping state.\n\n    def set_until(self, frame): #the name \"until\" is borrowed from gdb\n        \"\"\"Stop when the line with the line no greater than the current one is\n        reached or when returning from current frame\"\"\"\n        self._set_stopinfo(frame, frame, frame.f_lineno+1)\n\n    def set_step(self):\n        \"\"\"Stop after one line of code.\"\"\"\n        # Issue #13183: pdb skips frames after hitting a breakpoint and running\n        # step commands.\n        # Restore the trace function in the caller (that may not have been set\n        # for performance reasons) when returning from the current frame.\n        if self.frame_returning:\n            caller_frame = self.frame_returning.f_back\n            if caller_frame and not caller_frame.f_trace:\n                caller_frame.f_trace = self.trace_dispatch\n        self._set_stopinfo(None, None)\n\n    def set_next(self, frame):\n        \"\"\"Stop on the next line in or below the given frame.\"\"\"\n        self._set_stopinfo(frame, None)\n\n    def set_return(self, frame):\n        \"\"\"Stop when returning from the given frame.\"\"\"\n        self._set_stopinfo(frame.f_back, frame)\n\n    def set_trace(self, frame=None):\n        \"\"\"Start debugging from `frame`.\n\n        If frame is not specified, debugging starts from caller's frame.\n        \"\"\"\n        if frame is None:\n            frame = sys._getframe().f_back\n        self.reset()\n        while frame:\n            frame.f_trace = self.trace_dispatch\n            self.botframe = frame\n            frame = frame.f_back\n        self.set_step()\n        sys.settrace(self.trace_dispatch)\n\n    def set_continue(self):\n        # Don't stop except at breakpoints or when finished\n        self._set_stopinfo(self.botframe, None, -1)\n        if not self.breaks:\n            # no breakpoints; run without debugger overhead\n            sys.settrace(None)\n            frame = sys._getframe().f_back\n            while frame and frame is not self.botframe:\n                del frame.f_trace\n                frame = frame.f_back\n\n    def set_quit(self):\n        self.stopframe = self.botframe\n        self.returnframe = None\n        self.quitting = 1\n        sys.settrace(None)\n\n    # Derived classes and clients can call the following methods\n    # to manipulate breakpoints.  These methods return an\n    # error message is something went wrong, None if all is well.\n    # Set_break prints out the breakpoint line and file:lineno.\n    # Call self.get_*break*() to see the breakpoints or better\n    # for bp in Breakpoint.bpbynumber: if bp: bp.bpprint().\n\n    def set_break(self, filename, lineno, temporary=0, cond = None,\n                  funcname=None):\n        filename = self.canonic(filename)\n        import linecache # Import as late as possible\n        line = linecache.getline(filename, lineno)\n        if not line:\n            return 'Line %s:%d does not exist' % (filename,\n                                   lineno)\n        if not filename in self.breaks:\n            self.breaks[filename] = []\n        list = self.breaks[filename]\n        if not lineno in list:\n            list.append(lineno)\n        bp = Breakpoint(filename, lineno, temporary, cond, funcname)\n\n    def _prune_breaks(self, filename, lineno):\n        if (filename, lineno) not in Breakpoint.bplist:\n            self.breaks[filename].remove(lineno)\n        if not self.breaks[filename]:\n            del self.breaks[filename]\n\n    def clear_break(self, filename, lineno):\n        filename = self.canonic(filename)\n        if not filename in self.breaks:\n            return 'There are no breakpoints in %s' % filename\n        if lineno not in self.breaks[filename]:\n            return 'There is no breakpoint at %s:%d' % (filename,\n                                    lineno)\n        # If there's only one bp in the list for that file,line\n        # pair, then remove the breaks entry\n        for bp in Breakpoint.bplist[filename, lineno][:]:\n            bp.deleteMe()\n        self._prune_breaks(filename, lineno)\n\n    def clear_bpbynumber(self, arg):\n        try:\n            number = int(arg)\n        except:\n            return 'Non-numeric breakpoint number (%s)' % arg\n        try:\n            bp = Breakpoint.bpbynumber[number]\n        except IndexError:\n            return 'Breakpoint number (%d) out of range' % number\n        if not bp:\n            return 'Breakpoint (%d) already deleted' % number\n        bp.deleteMe()\n        self._prune_breaks(bp.file, bp.line)\n\n    def clear_all_file_breaks(self, filename):\n        filename = self.canonic(filename)\n        if not filename in self.breaks:\n            return 'There are no breakpoints in %s' % filename\n        for line in self.breaks[filename]:\n            blist = Breakpoint.bplist[filename, line]\n            for bp in blist:\n                bp.deleteMe()\n        del self.breaks[filename]\n\n    def clear_all_breaks(self):\n        if not self.breaks:\n            return 'There are no breakpoints'\n        for bp in Breakpoint.bpbynumber:\n            if bp:\n                bp.deleteMe()\n        self.breaks = {}\n\n    def get_break(self, filename, lineno):\n        filename = self.canonic(filename)\n        return filename in self.breaks and \\\n            lineno in self.breaks[filename]\n\n    def get_breaks(self, filename, lineno):\n        filename = self.canonic(filename)\n        return filename in self.breaks and \\\n            lineno in self.breaks[filename] and \\\n            Breakpoint.bplist[filename, lineno] or []\n\n    def get_file_breaks(self, filename):\n        filename = self.canonic(filename)\n        if filename in self.breaks:\n            return self.breaks[filename]\n        else:\n            return []\n\n    def get_all_breaks(self):\n        return self.breaks\n\n    # Derived classes and clients can call the following method\n    # to get a data structure representing a stack trace.\n\n    def get_stack(self, f, t):\n        stack = []\n        if t and t.tb_frame is f:\n            t = t.tb_next\n        while f is not None:\n            stack.append((f, f.f_lineno))\n            if f is self.botframe:\n                break\n            f = f.f_back\n        stack.reverse()\n        i = max(0, len(stack) - 1)\n        while t is not None:\n            stack.append((t.tb_frame, t.tb_lineno))\n            t = t.tb_next\n        if f is None:\n            i = max(0, len(stack) - 1)\n        return stack, i\n\n    #\n\n    def format_stack_entry(self, frame_lineno, lprefix=': '):\n        import linecache, repr\n        frame, lineno = frame_lineno\n        filename = self.canonic(frame.f_code.co_filename)\n        s = '%s(%r)' % (filename, lineno)\n        if frame.f_code.co_name:\n            s = s + frame.f_code.co_name\n        else:\n            s = s + \"<lambda>\"\n        if '__args__' in frame.f_locals:\n            args = frame.f_locals['__args__']\n        else:\n            args = None\n        if args:\n            s = s + repr.repr(args)\n        else:\n            s = s + '()'\n        if '__return__' in frame.f_locals:\n            rv = frame.f_locals['__return__']\n            s = s + '->'\n            s = s + repr.repr(rv)\n        line = linecache.getline(filename, lineno, frame.f_globals)\n        if line: s = s + lprefix + line.strip()\n        return s\n\n    # The following two methods can be called by clients to use\n    # a debugger to debug a statement, given as a string.\n\n    def run(self, cmd, globals=None, locals=None):\n        if globals is None:\n            import __main__\n            globals = __main__.__dict__\n        if locals is None:\n            locals = globals\n        self.reset()\n        sys.settrace(self.trace_dispatch)\n        if not isinstance(cmd, types.CodeType):\n            cmd = cmd+'\\n'\n        try:\n            exec cmd in globals, locals\n        except BdbQuit:\n            pass\n        finally:\n            self.quitting = 1\n            sys.settrace(None)\n\n    def runeval(self, expr, globals=None, locals=None):\n        if globals is None:\n            import __main__\n            globals = __main__.__dict__\n        if locals is None:\n            locals = globals\n        self.reset()\n        sys.settrace(self.trace_dispatch)\n        if not isinstance(expr, types.CodeType):\n            expr = expr+'\\n'\n        try:\n            return eval(expr, globals, locals)\n        except BdbQuit:\n            pass\n        finally:\n            self.quitting = 1\n            sys.settrace(None)\n\n    def runctx(self, cmd, globals, locals):\n        # B/W compatibility\n        self.run(cmd, globals, locals)\n\n    # This method is more useful to debug a single function call.\n\n    def runcall(self, func, *args, **kwds):\n        self.reset()\n        sys.settrace(self.trace_dispatch)\n        res = None\n        try:\n            res = func(*args, **kwds)\n        except BdbQuit:\n            pass\n        finally:\n            self.quitting = 1\n            sys.settrace(None)\n        return res\n\n\ndef set_trace():\n    Bdb().set_trace()\n\n\nclass Breakpoint:\n\n    \"\"\"Breakpoint class\n\n    Implements temporary breakpoints, ignore counts, disabling and\n    (re)-enabling, and conditionals.\n\n    Breakpoints are indexed by number through bpbynumber and by\n    the file,line tuple using bplist.  The former points to a\n    single instance of class Breakpoint.  The latter points to a\n    list of such instances since there may be more than one\n    breakpoint per line.\n\n    \"\"\"\n\n    # XXX Keeping state in the class is a mistake -- this means\n    # you cannot have more than one active Bdb instance.\n\n    next = 1        # Next bp to be assigned\n    bplist = {}     # indexed by (file, lineno) tuple\n    bpbynumber = [None] # Each entry is None or an instance of Bpt\n                # index 0 is unused, except for marking an\n                # effective break .... see effective()\n\n    def __init__(self, file, line, temporary=0, cond=None, funcname=None):\n        self.funcname = funcname\n        # Needed if funcname is not None.\n        self.func_first_executable_line = None\n        self.file = file    # This better be in canonical form!\n        self.line = line\n        self.temporary = temporary\n        self.cond = cond\n        self.enabled = 1\n        self.ignore = 0\n        self.hits = 0\n        self.number = Breakpoint.next\n        Breakpoint.next = Breakpoint.next + 1\n        # Build the two lists\n        self.bpbynumber.append(self)\n        if (file, line) in self.bplist:\n            self.bplist[file, line].append(self)\n        else:\n            self.bplist[file, line] = [self]\n\n\n    def deleteMe(self):\n        index = (self.file, self.line)\n        self.bpbynumber[self.number] = None   # No longer in list\n        self.bplist[index].remove(self)\n        if not self.bplist[index]:\n            # No more bp for this f:l combo\n            del self.bplist[index]\n\n    def enable(self):\n        self.enabled = 1\n\n    def disable(self):\n        self.enabled = 0\n\n    def bpprint(self, out=None):\n        if out is None:\n            out = sys.stdout\n        if self.temporary:\n            disp = 'del  '\n        else:\n            disp = 'keep '\n        if self.enabled:\n            disp = disp + 'yes  '\n        else:\n            disp = disp + 'no   '\n        print >>out, '%-4dbreakpoint   %s at %s:%d' % (self.number, disp,\n                                                       self.file, self.line)\n        if self.cond:\n            print >>out, '\\tstop only if %s' % (self.cond,)\n        if self.ignore:\n            print >>out, '\\tignore next %d hits' % (self.ignore)\n        if (self.hits):\n            if (self.hits > 1): ss = 's'\n            else: ss = ''\n            print >>out, ('\\tbreakpoint already hit %d time%s' %\n                          (self.hits, ss))\n\n# -----------end of Breakpoint class----------\n\ndef checkfuncname(b, frame):\n    \"\"\"Check whether we should break here because of `b.funcname`.\"\"\"\n    if not b.funcname:\n        # Breakpoint was set via line number.\n        if b.line != frame.f_lineno:\n            # Breakpoint was set at a line with a def statement and the function\n            # defined is called: don't break.\n            return False\n        return True\n\n    # Breakpoint set via function name.\n\n    if frame.f_code.co_name != b.funcname:\n        # It's not a function call, but rather execution of def statement.\n        return False\n\n    # We are in the right frame.\n    if not b.func_first_executable_line:\n        # The function is entered for the 1st time.\n        b.func_first_executable_line = frame.f_lineno\n\n    if  b.func_first_executable_line != frame.f_lineno:\n        # But we are not at the first line number: don't break.\n        return False\n    return True\n\n# Determines if there is an effective (active) breakpoint at this\n# line of code.  Returns breakpoint number or 0 if none\ndef effective(file, line, frame):\n    \"\"\"Determine which breakpoint for this file:line is to be acted upon.\n\n    Called only if we know there is a bpt at this\n    location.  Returns breakpoint that was triggered and a flag\n    that indicates if it is ok to delete a temporary bp.\n\n    \"\"\"\n    possibles = Breakpoint.bplist[file,line]\n    for i in range(0, len(possibles)):\n        b = possibles[i]\n        if b.enabled == 0:\n            continue\n        if not checkfuncname(b, frame):\n            continue\n        # Count every hit when bp is enabled\n        b.hits = b.hits + 1\n        if not b.cond:\n            # If unconditional, and ignoring,\n            # go on to next, else break\n            if b.ignore > 0:\n                b.ignore = b.ignore -1\n                continue\n            else:\n                # breakpoint and marker that's ok\n                # to delete if temporary\n                return (b,1)\n        else:\n            # Conditional bp.\n            # Ignore count applies only to those bpt hits where the\n            # condition evaluates to true.\n            try:\n                val = eval(b.cond, frame.f_globals,\n                       frame.f_locals)\n                if val:\n                    if b.ignore > 0:\n                        b.ignore = b.ignore -1\n                        # continue\n                    else:\n                        return (b,1)\n                # else:\n                #   continue\n            except:\n                # if eval fails, most conservative\n                # thing is to stop on breakpoint\n                # regardless of ignore count.\n                # Don't delete temporary,\n                # as another hint to user.\n                return (b,0)\n    return (None, None)\n\n# -------------------- testing --------------------\n\nclass Tdb(Bdb):\n    def user_call(self, frame, args):\n        name = frame.f_code.co_name\n        if not name: name = '???'\n        print '+++ call', name, args\n    def user_line(self, frame):\n        import linecache\n        name = frame.f_code.co_name\n        if not name: name = '???'\n        fn = self.canonic(frame.f_code.co_filename)\n        line = linecache.getline(fn, frame.f_lineno, frame.f_globals)\n        print '+++', fn, frame.f_lineno, name, ':', line.strip()\n    def user_return(self, frame, retval):\n        print '+++ return', retval\n    def user_exception(self, frame, exc_stuff):\n        print '+++ exception', exc_stuff\n        self.set_continue()\n\ndef foo(n):\n    print 'foo(', n, ')'\n    x = bar(n*10)\n    print 'bar returned', x\n\ndef bar(a):\n    print 'bar(', a, ')'\n    return a/2\n\ndef test():\n    t = Tdb()\n    t.run('import bdb; bdb.foo(10)')\n\n# end\n", 
    "cPickle": "#\n# Reimplementation of cPickle, mostly as a copy of pickle.py\n#\n\nfrom pickle import Pickler, dump, dumps, PickleError, PicklingError, UnpicklingError, _EmptyClass\nfrom pickle import __doc__, __version__, format_version, compatible_formats\nfrom types import *\nfrom copy_reg import dispatch_table\nfrom copy_reg import _extension_registry, _inverted_registry, _extension_cache\nimport marshal, struct, sys\n\ntry: from __pypy__ import builtinify\nexcept ImportError: builtinify = lambda f: f\n\n# These are purely informational; no code uses these.\nformat_version = \"2.0\"                  # File format version we write\ncompatible_formats = [\"1.0\",            # Original protocol 0\n                      \"1.1\",            # Protocol 0 with INST added\n                      \"1.2\",            # Original protocol 1\n                      \"1.3\",            # Protocol 1 with BINFLOAT added\n                      \"2.0\",            # Protocol 2\n                      ]                 # Old format versions we can read\n\n# Keep in synch with cPickle.  This is the highest protocol number we\n# know how to read.\nHIGHEST_PROTOCOL = 2\n\nBadPickleGet = KeyError\nUnpickleableError = PicklingError\n\nMARK            = ord('(')   # push special markobject on stack\nSTOP            = ord('.')   # every pickle ends with STOP\nPOP             = ord('0')   # discard topmost stack item\nPOP_MARK        = ord('1')   # discard stack top through topmost markobject\nDUP             = ord('2')   # duplicate top stack item\nFLOAT           = ord('F')   # push float object; decimal string argument\nINT             = ord('I')   # push integer or bool; decimal string argument\nBININT          = ord('J')   # push four-byte signed int\nBININT1         = ord('K')   # push 1-byte unsigned int\nLONG            = ord('L')   # push long; decimal string argument\nBININT2         = ord('M')   # push 2-byte unsigned int\nNONE            = ord('N')   # push None\nPERSID          = ord('P')   # push persistent object; id is taken from string arg\nBINPERSID       = ord('Q')   #  \"       \"         \"  ;  \"  \"   \"     \"  stack\nREDUCE          = ord('R')   # apply callable to argtuple, both on stack\nSTRING          = ord('S')   # push string; NL-terminated string argument\nBINSTRING       = ord('T')   # push string; counted binary string argument\nSHORT_BINSTRING = ord('U')   #  \"     \"   ;    \"      \"       \"      \" < 256 bytes\nUNICODE         = ord('V')   # push Unicode string; raw-unicode-escaped'd argument\nBINUNICODE      = ord('X')   #   \"     \"       \"  ; counted UTF-8 string argument\nAPPEND          = ord('a')   # append stack top to list below it\nBUILD           = ord('b')   # call __setstate__ or __dict__.update()\nGLOBAL          = ord('c')   # push self.find_class(modname, name); 2 string args\nDICT            = ord('d')   # build a dict from stack items\nEMPTY_DICT      = ord('}')   # push empty dict\nAPPENDS         = ord('e')   # extend list on stack by topmost stack slice\nGET             = ord('g')   # push item from memo on stack; index is string arg\nBINGET          = ord('h')   #   \"    \"    \"    \"   \"   \"  ;   \"    \" 1-byte arg\nINST            = ord('i')   # build & push class instance\nLONG_BINGET     = ord('j')   # push item from memo on stack; index is 4-byte arg\nLIST            = ord('l')   # build list from topmost stack items\nEMPTY_LIST      = ord(']')   # push empty list\nOBJ             = ord('o')   # build & push class instance\nPUT             = ord('p')   # store stack top in memo; index is string arg\nBINPUT          = ord('q')   #   \"     \"    \"   \"   \" ;   \"    \" 1-byte arg\nLONG_BINPUT     = ord('r')   #   \"     \"    \"   \"   \" ;   \"    \" 4-byte arg\nSETITEM         = ord('s')   # add key+value pair to dict\nTUPLE           = ord('t')   # build tuple from topmost stack items\nEMPTY_TUPLE     = ord(')')   # push empty tuple\nSETITEMS        = ord('u')   # modify dict by adding topmost key+value pairs\nBINFLOAT        = ord('G')   # push float; arg is 8-byte float encoding\n\nTRUE            = 'I01\\n'  # not an opcode; see INT docs in pickletools.py\nFALSE           = 'I00\\n'  # not an opcode; see INT docs in pickletools.py\n\n# Protocol 2\n\nPROTO           = ord('\\x80')  # identify pickle protocol\nNEWOBJ          = ord('\\x81')  # build object by applying cls.__new__ to argtuple\nEXT1            = ord('\\x82')  # push object from extension registry; 1-byte index\nEXT2            = ord('\\x83')  # ditto, but 2-byte index\nEXT4            = ord('\\x84')  # ditto, but 4-byte index\nTUPLE1          = ord('\\x85')  # build 1-tuple from stack top\nTUPLE2          = ord('\\x86')  # build 2-tuple from two topmost stack items\nTUPLE3          = ord('\\x87')  # build 3-tuple from three topmost stack items\nNEWTRUE         = ord('\\x88')  # push True\nNEWFALSE        = ord('\\x89')  # push False\nLONG1           = ord('\\x8a')  # push long from < 256 bytes\nLONG4           = ord('\\x8b')  # push really big long\n\n_tuplesize2code = [EMPTY_TUPLE, TUPLE1, TUPLE2, TUPLE3]\n\n\n# ____________________________________________________________\n# XXX some temporary dark magic to produce pickled dumps that are\n#     closer to the ones produced by cPickle in CPython\n\nfrom pickle import StringIO\n\nPythonPickler = Pickler\nclass Pickler(PythonPickler):\n    def __init__(self, *args, **kw):\n        self.__f = None\n        if len(args) == 1 and isinstance(args[0], int):\n            self.__f = StringIO()\n            PythonPickler.__init__(self, self.__f, args[0], **kw)\n        else:\n            PythonPickler.__init__(self, *args, **kw)\n\n    def memoize(self, obj):\n        self.memo[id(None)] = None   # cPickle starts counting at one\n        return PythonPickler.memoize(self, obj)\n\n    def getvalue(self):\n        return self.__f and self.__f.getvalue()\n\n@builtinify\ndef dump(obj, file, protocol=None):\n    Pickler(file, protocol).dump(obj)\n\n@builtinify\ndef dumps(obj, protocol=None):\n    file = StringIO()\n    Pickler(file, protocol).dump(obj)\n    return file.getvalue()\n\n# Why use struct.pack() for pickling but marshal.loads() for\n# unpickling?  struct.pack() is 40% faster than marshal.dumps(), but\n# marshal.loads() is twice as fast as struct.unpack()!\nmloads = marshal.loads\n\n# Unpickling machinery\n\nclass _Stack(list):\n    def pop(self, index=-1):\n        try:\n            return list.pop(self, index)\n        except IndexError:\n            raise UnpicklingError(\"unpickling stack underflow\")\n\nclass Unpickler(object):\n\n    def __init__(self, file):\n        \"\"\"This takes a file-like object for reading a pickle data stream.\n\n        The protocol version of the pickle is detected automatically, so no\n        proto argument is needed.\n\n        The file-like object must have two methods, a read() method that\n        takes an integer argument, and a readline() method that requires no\n        arguments.  Both methods should return a string.  Thus file-like\n        object can be a file object opened for reading, a StringIO object,\n        or any other custom object that meets this interface.\n        \"\"\"\n        self.readline = file.readline\n        self.read = file.read\n        self.memo = {}\n\n    def load(self):\n        \"\"\"Read a pickled object representation from the open file.\n\n        Return the reconstituted object hierarchy specified in the file.\n        \"\"\"\n        self.mark = object() # any new unique object\n        self.stack = _Stack()\n        self.append = self.stack.append\n        try:\n            key = ord(self.read(1))\n            while key != STOP:\n                self.dispatch[key](self)\n                key = ord(self.read(1))\n        except TypeError:\n            if self.read(1) == '':\n                raise EOFError\n            raise\n        return self.stack.pop()\n\n    # Return largest index k such that self.stack[k] is self.mark.\n    # If the stack doesn't contain a mark, eventually raises IndexError.\n    # This could be sped by maintaining another stack, of indices at which\n    # the mark appears.  For that matter, the latter stack would suffice,\n    # and we wouldn't need to push mark objects on self.stack at all.\n    # Doing so is probably a good thing, though, since if the pickle is\n    # corrupt (or hostile) we may get a clue from finding self.mark embedded\n    # in unpickled objects.\n    def marker(self):\n        k = len(self.stack)-1\n        while self.stack[k] is not self.mark: k -= 1\n        return k\n\n    dispatch = {}\n\n    def load_proto(self):\n        proto = ord(self.read(1))\n        if not 0 <= proto <= 2:\n            raise ValueError, \"unsupported pickle protocol: %d\" % proto\n    dispatch[PROTO] = load_proto\n\n    def load_persid(self):\n        pid = self.readline()[:-1]\n        self.append(self.persistent_load(pid))\n    dispatch[PERSID] = load_persid\n\n    def load_binpersid(self):\n        pid = self.stack.pop()\n        self.append(self.persistent_load(pid))\n    dispatch[BINPERSID] = load_binpersid\n\n    def load_none(self):\n        self.append(None)\n    dispatch[NONE] = load_none\n\n    def load_false(self):\n        self.append(False)\n    dispatch[NEWFALSE] = load_false\n\n    def load_true(self):\n        self.append(True)\n    dispatch[NEWTRUE] = load_true\n\n    def load_int(self):\n        data = self.readline()\n        if data == FALSE[1:]:\n            val = False\n        elif data == TRUE[1:]:\n            val = True\n        else:\n            val = int(data)\n        self.append(val)\n    dispatch[INT] = load_int\n\n    def load_binint(self):\n        self.append(mloads('i' + self.read(4)))\n    dispatch[BININT] = load_binint\n\n    def load_binint1(self):\n        self.append(ord(self.read(1)))\n    dispatch[BININT1] = load_binint1\n\n    def load_binint2(self):\n        self.append(mloads('i' + self.read(2) + '\\000\\000'))\n    dispatch[BININT2] = load_binint2\n\n    def load_long(self):\n        self.append(long(self.readline()[:-1], 0))\n    dispatch[LONG] = load_long\n\n    def load_long1(self):\n        n = ord(self.read(1))\n        bytes = self.read(n)\n        self.append(decode_long(bytes))\n    dispatch[LONG1] = load_long1\n\n    def load_long4(self):\n        n = mloads('i' + self.read(4))\n        bytes = self.read(n)\n        self.append(decode_long(bytes))\n    dispatch[LONG4] = load_long4\n\n    def load_float(self):\n        self.append(float(self.readline()[:-1]))\n    dispatch[FLOAT] = load_float\n\n    def load_binfloat(self, unpack=struct.unpack):\n        self.append(unpack('>d', self.read(8))[0])\n    dispatch[BINFLOAT] = load_binfloat\n\n    def load_string(self):\n        rep = self.readline()\n        if len(rep) < 3:\n            raise ValueError, \"insecure string pickle\"\n        if rep[0] == \"'\" == rep[-2]:\n            rep = rep[1:-2]\n        elif rep[0] == '\"' == rep[-2]:\n            rep = rep[1:-2]\n        else:\n            raise ValueError, \"insecure string pickle\"\n        self.append(rep.decode(\"string-escape\"))\n    dispatch[STRING] = load_string\n\n    def load_binstring(self):\n        L = mloads('i' + self.read(4))\n        self.append(self.read(L))\n    dispatch[BINSTRING] = load_binstring\n\n    def load_unicode(self):\n        self.append(unicode(self.readline()[:-1],'raw-unicode-escape'))\n    dispatch[UNICODE] = load_unicode\n\n    def load_binunicode(self):\n        L = mloads('i' + self.read(4))\n        self.append(unicode(self.read(L),'utf-8'))\n    dispatch[BINUNICODE] = load_binunicode\n\n    def load_short_binstring(self):\n        L = ord(self.read(1))\n        self.append(self.read(L))\n    dispatch[SHORT_BINSTRING] = load_short_binstring\n\n    def load_tuple(self):\n        k = self.marker()\n        self.stack[k:] = [tuple(self.stack[k+1:])]\n    dispatch[TUPLE] = load_tuple\n\n    def load_empty_tuple(self):\n        self.stack.append(())\n    dispatch[EMPTY_TUPLE] = load_empty_tuple\n\n    def load_tuple1(self):\n        self.stack[-1] = (self.stack[-1],)\n    dispatch[TUPLE1] = load_tuple1\n\n    def load_tuple2(self):\n        self.stack[-2:] = [(self.stack[-2], self.stack[-1])]\n    dispatch[TUPLE2] = load_tuple2\n\n    def load_tuple3(self):\n        self.stack[-3:] = [(self.stack[-3], self.stack[-2], self.stack[-1])]\n    dispatch[TUPLE3] = load_tuple3\n\n    def load_empty_list(self):\n        self.stack.append([])\n    dispatch[EMPTY_LIST] = load_empty_list\n\n    def load_empty_dictionary(self):\n        self.stack.append({})\n    dispatch[EMPTY_DICT] = load_empty_dictionary\n\n    def load_list(self):\n        k = self.marker()\n        self.stack[k:] = [self.stack[k+1:]]\n    dispatch[LIST] = load_list\n\n    def load_dict(self):\n        k = self.marker()\n        d = {}\n        items = self.stack[k+1:]\n        for i in range(0, len(items), 2):\n            key = items[i]\n            value = items[i+1]\n            d[key] = value\n        self.stack[k:] = [d]\n    dispatch[DICT] = load_dict\n\n    # INST and OBJ differ only in how they get a class object.  It's not\n    # only sensible to do the rest in a common routine, the two routines\n    # previously diverged and grew different bugs.\n    # klass is the class to instantiate, and k points to the topmost mark\n    # object, following which are the arguments for klass.__init__.\n    def _instantiate(self, klass, k):\n        args = tuple(self.stack[k+1:])\n        del self.stack[k:]\n        instantiated = 0\n        if (not args and\n                type(klass) is ClassType and\n                not hasattr(klass, \"__getinitargs__\")):\n            try:\n                value = _EmptyClass()\n                value.__class__ = klass\n                instantiated = 1\n            except RuntimeError:\n                # In restricted execution, assignment to inst.__class__ is\n                # prohibited\n                pass\n        if not instantiated:\n            try:\n                value = klass(*args)\n            except TypeError, err:\n                raise TypeError, \"in constructor for %s: %s\" % (\n                    klass.__name__, str(err)), sys.exc_info()[2]\n        self.append(value)\n\n    def load_inst(self):\n        module = self.readline()[:-1]\n        name = self.readline()[:-1]\n        klass = self.find_class(module, name)\n        self._instantiate(klass, self.marker())\n    dispatch[INST] = load_inst\n\n    def load_obj(self):\n        # Stack is ... markobject classobject arg1 arg2 ...\n        k = self.marker()\n        klass = self.stack.pop(k+1)\n        self._instantiate(klass, k)\n    dispatch[OBJ] = load_obj\n\n    def load_newobj(self):\n        args = self.stack.pop()\n        cls = self.stack[-1]\n        obj = cls.__new__(cls, *args)\n        self.stack[-1] = obj\n    dispatch[NEWOBJ] = load_newobj\n\n    def load_global(self):\n        module = self.readline()[:-1]\n        name = self.readline()[:-1]\n        klass = self.find_class(module, name)\n        self.append(klass)\n    dispatch[GLOBAL] = load_global\n\n    def load_ext1(self):\n        code = ord(self.read(1))\n        self.get_extension(code)\n    dispatch[EXT1] = load_ext1\n\n    def load_ext2(self):\n        code = mloads('i' + self.read(2) + '\\000\\000')\n        self.get_extension(code)\n    dispatch[EXT2] = load_ext2\n\n    def load_ext4(self):\n        code = mloads('i' + self.read(4))\n        self.get_extension(code)\n    dispatch[EXT4] = load_ext4\n\n    def get_extension(self, code):\n        nil = []\n        obj = _extension_cache.get(code, nil)\n        if obj is not nil:\n            self.append(obj)\n            return\n        key = _inverted_registry.get(code)\n        if not key:\n            raise ValueError(\"unregistered extension code %d\" % code)\n        obj = self.find_class(*key)\n        _extension_cache[code] = obj\n        self.append(obj)\n\n    def find_class(self, module, name):\n        # Subclasses may override this\n        __import__(module)\n        mod = sys.modules[module]\n        klass = getattr(mod, name)\n        return klass\n\n    def load_reduce(self):\n        args = self.stack.pop()\n        func = self.stack[-1]\n        value = self.stack[-1](*args)\n        self.stack[-1] = value\n    dispatch[REDUCE] = load_reduce\n\n    def load_pop(self):\n        del self.stack[-1]\n    dispatch[POP] = load_pop\n\n    def load_pop_mark(self):\n        k = self.marker()\n        del self.stack[k:]\n    dispatch[POP_MARK] = load_pop_mark\n\n    def load_dup(self):\n        self.append(self.stack[-1])\n    dispatch[DUP] = load_dup\n\n    def load_get(self):\n        self.append(self.memo[self.readline()[:-1]])\n    dispatch[GET] = load_get\n\n    def load_binget(self):\n        i = ord(self.read(1))\n        self.append(self.memo[repr(i)])\n    dispatch[BINGET] = load_binget\n\n    def load_long_binget(self):\n        i = mloads('i' + self.read(4))\n        self.append(self.memo[repr(i)])\n    dispatch[LONG_BINGET] = load_long_binget\n\n    def load_put(self):\n        self.memo[self.readline()[:-1]] = self.stack[-1]\n    dispatch[PUT] = load_put\n\n    def load_binput(self):\n        i = ord(self.read(1))\n        self.memo[repr(i)] = self.stack[-1]\n    dispatch[BINPUT] = load_binput\n\n    def load_long_binput(self):\n        i = mloads('i' + self.read(4))\n        self.memo[repr(i)] = self.stack[-1]\n    dispatch[LONG_BINPUT] = load_long_binput\n\n    def load_append(self):\n        value = self.stack.pop()\n        self.stack[-1].append(value)\n    dispatch[APPEND] = load_append\n\n    def load_appends(self):\n        stack = self.stack\n        mark = self.marker()\n        lst = stack[mark - 1]\n        lst.extend(stack[mark + 1:])\n        del stack[mark:]\n    dispatch[APPENDS] = load_appends\n\n    def load_setitem(self):\n        stack = self.stack\n        value = stack.pop()\n        key = stack.pop()\n        dict = stack[-1]\n        dict[key] = value\n    dispatch[SETITEM] = load_setitem\n\n    def load_setitems(self):\n        stack = self.stack\n        mark = self.marker()\n        dict = stack[mark - 1]\n        for i in range(mark + 1, len(stack), 2):\n            dict[stack[i]] = stack[i + 1]\n\n        del stack[mark:]\n    dispatch[SETITEMS] = load_setitems\n\n    def load_build(self):\n        stack = self.stack\n        state = stack.pop()\n        inst = stack[-1]\n        setstate = getattr(inst, \"__setstate__\", None)\n        if setstate:\n            setstate(state)\n            return\n        slotstate = None\n        if isinstance(state, tuple) and len(state) == 2:\n            state, slotstate = state\n        if state:\n            try:\n                d = inst.__dict__\n                try:\n                    for k, v in state.iteritems():\n                        d[intern(k)] = v\n                # keys in state don't have to be strings\n                # don't blow up, but don't go out of our way\n                except TypeError:\n                    d.update(state)\n\n            except RuntimeError:\n                # XXX In restricted execution, the instance's __dict__\n                # is not accessible.  Use the old way of unpickling\n                # the instance variables.  This is a semantic\n                # difference when unpickling in restricted\n                # vs. unrestricted modes.\n                # Note, however, that cPickle has never tried to do the\n                # .update() business, and always uses\n                #     PyObject_SetItem(inst.__dict__, key, value) in a\n                # loop over state.items().\n                for k, v in state.items():\n                    setattr(inst, k, v)\n        if slotstate:\n            for k, v in slotstate.items():\n                setattr(inst, k, v)\n    dispatch[BUILD] = load_build\n\n    def load_mark(self):\n        self.append(self.mark)\n    dispatch[MARK] = load_mark\n\n#from pickle import decode_long\n\ndef decode_long(data):\n    r\"\"\"Decode a long from a two's complement little-endian binary string.\n\n    >>> decode_long('')\n    0L\n    >>> decode_long(\"\\xff\\x00\")\n    255L\n    >>> decode_long(\"\\xff\\x7f\")\n    32767L\n    >>> decode_long(\"\\x00\\xff\")\n    -256L\n    >>> decode_long(\"\\x00\\x80\")\n    -32768L\n    >>> decode_long(\"\\x80\")\n    -128L\n    >>> decode_long(\"\\x7f\")\n    127L\n    \"\"\"\n\n    nbytes = len(data)\n    if nbytes == 0:\n        return 0L\n    ind = nbytes - 1\n    while ind and ord(data[ind]) == 0:\n        ind -= 1\n    n = ord(data[ind])\n    while ind:\n        n <<= 8\n        ind -= 1\n        if ord(data[ind]):\n            n += ord(data[ind])\n    if ord(data[nbytes - 1]) >= 128:\n        n -= 1L << (nbytes << 3)\n    return n\n\ndef load(f):\n    return Unpickler(f).load()\n\ndef loads(str):\n    f = StringIO(str)\n    return Unpickler(f).load()\n", 
    "cStringIO": "#\n# StringIO-based cStringIO implementation.\n#\n\n# Note that PyPy also contains a built-in module 'cStringIO' which will hide\n# this one if compiled in.\n\nfrom StringIO import *\nfrom StringIO import __doc__\n\nclass StringIO(StringIO):\n    def reset(self):\n        \"\"\"\n        reset() -- Reset the file position to the beginning\n        \"\"\"\n        self.seek(0, 0)\n", 
    "calendar": "\"\"\"Calendar printing functions\n\nNote when comparing these calendars to the ones printed by cal(1): By\ndefault, these calendars have Monday as the first day of the week, and\nSunday as the last (the European convention). Use setfirstweekday() to\nset the first day of the week (0=Monday, 6=Sunday).\"\"\"\n\nimport sys\nimport datetime\nimport locale as _locale\n\n__all__ = [\"IllegalMonthError\", \"IllegalWeekdayError\", \"setfirstweekday\",\n           \"firstweekday\", \"isleap\", \"leapdays\", \"weekday\", \"monthrange\",\n           \"monthcalendar\", \"prmonth\", \"month\", \"prcal\", \"calendar\",\n           \"timegm\", \"month_name\", \"month_abbr\", \"day_name\", \"day_abbr\"]\n\n# Exception raised for bad input (with string parameter for details)\nerror = ValueError\n\n# Exceptions raised for bad input\nclass IllegalMonthError(ValueError):\n    def __init__(self, month):\n        self.month = month\n    def __str__(self):\n        return \"bad month number %r; must be 1-12\" % self.month\n\n\nclass IllegalWeekdayError(ValueError):\n    def __init__(self, weekday):\n        self.weekday = weekday\n    def __str__(self):\n        return \"bad weekday number %r; must be 0 (Monday) to 6 (Sunday)\" % self.weekday\n\n\n# Constants for months referenced later\nJanuary = 1\nFebruary = 2\n\n# Number of days per month (except for February in leap years)\nmdays = [0, 31, 28, 31, 30, 31, 30, 31, 31, 30, 31, 30, 31]\n\n# This module used to have hard-coded lists of day and month names, as\n# English strings.  The classes following emulate a read-only version of\n# that, but supply localized names.  Note that the values are computed\n# fresh on each call, in case the user changes locale between calls.\n\nclass _localized_month:\n\n    _months = [datetime.date(2001, i+1, 1).strftime for i in range(12)]\n    _months.insert(0, lambda x: \"\")\n\n    def __init__(self, format):\n        self.format = format\n\n    def __getitem__(self, i):\n        funcs = self._months[i]\n        if isinstance(i, slice):\n            return [f(self.format) for f in funcs]\n        else:\n            return funcs(self.format)\n\n    def __len__(self):\n        return 13\n\n\nclass _localized_day:\n\n    # January 1, 2001, was a Monday.\n    _days = [datetime.date(2001, 1, i+1).strftime for i in range(7)]\n\n    def __init__(self, format):\n        self.format = format\n\n    def __getitem__(self, i):\n        funcs = self._days[i]\n        if isinstance(i, slice):\n            return [f(self.format) for f in funcs]\n        else:\n            return funcs(self.format)\n\n    def __len__(self):\n        return 7\n\n\n# Full and abbreviated names of weekdays\nday_name = _localized_day('%A')\nday_abbr = _localized_day('%a')\n\n# Full and abbreviated names of months (1-based arrays!!!)\nmonth_name = _localized_month('%B')\nmonth_abbr = _localized_month('%b')\n\n# Constants for weekdays\n(MONDAY, TUESDAY, WEDNESDAY, THURSDAY, FRIDAY, SATURDAY, SUNDAY) = range(7)\n\n\ndef isleap(year):\n    \"\"\"Return True for leap years, False for non-leap years.\"\"\"\n    return year % 4 == 0 and (year % 100 != 0 or year % 400 == 0)\n\n\ndef leapdays(y1, y2):\n    \"\"\"Return number of leap years in range [y1, y2).\n       Assume y1 <= y2.\"\"\"\n    y1 -= 1\n    y2 -= 1\n    return (y2//4 - y1//4) - (y2//100 - y1//100) + (y2//400 - y1//400)\n\n\ndef weekday(year, month, day):\n    \"\"\"Return weekday (0-6 ~ Mon-Sun) for year (1970-...), month (1-12),\n       day (1-31).\"\"\"\n    return datetime.date(year, month, day).weekday()\n\n\ndef monthrange(year, month):\n    \"\"\"Return weekday (0-6 ~ Mon-Sun) and number of days (28-31) for\n       year, month.\"\"\"\n    if not 1 <= month <= 12:\n        raise IllegalMonthError(month)\n    day1 = weekday(year, month, 1)\n    ndays = mdays[month] + (month == February and isleap(year))\n    return day1, ndays\n\n\nclass Calendar(object):\n    \"\"\"\n    Base calendar class. This class doesn't do any formatting. It simply\n    provides data to subclasses.\n    \"\"\"\n\n    def __init__(self, firstweekday=0):\n        self.firstweekday = firstweekday # 0 = Monday, 6 = Sunday\n\n    def getfirstweekday(self):\n        return self._firstweekday % 7\n\n    def setfirstweekday(self, firstweekday):\n        self._firstweekday = firstweekday\n\n    firstweekday = property(getfirstweekday, setfirstweekday)\n\n    def iterweekdays(self):\n        \"\"\"\n        Return a iterator for one week of weekday numbers starting with the\n        configured first one.\n        \"\"\"\n        for i in range(self.firstweekday, self.firstweekday + 7):\n            yield i%7\n\n    def itermonthdates(self, year, month):\n        \"\"\"\n        Return an iterator for one month. The iterator will yield datetime.date\n        values and will always iterate through complete weeks, so it will yield\n        dates outside the specified month.\n        \"\"\"\n        date = datetime.date(year, month, 1)\n        # Go back to the beginning of the week\n        days = (date.weekday() - self.firstweekday) % 7\n        date -= datetime.timedelta(days=days)\n        oneday = datetime.timedelta(days=1)\n        while True:\n            yield date\n            try:\n                date += oneday\n            except OverflowError:\n                # Adding one day could fail after datetime.MAXYEAR\n                break\n            if date.month != month and date.weekday() == self.firstweekday:\n                break\n\n    def itermonthdays2(self, year, month):\n        \"\"\"\n        Like itermonthdates(), but will yield (day number, weekday number)\n        tuples. For days outside the specified month the day number is 0.\n        \"\"\"\n        for date in self.itermonthdates(year, month):\n            if date.month != month:\n                yield (0, date.weekday())\n            else:\n                yield (date.day, date.weekday())\n\n    def itermonthdays(self, year, month):\n        \"\"\"\n        Like itermonthdates(), but will yield day numbers. For days outside\n        the specified month the day number is 0.\n        \"\"\"\n        for date in self.itermonthdates(year, month):\n            if date.month != month:\n                yield 0\n            else:\n                yield date.day\n\n    def monthdatescalendar(self, year, month):\n        \"\"\"\n        Return a matrix (list of lists) representing a month's calendar.\n        Each row represents a week; week entries are datetime.date values.\n        \"\"\"\n        dates = list(self.itermonthdates(year, month))\n        return [ dates[i:i+7] for i in range(0, len(dates), 7) ]\n\n    def monthdays2calendar(self, year, month):\n        \"\"\"\n        Return a matrix representing a month's calendar.\n        Each row represents a week; week entries are\n        (day number, weekday number) tuples. Day numbers outside this month\n        are zero.\n        \"\"\"\n        days = list(self.itermonthdays2(year, month))\n        return [ days[i:i+7] for i in range(0, len(days), 7) ]\n\n    def monthdayscalendar(self, year, month):\n        \"\"\"\n        Return a matrix representing a month's calendar.\n        Each row represents a week; days outside this month are zero.\n        \"\"\"\n        days = list(self.itermonthdays(year, month))\n        return [ days[i:i+7] for i in range(0, len(days), 7) ]\n\n    def yeardatescalendar(self, year, width=3):\n        \"\"\"\n        Return the data for the specified year ready for formatting. The return\n        value is a list of month rows. Each month row contains up to width months.\n        Each month contains between 4 and 6 weeks and each week contains 1-7\n        days. Days are datetime.date objects.\n        \"\"\"\n        months = [\n            self.monthdatescalendar(year, i)\n            for i in range(January, January+12)\n        ]\n        return [months[i:i+width] for i in range(0, len(months), width) ]\n\n    def yeardays2calendar(self, year, width=3):\n        \"\"\"\n        Return the data for the specified year ready for formatting (similar to\n        yeardatescalendar()). Entries in the week lists are\n        (day number, weekday number) tuples. Day numbers outside this month are\n        zero.\n        \"\"\"\n        months = [\n            self.monthdays2calendar(year, i)\n            for i in range(January, January+12)\n        ]\n        return [months[i:i+width] for i in range(0, len(months), width) ]\n\n    def yeardayscalendar(self, year, width=3):\n        \"\"\"\n        Return the data for the specified year ready for formatting (similar to\n        yeardatescalendar()). Entries in the week lists are day numbers.\n        Day numbers outside this month are zero.\n        \"\"\"\n        months = [\n            self.monthdayscalendar(year, i)\n            for i in range(January, January+12)\n        ]\n        return [months[i:i+width] for i in range(0, len(months), width) ]\n\n\nclass TextCalendar(Calendar):\n    \"\"\"\n    Subclass of Calendar that outputs a calendar as a simple plain text\n    similar to the UNIX program cal.\n    \"\"\"\n\n    def prweek(self, theweek, width):\n        \"\"\"\n        Print a single week (no newline).\n        \"\"\"\n        print self.formatweek(theweek, width),\n\n    def formatday(self, day, weekday, width):\n        \"\"\"\n        Returns a formatted day.\n        \"\"\"\n        if day == 0:\n            s = ''\n        else:\n            s = '%2i' % day             # right-align single-digit days\n        return s.center(width)\n\n    def formatweek(self, theweek, width):\n        \"\"\"\n        Returns a single week in a string (no newline).\n        \"\"\"\n        return ' '.join(self.formatday(d, wd, width) for (d, wd) in theweek)\n\n    def formatweekday(self, day, width):\n        \"\"\"\n        Returns a formatted week day name.\n        \"\"\"\n        if width >= 9:\n            names = day_name\n        else:\n            names = day_abbr\n        return names[day][:width].center(width)\n\n    def formatweekheader(self, width):\n        \"\"\"\n        Return a header for a week.\n        \"\"\"\n        return ' '.join(self.formatweekday(i, width) for i in self.iterweekdays())\n\n    def formatmonthname(self, theyear, themonth, width, withyear=True):\n        \"\"\"\n        Return a formatted month name.\n        \"\"\"\n        s = month_name[themonth]\n        if withyear:\n            s = \"%s %r\" % (s, theyear)\n        return s.center(width)\n\n    def prmonth(self, theyear, themonth, w=0, l=0):\n        \"\"\"\n        Print a month's calendar.\n        \"\"\"\n        print self.formatmonth(theyear, themonth, w, l),\n\n    def formatmonth(self, theyear, themonth, w=0, l=0):\n        \"\"\"\n        Return a month's calendar string (multi-line).\n        \"\"\"\n        w = max(2, w)\n        l = max(1, l)\n        s = self.formatmonthname(theyear, themonth, 7 * (w + 1) - 1)\n        s = s.rstrip()\n        s += '\\n' * l\n        s += self.formatweekheader(w).rstrip()\n        s += '\\n' * l\n        for week in self.monthdays2calendar(theyear, themonth):\n            s += self.formatweek(week, w).rstrip()\n            s += '\\n' * l\n        return s\n\n    def formatyear(self, theyear, w=2, l=1, c=6, m=3):\n        \"\"\"\n        Returns a year's calendar as a multi-line string.\n        \"\"\"\n        w = max(2, w)\n        l = max(1, l)\n        c = max(2, c)\n        colwidth = (w + 1) * 7 - 1\n        v = []\n        a = v.append\n        a(repr(theyear).center(colwidth*m+c*(m-1)).rstrip())\n        a('\\n'*l)\n        header = self.formatweekheader(w)\n        for (i, row) in enumerate(self.yeardays2calendar(theyear, m)):\n            # months in this row\n            months = range(m*i+1, min(m*(i+1)+1, 13))\n            a('\\n'*l)\n            names = (self.formatmonthname(theyear, k, colwidth, False)\n                     for k in months)\n            a(formatstring(names, colwidth, c).rstrip())\n            a('\\n'*l)\n            headers = (header for k in months)\n            a(formatstring(headers, colwidth, c).rstrip())\n            a('\\n'*l)\n            # max number of weeks for this row\n            height = max(len(cal) for cal in row)\n            for j in range(height):\n                weeks = []\n                for cal in row:\n                    if j >= len(cal):\n                        weeks.append('')\n                    else:\n                        weeks.append(self.formatweek(cal[j], w))\n                a(formatstring(weeks, colwidth, c).rstrip())\n                a('\\n' * l)\n        return ''.join(v)\n\n    def pryear(self, theyear, w=0, l=0, c=6, m=3):\n        \"\"\"Print a year's calendar.\"\"\"\n        print self.formatyear(theyear, w, l, c, m)\n\n\nclass HTMLCalendar(Calendar):\n    \"\"\"\n    This calendar returns complete HTML pages.\n    \"\"\"\n\n    # CSS classes for the day <td>s\n    cssclasses = [\"mon\", \"tue\", \"wed\", \"thu\", \"fri\", \"sat\", \"sun\"]\n\n    def formatday(self, day, weekday):\n        \"\"\"\n        Return a day as a table cell.\n        \"\"\"\n        if day == 0:\n            return '<td class=\"noday\">&nbsp;</td>' # day outside month\n        else:\n            return '<td class=\"%s\">%d</td>' % (self.cssclasses[weekday], day)\n\n    def formatweek(self, theweek):\n        \"\"\"\n        Return a complete week as a table row.\n        \"\"\"\n        s = ''.join(self.formatday(d, wd) for (d, wd) in theweek)\n        return '<tr>%s</tr>' % s\n\n    def formatweekday(self, day):\n        \"\"\"\n        Return a weekday name as a table header.\n        \"\"\"\n        return '<th class=\"%s\">%s</th>' % (self.cssclasses[day], day_abbr[day])\n\n    def formatweekheader(self):\n        \"\"\"\n        Return a header for a week as a table row.\n        \"\"\"\n        s = ''.join(self.formatweekday(i) for i in self.iterweekdays())\n        return '<tr>%s</tr>' % s\n\n    def formatmonthname(self, theyear, themonth, withyear=True):\n        \"\"\"\n        Return a month name as a table row.\n        \"\"\"\n        if withyear:\n            s = '%s %s' % (month_name[themonth], theyear)\n        else:\n            s = '%s' % month_name[themonth]\n        return '<tr><th colspan=\"7\" class=\"month\">%s</th></tr>' % s\n\n    def formatmonth(self, theyear, themonth, withyear=True):\n        \"\"\"\n        Return a formatted month as a table.\n        \"\"\"\n        v = []\n        a = v.append\n        a('<table border=\"0\" cellpadding=\"0\" cellspacing=\"0\" class=\"month\">')\n        a('\\n')\n        a(self.formatmonthname(theyear, themonth, withyear=withyear))\n        a('\\n')\n        a(self.formatweekheader())\n        a('\\n')\n        for week in self.monthdays2calendar(theyear, themonth):\n            a(self.formatweek(week))\n            a('\\n')\n        a('</table>')\n        a('\\n')\n        return ''.join(v)\n\n    def formatyear(self, theyear, width=3):\n        \"\"\"\n        Return a formatted year as a table of tables.\n        \"\"\"\n        v = []\n        a = v.append\n        width = max(width, 1)\n        a('<table border=\"0\" cellpadding=\"0\" cellspacing=\"0\" class=\"year\">')\n        a('\\n')\n        a('<tr><th colspan=\"%d\" class=\"year\">%s</th></tr>' % (width, theyear))\n        for i in range(January, January+12, width):\n            # months in this row\n            months = range(i, min(i+width, 13))\n            a('<tr>')\n            for m in months:\n                a('<td>')\n                a(self.formatmonth(theyear, m, withyear=False))\n                a('</td>')\n            a('</tr>')\n        a('</table>')\n        return ''.join(v)\n\n    def formatyearpage(self, theyear, width=3, css='calendar.css', encoding=None):\n        \"\"\"\n        Return a formatted year as a complete HTML page.\n        \"\"\"\n        if encoding is None:\n            encoding = sys.getdefaultencoding()\n        v = []\n        a = v.append\n        a('<?xml version=\"1.0\" encoding=\"%s\"?>\\n' % encoding)\n        a('<!DOCTYPE html PUBLIC \"-//W3C//DTD XHTML 1.0 Strict//EN\" \"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd\">\\n')\n        a('<html>\\n')\n        a('<head>\\n')\n        a('<meta http-equiv=\"Content-Type\" content=\"text/html; charset=%s\" />\\n' % encoding)\n        if css is not None:\n            a('<link rel=\"stylesheet\" type=\"text/css\" href=\"%s\" />\\n' % css)\n        a('<title>Calendar for %d</title>\\n' % theyear)\n        a('</head>\\n')\n        a('<body>\\n')\n        a(self.formatyear(theyear, width))\n        a('</body>\\n')\n        a('</html>\\n')\n        return ''.join(v).encode(encoding, \"xmlcharrefreplace\")\n\n\nclass TimeEncoding:\n    def __init__(self, locale):\n        self.locale = locale\n\n    def __enter__(self):\n        self.oldlocale = _locale.getlocale(_locale.LC_TIME)\n        _locale.setlocale(_locale.LC_TIME, self.locale)\n        return _locale.getlocale(_locale.LC_TIME)[1]\n\n    def __exit__(self, *args):\n        _locale.setlocale(_locale.LC_TIME, self.oldlocale)\n\n\nclass LocaleTextCalendar(TextCalendar):\n    \"\"\"\n    This class can be passed a locale name in the constructor and will return\n    month and weekday names in the specified locale. If this locale includes\n    an encoding all strings containing month and weekday names will be returned\n    as unicode.\n    \"\"\"\n\n    def __init__(self, firstweekday=0, locale=None):\n        TextCalendar.__init__(self, firstweekday)\n        if locale is None:\n            locale = _locale.getdefaultlocale()\n        self.locale = locale\n\n    def formatweekday(self, day, width):\n        with TimeEncoding(self.locale) as encoding:\n            if width >= 9:\n                names = day_name\n            else:\n                names = day_abbr\n            name = names[day]\n            if encoding is not None:\n                name = name.decode(encoding)\n            return name[:width].center(width)\n\n    def formatmonthname(self, theyear, themonth, width, withyear=True):\n        with TimeEncoding(self.locale) as encoding:\n            s = month_name[themonth]\n            if encoding is not None:\n                s = s.decode(encoding)\n            if withyear:\n                s = \"%s %r\" % (s, theyear)\n            return s.center(width)\n\n\nclass LocaleHTMLCalendar(HTMLCalendar):\n    \"\"\"\n    This class can be passed a locale name in the constructor and will return\n    month and weekday names in the specified locale. If this locale includes\n    an encoding all strings containing month and weekday names will be returned\n    as unicode.\n    \"\"\"\n    def __init__(self, firstweekday=0, locale=None):\n        HTMLCalendar.__init__(self, firstweekday)\n        if locale is None:\n            locale = _locale.getdefaultlocale()\n        self.locale = locale\n\n    def formatweekday(self, day):\n        with TimeEncoding(self.locale) as encoding:\n            s = day_abbr[day]\n            if encoding is not None:\n                s = s.decode(encoding)\n            return '<th class=\"%s\">%s</th>' % (self.cssclasses[day], s)\n\n    def formatmonthname(self, theyear, themonth, withyear=True):\n        with TimeEncoding(self.locale) as encoding:\n            s = month_name[themonth]\n            if encoding is not None:\n                s = s.decode(encoding)\n            if withyear:\n                s = '%s %s' % (s, theyear)\n            return '<tr><th colspan=\"7\" class=\"month\">%s</th></tr>' % s\n\n\n# Support for old module level interface\nc = TextCalendar()\n\nfirstweekday = c.getfirstweekday\n\ndef setfirstweekday(firstweekday):\n    try:\n        firstweekday.__index__\n    except AttributeError:\n        raise IllegalWeekdayError(firstweekday)\n    if not MONDAY <= firstweekday <= SUNDAY:\n        raise IllegalWeekdayError(firstweekday)\n    c.firstweekday = firstweekday\n\nmonthcalendar = c.monthdayscalendar\nprweek = c.prweek\nweek = c.formatweek\nweekheader = c.formatweekheader\nprmonth = c.prmonth\nmonth = c.formatmonth\ncalendar = c.formatyear\nprcal = c.pryear\n\n\n# Spacing of month columns for multi-column year calendar\n_colwidth = 7*3 - 1         # Amount printed by prweek()\n_spacing = 6                # Number of spaces between columns\n\n\ndef format(cols, colwidth=_colwidth, spacing=_spacing):\n    \"\"\"Prints multi-column formatting for year calendars\"\"\"\n    print formatstring(cols, colwidth, spacing)\n\n\ndef formatstring(cols, colwidth=_colwidth, spacing=_spacing):\n    \"\"\"Returns a string formatted from n strings, centered within n columns.\"\"\"\n    spacing *= ' '\n    return spacing.join(c.center(colwidth) for c in cols)\n\n\nEPOCH = 1970\n_EPOCH_ORD = datetime.date(EPOCH, 1, 1).toordinal()\n\n\ndef timegm(tuple):\n    \"\"\"Unrelated but handy function to calculate Unix timestamp from GMT.\"\"\"\n    year, month, day, hour, minute, second = tuple[:6]\n    days = datetime.date(year, month, 1).toordinal() - _EPOCH_ORD + day - 1\n    hours = days*24 + hour\n    minutes = hours*60 + minute\n    seconds = minutes*60 + second\n    return seconds\n\n\ndef main(args):\n    import optparse\n    parser = optparse.OptionParser(usage=\"usage: %prog [options] [year [month]]\")\n    parser.add_option(\n        \"-w\", \"--width\",\n        dest=\"width\", type=\"int\", default=2,\n        help=\"width of date column (default 2, text only)\"\n    )\n    parser.add_option(\n        \"-l\", \"--lines\",\n        dest=\"lines\", type=\"int\", default=1,\n        help=\"number of lines for each week (default 1, text only)\"\n    )\n    parser.add_option(\n        \"-s\", \"--spacing\",\n        dest=\"spacing\", type=\"int\", default=6,\n        help=\"spacing between months (default 6, text only)\"\n    )\n    parser.add_option(\n        \"-m\", \"--months\",\n        dest=\"months\", type=\"int\", default=3,\n        help=\"months per row (default 3, text only)\"\n    )\n    parser.add_option(\n        \"-c\", \"--css\",\n        dest=\"css\", default=\"calendar.css\",\n        help=\"CSS to use for page (html only)\"\n    )\n    parser.add_option(\n        \"-L\", \"--locale\",\n        dest=\"locale\", default=None,\n        help=\"locale to be used from month and weekday names\"\n    )\n    parser.add_option(\n        \"-e\", \"--encoding\",\n        dest=\"encoding\", default=None,\n        help=\"Encoding to use for output\"\n    )\n    parser.add_option(\n        \"-t\", \"--type\",\n        dest=\"type\", default=\"text\",\n        choices=(\"text\", \"html\"),\n        help=\"output type (text or html)\"\n    )\n\n    (options, args) = parser.parse_args(args)\n\n    if options.locale and not options.encoding:\n        parser.error(\"if --locale is specified --encoding is required\")\n        sys.exit(1)\n\n    locale = options.locale, options.encoding\n\n    if options.type == \"html\":\n        if options.locale:\n            cal = LocaleHTMLCalendar(locale=locale)\n        else:\n            cal = HTMLCalendar()\n        encoding = options.encoding\n        if encoding is None:\n            encoding = sys.getdefaultencoding()\n        optdict = dict(encoding=encoding, css=options.css)\n        if len(args) == 1:\n            print cal.formatyearpage(datetime.date.today().year, **optdict)\n        elif len(args) == 2:\n            print cal.formatyearpage(int(args[1]), **optdict)\n        else:\n            parser.error(\"incorrect number of arguments\")\n            sys.exit(1)\n    else:\n        if options.locale:\n            cal = LocaleTextCalendar(locale=locale)\n        else:\n            cal = TextCalendar()\n        optdict = dict(w=options.width, l=options.lines)\n        if len(args) != 3:\n            optdict[\"c\"] = options.spacing\n            optdict[\"m\"] = options.months\n        if len(args) == 1:\n            result = cal.formatyear(datetime.date.today().year, **optdict)\n        elif len(args) == 2:\n            result = cal.formatyear(int(args[1]), **optdict)\n        elif len(args) == 3:\n            result = cal.formatmonth(int(args[1]), int(args[2]), **optdict)\n        else:\n            parser.error(\"incorrect number of arguments\")\n            sys.exit(1)\n        if options.encoding:\n            result = result.encode(options.encoding)\n        print result\n\n\nif __name__ == \"__main__\":\n    main(sys.argv)\n", 
    "cmd": "\"\"\"A generic class to build line-oriented command interpreters.\n\nInterpreters constructed with this class obey the following conventions:\n\n1. End of file on input is processed as the command 'EOF'.\n2. A command is parsed out of each line by collecting the prefix composed\n   of characters in the identchars member.\n3. A command `foo' is dispatched to a method 'do_foo()'; the do_ method\n   is passed a single argument consisting of the remainder of the line.\n4. Typing an empty line repeats the last command.  (Actually, it calls the\n   method `emptyline', which may be overridden in a subclass.)\n5. There is a predefined `help' method.  Given an argument `topic', it\n   calls the command `help_topic'.  With no arguments, it lists all topics\n   with defined help_ functions, broken into up to three topics; documented\n   commands, miscellaneous help topics, and undocumented commands.\n6. The command '?' is a synonym for `help'.  The command '!' is a synonym\n   for `shell', if a do_shell method exists.\n7. If completion is enabled, completing commands will be done automatically,\n   and completing of commands args is done by calling complete_foo() with\n   arguments text, line, begidx, endidx.  text is string we are matching\n   against, all returned matches must begin with it.  line is the current\n   input line (lstripped), begidx and endidx are the beginning and end\n   indexes of the text being matched, which could be used to provide\n   different completion depending upon which position the argument is in.\n\nThe `default' method may be overridden to intercept commands for which there\nis no do_ method.\n\nThe `completedefault' method may be overridden to intercept completions for\ncommands that have no complete_ method.\n\nThe data member `self.ruler' sets the character used to draw separator lines\nin the help messages.  If empty, no ruler line is drawn.  It defaults to \"=\".\n\nIf the value of `self.intro' is nonempty when the cmdloop method is called,\nit is printed out on interpreter startup.  This value may be overridden\nvia an optional argument to the cmdloop() method.\n\nThe data members `self.doc_header', `self.misc_header', and\n`self.undoc_header' set the headers used for the help function's\nlistings of documented functions, miscellaneous topics, and undocumented\nfunctions respectively.\n\nThese interpreters use raw_input; thus, if the readline module is loaded,\nthey automatically support Emacs-like command history and editing features.\n\"\"\"\n\nimport string\n\n__all__ = [\"Cmd\"]\n\nPROMPT = '(Cmd) '\nIDENTCHARS = string.ascii_letters + string.digits + '_'\n\nclass Cmd:\n    \"\"\"A simple framework for writing line-oriented command interpreters.\n\n    These are often useful for test harnesses, administrative tools, and\n    prototypes that will later be wrapped in a more sophisticated interface.\n\n    A Cmd instance or subclass instance is a line-oriented interpreter\n    framework.  There is no good reason to instantiate Cmd itself; rather,\n    it's useful as a superclass of an interpreter class you define yourself\n    in order to inherit Cmd's methods and encapsulate action methods.\n\n    \"\"\"\n    prompt = PROMPT\n    identchars = IDENTCHARS\n    ruler = '='\n    lastcmd = ''\n    intro = None\n    doc_leader = \"\"\n    doc_header = \"Documented commands (type help <topic>):\"\n    misc_header = \"Miscellaneous help topics:\"\n    undoc_header = \"Undocumented commands:\"\n    nohelp = \"*** No help on %s\"\n    use_rawinput = 1\n\n    def __init__(self, completekey='tab', stdin=None, stdout=None):\n        \"\"\"Instantiate a line-oriented interpreter framework.\n\n        The optional argument 'completekey' is the readline name of a\n        completion key; it defaults to the Tab key. If completekey is\n        not None and the readline module is available, command completion\n        is done automatically. The optional arguments stdin and stdout\n        specify alternate input and output file objects; if not specified,\n        sys.stdin and sys.stdout are used.\n\n        \"\"\"\n        import sys\n        if stdin is not None:\n            self.stdin = stdin\n        else:\n            self.stdin = sys.stdin\n        if stdout is not None:\n            self.stdout = stdout\n        else:\n            self.stdout = sys.stdout\n        self.cmdqueue = []\n        self.completekey = completekey\n\n    def cmdloop(self, intro=None):\n        \"\"\"Repeatedly issue a prompt, accept input, parse an initial prefix\n        off the received input, and dispatch to action methods, passing them\n        the remainder of the line as argument.\n\n        \"\"\"\n\n        self.preloop()\n        if self.use_rawinput and self.completekey:\n            try:\n                import readline\n                self.old_completer = readline.get_completer()\n                readline.set_completer(self.complete)\n                readline.parse_and_bind(self.completekey+\": complete\")\n            except ImportError:\n                pass\n        try:\n            if intro is not None:\n                self.intro = intro\n            if self.intro:\n                self.stdout.write(str(self.intro)+\"\\n\")\n            stop = None\n            while not stop:\n                if self.cmdqueue:\n                    line = self.cmdqueue.pop(0)\n                else:\n                    if self.use_rawinput:\n                        try:\n                            line = raw_input(self.prompt)\n                        except EOFError:\n                            line = 'EOF'\n                    else:\n                        self.stdout.write(self.prompt)\n                        self.stdout.flush()\n                        line = self.stdin.readline()\n                        if not len(line):\n                            line = 'EOF'\n                        else:\n                            line = line.rstrip('\\r\\n')\n                line = self.precmd(line)\n                stop = self.onecmd(line)\n                stop = self.postcmd(stop, line)\n            self.postloop()\n        finally:\n            if self.use_rawinput and self.completekey:\n                try:\n                    import readline\n                    readline.set_completer(self.old_completer)\n                except ImportError:\n                    pass\n\n\n    def precmd(self, line):\n        \"\"\"Hook method executed just before the command line is\n        interpreted, but after the input prompt is generated and issued.\n\n        \"\"\"\n        return line\n\n    def postcmd(self, stop, line):\n        \"\"\"Hook method executed just after a command dispatch is finished.\"\"\"\n        return stop\n\n    def preloop(self):\n        \"\"\"Hook method executed once when the cmdloop() method is called.\"\"\"\n        pass\n\n    def postloop(self):\n        \"\"\"Hook method executed once when the cmdloop() method is about to\n        return.\n\n        \"\"\"\n        pass\n\n    def parseline(self, line):\n        \"\"\"Parse the line into a command name and a string containing\n        the arguments.  Returns a tuple containing (command, args, line).\n        'command' and 'args' may be None if the line couldn't be parsed.\n        \"\"\"\n        line = line.strip()\n        if not line:\n            return None, None, line\n        elif line[0] == '?':\n            line = 'help ' + line[1:]\n        elif line[0] == '!':\n            if hasattr(self, 'do_shell'):\n                line = 'shell ' + line[1:]\n            else:\n                return None, None, line\n        i, n = 0, len(line)\n        while i < n and line[i] in self.identchars: i = i+1\n        cmd, arg = line[:i], line[i:].strip()\n        return cmd, arg, line\n\n    def onecmd(self, line):\n        \"\"\"Interpret the argument as though it had been typed in response\n        to the prompt.\n\n        This may be overridden, but should not normally need to be;\n        see the precmd() and postcmd() methods for useful execution hooks.\n        The return value is a flag indicating whether interpretation of\n        commands by the interpreter should stop.\n\n        \"\"\"\n        cmd, arg, line = self.parseline(line)\n        if not line:\n            return self.emptyline()\n        if cmd is None:\n            return self.default(line)\n        self.lastcmd = line\n        if line == 'EOF' :\n            self.lastcmd = ''\n        if cmd == '':\n            return self.default(line)\n        else:\n            try:\n                func = getattr(self, 'do_' + cmd)\n            except AttributeError:\n                return self.default(line)\n            return func(arg)\n\n    def emptyline(self):\n        \"\"\"Called when an empty line is entered in response to the prompt.\n\n        If this method is not overridden, it repeats the last nonempty\n        command entered.\n\n        \"\"\"\n        if self.lastcmd:\n            return self.onecmd(self.lastcmd)\n\n    def default(self, line):\n        \"\"\"Called on an input line when the command prefix is not recognized.\n\n        If this method is not overridden, it prints an error message and\n        returns.\n\n        \"\"\"\n        self.stdout.write('*** Unknown syntax: %s\\n'%line)\n\n    def completedefault(self, *ignored):\n        \"\"\"Method called to complete an input line when no command-specific\n        complete_*() method is available.\n\n        By default, it returns an empty list.\n\n        \"\"\"\n        return []\n\n    def completenames(self, text, *ignored):\n        dotext = 'do_'+text\n        return [a[3:] for a in self.get_names() if a.startswith(dotext)]\n\n    def complete(self, text, state):\n        \"\"\"Return the next possible completion for 'text'.\n\n        If a command has not been entered, then complete against command list.\n        Otherwise try to call complete_<command> to get list of completions.\n        \"\"\"\n        if state == 0:\n            import readline\n            origline = readline.get_line_buffer()\n            line = origline.lstrip()\n            stripped = len(origline) - len(line)\n            begidx = readline.get_begidx() - stripped\n            endidx = readline.get_endidx() - stripped\n            if begidx>0:\n                cmd, args, foo = self.parseline(line)\n                if cmd == '':\n                    compfunc = self.completedefault\n                else:\n                    try:\n                        compfunc = getattr(self, 'complete_' + cmd)\n                    except AttributeError:\n                        compfunc = self.completedefault\n            else:\n                compfunc = self.completenames\n            self.completion_matches = compfunc(text, line, begidx, endidx)\n        try:\n            return self.completion_matches[state]\n        except IndexError:\n            return None\n\n    def get_names(self):\n        # This method used to pull in base class attributes\n        # at a time dir() didn't do it yet.\n        return dir(self.__class__)\n\n    def complete_help(self, *args):\n        commands = set(self.completenames(*args))\n        topics = set(a[5:] for a in self.get_names()\n                     if a.startswith('help_' + args[0]))\n        return list(commands | topics)\n\n    def do_help(self, arg):\n        'List available commands with \"help\" or detailed help with \"help cmd\".'\n        if arg:\n            # XXX check arg syntax\n            try:\n                func = getattr(self, 'help_' + arg)\n            except AttributeError:\n                try:\n                    doc=getattr(self, 'do_' + arg).__doc__\n                    if doc:\n                        self.stdout.write(\"%s\\n\"%str(doc))\n                        return\n                except AttributeError:\n                    pass\n                self.stdout.write(\"%s\\n\"%str(self.nohelp % (arg,)))\n                return\n            func()\n        else:\n            names = self.get_names()\n            cmds_doc = []\n            cmds_undoc = []\n            help = {}\n            for name in names:\n                if name[:5] == 'help_':\n                    help[name[5:]]=1\n            names.sort()\n            # There can be duplicates if routines overridden\n            prevname = ''\n            for name in names:\n                if name[:3] == 'do_':\n                    if name == prevname:\n                        continue\n                    prevname = name\n                    cmd=name[3:]\n                    if cmd in help:\n                        cmds_doc.append(cmd)\n                        del help[cmd]\n                    elif getattr(self, name).__doc__:\n                        cmds_doc.append(cmd)\n                    else:\n                        cmds_undoc.append(cmd)\n            self.stdout.write(\"%s\\n\"%str(self.doc_leader))\n            self.print_topics(self.doc_header,   cmds_doc,   15,80)\n            self.print_topics(self.misc_header,  help.keys(),15,80)\n            self.print_topics(self.undoc_header, cmds_undoc, 15,80)\n\n    def print_topics(self, header, cmds, cmdlen, maxcol):\n        if cmds:\n            self.stdout.write(\"%s\\n\"%str(header))\n            if self.ruler:\n                self.stdout.write(\"%s\\n\"%str(self.ruler * len(header)))\n            self.columnize(cmds, maxcol-1)\n            self.stdout.write(\"\\n\")\n\n    def columnize(self, list, displaywidth=80):\n        \"\"\"Display a list of strings as a compact set of columns.\n\n        Each column is only as wide as necessary.\n        Columns are separated by two spaces (one was not legible enough).\n        \"\"\"\n        if not list:\n            self.stdout.write(\"<empty>\\n\")\n            return\n        nonstrings = [i for i in range(len(list))\n                        if not isinstance(list[i], str)]\n        if nonstrings:\n            raise TypeError, (\"list[i] not a string for i in %s\" %\n                              \", \".join(map(str, nonstrings)))\n        size = len(list)\n        if size == 1:\n            self.stdout.write('%s\\n'%str(list[0]))\n            return\n        # Try every row count from 1 upwards\n        for nrows in range(1, len(list)):\n            ncols = (size+nrows-1) // nrows\n            colwidths = []\n            totwidth = -2\n            for col in range(ncols):\n                colwidth = 0\n                for row in range(nrows):\n                    i = row + nrows*col\n                    if i >= size:\n                        break\n                    x = list[i]\n                    colwidth = max(colwidth, len(x))\n                colwidths.append(colwidth)\n                totwidth += colwidth + 2\n                if totwidth > displaywidth:\n                    break\n            if totwidth <= displaywidth:\n                break\n        else:\n            nrows = len(list)\n            ncols = 1\n            colwidths = [0]\n        for row in range(nrows):\n            texts = []\n            for col in range(ncols):\n                i = row + nrows*col\n                if i >= size:\n                    x = \"\"\n                else:\n                    x = list[i]\n                texts.append(x)\n            while texts and not texts[-1]:\n                del texts[-1]\n            for col in range(len(texts)):\n                texts[col] = texts[col].ljust(colwidths[col])\n            self.stdout.write(\"%s\\n\"%str(\"  \".join(texts)))\n", 
    "code": "\"\"\"Utilities needed to emulate Python's interactive interpreter.\n\n\"\"\"\n\n# Inspired by similar code by Jeff Epler and Fredrik Lundh.\n\n\nimport sys\nimport traceback\nfrom codeop import CommandCompiler, compile_command\n\n__all__ = [\"InteractiveInterpreter\", \"InteractiveConsole\", \"interact\",\n           \"compile_command\"]\n\ndef softspace(file, newvalue):\n    oldvalue = 0\n    try:\n        oldvalue = file.softspace\n    except AttributeError:\n        pass\n    try:\n        file.softspace = newvalue\n    except (AttributeError, TypeError):\n        # \"attribute-less object\" or \"read-only attributes\"\n        pass\n    return oldvalue\n\nclass InteractiveInterpreter:\n    \"\"\"Base class for InteractiveConsole.\n\n    This class deals with parsing and interpreter state (the user's\n    namespace); it doesn't deal with input buffering or prompting or\n    input file naming (the filename is always passed in explicitly).\n\n    \"\"\"\n\n    def __init__(self, locals=None):\n        \"\"\"Constructor.\n\n        The optional 'locals' argument specifies the dictionary in\n        which code will be executed; it defaults to a newly created\n        dictionary with key \"__name__\" set to \"__console__\" and key\n        \"__doc__\" set to None.\n\n        \"\"\"\n        if locals is None:\n            locals = {\"__name__\": \"__console__\", \"__doc__\": None}\n        self.locals = locals\n        self.compile = CommandCompiler()\n\n    def runsource(self, source, filename=\"<input>\", symbol=\"single\"):\n        \"\"\"Compile and run some source in the interpreter.\n\n        Arguments are as for compile_command().\n\n        One several things can happen:\n\n        1) The input is incorrect; compile_command() raised an\n        exception (SyntaxError or OverflowError).  A syntax traceback\n        will be printed by calling the showsyntaxerror() method.\n\n        2) The input is incomplete, and more input is required;\n        compile_command() returned None.  Nothing happens.\n\n        3) The input is complete; compile_command() returned a code\n        object.  The code is executed by calling self.runcode() (which\n        also handles run-time exceptions, except for SystemExit).\n\n        The return value is True in case 2, False in the other cases (unless\n        an exception is raised).  The return value can be used to\n        decide whether to use sys.ps1 or sys.ps2 to prompt the next\n        line.\n\n        \"\"\"\n        try:\n            code = self.compile(source, filename, symbol)\n        except (OverflowError, SyntaxError, ValueError):\n            # Case 1\n            self.showsyntaxerror(filename)\n            return False\n\n        if code is None:\n            # Case 2\n            return True\n\n        # Case 3\n        self.runcode(code)\n        return False\n\n    def runcode(self, code):\n        \"\"\"Execute a code object.\n\n        When an exception occurs, self.showtraceback() is called to\n        display a traceback.  All exceptions are caught except\n        SystemExit, which is reraised.\n\n        A note about KeyboardInterrupt: this exception may occur\n        elsewhere in this code, and may not always be caught.  The\n        caller should be prepared to deal with it.\n\n        \"\"\"\n        try:\n            exec code in self.locals\n        except SystemExit:\n            raise\n        except:\n            self.showtraceback()\n        else:\n            if softspace(sys.stdout, 0):\n                print\n\n    def showsyntaxerror(self, filename=None):\n        \"\"\"Display the syntax error that just occurred.\n\n        This doesn't display a stack trace because there isn't one.\n\n        If a filename is given, it is stuffed in the exception instead\n        of what was there before (because Python's parser always uses\n        \"<string>\" when reading from a string).\n\n        The output is written by self.write(), below.\n\n        \"\"\"\n        type, value, sys.last_traceback = sys.exc_info()\n        sys.last_type = type\n        sys.last_value = value\n        if filename and type is SyntaxError:\n            # Work hard to stuff the correct filename in the exception\n            try:\n                msg, (dummy_filename, lineno, offset, line) = value\n            except:\n                # Not the format we expect; leave it alone\n                pass\n            else:\n                # Stuff in the right filename\n                value = SyntaxError(msg, (filename, lineno, offset, line))\n                sys.last_value = value\n        list = traceback.format_exception_only(type, value)\n        map(self.write, list)\n\n    def showtraceback(self):\n        \"\"\"Display the exception that just occurred.\n\n        We remove the first stack item because it is our own code.\n\n        The output is written by self.write(), below.\n\n        \"\"\"\n        try:\n            type, value, tb = sys.exc_info()\n            sys.last_type = type\n            sys.last_value = value\n            sys.last_traceback = tb\n            tblist = traceback.extract_tb(tb)\n            del tblist[:1]\n            list = traceback.format_list(tblist)\n            if list:\n                list.insert(0, \"Traceback (most recent call last):\\n\")\n            list[len(list):] = traceback.format_exception_only(type, value)\n        finally:\n            tblist = tb = None\n        map(self.write, list)\n\n    def write(self, data):\n        \"\"\"Write a string.\n\n        The base implementation writes to sys.stderr; a subclass may\n        replace this with a different implementation.\n\n        \"\"\"\n        sys.stderr.write(data)\n\n\nclass InteractiveConsole(InteractiveInterpreter):\n    \"\"\"Closely emulate the behavior of the interactive Python interpreter.\n\n    This class builds on InteractiveInterpreter and adds prompting\n    using the familiar sys.ps1 and sys.ps2, and input buffering.\n\n    \"\"\"\n\n    def __init__(self, locals=None, filename=\"<console>\"):\n        \"\"\"Constructor.\n\n        The optional locals argument will be passed to the\n        InteractiveInterpreter base class.\n\n        The optional filename argument should specify the (file)name\n        of the input stream; it will show up in tracebacks.\n\n        \"\"\"\n        InteractiveInterpreter.__init__(self, locals)\n        self.filename = filename\n        self.resetbuffer()\n\n    def resetbuffer(self):\n        \"\"\"Reset the input buffer.\"\"\"\n        self.buffer = []\n\n    def interact(self, banner=None):\n        \"\"\"Closely emulate the interactive Python console.\n\n        The optional banner argument specify the banner to print\n        before the first interaction; by default it prints a banner\n        similar to the one printed by the real Python interpreter,\n        followed by the current class name in parentheses (so as not\n        to confuse this with the real interpreter -- since it's so\n        close!).\n\n        \"\"\"\n        try:\n            sys.ps1\n        except AttributeError:\n            sys.ps1 = \">>> \"\n        try:\n            sys.ps2\n        except AttributeError:\n            sys.ps2 = \"... \"\n        cprt = 'Type \"help\", \"copyright\", \"credits\" or \"license\" for more information.'\n        if banner is None:\n            self.write(\"Python %s on %s\\n%s\\n(%s)\\n\" %\n                       (sys.version, sys.platform, cprt,\n                        self.__class__.__name__))\n        else:\n            self.write(\"%s\\n\" % str(banner))\n        more = 0\n        while 1:\n            try:\n                if more:\n                    prompt = sys.ps2\n                else:\n                    prompt = sys.ps1\n                try:\n                    line = self.raw_input(prompt)\n                    # Can be None if sys.stdin was redefined\n                    encoding = getattr(sys.stdin, \"encoding\", None)\n                    if encoding and not isinstance(line, unicode):\n                        line = line.decode(encoding)\n                except EOFError:\n                    self.write(\"\\n\")\n                    break\n                else:\n                    more = self.push(line)\n            except KeyboardInterrupt:\n                self.write(\"\\nKeyboardInterrupt\\n\")\n                self.resetbuffer()\n                more = 0\n\n    def push(self, line):\n        \"\"\"Push a line to the interpreter.\n\n        The line should not have a trailing newline; it may have\n        internal newlines.  The line is appended to a buffer and the\n        interpreter's runsource() method is called with the\n        concatenated contents of the buffer as source.  If this\n        indicates that the command was executed or invalid, the buffer\n        is reset; otherwise, the command is incomplete, and the buffer\n        is left as it was after the line was appended.  The return\n        value is 1 if more input is required, 0 if the line was dealt\n        with in some way (this is the same as runsource()).\n\n        \"\"\"\n        self.buffer.append(line)\n        source = \"\\n\".join(self.buffer)\n        more = self.runsource(source, self.filename)\n        if not more:\n            self.resetbuffer()\n        return more\n\n    def raw_input(self, prompt=\"\"):\n        \"\"\"Write a prompt and read a line.\n\n        The returned line does not include the trailing newline.\n        When the user enters the EOF key sequence, EOFError is raised.\n\n        The base implementation uses the built-in function\n        raw_input(); a subclass may replace this with a different\n        implementation.\n\n        \"\"\"\n        return raw_input(prompt)\n\n\ndef interact(banner=None, readfunc=None, local=None):\n    \"\"\"Closely emulate the interactive Python interpreter.\n\n    This is a backwards compatible interface to the InteractiveConsole\n    class.  When readfunc is not specified, it attempts to import the\n    readline module to enable GNU readline if it is available.\n\n    Arguments (all optional, all default to None):\n\n    banner -- passed to InteractiveConsole.interact()\n    readfunc -- if not None, replaces InteractiveConsole.raw_input()\n    local -- passed to InteractiveInterpreter.__init__()\n\n    \"\"\"\n    console = InteractiveConsole(local)\n    if readfunc is not None:\n        console.raw_input = readfunc\n    else:\n        try:\n            import readline\n        except ImportError:\n            pass\n    console.interact(banner)\n\n\nif __name__ == \"__main__\":\n    interact()\n", 
    "codecs": "\"\"\" codecs -- Python Codec Registry, API and helpers.\n\n\nWritten by Marc-Andre Lemburg (mal@lemburg.com).\n\n(c) Copyright CNRI, All Rights Reserved. NO WARRANTY.\n\n\"\"\"#\"\n\nimport __builtin__, sys\n\n### Registry and builtin stateless codec functions\n\ntry:\n    from _codecs import *\nexcept ImportError, why:\n    raise SystemError('Failed to load the builtin codecs: %s' % why)\n\n__all__ = [\"register\", \"lookup\", \"open\", \"EncodedFile\", \"BOM\", \"BOM_BE\",\n           \"BOM_LE\", \"BOM32_BE\", \"BOM32_LE\", \"BOM64_BE\", \"BOM64_LE\",\n           \"BOM_UTF8\", \"BOM_UTF16\", \"BOM_UTF16_LE\", \"BOM_UTF16_BE\",\n           \"BOM_UTF32\", \"BOM_UTF32_LE\", \"BOM_UTF32_BE\",\n           \"strict_errors\", \"ignore_errors\", \"replace_errors\",\n           \"xmlcharrefreplace_errors\",\n           \"register_error\", \"lookup_error\"]\n\n### Constants\n\n#\n# Byte Order Mark (BOM = ZERO WIDTH NO-BREAK SPACE = U+FEFF)\n# and its possible byte string values\n# for UTF8/UTF16/UTF32 output and little/big endian machines\n#\n\n# UTF-8\nBOM_UTF8 = '\\xef\\xbb\\xbf'\n\n# UTF-16, little endian\nBOM_LE = BOM_UTF16_LE = '\\xff\\xfe'\n\n# UTF-16, big endian\nBOM_BE = BOM_UTF16_BE = '\\xfe\\xff'\n\n# UTF-32, little endian\nBOM_UTF32_LE = '\\xff\\xfe\\x00\\x00'\n\n# UTF-32, big endian\nBOM_UTF32_BE = '\\x00\\x00\\xfe\\xff'\n\nif sys.byteorder == 'little':\n\n    # UTF-16, native endianness\n    BOM = BOM_UTF16 = BOM_UTF16_LE\n\n    # UTF-32, native endianness\n    BOM_UTF32 = BOM_UTF32_LE\n\nelse:\n\n    # UTF-16, native endianness\n    BOM = BOM_UTF16 = BOM_UTF16_BE\n\n    # UTF-32, native endianness\n    BOM_UTF32 = BOM_UTF32_BE\n\n# Old broken names (don't use in new code)\nBOM32_LE = BOM_UTF16_LE\nBOM32_BE = BOM_UTF16_BE\nBOM64_LE = BOM_UTF32_LE\nBOM64_BE = BOM_UTF32_BE\n\n\n### Codec base classes (defining the API)\n\nclass CodecInfo(tuple):\n\n    def __new__(cls, encode, decode, streamreader=None, streamwriter=None,\n        incrementalencoder=None, incrementaldecoder=None, name=None):\n        self = tuple.__new__(cls, (encode, decode, streamreader, streamwriter))\n        self.name = name\n        self.encode = encode\n        self.decode = decode\n        self.incrementalencoder = incrementalencoder\n        self.incrementaldecoder = incrementaldecoder\n        self.streamwriter = streamwriter\n        self.streamreader = streamreader\n        return self\n\n    def __repr__(self):\n        return \"<%s.%s object for encoding %s at 0x%x>\" % (self.__class__.__module__, self.__class__.__name__, self.name, id(self))\n\nclass Codec:\n\n    \"\"\" Defines the interface for stateless encoders/decoders.\n\n        The .encode()/.decode() methods may use different error\n        handling schemes by providing the errors argument. These\n        string values are predefined:\n\n         'strict' - raise a ValueError error (or a subclass)\n         'ignore' - ignore the character and continue with the next\n         'replace' - replace with a suitable replacement character;\n                    Python will use the official U+FFFD REPLACEMENT\n                    CHARACTER for the builtin Unicode codecs on\n                    decoding and '?' on encoding.\n         'xmlcharrefreplace' - Replace with the appropriate XML\n                               character reference (only for encoding).\n         'backslashreplace'  - Replace with backslashed escape sequences\n                               (only for encoding).\n\n        The set of allowed values can be extended via register_error.\n\n    \"\"\"\n    def encode(self, input, errors='strict'):\n\n        \"\"\" Encodes the object input and returns a tuple (output\n            object, length consumed).\n\n            errors defines the error handling to apply. It defaults to\n            'strict' handling.\n\n            The method may not store state in the Codec instance. Use\n            StreamCodec for codecs which have to keep state in order to\n            make encoding/decoding efficient.\n\n            The encoder must be able to handle zero length input and\n            return an empty object of the output object type in this\n            situation.\n\n        \"\"\"\n        raise NotImplementedError\n\n    def decode(self, input, errors='strict'):\n\n        \"\"\" Decodes the object input and returns a tuple (output\n            object, length consumed).\n\n            input must be an object which provides the bf_getreadbuf\n            buffer slot. Python strings, buffer objects and memory\n            mapped files are examples of objects providing this slot.\n\n            errors defines the error handling to apply. It defaults to\n            'strict' handling.\n\n            The method may not store state in the Codec instance. Use\n            StreamCodec for codecs which have to keep state in order to\n            make encoding/decoding efficient.\n\n            The decoder must be able to handle zero length input and\n            return an empty object of the output object type in this\n            situation.\n\n        \"\"\"\n        raise NotImplementedError\n\nclass IncrementalEncoder(object):\n    \"\"\"\n    An IncrementalEncoder encodes an input in multiple steps. The input can be\n    passed piece by piece to the encode() method. The IncrementalEncoder remembers\n    the state of the Encoding process between calls to encode().\n    \"\"\"\n    def __init__(self, errors='strict'):\n        \"\"\"\n        Creates an IncrementalEncoder instance.\n\n        The IncrementalEncoder may use different error handling schemes by\n        providing the errors keyword argument. See the module docstring\n        for a list of possible values.\n        \"\"\"\n        self.errors = errors\n        self.buffer = \"\"\n\n    def encode(self, input, final=False):\n        \"\"\"\n        Encodes input and returns the resulting object.\n        \"\"\"\n        raise NotImplementedError\n\n    def reset(self):\n        \"\"\"\n        Resets the encoder to the initial state.\n        \"\"\"\n\n    def getstate(self):\n        \"\"\"\n        Return the current state of the encoder.\n        \"\"\"\n        return 0\n\n    def setstate(self, state):\n        \"\"\"\n        Set the current state of the encoder. state must have been\n        returned by getstate().\n        \"\"\"\n\nclass BufferedIncrementalEncoder(IncrementalEncoder):\n    \"\"\"\n    This subclass of IncrementalEncoder can be used as the baseclass for an\n    incremental encoder if the encoder must keep some of the output in a\n    buffer between calls to encode().\n    \"\"\"\n    def __init__(self, errors='strict'):\n        IncrementalEncoder.__init__(self, errors)\n        self.buffer = \"\" # unencoded input that is kept between calls to encode()\n\n    def _buffer_encode(self, input, errors, final):\n        # Overwrite this method in subclasses: It must encode input\n        # and return an (output, length consumed) tuple\n        raise NotImplementedError\n\n    def encode(self, input, final=False):\n        # encode input (taking the buffer into account)\n        data = self.buffer + input\n        (result, consumed) = self._buffer_encode(data, self.errors, final)\n        # keep unencoded input until the next call\n        self.buffer = data[consumed:]\n        return result\n\n    def reset(self):\n        IncrementalEncoder.reset(self)\n        self.buffer = \"\"\n\n    def getstate(self):\n        return self.buffer or 0\n\n    def setstate(self, state):\n        self.buffer = state or \"\"\n\nclass IncrementalDecoder(object):\n    \"\"\"\n    An IncrementalDecoder decodes an input in multiple steps. The input can be\n    passed piece by piece to the decode() method. The IncrementalDecoder\n    remembers the state of the decoding process between calls to decode().\n    \"\"\"\n    def __init__(self, errors='strict'):\n        \"\"\"\n        Creates a IncrementalDecoder instance.\n\n        The IncrementalDecoder may use different error handling schemes by\n        providing the errors keyword argument. See the module docstring\n        for a list of possible values.\n        \"\"\"\n        self.errors = errors\n\n    def decode(self, input, final=False):\n        \"\"\"\n        Decodes input and returns the resulting object.\n        \"\"\"\n        raise NotImplementedError\n\n    def reset(self):\n        \"\"\"\n        Resets the decoder to the initial state.\n        \"\"\"\n\n    def getstate(self):\n        \"\"\"\n        Return the current state of the decoder.\n\n        This must be a (buffered_input, additional_state_info) tuple.\n        buffered_input must be a bytes object containing bytes that\n        were passed to decode() that have not yet been converted.\n        additional_state_info must be a non-negative integer\n        representing the state of the decoder WITHOUT yet having\n        processed the contents of buffered_input.  In the initial state\n        and after reset(), getstate() must return (b\"\", 0).\n        \"\"\"\n        return (b\"\", 0)\n\n    def setstate(self, state):\n        \"\"\"\n        Set the current state of the decoder.\n\n        state must have been returned by getstate().  The effect of\n        setstate((b\"\", 0)) must be equivalent to reset().\n        \"\"\"\n\nclass BufferedIncrementalDecoder(IncrementalDecoder):\n    \"\"\"\n    This subclass of IncrementalDecoder can be used as the baseclass for an\n    incremental decoder if the decoder must be able to handle incomplete byte\n    sequences.\n    \"\"\"\n    def __init__(self, errors='strict'):\n        IncrementalDecoder.__init__(self, errors)\n        self.buffer = \"\" # undecoded input that is kept between calls to decode()\n\n    def _buffer_decode(self, input, errors, final):\n        # Overwrite this method in subclasses: It must decode input\n        # and return an (output, length consumed) tuple\n        raise NotImplementedError\n\n    def decode(self, input, final=False):\n        # decode input (taking the buffer into account)\n        data = self.buffer + input\n        (result, consumed) = self._buffer_decode(data, self.errors, final)\n        # keep undecoded input until the next call\n        self.buffer = data[consumed:]\n        return result\n\n    def reset(self):\n        IncrementalDecoder.reset(self)\n        self.buffer = \"\"\n\n    def getstate(self):\n        # additional state info is always 0\n        return (self.buffer, 0)\n\n    def setstate(self, state):\n        # ignore additional state info\n        self.buffer = state[0]\n\n#\n# The StreamWriter and StreamReader class provide generic working\n# interfaces which can be used to implement new encoding submodules\n# very easily. See encodings/utf_8.py for an example on how this is\n# done.\n#\n\nclass StreamWriter(Codec):\n\n    def __init__(self, stream, errors='strict'):\n\n        \"\"\" Creates a StreamWriter instance.\n\n            stream must be a file-like object open for writing\n            (binary) data.\n\n            The StreamWriter may use different error handling\n            schemes by providing the errors keyword argument. These\n            parameters are predefined:\n\n             'strict' - raise a ValueError (or a subclass)\n             'ignore' - ignore the character and continue with the next\n             'replace'- replace with a suitable replacement character\n             'xmlcharrefreplace' - Replace with the appropriate XML\n                                   character reference.\n             'backslashreplace'  - Replace with backslashed escape\n                                   sequences (only for encoding).\n\n            The set of allowed parameter values can be extended via\n            register_error.\n        \"\"\"\n        self.stream = stream\n        self.errors = errors\n\n    def write(self, object):\n\n        \"\"\" Writes the object's contents encoded to self.stream.\n        \"\"\"\n        data, consumed = self.encode(object, self.errors)\n        self.stream.write(data)\n\n    def writelines(self, list):\n\n        \"\"\" Writes the concatenated list of strings to the stream\n            using .write().\n        \"\"\"\n        self.write(''.join(list))\n\n    def reset(self):\n\n        \"\"\" Flushes and resets the codec buffers used for keeping state.\n\n            Calling this method should ensure that the data on the\n            output is put into a clean state, that allows appending\n            of new fresh data without having to rescan the whole\n            stream to recover state.\n\n        \"\"\"\n        pass\n\n    def seek(self, offset, whence=0):\n        self.stream.seek(offset, whence)\n        if whence == 0 and offset == 0:\n            self.reset()\n\n    def __getattr__(self, name,\n                    getattr=getattr):\n\n        \"\"\" Inherit all other methods from the underlying stream.\n        \"\"\"\n        return getattr(self.stream, name)\n\n    def __enter__(self):\n        return self\n\n    def __exit__(self, type, value, tb):\n        self.stream.close()\n\n###\n\nclass StreamReader(Codec):\n\n    def __init__(self, stream, errors='strict'):\n\n        \"\"\" Creates a StreamReader instance.\n\n            stream must be a file-like object open for reading\n            (binary) data.\n\n            The StreamReader may use different error handling\n            schemes by providing the errors keyword argument. These\n            parameters are predefined:\n\n             'strict' - raise a ValueError (or a subclass)\n             'ignore' - ignore the character and continue with the next\n             'replace'- replace with a suitable replacement character;\n\n            The set of allowed parameter values can be extended via\n            register_error.\n        \"\"\"\n        self.stream = stream\n        self.errors = errors\n        self.bytebuffer = \"\"\n        # For str->str decoding this will stay a str\n        # For str->unicode decoding the first read will promote it to unicode\n        self.charbuffer = \"\"\n        self.linebuffer = None\n\n    def decode(self, input, errors='strict'):\n        raise NotImplementedError\n\n    def read(self, size=-1, chars=-1, firstline=False):\n\n        \"\"\" Decodes data from the stream self.stream and returns the\n            resulting object.\n\n            chars indicates the number of characters to read from the\n            stream. read() will never return more than chars\n            characters, but it might return less, if there are not enough\n            characters available.\n\n            size indicates the approximate maximum number of bytes to\n            read from the stream for decoding purposes. The decoder\n            can modify this setting as appropriate. The default value\n            -1 indicates to read and decode as much as possible.  size\n            is intended to prevent having to decode huge files in one\n            step.\n\n            If firstline is true, and a UnicodeDecodeError happens\n            after the first line terminator in the input only the first line\n            will be returned, the rest of the input will be kept until the\n            next call to read().\n\n            The method should use a greedy read strategy meaning that\n            it should read as much data as is allowed within the\n            definition of the encoding and the given size, e.g.  if\n            optional encoding endings or state markers are available\n            on the stream, these should be read too.\n        \"\"\"\n        # If we have lines cached, first merge them back into characters\n        if self.linebuffer:\n            self.charbuffer = \"\".join(self.linebuffer)\n            self.linebuffer = None\n\n        # read until we get the required number of characters (if available)\n        while True:\n            # can the request be satisfied from the character buffer?\n            if chars >= 0:\n                if len(self.charbuffer) >= chars:\n                    break\n            elif size >= 0:\n                if len(self.charbuffer) >= size:\n                    break\n            # we need more data\n            if size < 0:\n                newdata = self.stream.read()\n            else:\n                newdata = self.stream.read(size)\n            # decode bytes (those remaining from the last call included)\n            data = self.bytebuffer + newdata\n            try:\n                newchars, decodedbytes = self.decode(data, self.errors)\n            except UnicodeDecodeError, exc:\n                if firstline:\n                    newchars, decodedbytes = self.decode(data[:exc.start], self.errors)\n                    lines = newchars.splitlines(True)\n                    if len(lines)<=1:\n                        raise\n                else:\n                    raise\n            # keep undecoded bytes until the next call\n            self.bytebuffer = data[decodedbytes:]\n            # put new characters in the character buffer\n            self.charbuffer += newchars\n            # there was no data available\n            if not newdata:\n                break\n        if chars < 0:\n            # Return everything we've got\n            result = self.charbuffer\n            self.charbuffer = \"\"\n        else:\n            # Return the first chars characters\n            result = self.charbuffer[:chars]\n            self.charbuffer = self.charbuffer[chars:]\n        return result\n\n    def readline(self, size=None, keepends=True):\n\n        \"\"\" Read one line from the input stream and return the\n            decoded data.\n\n            size, if given, is passed as size argument to the\n            read() method.\n\n        \"\"\"\n        # If we have lines cached from an earlier read, return\n        # them unconditionally\n        if self.linebuffer:\n            line = self.linebuffer[0]\n            del self.linebuffer[0]\n            if len(self.linebuffer) == 1:\n                # revert to charbuffer mode; we might need more data\n                # next time\n                self.charbuffer = self.linebuffer[0]\n                self.linebuffer = None\n            if not keepends:\n                line = line.splitlines(False)[0]\n            return line\n\n        readsize = size or 72\n        line = \"\"\n        # If size is given, we call read() only once\n        while True:\n            data = self.read(readsize, firstline=True)\n            if data:\n                # If we're at a \"\\r\" read one extra character (which might\n                # be a \"\\n\") to get a proper line ending. If the stream is\n                # temporarily exhausted we return the wrong line ending.\n                if data.endswith(\"\\r\"):\n                    data += self.read(size=1, chars=1)\n\n            line += data\n            lines = line.splitlines(True)\n            if lines:\n                if len(lines) > 1:\n                    # More than one line result; the first line is a full line\n                    # to return\n                    line = lines[0]\n                    del lines[0]\n                    if len(lines) > 1:\n                        # cache the remaining lines\n                        lines[-1] += self.charbuffer\n                        self.linebuffer = lines\n                        self.charbuffer = None\n                    else:\n                        # only one remaining line, put it back into charbuffer\n                        self.charbuffer = lines[0] + self.charbuffer\n                    if not keepends:\n                        line = line.splitlines(False)[0]\n                    break\n                line0withend = lines[0]\n                line0withoutend = lines[0].splitlines(False)[0]\n                if line0withend != line0withoutend: # We really have a line end\n                    # Put the rest back together and keep it until the next call\n                    self.charbuffer = \"\".join(lines[1:]) + self.charbuffer\n                    if keepends:\n                        line = line0withend\n                    else:\n                        line = line0withoutend\n                    break\n            # we didn't get anything or this was our only try\n            if not data or size is not None:\n                if line and not keepends:\n                    line = line.splitlines(False)[0]\n                break\n            if readsize<8000:\n                readsize *= 2\n        return line\n\n    def readlines(self, sizehint=None, keepends=True):\n\n        \"\"\" Read all lines available on the input stream\n            and return them as list of lines.\n\n            Line breaks are implemented using the codec's decoder\n            method and are included in the list entries.\n\n            sizehint, if given, is ignored since there is no efficient\n            way to finding the true end-of-line.\n\n        \"\"\"\n        data = self.read()\n        return data.splitlines(keepends)\n\n    def reset(self):\n\n        \"\"\" Resets the codec buffers used for keeping state.\n\n            Note that no stream repositioning should take place.\n            This method is primarily intended to be able to recover\n            from decoding errors.\n\n        \"\"\"\n        self.bytebuffer = \"\"\n        self.charbuffer = u\"\"\n        self.linebuffer = None\n\n    def seek(self, offset, whence=0):\n        \"\"\" Set the input stream's current position.\n\n            Resets the codec buffers used for keeping state.\n        \"\"\"\n        self.stream.seek(offset, whence)\n        self.reset()\n\n    def next(self):\n\n        \"\"\" Return the next decoded line from the input stream.\"\"\"\n        line = self.readline()\n        if line:\n            return line\n        raise StopIteration\n\n    def __iter__(self):\n        return self\n\n    def __getattr__(self, name,\n                    getattr=getattr):\n\n        \"\"\" Inherit all other methods from the underlying stream.\n        \"\"\"\n        return getattr(self.stream, name)\n\n    def __enter__(self):\n        return self\n\n    def __exit__(self, type, value, tb):\n        self.stream.close()\n\n###\n\nclass StreamReaderWriter:\n\n    \"\"\" StreamReaderWriter instances allow wrapping streams which\n        work in both read and write modes.\n\n        The design is such that one can use the factory functions\n        returned by the codec.lookup() function to construct the\n        instance.\n\n    \"\"\"\n    # Optional attributes set by the file wrappers below\n    encoding = 'unknown'\n\n    def __init__(self, stream, Reader, Writer, errors='strict'):\n\n        \"\"\" Creates a StreamReaderWriter instance.\n\n            stream must be a Stream-like object.\n\n            Reader, Writer must be factory functions or classes\n            providing the StreamReader, StreamWriter interface resp.\n\n            Error handling is done in the same way as defined for the\n            StreamWriter/Readers.\n\n        \"\"\"\n        self.stream = stream\n        self.reader = Reader(stream, errors)\n        self.writer = Writer(stream, errors)\n        self.errors = errors\n\n    def read(self, size=-1):\n\n        return self.reader.read(size)\n\n    def readline(self, size=None):\n\n        return self.reader.readline(size)\n\n    def readlines(self, sizehint=None):\n\n        return self.reader.readlines(sizehint)\n\n    def next(self):\n\n        \"\"\" Return the next decoded line from the input stream.\"\"\"\n        return self.reader.next()\n\n    def __iter__(self):\n        return self\n\n    def write(self, data):\n\n        return self.writer.write(data)\n\n    def writelines(self, list):\n\n        return self.writer.writelines(list)\n\n    def reset(self):\n\n        self.reader.reset()\n        self.writer.reset()\n\n    def seek(self, offset, whence=0):\n        self.stream.seek(offset, whence)\n        self.reader.reset()\n        if whence == 0 and offset == 0:\n            self.writer.reset()\n\n    def __getattr__(self, name,\n                    getattr=getattr):\n\n        \"\"\" Inherit all other methods from the underlying stream.\n        \"\"\"\n        return getattr(self.stream, name)\n\n    # these are needed to make \"with codecs.open(...)\" work properly\n\n    def __enter__(self):\n        return self\n\n    def __exit__(self, type, value, tb):\n        self.stream.close()\n\n###\n\nclass StreamRecoder:\n\n    \"\"\" StreamRecoder instances provide a frontend - backend\n        view of encoding data.\n\n        They use the complete set of APIs returned by the\n        codecs.lookup() function to implement their task.\n\n        Data written to the stream is first decoded into an\n        intermediate format (which is dependent on the given codec\n        combination) and then written to the stream using an instance\n        of the provided Writer class.\n\n        In the other direction, data is read from the stream using a\n        Reader instance and then return encoded data to the caller.\n\n    \"\"\"\n    # Optional attributes set by the file wrappers below\n    data_encoding = 'unknown'\n    file_encoding = 'unknown'\n\n    def __init__(self, stream, encode, decode, Reader, Writer,\n                 errors='strict'):\n\n        \"\"\" Creates a StreamRecoder instance which implements a two-way\n            conversion: encode and decode work on the frontend (the\n            input to .read() and output of .write()) while\n            Reader and Writer work on the backend (reading and\n            writing to the stream).\n\n            You can use these objects to do transparent direct\n            recodings from e.g. latin-1 to utf-8 and back.\n\n            stream must be a file-like object.\n\n            encode, decode must adhere to the Codec interface, Reader,\n            Writer must be factory functions or classes providing the\n            StreamReader, StreamWriter interface resp.\n\n            encode and decode are needed for the frontend translation,\n            Reader and Writer for the backend translation. Unicode is\n            used as intermediate encoding.\n\n            Error handling is done in the same way as defined for the\n            StreamWriter/Readers.\n\n        \"\"\"\n        self.stream = stream\n        self.encode = encode\n        self.decode = decode\n        self.reader = Reader(stream, errors)\n        self.writer = Writer(stream, errors)\n        self.errors = errors\n\n    def read(self, size=-1):\n\n        data = self.reader.read(size)\n        data, bytesencoded = self.encode(data, self.errors)\n        return data\n\n    def readline(self, size=None):\n\n        if size is None:\n            data = self.reader.readline()\n        else:\n            data = self.reader.readline(size)\n        data, bytesencoded = self.encode(data, self.errors)\n        return data\n\n    def readlines(self, sizehint=None):\n\n        data = self.reader.read()\n        data, bytesencoded = self.encode(data, self.errors)\n        return data.splitlines(1)\n\n    def next(self):\n\n        \"\"\" Return the next decoded line from the input stream.\"\"\"\n        data = self.reader.next()\n        data, bytesencoded = self.encode(data, self.errors)\n        return data\n\n    def __iter__(self):\n        return self\n\n    def write(self, data):\n\n        data, bytesdecoded = self.decode(data, self.errors)\n        return self.writer.write(data)\n\n    def writelines(self, list):\n\n        data = ''.join(list)\n        data, bytesdecoded = self.decode(data, self.errors)\n        return self.writer.write(data)\n\n    def reset(self):\n\n        self.reader.reset()\n        self.writer.reset()\n\n    def __getattr__(self, name,\n                    getattr=getattr):\n\n        \"\"\" Inherit all other methods from the underlying stream.\n        \"\"\"\n        return getattr(self.stream, name)\n\n    def __enter__(self):\n        return self\n\n    def __exit__(self, type, value, tb):\n        self.stream.close()\n\n### Shortcuts\n\ndef open(filename, mode='rb', encoding=None, errors='strict', buffering=1):\n\n    \"\"\" Open an encoded file using the given mode and return\n        a wrapped version providing transparent encoding/decoding.\n\n        Note: The wrapped version will only accept the object format\n        defined by the codecs, i.e. Unicode objects for most builtin\n        codecs. Output is also codec dependent and will usually be\n        Unicode as well.\n\n        Files are always opened in binary mode, even if no binary mode\n        was specified. This is done to avoid data loss due to encodings\n        using 8-bit values. The default file mode is 'rb' meaning to\n        open the file in binary read mode.\n\n        encoding specifies the encoding which is to be used for the\n        file.\n\n        errors may be given to define the error handling. It defaults\n        to 'strict' which causes ValueErrors to be raised in case an\n        encoding error occurs.\n\n        buffering has the same meaning as for the builtin open() API.\n        It defaults to line buffered.\n\n        The returned wrapped file object provides an extra attribute\n        .encoding which allows querying the used encoding. This\n        attribute is only available if an encoding was specified as\n        parameter.\n\n    \"\"\"\n    if encoding is not None:\n        if 'U' in mode:\n            # No automatic conversion of '\\n' is done on reading and writing\n            mode = mode.strip().replace('U', '')\n            if mode[:1] not in set('rwa'):\n                mode = 'r' + mode\n        if 'b' not in mode:\n            # Force opening of the file in binary mode\n            mode = mode + 'b'\n    file = __builtin__.open(filename, mode, buffering)\n    if encoding is None:\n        return file\n    info = lookup(encoding)\n    srw = StreamReaderWriter(file, info.streamreader, info.streamwriter, errors)\n    # Add attributes to simplify introspection\n    srw.encoding = encoding\n    return srw\n\ndef EncodedFile(file, data_encoding, file_encoding=None, errors='strict'):\n\n    \"\"\" Return a wrapped version of file which provides transparent\n        encoding translation.\n\n        Strings written to the wrapped file are interpreted according\n        to the given data_encoding and then written to the original\n        file as string using file_encoding. The intermediate encoding\n        will usually be Unicode but depends on the specified codecs.\n\n        Strings are read from the file using file_encoding and then\n        passed back to the caller as string using data_encoding.\n\n        If file_encoding is not given, it defaults to data_encoding.\n\n        errors may be given to define the error handling. It defaults\n        to 'strict' which causes ValueErrors to be raised in case an\n        encoding error occurs.\n\n        The returned wrapped file object provides two extra attributes\n        .data_encoding and .file_encoding which reflect the given\n        parameters of the same name. The attributes can be used for\n        introspection by Python programs.\n\n    \"\"\"\n    if file_encoding is None:\n        file_encoding = data_encoding\n    data_info = lookup(data_encoding)\n    file_info = lookup(file_encoding)\n    sr = StreamRecoder(file, data_info.encode, data_info.decode,\n                       file_info.streamreader, file_info.streamwriter, errors)\n    # Add attributes to simplify introspection\n    sr.data_encoding = data_encoding\n    sr.file_encoding = file_encoding\n    return sr\n\n### Helpers for codec lookup\n\ndef getencoder(encoding):\n\n    \"\"\" Lookup up the codec for the given encoding and return\n        its encoder function.\n\n        Raises a LookupError in case the encoding cannot be found.\n\n    \"\"\"\n    return lookup(encoding).encode\n\ndef getdecoder(encoding):\n\n    \"\"\" Lookup up the codec for the given encoding and return\n        its decoder function.\n\n        Raises a LookupError in case the encoding cannot be found.\n\n    \"\"\"\n    return lookup(encoding).decode\n\ndef getincrementalencoder(encoding):\n\n    \"\"\" Lookup up the codec for the given encoding and return\n        its IncrementalEncoder class or factory function.\n\n        Raises a LookupError in case the encoding cannot be found\n        or the codecs doesn't provide an incremental encoder.\n\n    \"\"\"\n    encoder = lookup(encoding).incrementalencoder\n    if encoder is None:\n        raise LookupError(encoding)\n    return encoder\n\ndef getincrementaldecoder(encoding):\n\n    \"\"\" Lookup up the codec for the given encoding and return\n        its IncrementalDecoder class or factory function.\n\n        Raises a LookupError in case the encoding cannot be found\n        or the codecs doesn't provide an incremental decoder.\n\n    \"\"\"\n    decoder = lookup(encoding).incrementaldecoder\n    if decoder is None:\n        raise LookupError(encoding)\n    return decoder\n\ndef getreader(encoding):\n\n    \"\"\" Lookup up the codec for the given encoding and return\n        its StreamReader class or factory function.\n\n        Raises a LookupError in case the encoding cannot be found.\n\n    \"\"\"\n    return lookup(encoding).streamreader\n\ndef getwriter(encoding):\n\n    \"\"\" Lookup up the codec for the given encoding and return\n        its StreamWriter class or factory function.\n\n        Raises a LookupError in case the encoding cannot be found.\n\n    \"\"\"\n    return lookup(encoding).streamwriter\n\ndef iterencode(iterator, encoding, errors='strict', **kwargs):\n    \"\"\"\n    Encoding iterator.\n\n    Encodes the input strings from the iterator using a IncrementalEncoder.\n\n    errors and kwargs are passed through to the IncrementalEncoder\n    constructor.\n    \"\"\"\n    encoder = getincrementalencoder(encoding)(errors, **kwargs)\n    for input in iterator:\n        output = encoder.encode(input)\n        if output:\n            yield output\n    output = encoder.encode(\"\", True)\n    if output:\n        yield output\n\ndef iterdecode(iterator, encoding, errors='strict', **kwargs):\n    \"\"\"\n    Decoding iterator.\n\n    Decodes the input strings from the iterator using a IncrementalDecoder.\n\n    errors and kwargs are passed through to the IncrementalDecoder\n    constructor.\n    \"\"\"\n    decoder = getincrementaldecoder(encoding)(errors, **kwargs)\n    for input in iterator:\n        output = decoder.decode(input)\n        if output:\n            yield output\n    output = decoder.decode(\"\", True)\n    if output:\n        yield output\n\n### Helpers for charmap-based codecs\n\ndef make_identity_dict(rng):\n\n    \"\"\" make_identity_dict(rng) -> dict\n\n        Return a dictionary where elements of the rng sequence are\n        mapped to themselves.\n\n    \"\"\"\n    res = {}\n    for i in rng:\n        res[i]=i\n    return res\n\ndef make_encoding_map(decoding_map):\n\n    \"\"\" Creates an encoding map from a decoding map.\n\n        If a target mapping in the decoding map occurs multiple\n        times, then that target is mapped to None (undefined mapping),\n        causing an exception when encountered by the charmap codec\n        during translation.\n\n        One example where this happens is cp875.py which decodes\n        multiple character to \\u001a.\n\n    \"\"\"\n    m = {}\n    for k,v in decoding_map.items():\n        if not v in m:\n            m[v] = k\n        else:\n            m[v] = None\n    return m\n\n### error handlers\n\ntry:\n    strict_errors = lookup_error(\"strict\")\n    ignore_errors = lookup_error(\"ignore\")\n    replace_errors = lookup_error(\"replace\")\n    xmlcharrefreplace_errors = lookup_error(\"xmlcharrefreplace\")\n    backslashreplace_errors = lookup_error(\"backslashreplace\")\nexcept LookupError:\n    # In --disable-unicode builds, these error handler are missing\n    strict_errors = None\n    ignore_errors = None\n    replace_errors = None\n    xmlcharrefreplace_errors = None\n    backslashreplace_errors = None\n\n# Tell modulefinder that using codecs probably needs the encodings\n# package\n_false = 0\nif _false:\n    import encodings\n\n### Tests\n\nif __name__ == '__main__':\n\n    # Make stdout translate Latin-1 output into UTF-8 output\n    sys.stdout = EncodedFile(sys.stdout, 'latin-1', 'utf-8')\n\n    # Have stdin translate Latin-1 input into UTF-8 input\n    sys.stdin = EncodedFile(sys.stdin, 'utf-8', 'latin-1')\n", 
    "codeop": "r\"\"\"Utilities to compile possibly incomplete Python source code.\n\nThis module provides two interfaces, broadly similar to the builtin\nfunction compile(), which take program text, a filename and a 'mode'\nand:\n\n- Return code object if the command is complete and valid\n- Return None if the command is incomplete\n- Raise SyntaxError, ValueError or OverflowError if the command is a\n  syntax error (OverflowError and ValueError can be produced by\n  malformed literals).\n\nApproach:\n\nFirst, check if the source consists entirely of blank lines and\ncomments; if so, replace it with 'pass', because the built-in\nparser doesn't always do the right thing for these.\n\nCompile three times: as is, with \\n, and with \\n\\n appended.  If it\ncompiles as is, it's complete.  If it compiles with one \\n appended,\nwe expect more.  If it doesn't compile either way, we compare the\nerror we get when compiling with \\n or \\n\\n appended.  If the errors\nare the same, the code is broken.  But if the errors are different, we\nexpect more.  Not intuitive; not even guaranteed to hold in future\nreleases; but this matches the compiler's behavior from Python 1.4\nthrough 2.2, at least.\n\nCaveat:\n\nIt is possible (but not likely) that the parser stops parsing with a\nsuccessful outcome before reaching the end of the source; in this\ncase, trailing symbols may be ignored instead of causing an error.\nFor example, a backslash followed by two newlines may be followed by\narbitrary garbage.  This will be fixed once the API for the parser is\nbetter.\n\nThe two interfaces are:\n\ncompile_command(source, filename, symbol):\n\n    Compiles a single command in the manner described above.\n\nCommandCompiler():\n\n    Instances of this class have __call__ methods identical in\n    signature to compile_command; the difference is that if the\n    instance compiles program text containing a __future__ statement,\n    the instance 'remembers' and compiles all subsequent program texts\n    with the statement in force.\n\nThe module also provides another class:\n\nCompile():\n\n    Instances of this class act like the built-in function compile,\n    but with 'memory' in the sense described above.\n\"\"\"\n\nimport __future__\n\n_features = [getattr(__future__, fname)\n             for fname in __future__.all_feature_names]\n\n__all__ = [\"compile_command\", \"Compile\", \"CommandCompiler\"]\n\nPyCF_DONT_IMPLY_DEDENT = 0x200          # Matches pythonrun.h\n\ndef _maybe_compile(compiler, source, filename, symbol):\n    # Check for source consisting of only blank lines and comments\n    for line in source.split(\"\\n\"):\n        line = line.strip()\n        if line and line[0] != '#':\n            break               # Leave it alone\n    else:\n        if symbol != \"eval\":\n            source = \"pass\"     # Replace it with a 'pass' statement\n\n    err = err1 = err2 = None\n    code = code1 = code2 = None\n\n    try:\n        code = compiler(source, filename, symbol)\n    except SyntaxError, err:\n        pass\n\n    try:\n        code1 = compiler(source + \"\\n\", filename, symbol)\n    except SyntaxError, err1:\n        pass\n\n    try:\n        code2 = compiler(source + \"\\n\\n\", filename, symbol)\n    except SyntaxError, err2:\n        pass\n\n    if code:\n        return code\n    if not code1 and repr(err1) == repr(err2):\n        raise SyntaxError, err1\n\ndef _compile(source, filename, symbol):\n    return compile(source, filename, symbol, PyCF_DONT_IMPLY_DEDENT)\n\ndef compile_command(source, filename=\"<input>\", symbol=\"single\"):\n    r\"\"\"Compile a command and determine whether it is incomplete.\n\n    Arguments:\n\n    source -- the source string; may contain \\n characters\n    filename -- optional filename from which source was read; default\n                \"<input>\"\n    symbol -- optional grammar start symbol; \"single\" (default) or \"eval\"\n\n    Return value / exceptions raised:\n\n    - Return a code object if the command is complete and valid\n    - Return None if the command is incomplete\n    - Raise SyntaxError, ValueError or OverflowError if the command is a\n      syntax error (OverflowError and ValueError can be produced by\n      malformed literals).\n    \"\"\"\n    return _maybe_compile(_compile, source, filename, symbol)\n\nclass Compile:\n    \"\"\"Instances of this class behave much like the built-in compile\n    function, but if one is used to compile text containing a future\n    statement, it \"remembers\" and compiles all subsequent program texts\n    with the statement in force.\"\"\"\n    def __init__(self):\n        self.flags = PyCF_DONT_IMPLY_DEDENT\n\n    def __call__(self, source, filename, symbol):\n        codeob = compile(source, filename, symbol, self.flags, 1)\n        for feature in _features:\n            if codeob.co_flags & feature.compiler_flag:\n                self.flags |= feature.compiler_flag\n        return codeob\n\nclass CommandCompiler:\n    \"\"\"Instances of this class have __call__ methods identical in\n    signature to compile_command; the difference is that if the\n    instance compiles program text containing a __future__ statement,\n    the instance 'remembers' and compiles all subsequent program texts\n    with the statement in force.\"\"\"\n\n    def __init__(self,):\n        self.compiler = Compile()\n\n    def __call__(self, source, filename=\"<input>\", symbol=\"single\"):\n        r\"\"\"Compile a command and determine whether it is incomplete.\n\n        Arguments:\n\n        source -- the source string; may contain \\n characters\n        filename -- optional filename from which source was read;\n                    default \"<input>\"\n        symbol -- optional grammar start symbol; \"single\" (default) or\n                  \"eval\"\n\n        Return value / exceptions raised:\n\n        - Return a code object if the command is complete and valid\n        - Return None if the command is incomplete\n        - Raise SyntaxError, ValueError or OverflowError if the command is a\n          syntax error (OverflowError and ValueError can be produced by\n          malformed literals).\n        \"\"\"\n        return _maybe_compile(self.compiler, source, filename, symbol)\n", 
    "collections": "__all__ = ['Counter', 'deque', 'defaultdict', 'namedtuple', 'OrderedDict']\n# For bootstrapping reasons, the collection ABCs are defined in _abcoll.py.\n# They should however be considered an integral part of collections.py.\nfrom _abcoll import *\nimport _abcoll\n__all__ += _abcoll.__all__\n\nfrom _collections import deque, defaultdict\nfrom operator import itemgetter as _itemgetter, eq as _eq\nfrom keyword import iskeyword as _iskeyword\nimport sys as _sys\nimport heapq as _heapq\nfrom itertools import repeat as _repeat, chain as _chain, starmap as _starmap\nfrom itertools import imap as _imap\ntry:\n    from __pypy__ import newdict\nexcept ImportError:\n    assert '__pypy__' not in _sys.builtin_module_names\n    newdict = lambda _ : {}\ntry:\n    from __pypy__ import reversed_dict\nexcept ImportError:\n    reversed_dict = lambda d: reversed(d.keys())\n\ntry:\n    from thread import get_ident as _get_ident\nexcept ImportError:\n    from dummy_thread import get_ident as _get_ident\n\n\n################################################################################\n### OrderedDict\n################################################################################\n\nclass OrderedDict(dict):\n    '''Dictionary that remembers insertion order.\n\n    In PyPy all dicts are ordered anyway.  This is mostly useful as a\n    placeholder to mean \"this dict must be ordered even on CPython\".\n\n    Known difference: iterating over an OrderedDict which is being\n    concurrently modified raises RuntimeError in PyPy.  In CPython\n    instead we get some behavior that appears reasonable in some\n    cases but is nonsensical in other cases.  This is officially\n    forbidden by the CPython docs, so we forbid it explicitly for now.\n    '''\n\n    def __reversed__(self):\n        return reversed_dict(self)\n\n    def popitem(self, last=True):\n        '''od.popitem() -> (k, v), return and remove a (key, value) pair.\n        Pairs are returned in LIFO order if last is true or FIFO order if false.\n\n        '''\n        if last:\n            return dict.popitem(self)\n        else:\n            it = dict.__iter__(self)\n            try:\n                k = it.next()\n            except StopIteration:\n                raise KeyError('dictionary is empty')\n            return (k, self.pop(k))\n\n    def __repr__(self, _repr_running={}):\n        'od.__repr__() <==> repr(od)'\n        call_key = id(self), _get_ident()\n        if call_key in _repr_running:\n            return '...'\n        _repr_running[call_key] = 1\n        try:\n            if not self:\n                return '%s()' % (self.__class__.__name__,)\n            return '%s(%r)' % (self.__class__.__name__, self.items())\n        finally:\n            del _repr_running[call_key]\n\n    def __reduce__(self):\n        'Return state information for pickling'\n        items = [[k, self[k]] for k in self]\n        inst_dict = vars(self).copy()\n        if inst_dict:\n            return (self.__class__, (items,), inst_dict)\n        return self.__class__, (items,)\n\n    def copy(self):\n        'od.copy() -> a shallow copy of od'\n        return self.__class__(self)\n\n    def __eq__(self, other):\n        '''od.__eq__(y) <==> od==y.  Comparison to another OD is order-sensitive\n        while comparison to a regular mapping is order-insensitive.\n\n        '''\n        if isinstance(other, OrderedDict):\n            return dict.__eq__(self, other) and all(_imap(_eq, self, other))\n        return dict.__eq__(self, other)\n\n    def __ne__(self, other):\n        'od.__ne__(y) <==> od!=y'\n        return not self == other\n\n    # -- the following methods support python 3.x style dictionary views --\n\n    def viewkeys(self):\n        \"od.viewkeys() -> a set-like object providing a view on od's keys\"\n        return KeysView(self)\n\n    def viewvalues(self):\n        \"od.viewvalues() -> an object providing a view on od's values\"\n        return ValuesView(self)\n\n    def viewitems(self):\n        \"od.viewitems() -> a set-like object providing a view on od's items\"\n        return ItemsView(self)\n\n\n################################################################################\n### namedtuple\n################################################################################\n\n_class_template = '''\\\nclass {typename}(tuple):\n    '{typename}({arg_list})'\n\n    __slots__ = ()\n\n    _fields = {field_names!r}\n\n    def __new__(_cls, {arg_list}):\n        'Create new instance of {typename}({arg_list})'\n        return _tuple.__new__(_cls, ({arg_list}))\n\n    @classmethod\n    def _make(cls, iterable, new=tuple.__new__, len=len):\n        'Make a new {typename} object from a sequence or iterable'\n        result = new(cls, iterable)\n        if len(result) != {num_fields:d}:\n            raise TypeError('Expected {num_fields:d} arguments, got %d' % len(result))\n        return result\n\n    def __repr__(self):\n        'Return a nicely formatted representation string'\n        return '{typename}({repr_fmt})' % self\n\n    def _asdict(self):\n        'Return a new OrderedDict which maps field names to their values'\n        return OrderedDict(zip(self._fields, self))\n\n    def _replace(_self, **kwds):\n        'Return a new {typename} object replacing specified fields with new values'\n        result = _self._make(map(kwds.pop, {field_names!r}, _self))\n        if kwds:\n            raise ValueError('Got unexpected field names: %r' % kwds.keys())\n        return result\n\n    def __getnewargs__(self):\n        'Return self as a plain tuple.  Used by copy and pickle.'\n        return tuple(self)\n\n    __dict__ = _property(_asdict)\n\n    def __getstate__(self):\n        'Exclude the OrderedDict from pickling'\n        pass\n\n{field_defs}\n'''\n\n_repr_template = '{name}=%r'\n\n_field_template = '''\\\n    {name} = _property(lambda self: self[{index:d}], doc='Alias for field number {index:d}')\n'''\n\ndef namedtuple(typename, field_names, verbose=False, rename=False):\n    \"\"\"Returns a new subclass of tuple with named fields.\n\n    >>> Point = namedtuple('Point', ['x', 'y'])\n    >>> Point.__doc__                   # docstring for the new class\n    'Point(x, y)'\n    >>> p = Point(11, y=22)             # instantiate with positional args or keywords\n    >>> p[0] + p[1]                     # indexable like a plain tuple\n    33\n    >>> x, y = p                        # unpack like a regular tuple\n    >>> x, y\n    (11, 22)\n    >>> p.x + p.y                       # fields also accessable by name\n    33\n    >>> d = p._asdict()                 # convert to a dictionary\n    >>> d['x']\n    11\n    >>> Point(**d)                      # convert from a dictionary\n    Point(x=11, y=22)\n    >>> p._replace(x=100)               # _replace() is like str.replace() but targets named fields\n    Point(x=100, y=22)\n\n    \"\"\"\n\n    # Validate the field names.  At the user's option, either generate an error\n    # message or automatically replace the field name with a valid name.\n    if isinstance(field_names, basestring):\n        field_names = field_names.replace(',', ' ').split()\n    field_names = map(str, field_names)\n    typename = str(typename)\n    if rename:\n        seen = set()\n        for index, name in enumerate(field_names):\n            if (not all(c.isalnum() or c=='_' for c in name)\n                or _iskeyword(name)\n                or not name\n                or name[0].isdigit()\n                or name.startswith('_')\n                or name in seen):\n                field_names[index] = '_%d' % index\n            seen.add(name)\n    for name in [typename] + field_names:\n        if type(name) != str:\n            raise TypeError('Type names and field names must be strings')\n        if not all(c.isalnum() or c=='_' for c in name):\n            raise ValueError('Type names and field names can only contain '\n                             'alphanumeric characters and underscores: %r' % name)\n        if _iskeyword(name):\n            raise ValueError('Type names and field names cannot be a '\n                             'keyword: %r' % name)\n        if name[0].isdigit():\n            raise ValueError('Type names and field names cannot start with '\n                             'a number: %r' % name)\n    seen = set()\n    for name in field_names:\n        if name.startswith('_') and not rename:\n            raise ValueError('Field names cannot start with an underscore: '\n                             '%r' % name)\n        if name in seen:\n            raise ValueError('Encountered duplicate field name: %r' % name)\n        seen.add(name)\n\n    # Fill-in the class template\n    class_definition = _class_template.format(\n        typename = typename,\n        field_names = tuple(field_names),\n        num_fields = len(field_names),\n        arg_list = repr(tuple(field_names)).replace(\"'\", \"\")[1:-1],\n        repr_fmt = ', '.join(_repr_template.format(name=name)\n                             for name in field_names),\n        field_defs = '\\n'.join(_field_template.format(index=index, name=name)\n                               for index, name in enumerate(field_names))\n    )\n    if verbose:\n        print class_definition\n\n    # Execute the template string in a temporary namespace and support\n    # tracing utilities by setting a value for frame.f_globals['__name__']\n    namespace = newdict('module')\n    namespace['__name__'] = 'namedtuple_%s' % typename\n    namespace['OrderedDict'] = OrderedDict\n    namespace['_property'] = property\n    namespace['_tuple'] = tuple\n    try:\n        exec class_definition in namespace\n    except SyntaxError as e:\n        raise SyntaxError(e.message + ':\\n' + class_definition)\n    result = namespace[typename]\n\n    # For pickling to work, the __module__ variable needs to be set to the frame\n    # where the named tuple is created.  Bypass this step in environments where\n    # sys._getframe is not defined (Jython for example) or sys._getframe is not\n    # defined for arguments greater than 0 (IronPython).\n    try:\n        result.__module__ = _sys._getframe(1).f_globals.get('__name__', '__main__')\n    except (AttributeError, ValueError):\n        pass\n\n    return result\n\n\n########################################################################\n###  Counter\n########################################################################\n\nclass Counter(dict):\n    '''Dict subclass for counting hashable items.  Sometimes called a bag\n    or multiset.  Elements are stored as dictionary keys and their counts\n    are stored as dictionary values.\n\n    >>> c = Counter('abcdeabcdabcaba')  # count elements from a string\n\n    >>> c.most_common(3)                # three most common elements\n    [('a', 5), ('b', 4), ('c', 3)]\n    >>> sorted(c)                       # list all unique elements\n    ['a', 'b', 'c', 'd', 'e']\n    >>> ''.join(sorted(c.elements()))   # list elements with repetitions\n    'aaaaabbbbcccdde'\n    >>> sum(c.values())                 # total of all counts\n    15\n\n    >>> c['a']                          # count of letter 'a'\n    5\n    >>> for elem in 'shazam':           # update counts from an iterable\n    ...     c[elem] += 1                # by adding 1 to each element's count\n    >>> c['a']                          # now there are seven 'a'\n    7\n    >>> del c['b']                      # remove all 'b'\n    >>> c['b']                          # now there are zero 'b'\n    0\n\n    >>> d = Counter('simsalabim')       # make another counter\n    >>> c.update(d)                     # add in the second counter\n    >>> c['a']                          # now there are nine 'a'\n    9\n\n    >>> c.clear()                       # empty the counter\n    >>> c\n    Counter()\n\n    Note:  If a count is set to zero or reduced to zero, it will remain\n    in the counter until the entry is deleted or the counter is cleared:\n\n    >>> c = Counter('aaabbc')\n    >>> c['b'] -= 2                     # reduce the count of 'b' by two\n    >>> c.most_common()                 # 'b' is still in, but its count is zero\n    [('a', 3), ('c', 1), ('b', 0)]\n\n    '''\n    # References:\n    #   http://en.wikipedia.org/wiki/Multiset\n    #   http://www.gnu.org/software/smalltalk/manual-base/html_node/Bag.html\n    #   http://www.demo2s.com/Tutorial/Cpp/0380__set-multiset/Catalog0380__set-multiset.htm\n    #   http://code.activestate.com/recipes/259174/\n    #   Knuth, TAOCP Vol. II section 4.6.3\n\n    def __init__(self, iterable=None, **kwds):\n        '''Create a new, empty Counter object.  And if given, count elements\n        from an input iterable.  Or, initialize the count from another mapping\n        of elements to their counts.\n\n        >>> c = Counter()                           # a new, empty counter\n        >>> c = Counter('gallahad')                 # a new counter from an iterable\n        >>> c = Counter({'a': 4, 'b': 2})           # a new counter from a mapping\n        >>> c = Counter(a=4, b=2)                   # a new counter from keyword args\n\n        '''\n        super(Counter, self).__init__()\n        self.update(iterable, **kwds)\n\n    def __missing__(self, key):\n        'The count of elements not in the Counter is zero.'\n        # Needed so that self[missing_item] does not raise KeyError\n        return 0\n\n    def most_common(self, n=None):\n        '''List the n most common elements and their counts from the most\n        common to the least.  If n is None, then list all element counts.\n\n        >>> Counter('abcdeabcdabcaba').most_common(3)\n        [('a', 5), ('b', 4), ('c', 3)]\n\n        '''\n        # Emulate Bag.sortedByCount from Smalltalk\n        if n is None:\n            return sorted(self.iteritems(), key=_itemgetter(1), reverse=True)\n        return _heapq.nlargest(n, self.iteritems(), key=_itemgetter(1))\n\n    def elements(self):\n        '''Iterator over elements repeating each as many times as its count.\n\n        >>> c = Counter('ABCABC')\n        >>> sorted(c.elements())\n        ['A', 'A', 'B', 'B', 'C', 'C']\n\n        # Knuth's example for prime factors of 1836:  2**2 * 3**3 * 17**1\n        >>> prime_factors = Counter({2: 2, 3: 3, 17: 1})\n        >>> product = 1\n        >>> for factor in prime_factors.elements():     # loop over factors\n        ...     product *= factor                       # and multiply them\n        >>> product\n        1836\n\n        Note, if an element's count has been set to zero or is a negative\n        number, elements() will ignore it.\n\n        '''\n        # Emulate Bag.do from Smalltalk and Multiset.begin from C++.\n        return _chain.from_iterable(_starmap(_repeat, self.iteritems()))\n\n    # Override dict methods where necessary\n\n    @classmethod\n    def fromkeys(cls, iterable, v=None):\n        # There is no equivalent method for counters because setting v=1\n        # means that no element can have a count greater than one.\n        raise NotImplementedError(\n            'Counter.fromkeys() is undefined.  Use Counter(iterable) instead.')\n\n    def update(self, iterable=None, **kwds):\n        '''Like dict.update() but add counts instead of replacing them.\n\n        Source can be an iterable, a dictionary, or another Counter instance.\n\n        >>> c = Counter('which')\n        >>> c.update('witch')           # add elements from another iterable\n        >>> d = Counter('watch')\n        >>> c.update(d)                 # add elements from another counter\n        >>> c['h']                      # four 'h' in which, witch, and watch\n        4\n\n        '''\n        # The regular dict.update() operation makes no sense here because the\n        # replace behavior results in the some of original untouched counts\n        # being mixed-in with all of the other counts for a mismash that\n        # doesn't have a straight-forward interpretation in most counting\n        # contexts.  Instead, we implement straight-addition.  Both the inputs\n        # and outputs are allowed to contain zero and negative counts.\n\n        if iterable is not None:\n            if isinstance(iterable, Mapping):\n                if self:\n                    self_get = self.get\n                    for elem, count in iterable.iteritems():\n                        self[elem] = self_get(elem, 0) + count\n                else:\n                    super(Counter, self).update(iterable) # fast path when counter is empty\n            else:\n                self_get = self.get\n                for elem in iterable:\n                    self[elem] = self_get(elem, 0) + 1\n        if kwds:\n            self.update(kwds)\n\n    def subtract(self, iterable=None, **kwds):\n        '''Like dict.update() but subtracts counts instead of replacing them.\n        Counts can be reduced below zero.  Both the inputs and outputs are\n        allowed to contain zero and negative counts.\n\n        Source can be an iterable, a dictionary, or another Counter instance.\n\n        >>> c = Counter('which')\n        >>> c.subtract('witch')             # subtract elements from another iterable\n        >>> c.subtract(Counter('watch'))    # subtract elements from another counter\n        >>> c['h']                          # 2 in which, minus 1 in witch, minus 1 in watch\n        0\n        >>> c['w']                          # 1 in which, minus 1 in witch, minus 1 in watch\n        -1\n\n        '''\n        if iterable is not None:\n            self_get = self.get\n            if isinstance(iterable, Mapping):\n                for elem, count in iterable.items():\n                    self[elem] = self_get(elem, 0) - count\n            else:\n                for elem in iterable:\n                    self[elem] = self_get(elem, 0) - 1\n        if kwds:\n            self.subtract(kwds)\n\n    def copy(self):\n        'Return a shallow copy.'\n        return self.__class__(self)\n\n    def __reduce__(self):\n        return self.__class__, (dict(self),)\n\n    def __delitem__(self, elem):\n        'Like dict.__delitem__() but does not raise KeyError for missing values.'\n        if elem in self:\n            super(Counter, self).__delitem__(elem)\n\n    def __repr__(self):\n        if not self:\n            return '%s()' % self.__class__.__name__\n        items = ', '.join(map('%r: %r'.__mod__, self.most_common()))\n        return '%s({%s})' % (self.__class__.__name__, items)\n\n    # Multiset-style mathematical operations discussed in:\n    #       Knuth TAOCP Volume II section 4.6.3 exercise 19\n    #       and at http://en.wikipedia.org/wiki/Multiset\n    #\n    # Outputs guaranteed to only include positive counts.\n    #\n    # To strip negative and zero counts, add-in an empty counter:\n    #       c += Counter()\n\n    def __add__(self, other):\n        '''Add counts from two counters.\n\n        >>> Counter('abbb') + Counter('bcc')\n        Counter({'b': 4, 'c': 2, 'a': 1})\n\n        '''\n        if not isinstance(other, Counter):\n            return NotImplemented\n        result = Counter()\n        for elem, count in self.items():\n            newcount = count + other[elem]\n            if newcount > 0:\n                result[elem] = newcount\n        for elem, count in other.items():\n            if elem not in self and count > 0:\n                result[elem] = count\n        return result\n\n    def __sub__(self, other):\n        ''' Subtract count, but keep only results with positive counts.\n\n        >>> Counter('abbbc') - Counter('bccd')\n        Counter({'b': 2, 'a': 1})\n\n        '''\n        if not isinstance(other, Counter):\n            return NotImplemented\n        result = Counter()\n        for elem, count in self.items():\n            newcount = count - other[elem]\n            if newcount > 0:\n                result[elem] = newcount\n        for elem, count in other.items():\n            if elem not in self and count < 0:\n                result[elem] = 0 - count\n        return result\n\n    def __or__(self, other):\n        '''Union is the maximum of value in either of the input counters.\n\n        >>> Counter('abbb') | Counter('bcc')\n        Counter({'b': 3, 'c': 2, 'a': 1})\n\n        '''\n        if not isinstance(other, Counter):\n            return NotImplemented\n        result = Counter()\n        for elem, count in self.items():\n            other_count = other[elem]\n            newcount = other_count if count < other_count else count\n            if newcount > 0:\n                result[elem] = newcount\n        for elem, count in other.items():\n            if elem not in self and count > 0:\n                result[elem] = count\n        return result\n\n    def __and__(self, other):\n        ''' Intersection is the minimum of corresponding counts.\n\n        >>> Counter('abbb') & Counter('bcc')\n        Counter({'b': 1})\n\n        '''\n        if not isinstance(other, Counter):\n            return NotImplemented\n        result = Counter()\n        for elem, count in self.items():\n            other_count = other[elem]\n            newcount = count if count < other_count else other_count\n            if newcount > 0:\n                result[elem] = newcount\n        return result\n\n\nif __name__ == '__main__':\n    # verify that instances can be pickled\n    from cPickle import loads, dumps\n    Point = namedtuple('Point', 'x, y', True)\n    p = Point(x=10, y=20)\n    assert p == loads(dumps(p))\n\n    # test and demonstrate ability to override methods\n    class Point(namedtuple('Point', 'x y')):\n        __slots__ = ()\n        @property\n        def hypot(self):\n            return (self.x ** 2 + self.y ** 2) ** 0.5\n        def __str__(self):\n            return 'Point: x=%6.3f  y=%6.3f  hypot=%6.3f' % (self.x, self.y, self.hypot)\n\n    for p in Point(3, 4), Point(14, 5/7.):\n        print p\n\n    class Point(namedtuple('Point', 'x y')):\n        'Point class with optimized _make() and _replace() without error-checking'\n        __slots__ = ()\n        _make = classmethod(tuple.__new__)\n        def _replace(self, _map=map, **kwds):\n            return self._make(_map(kwds.get, ('x', 'y'), self))\n\n    print Point(11, 22)._replace(x=100)\n\n    Point3D = namedtuple('Point3D', Point._fields + ('z',))\n    print Point3D.__doc__\n\n    import doctest\n    TestResults = namedtuple('TestResults', 'failed attempted')\n    print TestResults(*doctest.testmod())\n", 
    "copy": "\"\"\"Generic (shallow and deep) copying operations.\n\nInterface summary:\n\n        import copy\n\n        x = copy.copy(y)        # make a shallow copy of y\n        x = copy.deepcopy(y)    # make a deep copy of y\n\nFor module specific errors, copy.Error is raised.\n\nThe difference between shallow and deep copying is only relevant for\ncompound objects (objects that contain other objects, like lists or\nclass instances).\n\n- A shallow copy constructs a new compound object and then (to the\n  extent possible) inserts *the same objects* into it that the\n  original contains.\n\n- A deep copy constructs a new compound object and then, recursively,\n  inserts *copies* into it of the objects found in the original.\n\nTwo problems often exist with deep copy operations that don't exist\nwith shallow copy operations:\n\n a) recursive objects (compound objects that, directly or indirectly,\n    contain a reference to themselves) may cause a recursive loop\n\n b) because deep copy copies *everything* it may copy too much, e.g.\n    administrative data structures that should be shared even between\n    copies\n\nPython's deep copy operation avoids these problems by:\n\n a) keeping a table of objects already copied during the current\n    copying pass\n\n b) letting user-defined classes override the copying operation or the\n    set of components copied\n\nThis version does not copy types like module, class, function, method,\nnor stack trace, stack frame, nor file, socket, window, nor array, nor\nany similar types.\n\nClasses can use the same interfaces to control copying that they use\nto control pickling: they can define methods called __getinitargs__(),\n__getstate__() and __setstate__().  See the documentation for module\n\"pickle\" for information on these methods.\n\"\"\"\n\nimport types\nimport weakref\nfrom copy_reg import dispatch_table\n\nclass Error(Exception):\n    pass\nerror = Error   # backward compatibility\n\ntry:\n    from org.python.core import PyStringMap\nexcept ImportError:\n    PyStringMap = None\n\n__all__ = [\"Error\", \"copy\", \"deepcopy\"]\n\ndef copy(x):\n    \"\"\"Shallow copy operation on arbitrary Python objects.\n\n    See the module's __doc__ string for more info.\n    \"\"\"\n\n    cls = type(x)\n\n    copier = _copy_dispatch.get(cls)\n    if copier:\n        return copier(x)\n\n    copier = getattr(cls, \"__copy__\", None)\n    if copier:\n        return copier(x)\n\n    reductor = dispatch_table.get(cls)\n    if reductor:\n        rv = reductor(x)\n    else:\n        reductor = getattr(x, \"__reduce_ex__\", None)\n        if reductor:\n            rv = reductor(2)\n        else:\n            reductor = getattr(x, \"__reduce__\", None)\n            if reductor:\n                rv = reductor()\n            else:\n                raise Error(\"un(shallow)copyable object of type %s\" % cls)\n\n    return _reconstruct(x, rv, 0)\n\n\n_copy_dispatch = d = {}\n\ndef _copy_immutable(x):\n    return x\nfor t in (type(None), int, long, float, bool, str, tuple,\n          frozenset, type, xrange, types.ClassType,\n          types.BuiltinFunctionType, type(Ellipsis),\n          types.FunctionType, weakref.ref):\n    d[t] = _copy_immutable\nfor name in (\"ComplexType\", \"UnicodeType\", \"CodeType\"):\n    t = getattr(types, name, None)\n    if t is not None:\n        d[t] = _copy_immutable\n\ndef _copy_with_constructor(x):\n    return type(x)(x)\nfor t in (list, dict, set):\n    d[t] = _copy_with_constructor\n\ndef _copy_with_copy_method(x):\n    return x.copy()\nif PyStringMap is not None:\n    d[PyStringMap] = _copy_with_copy_method\n\ndef _copy_inst(x):\n    if hasattr(x, '__copy__'):\n        return x.__copy__()\n    if hasattr(x, '__getinitargs__'):\n        args = x.__getinitargs__()\n        y = x.__class__(*args)\n    else:\n        y = _EmptyClass()\n        y.__class__ = x.__class__\n    if hasattr(x, '__getstate__'):\n        state = x.__getstate__()\n    else:\n        state = x.__dict__\n    if hasattr(y, '__setstate__'):\n        y.__setstate__(state)\n    else:\n        y.__dict__.update(state)\n    return y\nd[types.InstanceType] = _copy_inst\n\ndel d\n\ndef deepcopy(x, memo=None, _nil=[]):\n    \"\"\"Deep copy operation on arbitrary Python objects.\n\n    See the module's __doc__ string for more info.\n    \"\"\"\n\n    if memo is None:\n        memo = {}\n\n    d = id(x)\n    y = memo.get(d, _nil)\n    if y is not _nil:\n        return y\n\n    cls = type(x)\n\n    copier = _deepcopy_dispatch.get(cls)\n    if copier:\n        y = copier(x, memo)\n    else:\n        try:\n            issc = issubclass(cls, type)\n        except TypeError: # cls is not a class (old Boost; see SF #502085)\n            issc = 0\n        if issc:\n            y = _deepcopy_atomic(x, memo)\n        else:\n            copier = getattr(x, \"__deepcopy__\", None)\n            if copier:\n                y = copier(memo)\n            else:\n                reductor = dispatch_table.get(cls)\n                if reductor:\n                    rv = reductor(x)\n                else:\n                    reductor = getattr(x, \"__reduce_ex__\", None)\n                    if reductor:\n                        rv = reductor(2)\n                    else:\n                        reductor = getattr(x, \"__reduce__\", None)\n                        if reductor:\n                            rv = reductor()\n                        else:\n                            raise Error(\n                                \"un(deep)copyable object of type %s\" % cls)\n                y = _reconstruct(x, rv, 1, memo)\n\n    memo[d] = y\n    _keep_alive(x, memo) # Make sure x lives at least as long as d\n    return y\n\n_deepcopy_dispatch = d = {}\n\ndef _deepcopy_atomic(x, memo):\n    return x\nd[type(None)] = _deepcopy_atomic\nd[type(Ellipsis)] = _deepcopy_atomic\nd[int] = _deepcopy_atomic\nd[long] = _deepcopy_atomic\nd[float] = _deepcopy_atomic\nd[bool] = _deepcopy_atomic\ntry:\n    d[complex] = _deepcopy_atomic\nexcept NameError:\n    pass\nd[str] = _deepcopy_atomic\ntry:\n    d[unicode] = _deepcopy_atomic\nexcept NameError:\n    pass\ntry:\n    d[types.CodeType] = _deepcopy_atomic\nexcept AttributeError:\n    pass\nd[type] = _deepcopy_atomic\nd[xrange] = _deepcopy_atomic\nd[types.ClassType] = _deepcopy_atomic\nd[types.BuiltinFunctionType] = _deepcopy_atomic\nd[types.FunctionType] = _deepcopy_atomic\nd[weakref.ref] = _deepcopy_atomic\n\ndef _deepcopy_list(x, memo):\n    y = []\n    memo[id(x)] = y\n    for a in x:\n        y.append(deepcopy(a, memo))\n    return y\nd[list] = _deepcopy_list\n\ndef _deepcopy_tuple(x, memo):\n    y = []\n    for a in x:\n        y.append(deepcopy(a, memo))\n    d = id(x)\n    try:\n        return memo[d]\n    except KeyError:\n        pass\n    for i in range(len(x)):\n        if x[i] is not y[i]:\n            y = tuple(y)\n            break\n    else:\n        y = x\n    memo[d] = y\n    return y\nd[tuple] = _deepcopy_tuple\n\ndef _deepcopy_dict(x, memo):\n    y = {}\n    memo[id(x)] = y\n    for key, value in x.iteritems():\n        y[deepcopy(key, memo)] = deepcopy(value, memo)\n    return y\nd[dict] = _deepcopy_dict\nif PyStringMap is not None:\n    d[PyStringMap] = _deepcopy_dict\n\ndef _deepcopy_method(x, memo): # Copy instance methods\n    return type(x)(x.im_func, deepcopy(x.im_self, memo), x.im_class)\n_deepcopy_dispatch[types.MethodType] = _deepcopy_method\n\ndef _keep_alive(x, memo):\n    \"\"\"Keeps a reference to the object x in the memo.\n\n    Because we remember objects by their id, we have\n    to assure that possibly temporary objects are kept\n    alive by referencing them.\n    We store a reference at the id of the memo, which should\n    normally not be used unless someone tries to deepcopy\n    the memo itself...\n    \"\"\"\n    try:\n        memo[id(memo)].append(x)\n    except KeyError:\n        # aha, this is the first one :-)\n        memo[id(memo)]=[x]\n\ndef _deepcopy_inst(x, memo):\n    if hasattr(x, '__deepcopy__'):\n        return x.__deepcopy__(memo)\n    if hasattr(x, '__getinitargs__'):\n        args = x.__getinitargs__()\n        args = deepcopy(args, memo)\n        y = x.__class__(*args)\n    else:\n        y = _EmptyClass()\n        y.__class__ = x.__class__\n    memo[id(x)] = y\n    if hasattr(x, '__getstate__'):\n        state = x.__getstate__()\n    else:\n        state = x.__dict__\n    state = deepcopy(state, memo)\n    if hasattr(y, '__setstate__'):\n        y.__setstate__(state)\n    else:\n        y.__dict__.update(state)\n    return y\nd[types.InstanceType] = _deepcopy_inst\n\ndef _reconstruct(x, info, deep, memo=None):\n    if isinstance(info, str):\n        return x\n    assert isinstance(info, tuple)\n    if memo is None:\n        memo = {}\n    n = len(info)\n    assert n in (2, 3, 4, 5)\n    callable, args = info[:2]\n    if n > 2:\n        state = info[2]\n    else:\n        state = {}\n    if n > 3:\n        listiter = info[3]\n    else:\n        listiter = None\n    if n > 4:\n        dictiter = info[4]\n    else:\n        dictiter = None\n    if deep:\n        args = deepcopy(args, memo)\n    y = callable(*args)\n    memo[id(x)] = y\n\n    if state:\n        if deep:\n            state = deepcopy(state, memo)\n        if hasattr(y, '__setstate__'):\n            y.__setstate__(state)\n        else:\n            if isinstance(state, tuple) and len(state) == 2:\n                state, slotstate = state\n            else:\n                slotstate = None\n            if state is not None:\n                y.__dict__.update(state)\n            if slotstate is not None:\n                for key, value in slotstate.iteritems():\n                    setattr(y, key, value)\n\n    if listiter is not None:\n        for item in listiter:\n            if deep:\n                item = deepcopy(item, memo)\n            y.append(item)\n    if dictiter is not None:\n        for key, value in dictiter:\n            if deep:\n                key = deepcopy(key, memo)\n                value = deepcopy(value, memo)\n            y[key] = value\n    return y\n\ndel d\n\ndel types\n\n# Helper for instance creation without calling __init__\nclass _EmptyClass:\n    pass\n\ndef _test():\n    l = [None, 1, 2L, 3.14, 'xyzzy', (1, 2L), [3.14, 'abc'],\n         {'abc': 'ABC'}, (), [], {}]\n    l1 = copy(l)\n    print l1==l\n    l1 = map(copy, l)\n    print l1==l\n    l1 = deepcopy(l)\n    print l1==l\n    class C:\n        def __init__(self, arg=None):\n            self.a = 1\n            self.arg = arg\n            if __name__ == '__main__':\n                import sys\n                file = sys.argv[0]\n            else:\n                file = __file__\n            self.fp = open(file)\n            self.fp.close()\n        def __getstate__(self):\n            return {'a': self.a, 'arg': self.arg}\n        def __setstate__(self, state):\n            for key, value in state.iteritems():\n                setattr(self, key, value)\n        def __deepcopy__(self, memo=None):\n            new = self.__class__(deepcopy(self.arg, memo))\n            new.a = self.a\n            return new\n    c = C('argument sketch')\n    l.append(c)\n    l2 = copy(l)\n    print l == l2\n    print l\n    print l2\n    l2 = deepcopy(l)\n    print l == l2\n    print l\n    print l2\n    l.append({l[1]: l, 'xyz': l[2]})\n    l3 = copy(l)\n    import repr\n    print map(repr.repr, l)\n    print map(repr.repr, l1)\n    print map(repr.repr, l2)\n    print map(repr.repr, l3)\n    l3 = deepcopy(l)\n    import repr\n    print map(repr.repr, l)\n    print map(repr.repr, l1)\n    print map(repr.repr, l2)\n    print map(repr.repr, l3)\n    class odict(dict):\n        def __init__(self, d = {}):\n            self.a = 99\n            dict.__init__(self, d)\n        def __setitem__(self, k, i):\n            dict.__setitem__(self, k, i)\n            self.a\n    o = odict({\"A\" : \"B\"})\n    x = deepcopy(o)\n    print(o, x)\n\nif __name__ == '__main__':\n    _test()\n", 
    "copy_reg": "\"\"\"Helper to provide extensibility for pickle/cPickle.\n\nThis is only useful to add pickle support for extension types defined in\nC, not for instances of user-defined classes.\n\"\"\"\n\nfrom types import ClassType as _ClassType\n\n__all__ = [\"pickle\", \"constructor\",\n           \"add_extension\", \"remove_extension\", \"clear_extension_cache\"]\n\ndispatch_table = {}\n\ndef pickle(ob_type, pickle_function, constructor_ob=None):\n    if type(ob_type) is _ClassType:\n        raise TypeError(\"copy_reg is not intended for use with classes\")\n\n    if not hasattr(pickle_function, '__call__'):\n        raise TypeError(\"reduction functions must be callable\")\n    dispatch_table[ob_type] = pickle_function\n\n    # The constructor_ob function is a vestige of safe for unpickling.\n    # There is no reason for the caller to pass it anymore.\n    if constructor_ob is not None:\n        constructor(constructor_ob)\n\ndef constructor(object):\n    if not hasattr(object, '__call__'):\n        raise TypeError(\"constructors must be callable\")\n\n# Example: provide pickling support for complex numbers.\n\ntry:\n    complex\nexcept NameError:\n    pass\nelse:\n\n    def pickle_complex(c):\n        return complex, (c.real, c.imag)\n\n    pickle(complex, pickle_complex, complex)\n\n# Support for pickling new-style objects\n\ndef _reconstructor(cls, base, state):\n    if base is object:\n        obj = object.__new__(cls)\n    else:\n        obj = base.__new__(cls, state)\n        if base.__init__ != object.__init__:\n            base.__init__(obj, state)\n    return obj\n\n_HEAPTYPE = 1<<9\n\n# Python code for object.__reduce_ex__ for protocols 0 and 1\n\ndef _reduce_ex(self, proto):\n    assert proto < 2\n    for base in self.__class__.__mro__:\n        if hasattr(base, '__flags__') and not base.__flags__ & _HEAPTYPE:\n            break\n    else:\n        base = object # not really reachable\n    if base is object:\n        state = None\n    else:\n        if base is self.__class__:\n            raise TypeError, \"can't pickle %s objects\" % base.__name__\n        state = base(self)\n    args = (self.__class__, base, state)\n    try:\n        getstate = self.__getstate__\n    except AttributeError:\n        if getattr(self, \"__slots__\", None):\n            raise TypeError(\"a class that defines __slots__ without \"\n                            \"defining __getstate__ cannot be pickled\")\n        try:\n            dict = self.__dict__\n        except AttributeError:\n            dict = None\n    else:\n        dict = getstate()\n    if dict:\n        return _reconstructor, args, dict\n    else:\n        return _reconstructor, args\n\n# Helper for __reduce_ex__ protocol 2\n\ndef __newobj__(cls, *args):\n    return cls.__new__(cls, *args)\n\ndef _slotnames(cls):\n    \"\"\"Return a list of slot names for a given class.\n\n    This needs to find slots defined by the class and its bases, so we\n    can't simply return the __slots__ attribute.  We must walk down\n    the Method Resolution Order and concatenate the __slots__ of each\n    class found there.  (This assumes classes don't modify their\n    __slots__ attribute to misrepresent their slots after the class is\n    defined.)\n    \"\"\"\n\n    # Get the value from a cache in the class if possible\n    names = cls.__dict__.get(\"__slotnames__\")\n    if names is not None:\n        return names\n\n    # Not cached -- calculate the value\n    names = []\n    if not hasattr(cls, \"__slots__\"):\n        # This class has no slots\n        pass\n    else:\n        # Slots found -- gather slot names from all base classes\n        for c in cls.__mro__:\n            if \"__slots__\" in c.__dict__:\n                slots = c.__dict__['__slots__']\n                # if class has a single slot, it can be given as a string\n                if isinstance(slots, basestring):\n                    slots = (slots,)\n                for name in slots:\n                    # special descriptors\n                    if name in (\"__dict__\", \"__weakref__\"):\n                        continue\n                    # mangled names\n                    elif name.startswith('__') and not name.endswith('__'):\n                        names.append('_%s%s' % (c.__name__, name))\n                    else:\n                        names.append(name)\n\n    # Cache the outcome in the class if at all possible\n    try:\n        cls.__slotnames__ = names\n    except:\n        pass # But don't die if we can't\n\n    return names\n\n# A registry of extension codes.  This is an ad-hoc compression\n# mechanism.  Whenever a global reference to <module>, <name> is about\n# to be pickled, the (<module>, <name>) tuple is looked up here to see\n# if it is a registered extension code for it.  Extension codes are\n# universal, so that the meaning of a pickle does not depend on\n# context.  (There are also some codes reserved for local use that\n# don't have this restriction.)  Codes are positive ints; 0 is\n# reserved.\n\n_extension_registry = {}                # key -> code\n_inverted_registry = {}                 # code -> key\n_extension_cache = {}                   # code -> object\n# Don't ever rebind those names:  cPickle grabs a reference to them when\n# it's initialized, and won't see a rebinding.\n\ndef add_extension(module, name, code):\n    \"\"\"Register an extension code.\"\"\"\n    code = int(code)\n    if not 1 <= code <= 0x7fffffff:\n        raise ValueError, \"code out of range\"\n    key = (module, name)\n    if (_extension_registry.get(key) == code and\n        _inverted_registry.get(code) == key):\n        return # Redundant registrations are benign\n    if key in _extension_registry:\n        raise ValueError(\"key %s is already registered with code %s\" %\n                         (key, _extension_registry[key]))\n    if code in _inverted_registry:\n        raise ValueError(\"code %s is already in use for key %s\" %\n                         (code, _inverted_registry[code]))\n    _extension_registry[key] = code\n    _inverted_registry[code] = key\n\ndef remove_extension(module, name, code):\n    \"\"\"Unregister an extension code.  For testing only.\"\"\"\n    key = (module, name)\n    if (_extension_registry.get(key) != code or\n        _inverted_registry.get(code) != key):\n        raise ValueError(\"key %s is not registered with code %s\" %\n                         (key, code))\n    del _extension_registry[key]\n    del _inverted_registry[code]\n    if code in _extension_cache:\n        del _extension_cache[code]\n\ndef clear_extension_cache():\n    _extension_cache.clear()\n\n# Standard extension code assignments\n\n# Reserved ranges\n\n# First  Last Count  Purpose\n#     1   127   127  Reserved for Python standard library\n#   128   191    64  Reserved for Zope\n#   192   239    48  Reserved for 3rd parties\n#   240   255    16  Reserved for private use (will never be assigned)\n#   256   Inf   Inf  Reserved for future assignment\n\n# Extension codes are assigned by the Python Software Foundation.\n", 
    "datetime": "\"\"\"Concrete date/time and related types -- prototype implemented in Python.\n\nSee http://www.zope.org/Members/fdrake/DateTimeWiki/FrontPage\n\nSee also http://dir.yahoo.com/Reference/calendars/\n\nFor a primer on DST, including many current DST rules, see\nhttp://webexhibits.org/daylightsaving/\n\nFor more about DST than you ever wanted to know, see\nftp://elsie.nci.nih.gov/pub/\n\nSources for time zone and DST data: http://www.twinsun.com/tz/tz-link.htm\n\nThis was originally copied from the sandbox of the CPython CVS repository.\nThanks to Tim Peters for suggesting using it.\n\"\"\"\n\nfrom __future__ import division\nimport time as _time\nimport math as _math\nimport struct as _struct\n\ndef _cmp(x, y):\n    return 0 if x == y else 1 if x > y else -1\n\ndef _round(x):\n    return int(_math.floor(x + 0.5) if x >= 0.0 else _math.ceil(x - 0.5))\n\nMINYEAR = 1\nMAXYEAR = 9999\n_MINYEARFMT = 1900\n\n# Utility functions, adapted from Python's Demo/classes/Dates.py, which\n# also assumes the current Gregorian calendar indefinitely extended in\n# both directions.  Difference:  Dates.py calls January 1 of year 0 day\n# number 1.  The code here calls January 1 of year 1 day number 1.  This is\n# to match the definition of the \"proleptic Gregorian\" calendar in Dershowitz\n# and Reingold's \"Calendrical Calculations\", where it's the base calendar\n# for all computations.  See the book for algorithms for converting between\n# proleptic Gregorian ordinals and many other calendar systems.\n\n_DAYS_IN_MONTH = [-1, 31, 28, 31, 30, 31, 30, 31, 31, 30, 31, 30, 31]\n\n_DAYS_BEFORE_MONTH = [-1]\ndbm = 0\nfor dim in _DAYS_IN_MONTH[1:]:\n    _DAYS_BEFORE_MONTH.append(dbm)\n    dbm += dim\ndel dbm, dim\n\ndef _is_leap(year):\n    \"year -> 1 if leap year, else 0.\"\n    return year % 4 == 0 and (year % 100 != 0 or year % 400 == 0)\n\ndef _days_before_year(year):\n    \"year -> number of days before January 1st of year.\"\n    y = year - 1\n    return y*365 + y//4 - y//100 + y//400\n\ndef _days_in_month(year, month):\n    \"year, month -> number of days in that month in that year.\"\n    assert 1 <= month <= 12, month\n    if month == 2 and _is_leap(year):\n        return 29\n    return _DAYS_IN_MONTH[month]\n\ndef _days_before_month(year, month):\n    \"year, month -> number of days in year preceding first day of month.\"\n    assert 1 <= month <= 12, 'month must be in 1..12'\n    return _DAYS_BEFORE_MONTH[month] + (month > 2 and _is_leap(year))\n\ndef _ymd2ord(year, month, day):\n    \"year, month, day -> ordinal, considering 01-Jan-0001 as day 1.\"\n    assert 1 <= month <= 12, 'month must be in 1..12'\n    dim = _days_in_month(year, month)\n    assert 1 <= day <= dim, ('day must be in 1..%d' % dim)\n    return (_days_before_year(year) +\n            _days_before_month(year, month) +\n            day)\n\n_DI400Y = _days_before_year(401)    # number of days in 400 years\n_DI100Y = _days_before_year(101)    #    \"    \"   \"   \" 100   \"\n_DI4Y   = _days_before_year(5)      #    \"    \"   \"   \"   4   \"\n\n# A 4-year cycle has an extra leap day over what we'd get from pasting\n# together 4 single years.\nassert _DI4Y == 4 * 365 + 1\n\n# Similarly, a 400-year cycle has an extra leap day over what we'd get from\n# pasting together 4 100-year cycles.\nassert _DI400Y == 4 * _DI100Y + 1\n\n# OTOH, a 100-year cycle has one fewer leap day than we'd get from\n# pasting together 25 4-year cycles.\nassert _DI100Y == 25 * _DI4Y - 1\n\ndef _ord2ymd(n):\n    \"ordinal -> (year, month, day), considering 01-Jan-0001 as day 1.\"\n\n    # n is a 1-based index, starting at 1-Jan-1.  The pattern of leap years\n    # repeats exactly every 400 years.  The basic strategy is to find the\n    # closest 400-year boundary at or before n, then work with the offset\n    # from that boundary to n.  Life is much clearer if we subtract 1 from\n    # n first -- then the values of n at 400-year boundaries are exactly\n    # those divisible by _DI400Y:\n    #\n    #     D  M   Y            n              n-1\n    #     -- --- ----        ----------     ----------------\n    #     31 Dec -400        -_DI400Y       -_DI400Y -1\n    #      1 Jan -399         -_DI400Y +1   -_DI400Y      400-year boundary\n    #     ...\n    #     30 Dec  000        -1             -2\n    #     31 Dec  000         0             -1\n    #      1 Jan  001         1              0            400-year boundary\n    #      2 Jan  001         2              1\n    #      3 Jan  001         3              2\n    #     ...\n    #     31 Dec  400         _DI400Y        _DI400Y -1\n    #      1 Jan  401         _DI400Y +1     _DI400Y      400-year boundary\n    n -= 1\n    n400, n = divmod(n, _DI400Y)\n    year = n400 * 400 + 1   # ..., -399, 1, 401, ...\n\n    # Now n is the (non-negative) offset, in days, from January 1 of year, to\n    # the desired date.  Now compute how many 100-year cycles precede n.\n    # Note that it's possible for n100 to equal 4!  In that case 4 full\n    # 100-year cycles precede the desired day, which implies the desired\n    # day is December 31 at the end of a 400-year cycle.\n    n100, n = divmod(n, _DI100Y)\n\n    # Now compute how many 4-year cycles precede it.\n    n4, n = divmod(n, _DI4Y)\n\n    # And now how many single years.  Again n1 can be 4, and again meaning\n    # that the desired day is December 31 at the end of the 4-year cycle.\n    n1, n = divmod(n, 365)\n\n    year += n100 * 100 + n4 * 4 + n1\n    if n1 == 4 or n100 == 4:\n        assert n == 0\n        return year-1, 12, 31\n\n    # Now the year is correct, and n is the offset from January 1.  We find\n    # the month via an estimate that's either exact or one too large.\n    leapyear = n1 == 3 and (n4 != 24 or n100 == 3)\n    assert leapyear == _is_leap(year)\n    month = (n + 50) >> 5\n    preceding = _DAYS_BEFORE_MONTH[month] + (month > 2 and leapyear)\n    if preceding > n:  # estimate is too large\n        month -= 1\n        preceding -= _DAYS_IN_MONTH[month] + (month == 2 and leapyear)\n    n -= preceding\n    assert 0 <= n < _days_in_month(year, month)\n\n    # Now the year and month are correct, and n is the offset from the\n    # start of that month:  we're done!\n    return year, month, n+1\n\n# Month and day names.  For localized versions, see the calendar module.\n_MONTHNAMES = [None, \"Jan\", \"Feb\", \"Mar\", \"Apr\", \"May\", \"Jun\",\n                     \"Jul\", \"Aug\", \"Sep\", \"Oct\", \"Nov\", \"Dec\"]\n_DAYNAMES = [None, \"Mon\", \"Tue\", \"Wed\", \"Thu\", \"Fri\", \"Sat\", \"Sun\"]\n\n\ndef _build_struct_time(y, m, d, hh, mm, ss, dstflag):\n    wday = (_ymd2ord(y, m, d) + 6) % 7\n    dnum = _days_before_month(y, m) + d\n    return _time.struct_time((y, m, d, hh, mm, ss, wday, dnum, dstflag))\n\ndef _format_time(hh, mm, ss, us):\n    # Skip trailing microseconds when us==0.\n    result = \"%02d:%02d:%02d\" % (hh, mm, ss)\n    if us:\n        result += \".%06d\" % us\n    return result\n\n# Correctly substitute for %z and %Z escapes in strftime formats.\ndef _wrap_strftime(object, format, timetuple):\n    year = timetuple[0]\n    if year < _MINYEARFMT:\n        raise ValueError(\"year=%d is before %d; the datetime strftime() \"\n                         \"methods require year >= %d\" %\n                         (year, _MINYEARFMT, _MINYEARFMT))\n    # Don't call utcoffset() or tzname() unless actually needed.\n    freplace = None  # the string to use for %f\n    zreplace = None  # the string to use for %z\n    Zreplace = None  # the string to use for %Z\n\n    # Scan format for %z and %Z escapes, replacing as needed.\n    newformat = []\n    push = newformat.append\n    i, n = 0, len(format)\n    while i < n:\n        ch = format[i]\n        i += 1\n        if ch == '%':\n            if i < n:\n                ch = format[i]\n                i += 1\n                if ch == 'f':\n                    if freplace is None:\n                        freplace = '%06d' % getattr(object,\n                                                    'microsecond', 0)\n                    newformat.append(freplace)\n                elif ch == 'z':\n                    if zreplace is None:\n                        zreplace = \"\"\n                        if hasattr(object, \"_utcoffset\"):\n                            offset = object._utcoffset()\n                            if offset is not None:\n                                sign = '+'\n                                if offset < 0:\n                                    offset = -offset\n                                    sign = '-'\n                                h, m = divmod(offset, 60)\n                                zreplace = '%c%02d%02d' % (sign, h, m)\n                    assert '%' not in zreplace\n                    newformat.append(zreplace)\n                elif ch == 'Z':\n                    if Zreplace is None:\n                        Zreplace = \"\"\n                        if hasattr(object, \"tzname\"):\n                            s = object.tzname()\n                            if s is not None:\n                                # strftime is going to have at this: escape %\n                                Zreplace = s.replace('%', '%%')\n                    newformat.append(Zreplace)\n                else:\n                    push('%')\n                    push(ch)\n            else:\n                push('%')\n        else:\n            push(ch)\n    newformat = \"\".join(newformat)\n    return _time.strftime(newformat, timetuple)\n\n# Just raise TypeError if the arg isn't None or a string.\ndef _check_tzname(name):\n    if name is not None and not isinstance(name, str):\n        raise TypeError(\"tzinfo.tzname() must return None or string, \"\n                        \"not '%s'\" % type(name))\n\n# name is the offset-producing method, \"utcoffset\" or \"dst\".\n# offset is what it returned.\n# If offset isn't None or timedelta, raises TypeError.\n# If offset is None, returns None.\n# Else offset is checked for being in range, and a whole # of minutes.\n# If it is, its integer value is returned.  Else ValueError is raised.\ndef _check_utc_offset(name, offset):\n    assert name in (\"utcoffset\", \"dst\")\n    if offset is None:\n        return\n    if not isinstance(offset, timedelta):\n        raise TypeError(\"tzinfo.%s() must return None \"\n                        \"or timedelta, not '%s'\" % (name, type(offset)))\n    days = offset.days\n    if days < -1 or days > 0:\n        offset = 1440  # trigger out-of-range\n    else:\n        seconds = days * 86400 + offset.seconds\n        minutes, seconds = divmod(seconds, 60)\n        if seconds or offset.microseconds:\n            raise ValueError(\"tzinfo.%s() must return a whole number \"\n                             \"of minutes\" % name)\n        offset = minutes\n    if not -1440 < offset < 1440:\n        raise ValueError(\"%s()=%d, must be in -1439..1439\" % (name, offset))\n    return offset\n\ndef _check_int_field(value):\n    if isinstance(value, int):\n        return value\n    if not isinstance(value, float):\n        try:\n            value = value.__int__()\n        except AttributeError:\n            pass\n        else:\n            if isinstance(value, (int, long)):\n                return value\n            raise TypeError('__int__ method should return an integer')\n        raise TypeError('an integer is required')\n    raise TypeError('integer argument expected, got float')\n\ndef _check_date_fields(year, month, day):\n    year = _check_int_field(year)\n    month = _check_int_field(month)\n    day = _check_int_field(day)\n    if not MINYEAR <= year <= MAXYEAR:\n        raise ValueError('year must be in %d..%d' % (MINYEAR, MAXYEAR), year)\n    if not 1 <= month <= 12:\n        raise ValueError('month must be in 1..12', month)\n    dim = _days_in_month(year, month)\n    if not 1 <= day <= dim:\n        raise ValueError('day must be in 1..%d' % dim, day)\n    return year, month, day\n\ndef _check_time_fields(hour, minute, second, microsecond):\n    hour = _check_int_field(hour)\n    minute = _check_int_field(minute)\n    second = _check_int_field(second)\n    microsecond = _check_int_field(microsecond)\n    if not 0 <= hour <= 23:\n        raise ValueError('hour must be in 0..23', hour)\n    if not 0 <= minute <= 59:\n        raise ValueError('minute must be in 0..59', minute)\n    if not 0 <= second <= 59:\n        raise ValueError('second must be in 0..59', second)\n    if not 0 <= microsecond <= 999999:\n        raise ValueError('microsecond must be in 0..999999', microsecond)\n    return hour, minute, second, microsecond\n\ndef _check_tzinfo_arg(tz):\n    if tz is not None and not isinstance(tz, tzinfo):\n        raise TypeError(\"tzinfo argument must be None or of a tzinfo subclass\")\n\n\n# Notes on comparison:  In general, datetime module comparison operators raise\n# TypeError when they don't know how to do a comparison themself.  If they\n# returned NotImplemented instead, comparison could (silently) fall back to\n# the default compare-objects-by-comparing-their-memory-addresses strategy,\n# and that's not helpful.  There are two exceptions:\n#\n# 1. For date and datetime, if the other object has a \"timetuple\" attr,\n#    NotImplemented is returned.  This is a hook to allow other kinds of\n#    datetime-like objects a chance to intercept the comparison.\n#\n# 2. Else __eq__ and __ne__ return False and True, respectively.  This is\n#    so opertaions like\n#\n#        x == y\n#        x != y\n#        x in sequence\n#        x not in sequence\n#        dict[x] = y\n#\n#    don't raise annoying TypeErrors just because a datetime object\n#    is part of a heterogeneous collection.  If there's no known way to\n#    compare X to a datetime, saying they're not equal is reasonable.\n\ndef _cmperror(x, y):\n    raise TypeError(\"can't compare '%s' to '%s'\" % (\n                    type(x).__name__, type(y).__name__))\n\n# This is a start at a struct tm workalike.  Goals:\n#\n# + Works the same way across platforms.\n# + Handles all the fields datetime needs handled, without 1970-2038 glitches.\n#\n# Note:  I suspect it's best if this flavor of tm does *not* try to\n# second-guess timezones or DST.  Instead fold whatever adjustments you want\n# into the minutes argument (and the constructor will normalize).\n\nclass _tmxxx:\n\n    ordinal = None\n\n    def __init__(self, year, month, day, hour=0, minute=0, second=0,\n                 microsecond=0):\n        # Normalize all the inputs, and store the normalized values.\n        if not 0 <= microsecond <= 999999:\n            carry, microsecond = divmod(microsecond, 1000000)\n            second += carry\n        if not 0 <= second <= 59:\n            carry, second = divmod(second, 60)\n            minute += carry\n        if not 0 <= minute <= 59:\n            carry, minute = divmod(minute, 60)\n            hour += carry\n        if not 0 <= hour <= 23:\n            carry, hour = divmod(hour, 24)\n            day += carry\n\n        # That was easy.  Now it gets muddy:  the proper range for day\n        # can't be determined without knowing the correct month and year,\n        # but if day is, e.g., plus or minus a million, the current month\n        # and year values make no sense (and may also be out of bounds\n        # themselves).\n        # Saying 12 months == 1 year should be non-controversial.\n        if not 1 <= month <= 12:\n            carry, month = divmod(month-1, 12)\n            year += carry\n            month += 1\n            assert 1 <= month <= 12\n\n        # Now only day can be out of bounds (year may also be out of bounds\n        # for a datetime object, but we don't care about that here).\n        # If day is out of bounds, what to do is arguable, but at least the\n        # method here is principled and explainable.\n        dim = _days_in_month(year, month)\n        if not 1 <= day <= dim:\n            # Move day-1 days from the first of the month.  First try to\n            # get off cheap if we're only one day out of range (adjustments\n            # for timezone alone can't be worse than that).\n            if day == 0:    # move back a day\n                month -= 1\n                if month > 0:\n                    day = _days_in_month(year, month)\n                else:\n                    year, month, day = year-1, 12, 31\n            elif day == dim + 1:    # move forward a day\n                month += 1\n                day = 1\n                if month > 12:\n                    month = 1\n                    year += 1\n            else:\n                self.ordinal = _ymd2ord(year, month, 1) + (day - 1)\n                year, month, day = _ord2ymd(self.ordinal)\n\n        self.year, self.month, self.day = year, month, day\n        self.hour, self.minute, self.second = hour, minute, second\n        self.microsecond = microsecond\n\nclass timedelta(object):\n    \"\"\"Represent the difference between two datetime objects.\n\n    Supported operators:\n\n    - add, subtract timedelta\n    - unary plus, minus, abs\n    - compare to timedelta\n    - multiply, divide by int/long\n\n    In addition, datetime supports subtraction of two datetime objects\n    returning a timedelta, and addition or subtraction of a datetime\n    and a timedelta giving a datetime.\n\n    Representation: (days, seconds, microseconds).  Why?  Because I\n    felt like it.\n    \"\"\"\n    __slots__ = '_days', '_seconds', '_microseconds', '_hashcode'\n\n    def __new__(cls, days=0, seconds=0, microseconds=0,\n                milliseconds=0, minutes=0, hours=0, weeks=0):\n        # Doing this efficiently and accurately in C is going to be difficult\n        # and error-prone, due to ubiquitous overflow possibilities, and that\n        # C double doesn't have enough bits of precision to represent\n        # microseconds over 10K years faithfully.  The code here tries to make\n        # explicit where go-fast assumptions can be relied on, in order to\n        # guide the C implementation; it's way more convoluted than speed-\n        # ignoring auto-overflow-to-long idiomatic Python could be.\n\n        # XXX Check that all inputs are ints, longs or floats.\n\n        # Final values, all integer.\n        # s and us fit in 32-bit signed ints; d isn't bounded.\n        d = s = us = 0\n\n        # Normalize everything to days, seconds, microseconds.\n        days += weeks*7\n        seconds += minutes*60 + hours*3600\n        microseconds += milliseconds*1000\n\n        # Get rid of all fractions, and normalize s and us.\n        # Take a deep breath <wink>.\n        if isinstance(days, float):\n            dayfrac, days = _math.modf(days)\n            daysecondsfrac, daysecondswhole = _math.modf(dayfrac * (24.*3600.))\n            assert daysecondswhole == int(daysecondswhole)  # can't overflow\n            s = int(daysecondswhole)\n            assert days == int(days)\n            d = int(days)\n        else:\n            daysecondsfrac = 0.0\n            d = days\n        assert isinstance(daysecondsfrac, float)\n        assert abs(daysecondsfrac) <= 1.0\n        assert isinstance(d, (int, long))\n        assert abs(s) <= 24 * 3600\n        # days isn't referenced again before redefinition\n\n        if isinstance(seconds, float):\n            secondsfrac, seconds = _math.modf(seconds)\n            assert seconds == int(seconds)\n            seconds = int(seconds)\n            secondsfrac += daysecondsfrac\n            assert abs(secondsfrac) <= 2.0\n        else:\n            secondsfrac = daysecondsfrac\n        # daysecondsfrac isn't referenced again\n        assert isinstance(secondsfrac, float)\n        assert abs(secondsfrac) <= 2.0\n\n        assert isinstance(seconds, (int, long))\n        days, seconds = divmod(seconds, 24*3600)\n        d += days\n        s += int(seconds)    # can't overflow\n        assert isinstance(s, int)\n        assert abs(s) <= 2 * 24 * 3600\n        # seconds isn't referenced again before redefinition\n\n        usdouble = secondsfrac * 1e6\n        assert abs(usdouble) < 2.1e6    # exact value not critical\n        # secondsfrac isn't referenced again\n\n        if isinstance(microseconds, float):\n            microseconds = _round(microseconds + usdouble)\n            seconds, microseconds = divmod(microseconds, 1000000)\n            days, seconds = divmod(seconds, 24*3600)\n            d += days\n            s += int(seconds)\n            microseconds = int(microseconds)\n        else:\n            microseconds = int(microseconds)\n            seconds, microseconds = divmod(microseconds, 1000000)\n            days, seconds = divmod(seconds, 24*3600)\n            d += days\n            s += int(seconds)\n            microseconds = _round(microseconds + usdouble)\n        assert isinstance(s, int)\n        assert isinstance(microseconds, int)\n        assert abs(s) <= 3 * 24 * 3600\n        assert abs(microseconds) < 3.1e6\n\n        # Just a little bit of carrying possible for microseconds and seconds.\n        seconds, us = divmod(microseconds, 1000000)\n        s += seconds\n        days, s = divmod(s, 24*3600)\n        d += days\n\n        assert isinstance(d, (int, long))\n        assert isinstance(s, int) and 0 <= s < 24*3600\n        assert isinstance(us, int) and 0 <= us < 1000000\n\n        if abs(d) > 999999999:\n            raise OverflowError(\"timedelta # of days is too large: %d\" % d)\n\n        self = object.__new__(cls)\n        self._days = d\n        self._seconds = s\n        self._microseconds = us\n        self._hashcode = -1\n        return self\n\n    def __repr__(self):\n        if self._microseconds:\n            return \"%s(%d, %d, %d)\" % ('datetime.' + self.__class__.__name__,\n                                       self._days,\n                                       self._seconds,\n                                       self._microseconds)\n        if self._seconds:\n            return \"%s(%d, %d)\" % ('datetime.' + self.__class__.__name__,\n                                   self._days,\n                                   self._seconds)\n        return \"%s(%d)\" % ('datetime.' + self.__class__.__name__, self._days)\n\n    def __str__(self):\n        mm, ss = divmod(self._seconds, 60)\n        hh, mm = divmod(mm, 60)\n        s = \"%d:%02d:%02d\" % (hh, mm, ss)\n        if self._days:\n            def plural(n):\n                return n, abs(n) != 1 and \"s\" or \"\"\n            s = (\"%d day%s, \" % plural(self._days)) + s\n        if self._microseconds:\n            s = s + \".%06d\" % self._microseconds\n        return s\n\n    def total_seconds(self):\n        \"\"\"Total seconds in the duration.\"\"\"\n        return ((self.days * 86400 + self.seconds) * 10**6 +\n                self.microseconds) / 10**6\n\n    # Read-only field accessors\n    @property\n    def days(self):\n        \"\"\"days\"\"\"\n        return self._days\n\n    @property\n    def seconds(self):\n        \"\"\"seconds\"\"\"\n        return self._seconds\n\n    @property\n    def microseconds(self):\n        \"\"\"microseconds\"\"\"\n        return self._microseconds\n\n    def __add__(self, other):\n        if isinstance(other, timedelta):\n            # for CPython compatibility, we cannot use\n            # our __class__ here, but need a real timedelta\n            return timedelta(self._days + other._days,\n                             self._seconds + other._seconds,\n                             self._microseconds + other._microseconds)\n        return NotImplemented\n\n    __radd__ = __add__\n\n    def __sub__(self, other):\n        if isinstance(other, timedelta):\n            # for CPython compatibility, we cannot use\n            # our __class__ here, but need a real timedelta\n            return timedelta(self._days - other._days,\n                             self._seconds - other._seconds,\n                             self._microseconds - other._microseconds)\n        return NotImplemented\n\n    def __rsub__(self, other):\n        if isinstance(other, timedelta):\n            return -self + other\n        return NotImplemented\n\n    def __neg__(self):\n        # for CPython compatibility, we cannot use\n        # our __class__ here, but need a real timedelta\n        return timedelta(-self._days,\n                         -self._seconds,\n                         -self._microseconds)\n\n    def __pos__(self):\n        return self\n\n    def __abs__(self):\n        if self._days < 0:\n            return -self\n        else:\n            return self\n\n    def __mul__(self, other):\n        if isinstance(other, (int, long)):\n            # for CPython compatibility, we cannot use\n            # our __class__ here, but need a real timedelta\n            return timedelta(self._days * other,\n                             self._seconds * other,\n                             self._microseconds * other)\n        return NotImplemented\n\n    __rmul__ = __mul__\n\n    def _to_microseconds(self):\n        return ((self._days * (24*3600) + self._seconds) * 1000000 +\n                self._microseconds)\n\n    def __div__(self, other):\n        if not isinstance(other, (int, long)):\n            return NotImplemented\n        usec = self._to_microseconds()\n        return timedelta(0, 0, usec // other)\n\n    __floordiv__ = __div__\n\n    # Comparisons of timedelta objects with other.\n\n    def __eq__(self, other):\n        if isinstance(other, timedelta):\n            return self._cmp(other) == 0\n        else:\n            return False\n\n    def __ne__(self, other):\n        if isinstance(other, timedelta):\n            return self._cmp(other) != 0\n        else:\n            return True\n\n    def __le__(self, other):\n        if isinstance(other, timedelta):\n            return self._cmp(other) <= 0\n        else:\n            _cmperror(self, other)\n\n    def __lt__(self, other):\n        if isinstance(other, timedelta):\n            return self._cmp(other) < 0\n        else:\n            _cmperror(self, other)\n\n    def __ge__(self, other):\n        if isinstance(other, timedelta):\n            return self._cmp(other) >= 0\n        else:\n            _cmperror(self, other)\n\n    def __gt__(self, other):\n        if isinstance(other, timedelta):\n            return self._cmp(other) > 0\n        else:\n            _cmperror(self, other)\n\n    def _cmp(self, other):\n        assert isinstance(other, timedelta)\n        return _cmp(self._getstate(), other._getstate())\n\n    def __hash__(self):\n        if self._hashcode == -1:\n            self._hashcode = hash(self._getstate())\n        return self._hashcode\n\n    def __nonzero__(self):\n        return (self._days != 0 or\n                self._seconds != 0 or\n                self._microseconds != 0)\n\n    # Pickle support.\n\n    def _getstate(self):\n        return (self._days, self._seconds, self._microseconds)\n\n    def __reduce__(self):\n        return (self.__class__, self._getstate())\n\ntimedelta.min = timedelta(-999999999)\ntimedelta.max = timedelta(days=999999999, hours=23, minutes=59, seconds=59,\n                          microseconds=999999)\ntimedelta.resolution = timedelta(microseconds=1)\n\nclass date(object):\n    \"\"\"Concrete date type.\n\n    Constructors:\n\n    __new__()\n    fromtimestamp()\n    today()\n    fromordinal()\n\n    Operators:\n\n    __repr__, __str__\n    __cmp__, __hash__\n    __add__, __radd__, __sub__ (add/radd only with timedelta arg)\n\n    Methods:\n\n    timetuple()\n    toordinal()\n    weekday()\n    isoweekday(), isocalendar(), isoformat()\n    ctime()\n    strftime()\n\n    Properties (readonly):\n    year, month, day\n    \"\"\"\n    __slots__ = '_year', '_month', '_day', '_hashcode'\n\n    def __new__(cls, year, month=None, day=None):\n        \"\"\"Constructor.\n\n        Arguments:\n\n        year, month, day (required, base 1)\n        \"\"\"\n        if month is None and isinstance(year, bytes) and len(year) == 4 and \\\n                1 <= ord(year[2]) <= 12:\n            # Pickle support\n            self = object.__new__(cls)\n            self.__setstate(year)\n            self._hashcode = -1\n            return self\n        year, month, day = _check_date_fields(year, month, day)\n        self = object.__new__(cls)\n        self._year = year\n        self._month = month\n        self._day = day\n        self._hashcode = -1\n        return self\n\n    # Additional constructors\n\n    @classmethod\n    def fromtimestamp(cls, t):\n        \"Construct a date from a POSIX timestamp (like time.time()).\"\n        y, m, d, hh, mm, ss, weekday, jday, dst = _time.localtime(t)\n        return cls(y, m, d)\n\n    @classmethod\n    def today(cls):\n        \"Construct a date from time.time().\"\n        t = _time.time()\n        return cls.fromtimestamp(t)\n\n    @classmethod\n    def fromordinal(cls, n):\n        \"\"\"Contruct a date from a proleptic Gregorian ordinal.\n\n        January 1 of year 1 is day 1.  Only the year, month and day are\n        non-zero in the result.\n        \"\"\"\n        y, m, d = _ord2ymd(n)\n        return cls(y, m, d)\n\n    # Conversions to string\n\n    def __repr__(self):\n        \"\"\"Convert to formal string, for repr().\n\n        >>> dt = datetime(2010, 1, 1)\n        >>> repr(dt)\n        'datetime.datetime(2010, 1, 1, 0, 0)'\n\n        >>> dt = datetime(2010, 1, 1, tzinfo=timezone.utc)\n        >>> repr(dt)\n        'datetime.datetime(2010, 1, 1, 0, 0, tzinfo=datetime.timezone.utc)'\n        \"\"\"\n        return \"%s(%d, %d, %d)\" % ('datetime.' + self.__class__.__name__,\n                                   self._year,\n                                   self._month,\n                                   self._day)\n\n    # XXX These shouldn't depend on time.localtime(), because that\n    # clips the usable dates to [1970 .. 2038).  At least ctime() is\n    # easily done without using strftime() -- that's better too because\n    # strftime(\"%c\", ...) is locale specific.\n\n    def ctime(self):\n        \"Return ctime() style string.\"\n        weekday = self.toordinal() % 7 or 7\n        return \"%s %s %2d 00:00:00 %04d\" % (\n            _DAYNAMES[weekday],\n            _MONTHNAMES[self._month],\n            self._day, self._year)\n\n    def strftime(self, format):\n        \"Format using strftime().\"\n        return _wrap_strftime(self, format, self.timetuple())\n\n    def __format__(self, fmt):\n        if not isinstance(fmt, (str, unicode)):\n            raise ValueError(\"__format__ expects str or unicode, not %s\" %\n                             fmt.__class__.__name__)\n        if len(fmt) != 0:\n            return self.strftime(fmt)\n        return str(self)\n\n    def isoformat(self):\n        \"\"\"Return the date formatted according to ISO.\n\n        This is 'YYYY-MM-DD'.\n\n        References:\n        - http://www.w3.org/TR/NOTE-datetime\n        - http://www.cl.cam.ac.uk/~mgk25/iso-time.html\n        \"\"\"\n        return \"%04d-%02d-%02d\" % (self._year, self._month, self._day)\n\n    __str__ = isoformat\n\n    # Read-only field accessors\n    @property\n    def year(self):\n        \"\"\"year (1-9999)\"\"\"\n        return self._year\n\n    @property\n    def month(self):\n        \"\"\"month (1-12)\"\"\"\n        return self._month\n\n    @property\n    def day(self):\n        \"\"\"day (1-31)\"\"\"\n        return self._day\n\n    # Standard conversions, __cmp__, __hash__ (and helpers)\n\n    def timetuple(self):\n        \"Return local time tuple compatible with time.localtime().\"\n        return _build_struct_time(self._year, self._month, self._day,\n                                  0, 0, 0, -1)\n\n    def toordinal(self):\n        \"\"\"Return proleptic Gregorian ordinal for the year, month and day.\n\n        January 1 of year 1 is day 1.  Only the year, month and day values\n        contribute to the result.\n        \"\"\"\n        return _ymd2ord(self._year, self._month, self._day)\n\n    def replace(self, year=None, month=None, day=None):\n        \"\"\"Return a new date with new values for the specified fields.\"\"\"\n        if year is None:\n            year = self._year\n        if month is None:\n            month = self._month\n        if day is None:\n            day = self._day\n        return date(year, month, day)\n\n    # Comparisons of date objects with other.\n\n    def __eq__(self, other):\n        if isinstance(other, date):\n            return self._cmp(other) == 0\n        elif hasattr(other, \"timetuple\"):\n            return NotImplemented\n        else:\n            return False\n\n    def __ne__(self, other):\n        if isinstance(other, date):\n            return self._cmp(other) != 0\n        elif hasattr(other, \"timetuple\"):\n            return NotImplemented\n        else:\n            return True\n\n    def __le__(self, other):\n        if isinstance(other, date):\n            return self._cmp(other) <= 0\n        elif hasattr(other, \"timetuple\"):\n            return NotImplemented\n        else:\n            _cmperror(self, other)\n\n    def __lt__(self, other):\n        if isinstance(other, date):\n            return self._cmp(other) < 0\n        elif hasattr(other, \"timetuple\"):\n            return NotImplemented\n        else:\n            _cmperror(self, other)\n\n    def __ge__(self, other):\n        if isinstance(other, date):\n            return self._cmp(other) >= 0\n        elif hasattr(other, \"timetuple\"):\n            return NotImplemented\n        else:\n            _cmperror(self, other)\n\n    def __gt__(self, other):\n        if isinstance(other, date):\n            return self._cmp(other) > 0\n        elif hasattr(other, \"timetuple\"):\n            return NotImplemented\n        else:\n            _cmperror(self, other)\n\n    def _cmp(self, other):\n        assert isinstance(other, date)\n        y, m, d = self._year, self._month, self._day\n        y2, m2, d2 = other._year, other._month, other._day\n        return _cmp((y, m, d), (y2, m2, d2))\n\n    def __hash__(self):\n        \"Hash.\"\n        if self._hashcode == -1:\n            self._hashcode = hash(self._getstate())\n        return self._hashcode\n\n    # Computations\n\n    def _checkOverflow(self, year):\n        if not MINYEAR <= year <= MAXYEAR:\n            raise OverflowError(\"date +/-: result year %d not in %d..%d\" %\n                                (year, MINYEAR, MAXYEAR))\n\n    def __add__(self, other):\n        \"Add a date to a timedelta.\"\n        if isinstance(other, timedelta):\n            t = _tmxxx(self._year,\n                      self._month,\n                      self._day + other.days)\n            self._checkOverflow(t.year)\n            result = date(t.year, t.month, t.day)\n            return result\n        return NotImplemented\n\n    __radd__ = __add__\n\n    def __sub__(self, other):\n        \"\"\"Subtract two dates, or a date and a timedelta.\"\"\"\n        if isinstance(other, timedelta):\n            return self + timedelta(-other.days)\n        if isinstance(other, date):\n            days1 = self.toordinal()\n            days2 = other.toordinal()\n            return timedelta(days1 - days2)\n        return NotImplemented\n\n    def weekday(self):\n        \"Return day of the week, where Monday == 0 ... Sunday == 6.\"\n        return (self.toordinal() + 6) % 7\n\n    # Day-of-the-week and week-of-the-year, according to ISO\n\n    def isoweekday(self):\n        \"Return day of the week, where Monday == 1 ... Sunday == 7.\"\n        # 1-Jan-0001 is a Monday\n        return self.toordinal() % 7 or 7\n\n    def isocalendar(self):\n        \"\"\"Return a 3-tuple containing ISO year, week number, and weekday.\n\n        The first ISO week of the year is the (Mon-Sun) week\n        containing the year's first Thursday; everything else derives\n        from that.\n\n        The first week is 1; Monday is 1 ... Sunday is 7.\n\n        ISO calendar algorithm taken from\n        http://www.phys.uu.nl/~vgent/calendar/isocalendar.htm\n        \"\"\"\n        year = self._year\n        week1monday = _isoweek1monday(year)\n        today = _ymd2ord(self._year, self._month, self._day)\n        # Internally, week and day have origin 0\n        week, day = divmod(today - week1monday, 7)\n        if week < 0:\n            year -= 1\n            week1monday = _isoweek1monday(year)\n            week, day = divmod(today - week1monday, 7)\n        elif week >= 52:\n            if today >= _isoweek1monday(year+1):\n                year += 1\n                week = 0\n        return year, week+1, day+1\n\n    # Pickle support.\n\n    def _getstate(self):\n        yhi, ylo = divmod(self._year, 256)\n        return (_struct.pack('4B', yhi, ylo, self._month, self._day),)\n\n    def __setstate(self, string):\n        yhi, ylo, self._month, self._day = (ord(string[0]), ord(string[1]),\n                                            ord(string[2]), ord(string[3]))\n        self._year = yhi * 256 + ylo\n\n    def __reduce__(self):\n        return (self.__class__, self._getstate())\n\n_date_class = date  # so functions w/ args named \"date\" can get at the class\n\ndate.min = date(1, 1, 1)\ndate.max = date(9999, 12, 31)\ndate.resolution = timedelta(days=1)\n\nclass tzinfo(object):\n    \"\"\"Abstract base class for time zone info classes.\n\n    Subclasses must override the name(), utcoffset() and dst() methods.\n    \"\"\"\n    __slots__ = ()\n\n    def tzname(self, dt):\n        \"datetime -> string name of time zone.\"\n        raise NotImplementedError(\"tzinfo subclass must override tzname()\")\n\n    def utcoffset(self, dt):\n        \"datetime -> minutes east of UTC (negative for west of UTC)\"\n        raise NotImplementedError(\"tzinfo subclass must override utcoffset()\")\n\n    def dst(self, dt):\n        \"\"\"datetime -> DST offset in minutes east of UTC.\n\n        Return 0 if DST not in effect.  utcoffset() must include the DST\n        offset.\n        \"\"\"\n        raise NotImplementedError(\"tzinfo subclass must override dst()\")\n\n    def fromutc(self, dt):\n        \"datetime in UTC -> datetime in local time.\"\n\n        if not isinstance(dt, datetime):\n            raise TypeError(\"fromutc() requires a datetime argument\")\n        if dt.tzinfo is not self:\n            raise ValueError(\"dt.tzinfo is not self\")\n\n        dtoff = dt.utcoffset()\n        if dtoff is None:\n            raise ValueError(\"fromutc() requires a non-None utcoffset() \"\n                             \"result\")\n\n        # See the long comment block at the end of this file for an\n        # explanation of this algorithm.\n        dtdst = dt.dst()\n        if dtdst is None:\n            raise ValueError(\"fromutc() requires a non-None dst() result\")\n        delta = dtoff - dtdst\n        if delta:\n            dt += delta\n            dtdst = dt.dst()\n            if dtdst is None:\n                raise ValueError(\"fromutc(): dt.dst gave inconsistent \"\n                                 \"results; cannot convert\")\n        if dtdst:\n            return dt + dtdst\n        else:\n            return dt\n\n    # Pickle support.\n\n    def __reduce__(self):\n        getinitargs = getattr(self, \"__getinitargs__\", None)\n        if getinitargs:\n            args = getinitargs()\n        else:\n            args = ()\n        getstate = getattr(self, \"__getstate__\", None)\n        if getstate:\n            state = getstate()\n        else:\n            state = getattr(self, \"__dict__\", None) or None\n        if state is None:\n            return (self.__class__, args)\n        else:\n            return (self.__class__, args, state)\n\n_tzinfo_class = tzinfo\n\nclass time(object):\n    \"\"\"Time with time zone.\n\n    Constructors:\n\n    __new__()\n\n    Operators:\n\n    __repr__, __str__\n    __cmp__, __hash__\n\n    Methods:\n\n    strftime()\n    isoformat()\n    utcoffset()\n    tzname()\n    dst()\n\n    Properties (readonly):\n    hour, minute, second, microsecond, tzinfo\n    \"\"\"\n    __slots__ = '_hour', '_minute', '_second', '_microsecond', '_tzinfo', '_hashcode'\n\n    def __new__(cls, hour=0, minute=0, second=0, microsecond=0, tzinfo=None):\n        \"\"\"Constructor.\n\n        Arguments:\n\n        hour, minute (required)\n        second, microsecond (default to zero)\n        tzinfo (default to None)\n        \"\"\"\n        if isinstance(hour, bytes) and len(hour) == 6 and ord(hour[0]) < 24:\n            # Pickle support\n            self = object.__new__(cls)\n            self.__setstate(hour, minute or None)\n            self._hashcode = -1\n            return self\n        hour, minute, second, microsecond = _check_time_fields(\n            hour, minute, second, microsecond)\n        _check_tzinfo_arg(tzinfo)\n        self = object.__new__(cls)\n        self._hour = hour\n        self._minute = minute\n        self._second = second\n        self._microsecond = microsecond\n        self._tzinfo = tzinfo\n        self._hashcode = -1\n        return self\n\n    # Read-only field accessors\n    @property\n    def hour(self):\n        \"\"\"hour (0-23)\"\"\"\n        return self._hour\n\n    @property\n    def minute(self):\n        \"\"\"minute (0-59)\"\"\"\n        return self._minute\n\n    @property\n    def second(self):\n        \"\"\"second (0-59)\"\"\"\n        return self._second\n\n    @property\n    def microsecond(self):\n        \"\"\"microsecond (0-999999)\"\"\"\n        return self._microsecond\n\n    @property\n    def tzinfo(self):\n        \"\"\"timezone info object\"\"\"\n        return self._tzinfo\n\n    # Standard conversions, __hash__ (and helpers)\n\n    # Comparisons of time objects with other.\n\n    def __eq__(self, other):\n        if isinstance(other, time):\n            return self._cmp(other) == 0\n        else:\n            return False\n\n    def __ne__(self, other):\n        if isinstance(other, time):\n            return self._cmp(other) != 0\n        else:\n            return True\n\n    def __le__(self, other):\n        if isinstance(other, time):\n            return self._cmp(other) <= 0\n        else:\n            _cmperror(self, other)\n\n    def __lt__(self, other):\n        if isinstance(other, time):\n            return self._cmp(other) < 0\n        else:\n            _cmperror(self, other)\n\n    def __ge__(self, other):\n        if isinstance(other, time):\n            return self._cmp(other) >= 0\n        else:\n            _cmperror(self, other)\n\n    def __gt__(self, other):\n        if isinstance(other, time):\n            return self._cmp(other) > 0\n        else:\n            _cmperror(self, other)\n\n    def _cmp(self, other):\n        assert isinstance(other, time)\n        mytz = self._tzinfo\n        ottz = other._tzinfo\n        myoff = otoff = None\n\n        if mytz is ottz:\n            base_compare = True\n        else:\n            myoff = self._utcoffset()\n            otoff = other._utcoffset()\n            base_compare = myoff == otoff\n\n        if base_compare:\n            return _cmp((self._hour, self._minute, self._second,\n                         self._microsecond),\n                        (other._hour, other._minute, other._second,\n                         other._microsecond))\n        if myoff is None or otoff is None:\n            raise TypeError(\"can't compare offset-naive and offset-aware times\")\n        myhhmm = self._hour * 60 + self._minute - myoff\n        othhmm = other._hour * 60 + other._minute - otoff\n        return _cmp((myhhmm, self._second, self._microsecond),\n                    (othhmm, other._second, other._microsecond))\n\n    def __hash__(self):\n        \"\"\"Hash.\"\"\"\n        if self._hashcode == -1:\n            tzoff = self._utcoffset()\n            if not tzoff:  # zero or None\n                self._hashcode = hash(self._getstate()[0])\n            else:\n                h, m = divmod(self.hour * 60 + self.minute - tzoff, 60)\n                if 0 <= h < 24:\n                    self._hashcode = hash(time(h, m, self.second, self.microsecond))\n                else:\n                    self._hashcode = hash((h, m, self.second, self.microsecond))\n        return self._hashcode\n\n    # Conversion to string\n\n    def _tzstr(self, sep=\":\"):\n        \"\"\"Return formatted timezone offset (+xx:xx) or None.\"\"\"\n        off = self._utcoffset()\n        if off is not None:\n            if off < 0:\n                sign = \"-\"\n                off = -off\n            else:\n                sign = \"+\"\n            hh, mm = divmod(off, 60)\n            assert 0 <= hh < 24\n            off = \"%s%02d%s%02d\" % (sign, hh, sep, mm)\n        return off\n\n    def __repr__(self):\n        \"\"\"Convert to formal string, for repr().\"\"\"\n        if self._microsecond != 0:\n            s = \", %d, %d\" % (self._second, self._microsecond)\n        elif self._second != 0:\n            s = \", %d\" % self._second\n        else:\n            s = \"\"\n        s= \"%s(%d, %d%s)\" % ('datetime.' + self.__class__.__name__,\n                             self._hour, self._minute, s)\n        if self._tzinfo is not None:\n            assert s[-1:] == \")\"\n            s = s[:-1] + \", tzinfo=%r\" % self._tzinfo + \")\"\n        return s\n\n    def isoformat(self):\n        \"\"\"Return the time formatted according to ISO.\n\n        This is 'HH:MM:SS.mmmmmm+zz:zz', or 'HH:MM:SS+zz:zz' if\n        self.microsecond == 0.\n        \"\"\"\n        s = _format_time(self._hour, self._minute, self._second,\n                         self._microsecond)\n        tz = self._tzstr()\n        if tz:\n            s += tz\n        return s\n\n    __str__ = isoformat\n\n    def strftime(self, format):\n        \"\"\"Format using strftime().  The date part of the timestamp passed\n        to underlying strftime should not be used.\n        \"\"\"\n        # The year must be >= _MINYEARFMT else Python's strftime implementation\n        # can raise a bogus exception.\n        timetuple = (1900, 1, 1,\n                     self._hour, self._minute, self._second,\n                     0, 1, -1)\n        return _wrap_strftime(self, format, timetuple)\n\n    def __format__(self, fmt):\n        if not isinstance(fmt, (str, unicode)):\n            raise ValueError(\"__format__ expects str or unicode, not %s\" %\n                             fmt.__class__.__name__)\n        if len(fmt) != 0:\n            return self.strftime(fmt)\n        return str(self)\n\n    # Timezone functions\n\n    def utcoffset(self):\n        \"\"\"Return the timezone offset in minutes east of UTC (negative west of\n        UTC).\"\"\"\n        if self._tzinfo is None:\n            return None\n        offset = self._tzinfo.utcoffset(None)\n        offset = _check_utc_offset(\"utcoffset\", offset)\n        if offset is not None:\n            offset = timedelta(minutes=offset)\n        return offset\n\n    # Return an integer (or None) instead of a timedelta (or None).\n    def _utcoffset(self):\n        if self._tzinfo is None:\n            return None\n        offset = self._tzinfo.utcoffset(None)\n        offset = _check_utc_offset(\"utcoffset\", offset)\n        return offset\n\n    def tzname(self):\n        \"\"\"Return the timezone name.\n\n        Note that the name is 100% informational -- there's no requirement that\n        it mean anything in particular. For example, \"GMT\", \"UTC\", \"-500\",\n        \"-5:00\", \"EDT\", \"US/Eastern\", \"America/New York\" are all valid replies.\n        \"\"\"\n        if self._tzinfo is None:\n            return None\n        name = self._tzinfo.tzname(None)\n        _check_tzname(name)\n        return name\n\n    def dst(self):\n        \"\"\"Return 0 if DST is not in effect, or the DST offset (in minutes\n        eastward) if DST is in effect.\n\n        This is purely informational; the DST offset has already been added to\n        the UTC offset returned by utcoffset() if applicable, so there's no\n        need to consult dst() unless you're interested in displaying the DST\n        info.\n        \"\"\"\n        if self._tzinfo is None:\n            return None\n        offset = self._tzinfo.dst(None)\n        offset = _check_utc_offset(\"dst\", offset)\n        if offset is not None:\n            offset = timedelta(minutes=offset)\n        return offset\n\n    # Return an integer (or None) instead of a timedelta (or None).\n    def _dst(self):\n        if self._tzinfo is None:\n            return None\n        offset = self._tzinfo.dst(None)\n        offset = _check_utc_offset(\"dst\", offset)\n        return offset\n\n    def replace(self, hour=None, minute=None, second=None, microsecond=None,\n                tzinfo=True):\n        \"\"\"Return a new time with new values for the specified fields.\"\"\"\n        if hour is None:\n            hour = self.hour\n        if minute is None:\n            minute = self.minute\n        if second is None:\n            second = self.second\n        if microsecond is None:\n            microsecond = self.microsecond\n        if tzinfo is True:\n            tzinfo = self.tzinfo\n        return time(hour, minute, second, microsecond, tzinfo)\n\n    def __nonzero__(self):\n        if self.second or self.microsecond:\n            return True\n        offset = self._utcoffset() or 0\n        return self.hour * 60 + self.minute != offset\n\n    # Pickle support.\n\n    def _getstate(self):\n        us2, us3 = divmod(self._microsecond, 256)\n        us1, us2 = divmod(us2, 256)\n        basestate = _struct.pack('6B', self._hour, self._minute, self._second,\n                                       us1, us2, us3)\n        if self._tzinfo is None:\n            return (basestate,)\n        else:\n            return (basestate, self._tzinfo)\n\n    def __setstate(self, string, tzinfo):\n        if tzinfo is not None and not isinstance(tzinfo, _tzinfo_class):\n            raise TypeError(\"bad tzinfo state arg\")\n        self._hour, self._minute, self._second, us1, us2, us3 = (\n            ord(string[0]), ord(string[1]), ord(string[2]),\n            ord(string[3]), ord(string[4]), ord(string[5]))\n        self._microsecond = (((us1 << 8) | us2) << 8) | us3\n        self._tzinfo = tzinfo\n\n    def __reduce__(self):\n        return (time, self._getstate())\n\n_time_class = time  # so functions w/ args named \"time\" can get at the class\n\ntime.min = time(0, 0, 0)\ntime.max = time(23, 59, 59, 999999)\ntime.resolution = timedelta(microseconds=1)\n\nclass datetime(date):\n    \"\"\"datetime(year, month, day[, hour[, minute[, second[, microsecond[,tzinfo]]]]])\n\n    The year, month and day arguments are required. tzinfo may be None, or an\n    instance of a tzinfo subclass. The remaining arguments may be ints or longs.\n    \"\"\"\n    __slots__ = date.__slots__ + time.__slots__\n\n    def __new__(cls, year, month=None, day=None, hour=0, minute=0, second=0,\n                microsecond=0, tzinfo=None):\n        if isinstance(year, bytes) and len(year) == 10 and \\\n                1 <= ord(year[2]) <= 12:\n            # Pickle support\n            self = object.__new__(cls)\n            self.__setstate(year, month)\n            self._hashcode = -1\n            return self\n        year, month, day = _check_date_fields(year, month, day)\n        hour, minute, second, microsecond = _check_time_fields(\n            hour, minute, second, microsecond)\n        _check_tzinfo_arg(tzinfo)\n        self = object.__new__(cls)\n        self._year = year\n        self._month = month\n        self._day = day\n        self._hour = hour\n        self._minute = minute\n        self._second = second\n        self._microsecond = microsecond\n        self._tzinfo = tzinfo\n        self._hashcode = -1\n        return self\n\n    # Read-only field accessors\n    @property\n    def hour(self):\n        \"\"\"hour (0-23)\"\"\"\n        return self._hour\n\n    @property\n    def minute(self):\n        \"\"\"minute (0-59)\"\"\"\n        return self._minute\n\n    @property\n    def second(self):\n        \"\"\"second (0-59)\"\"\"\n        return self._second\n\n    @property\n    def microsecond(self):\n        \"\"\"microsecond (0-999999)\"\"\"\n        return self._microsecond\n\n    @property\n    def tzinfo(self):\n        \"\"\"timezone info object\"\"\"\n        return self._tzinfo\n\n    @classmethod\n    def fromtimestamp(cls, timestamp, tz=None):\n        \"\"\"Construct a datetime from a POSIX timestamp (like time.time()).\n\n        A timezone info object may be passed in as well.\n        \"\"\"\n\n        _check_tzinfo_arg(tz)\n\n        converter = _time.localtime if tz is None else _time.gmtime\n\n        if isinstance(timestamp, int):\n            us = 0\n        else:\n            t_full = timestamp\n            timestamp = int(_math.floor(timestamp))\n            frac = t_full - timestamp\n            us = _round(frac * 1e6)\n\n        # If timestamp is less than one microsecond smaller than a\n        # full second, us can be rounded up to 1000000.  In this case,\n        # roll over to seconds, otherwise, ValueError is raised\n        # by the constructor.\n        if us == 1000000:\n            timestamp += 1\n            us = 0\n        y, m, d, hh, mm, ss, weekday, jday, dst = converter(timestamp)\n        ss = min(ss, 59)    # clamp out leap seconds if the platform has them\n        result = cls(y, m, d, hh, mm, ss, us, tz)\n        if tz is not None:\n            result = tz.fromutc(result)\n        return result\n\n    @classmethod\n    def utcfromtimestamp(cls, t):\n        \"Construct a UTC datetime from a POSIX timestamp (like time.time()).\"\n        if isinstance(t, int):\n            us = 0\n        else:\n            t_full = t\n            t = int(_math.floor(t))\n            frac = t_full - t\n            us = _round(frac * 1e6)\n\n        # If timestamp is less than one microsecond smaller than a\n        # full second, us can be rounded up to 1000000.  In this case,\n        # roll over to seconds, otherwise, ValueError is raised\n        # by the constructor.\n        if us == 1000000:\n            t += 1\n            us = 0\n        y, m, d, hh, mm, ss, weekday, jday, dst = _time.gmtime(t)\n        ss = min(ss, 59)    # clamp out leap seconds if the platform has them\n        return cls(y, m, d, hh, mm, ss, us)\n\n    @classmethod\n    def now(cls, tz=None):\n        \"Construct a datetime from time.time() and optional time zone info.\"\n        t = _time.time()\n        return cls.fromtimestamp(t, tz)\n\n    @classmethod\n    def utcnow(cls):\n        \"Construct a UTC datetime from time.time().\"\n        t = _time.time()\n        return cls.utcfromtimestamp(t)\n\n    @classmethod\n    def combine(cls, date, time):\n        \"Construct a datetime from a given date and a given time.\"\n        if not isinstance(date, _date_class):\n            raise TypeError(\"date argument must be a date instance\")\n        if not isinstance(time, _time_class):\n            raise TypeError(\"time argument must be a time instance\")\n        return cls(date.year, date.month, date.day,\n                   time.hour, time.minute, time.second, time.microsecond,\n                   time.tzinfo)\n\n    def timetuple(self):\n        \"Return local time tuple compatible with time.localtime().\"\n        dst = self._dst()\n        if dst is None:\n            dst = -1\n        elif dst:\n            dst = 1\n        return _build_struct_time(self.year, self.month, self.day,\n                                  self.hour, self.minute, self.second,\n                                  dst)\n\n    def utctimetuple(self):\n        \"Return UTC time tuple compatible with time.gmtime().\"\n        y, m, d = self.year, self.month, self.day\n        hh, mm, ss = self.hour, self.minute, self.second\n        offset = self._utcoffset()\n        if offset:  # neither None nor 0\n            tm = _tmxxx(y, m, d, hh, mm - offset)\n            y, m, d = tm.year, tm.month, tm.day\n            hh, mm = tm.hour, tm.minute\n        return _build_struct_time(y, m, d, hh, mm, ss, 0)\n\n    def date(self):\n        \"Return the date part.\"\n        return date(self._year, self._month, self._day)\n\n    def time(self):\n        \"Return the time part, with tzinfo None.\"\n        return time(self.hour, self.minute, self.second, self.microsecond)\n\n    def timetz(self):\n        \"Return the time part, with same tzinfo.\"\n        return time(self.hour, self.minute, self.second, self.microsecond,\n                    self._tzinfo)\n\n    def replace(self, year=None, month=None, day=None, hour=None,\n                minute=None, second=None, microsecond=None, tzinfo=True):\n        \"\"\"Return a new datetime with new values for the specified fields.\"\"\"\n        if year is None:\n            year = self.year\n        if month is None:\n            month = self.month\n        if day is None:\n            day = self.day\n        if hour is None:\n            hour = self.hour\n        if minute is None:\n            minute = self.minute\n        if second is None:\n            second = self.second\n        if microsecond is None:\n            microsecond = self.microsecond\n        if tzinfo is True:\n            tzinfo = self.tzinfo\n        return datetime(year, month, day, hour, minute, second, microsecond,\n                        tzinfo)\n\n    def astimezone(self, tz):\n        if not isinstance(tz, tzinfo):\n            raise TypeError(\"tz argument must be an instance of tzinfo\")\n\n        mytz = self.tzinfo\n        if mytz is None:\n            raise ValueError(\"astimezone() requires an aware datetime\")\n\n        if tz is mytz:\n            return self\n\n        # Convert self to UTC, and attach the new time zone object.\n        myoffset = self.utcoffset()\n        if myoffset is None:\n            raise ValueError(\"astimezone() requires an aware datetime\")\n        utc = (self - myoffset).replace(tzinfo=tz)\n\n        # Convert from UTC to tz's local time.\n        return tz.fromutc(utc)\n\n    # Ways to produce a string.\n\n    def ctime(self):\n        \"Return ctime() style string.\"\n        weekday = self.toordinal() % 7 or 7\n        return \"%s %s %2d %02d:%02d:%02d %04d\" % (\n            _DAYNAMES[weekday],\n            _MONTHNAMES[self._month],\n            self._day,\n            self._hour, self._minute, self._second,\n            self._year)\n\n    def isoformat(self, sep='T'):\n        \"\"\"Return the time formatted according to ISO.\n\n        This is 'YYYY-MM-DD HH:MM:SS.mmmmmm', or 'YYYY-MM-DD HH:MM:SS' if\n        self.microsecond == 0.\n\n        If self.tzinfo is not None, the UTC offset is also attached, giving\n        'YYYY-MM-DD HH:MM:SS.mmmmmm+HH:MM' or 'YYYY-MM-DD HH:MM:SS+HH:MM'.\n\n        Optional argument sep specifies the separator between date and\n        time, default 'T'.\n        \"\"\"\n        s = (\"%04d-%02d-%02d%c\" % (self._year, self._month, self._day, sep) +\n             _format_time(self._hour, self._minute, self._second,\n                          self._microsecond))\n        off = self._utcoffset()\n        if off is not None:\n            if off < 0:\n                sign = \"-\"\n                off = -off\n            else:\n                sign = \"+\"\n            hh, mm = divmod(off, 60)\n            s += \"%s%02d:%02d\" % (sign, hh, mm)\n        return s\n\n    def __repr__(self):\n        \"\"\"Convert to formal string, for repr().\"\"\"\n        L = [self._year, self._month, self._day,  # These are never zero\n             self._hour, self._minute, self._second, self._microsecond]\n        if L[-1] == 0:\n            del L[-1]\n        if L[-1] == 0:\n            del L[-1]\n        s = \", \".join(map(str, L))\n        s = \"%s(%s)\" % ('datetime.' + self.__class__.__name__, s)\n        if self._tzinfo is not None:\n            assert s[-1:] == \")\"\n            s = s[:-1] + \", tzinfo=%r\" % self._tzinfo + \")\"\n        return s\n\n    def __str__(self):\n        \"Convert to string, for str().\"\n        return self.isoformat(sep=' ')\n\n    @classmethod\n    def strptime(cls, date_string, format):\n        'string, format -> new datetime parsed from a string (like time.strptime()).'\n        from _strptime import _strptime\n        # _strptime._strptime returns a two-element tuple.  The first\n        # element is a time.struct_time object.  The second is the\n        # microseconds (which are not defined for time.struct_time).\n        struct, micros = _strptime(date_string, format)\n        return cls(*(struct[0:6] + (micros,)))\n\n    def utcoffset(self):\n        \"\"\"Return the timezone offset in minutes east of UTC (negative west of\n        UTC).\"\"\"\n        if self._tzinfo is None:\n            return None\n        offset = self._tzinfo.utcoffset(self)\n        offset = _check_utc_offset(\"utcoffset\", offset)\n        if offset is not None:\n            offset = timedelta(minutes=offset)\n        return offset\n\n    # Return an integer (or None) instead of a timedelta (or None).\n    def _utcoffset(self):\n        if self._tzinfo is None:\n            return None\n        offset = self._tzinfo.utcoffset(self)\n        offset = _check_utc_offset(\"utcoffset\", offset)\n        return offset\n\n    def tzname(self):\n        \"\"\"Return the timezone name.\n\n        Note that the name is 100% informational -- there's no requirement that\n        it mean anything in particular. For example, \"GMT\", \"UTC\", \"-500\",\n        \"-5:00\", \"EDT\", \"US/Eastern\", \"America/New York\" are all valid replies.\n        \"\"\"\n        if self._tzinfo is None:\n            return None\n        name = self._tzinfo.tzname(self)\n        _check_tzname(name)\n        return name\n\n    def dst(self):\n        \"\"\"Return 0 if DST is not in effect, or the DST offset (in minutes\n        eastward) if DST is in effect.\n\n        This is purely informational; the DST offset has already been added to\n        the UTC offset returned by utcoffset() if applicable, so there's no\n        need to consult dst() unless you're interested in displaying the DST\n        info.\n        \"\"\"\n        if self._tzinfo is None:\n            return None\n        offset = self._tzinfo.dst(self)\n        offset = _check_utc_offset(\"dst\", offset)\n        if offset is not None:\n            offset = timedelta(minutes=offset)\n        return offset\n\n    # Return an integer (or None) instead of a timedelta (or None).\n    def _dst(self):\n        if self._tzinfo is None:\n            return None\n        offset = self._tzinfo.dst(self)\n        offset = _check_utc_offset(\"dst\", offset)\n        return offset\n\n    # Comparisons of datetime objects with other.\n\n    def __eq__(self, other):\n        if isinstance(other, datetime):\n            return self._cmp(other) == 0\n        elif hasattr(other, \"timetuple\") and not isinstance(other, date):\n            return NotImplemented\n        else:\n            return False\n\n    def __ne__(self, other):\n        if isinstance(other, datetime):\n            return self._cmp(other) != 0\n        elif hasattr(other, \"timetuple\") and not isinstance(other, date):\n            return NotImplemented\n        else:\n            return True\n\n    def __le__(self, other):\n        if isinstance(other, datetime):\n            return self._cmp(other) <= 0\n        elif hasattr(other, \"timetuple\") and not isinstance(other, date):\n            return NotImplemented\n        else:\n            _cmperror(self, other)\n\n    def __lt__(self, other):\n        if isinstance(other, datetime):\n            return self._cmp(other) < 0\n        elif hasattr(other, \"timetuple\") and not isinstance(other, date):\n            return NotImplemented\n        else:\n            _cmperror(self, other)\n\n    def __ge__(self, other):\n        if isinstance(other, datetime):\n            return self._cmp(other) >= 0\n        elif hasattr(other, \"timetuple\") and not isinstance(other, date):\n            return NotImplemented\n        else:\n            _cmperror(self, other)\n\n    def __gt__(self, other):\n        if isinstance(other, datetime):\n            return self._cmp(other) > 0\n        elif hasattr(other, \"timetuple\") and not isinstance(other, date):\n            return NotImplemented\n        else:\n            _cmperror(self, other)\n\n    def _cmp(self, other):\n        assert isinstance(other, datetime)\n        mytz = self._tzinfo\n        ottz = other._tzinfo\n        myoff = otoff = None\n\n        if mytz is ottz:\n            base_compare = True\n        else:\n            if mytz is not None:\n                myoff = self._utcoffset()\n            if ottz is not None:\n                otoff = other._utcoffset()\n            base_compare = myoff == otoff\n\n        if base_compare:\n            return _cmp((self._year, self._month, self._day,\n                         self._hour, self._minute, self._second,\n                         self._microsecond),\n                        (other._year, other._month, other._day,\n                         other._hour, other._minute, other._second,\n                         other._microsecond))\n        if myoff is None or otoff is None:\n            raise TypeError(\"can't compare offset-naive and offset-aware datetimes\")\n        # XXX What follows could be done more efficiently...\n        diff = self - other     # this will take offsets into account\n        if diff.days < 0:\n            return -1\n        return diff and 1 or 0\n\n    def __add__(self, other):\n        \"Add a datetime and a timedelta.\"\n        if not isinstance(other, timedelta):\n            return NotImplemented\n        t = _tmxxx(self._year,\n                  self._month,\n                  self._day + other.days,\n                  self._hour,\n                  self._minute,\n                  self._second + other.seconds,\n                  self._microsecond + other.microseconds)\n        self._checkOverflow(t.year)\n        result = datetime(t.year, t.month, t.day,\n                                t.hour, t.minute, t.second,\n                                t.microsecond, tzinfo=self._tzinfo)\n        return result\n\n    __radd__ = __add__\n\n    def __sub__(self, other):\n        \"Subtract two datetimes, or a datetime and a timedelta.\"\n        if not isinstance(other, datetime):\n            if isinstance(other, timedelta):\n                return self + -other\n            return NotImplemented\n\n        days1 = self.toordinal()\n        days2 = other.toordinal()\n        secs1 = self._second + self._minute * 60 + self._hour * 3600\n        secs2 = other._second + other._minute * 60 + other._hour * 3600\n        base = timedelta(days1 - days2,\n                         secs1 - secs2,\n                         self._microsecond - other._microsecond)\n        if self._tzinfo is other._tzinfo:\n            return base\n        myoff = self._utcoffset()\n        otoff = other._utcoffset()\n        if myoff == otoff:\n            return base\n        if myoff is None or otoff is None:\n            raise TypeError(\"can't subtract offset-naive and offset-aware datetimes\")\n        return base + timedelta(minutes = otoff-myoff)\n\n    def __hash__(self):\n        if self._hashcode == -1:\n            tzoff = self._utcoffset()\n            if tzoff is None:\n                self._hashcode = hash(self._getstate()[0])\n            else:\n                days = _ymd2ord(self.year, self.month, self.day)\n                seconds = self.hour * 3600 + (self.minute - tzoff) * 60 + self.second\n                self._hashcode = hash(timedelta(days, seconds, self.microsecond))\n        return self._hashcode\n\n    # Pickle support.\n\n    def _getstate(self):\n        yhi, ylo = divmod(self._year, 256)\n        us2, us3 = divmod(self._microsecond, 256)\n        us1, us2 = divmod(us2, 256)\n        basestate = _struct.pack('10B', yhi, ylo, self._month, self._day,\n                                        self._hour, self._minute, self._second,\n                                        us1, us2, us3)\n        if self._tzinfo is None:\n            return (basestate,)\n        else:\n            return (basestate, self._tzinfo)\n\n    def __setstate(self, string, tzinfo):\n        if tzinfo is not None and not isinstance(tzinfo, _tzinfo_class):\n            raise TypeError(\"bad tzinfo state arg\")\n        (yhi, ylo, self._month, self._day, self._hour,\n            self._minute, self._second, us1, us2, us3) = (ord(string[0]),\n                ord(string[1]), ord(string[2]), ord(string[3]),\n                ord(string[4]), ord(string[5]), ord(string[6]),\n                ord(string[7]), ord(string[8]), ord(string[9]))\n        self._year = yhi * 256 + ylo\n        self._microsecond = (((us1 << 8) | us2) << 8) | us3\n        self._tzinfo = tzinfo\n\n    def __reduce__(self):\n        return (self.__class__, self._getstate())\n\n\ndatetime.min = datetime(1, 1, 1)\ndatetime.max = datetime(9999, 12, 31, 23, 59, 59, 999999)\ndatetime.resolution = timedelta(microseconds=1)\n\n\ndef _isoweek1monday(year):\n    # Helper to calculate the day number of the Monday starting week 1\n    # XXX This could be done more efficiently\n    THURSDAY = 3\n    firstday = _ymd2ord(year, 1, 1)\n    firstweekday = (firstday + 6) % 7  # See weekday() above\n    week1monday = firstday - firstweekday\n    if firstweekday > THURSDAY:\n        week1monday += 7\n    return week1monday\n\n\"\"\"\nSome time zone algebra.  For a datetime x, let\n    x.n = x stripped of its timezone -- its naive time.\n    x.o = x.utcoffset(), and assuming that doesn't raise an exception or\n          return None\n    x.d = x.dst(), and assuming that doesn't raise an exception or\n          return None\n    x.s = x's standard offset, x.o - x.d\n\nNow some derived rules, where k is a duration (timedelta).\n\n1. x.o = x.s + x.d\n   This follows from the definition of x.s.\n\n2. If x and y have the same tzinfo member, x.s = y.s.\n   This is actually a requirement, an assumption we need to make about\n   sane tzinfo classes.\n\n3. The naive UTC time corresponding to x is x.n - x.o.\n   This is again a requirement for a sane tzinfo class.\n\n4. (x+k).s = x.s\n   This follows from #2, and that datimetimetz+timedelta preserves tzinfo.\n\n5. (x+k).n = x.n + k\n   Again follows from how arithmetic is defined.\n\nNow we can explain tz.fromutc(x).  Let's assume it's an interesting case\n(meaning that the various tzinfo methods exist, and don't blow up or return\nNone when called).\n\nThe function wants to return a datetime y with timezone tz, equivalent to x.\nx is already in UTC.\n\nBy #3, we want\n\n    y.n - y.o = x.n                             [1]\n\nThe algorithm starts by attaching tz to x.n, and calling that y.  So\nx.n = y.n at the start.  Then it wants to add a duration k to y, so that [1]\nbecomes true; in effect, we want to solve [2] for k:\n\n   (y+k).n - (y+k).o = x.n                      [2]\n\nBy #1, this is the same as\n\n   (y+k).n - ((y+k).s + (y+k).d) = x.n          [3]\n\nBy #5, (y+k).n = y.n + k, which equals x.n + k because x.n=y.n at the start.\nSubstituting that into [3],\n\n   x.n + k - (y+k).s - (y+k).d = x.n; the x.n terms cancel, leaving\n   k - (y+k).s - (y+k).d = 0; rearranging,\n   k = (y+k).s - (y+k).d; by #4, (y+k).s == y.s, so\n   k = y.s - (y+k).d\n\nOn the RHS, (y+k).d can't be computed directly, but y.s can be, and we\napproximate k by ignoring the (y+k).d term at first.  Note that k can't be\nvery large, since all offset-returning methods return a duration of magnitude\nless than 24 hours.  For that reason, if y is firmly in std time, (y+k).d must\nbe 0, so ignoring it has no consequence then.\n\nIn any case, the new value is\n\n    z = y + y.s                                 [4]\n\nIt's helpful to step back at look at [4] from a higher level:  it's simply\nmapping from UTC to tz's standard time.\n\nAt this point, if\n\n    z.n - z.o = x.n                             [5]\n\nwe have an equivalent time, and are almost done.  The insecurity here is\nat the start of daylight time.  Picture US Eastern for concreteness.  The wall\ntime jumps from 1:59 to 3:00, and wall hours of the form 2:MM don't make good\nsense then.  The docs ask that an Eastern tzinfo class consider such a time to\nbe EDT (because it's \"after 2\"), which is a redundant spelling of 1:MM EST\non the day DST starts.  We want to return the 1:MM EST spelling because that's\nthe only spelling that makes sense on the local wall clock.\n\nIn fact, if [5] holds at this point, we do have the standard-time spelling,\nbut that takes a bit of proof.  We first prove a stronger result.  What's the\ndifference between the LHS and RHS of [5]?  Let\n\n    diff = x.n - (z.n - z.o)                    [6]\n\nNow\n    z.n =                       by [4]\n    (y + y.s).n =               by #5\n    y.n + y.s =                 since y.n = x.n\n    x.n + y.s =                 since z and y are have the same tzinfo member,\n                                    y.s = z.s by #2\n    x.n + z.s\n\nPlugging that back into [6] gives\n\n    diff =\n    x.n - ((x.n + z.s) - z.o) =     expanding\n    x.n - x.n - z.s + z.o =         cancelling\n    - z.s + z.o =                   by #2\n    z.d\n\nSo diff = z.d.\n\nIf [5] is true now, diff = 0, so z.d = 0 too, and we have the standard-time\nspelling we wanted in the endcase described above.  We're done.  Contrarily,\nif z.d = 0, then we have a UTC equivalent, and are also done.\n\nIf [5] is not true now, diff = z.d != 0, and z.d is the offset we need to\nadd to z (in effect, z is in tz's standard time, and we need to shift the\nlocal clock into tz's daylight time).\n\nLet\n\n    z' = z + z.d = z + diff                     [7]\n\nand we can again ask whether\n\n    z'.n - z'.o = x.n                           [8]\n\nIf so, we're done.  If not, the tzinfo class is insane, according to the\nassumptions we've made.  This also requires a bit of proof.  As before, let's\ncompute the difference between the LHS and RHS of [8] (and skipping some of\nthe justifications for the kinds of substitutions we've done several times\nalready):\n\n    diff' = x.n - (z'.n - z'.o) =           replacing z'.n via [7]\n            x.n  - (z.n + diff - z'.o) =    replacing diff via [6]\n            x.n - (z.n + x.n - (z.n - z.o) - z'.o) =\n            x.n - z.n - x.n + z.n - z.o + z'.o =    cancel x.n\n            - z.n + z.n - z.o + z'.o =              cancel z.n\n            - z.o + z'.o =                      #1 twice\n            -z.s - z.d + z'.s + z'.d =          z and z' have same tzinfo\n            z'.d - z.d\n\nSo z' is UTC-equivalent to x iff z'.d = z.d at this point.  If they are equal,\nwe've found the UTC-equivalent so are done.  In fact, we stop with [7] and\nreturn z', not bothering to compute z'.d.\n\nHow could z.d and z'd differ?  z' = z + z.d [7], so merely moving z' by\na dst() offset, and starting *from* a time already in DST (we know z.d != 0),\nwould have to change the result dst() returns:  we start in DST, and moving\na little further into it takes us out of DST.\n\nThere isn't a sane case where this can happen.  The closest it gets is at\nthe end of DST, where there's an hour in UTC with no spelling in a hybrid\ntzinfo class.  In US Eastern, that's 5:MM UTC = 0:MM EST = 1:MM EDT.  During\nthat hour, on an Eastern clock 1:MM is taken as being in standard time (6:MM\nUTC) because the docs insist on that, but 0:MM is taken as being in daylight\ntime (4:MM UTC).  There is no local time mapping to 5:MM UTC.  The local\nclock jumps from 1:59 back to 1:00 again, and repeats the 1:MM hour in\nstandard time.  Since that's what the local clock *does*, we want to map both\nUTC hours 5:MM and 6:MM to 1:MM Eastern.  The result is ambiguous\nin local time, but so it goes -- it's the way the local clock works.\n\nWhen x = 5:MM UTC is the input to this algorithm, x.o=0, y.o=-5 and y.d=0,\nso z=0:MM.  z.d=60 (minutes) then, so [5] doesn't hold and we keep going.\nz' = z + z.d = 1:MM then, and z'.d=0, and z'.d - z.d = -60 != 0 so [8]\n(correctly) concludes that z' is not UTC-equivalent to x.\n\nBecause we know z.d said z was in daylight time (else [5] would have held and\nwe would have stopped then), and we know z.d != z'.d (else [8] would have held\nand we have stopped then), and there are only 2 possible values dst() can\nreturn in Eastern, it follows that z'.d must be 0 (which it is in the example,\nbut the reasoning doesn't depend on the example -- it depends on there being\ntwo possible dst() outcomes, one zero and the other non-zero).  Therefore\nz' must be in standard time, and is the spelling we want in this case.\n\nNote again that z' is not UTC-equivalent as far as the hybrid tzinfo class is\nconcerned (because it takes z' as being in standard time rather than the\ndaylight time we intend here), but returning it gives the real-life \"local\nclock repeats an hour\" behavior when mapping the \"unspellable\" UTC hour into\ntz.\n\nWhen the input is 6:MM, z=1:MM and z.d=0, and we stop at once, again with\nthe 1:MM standard time spelling we want.\n\nSo how can this break?  One of the assumptions must be violated.  Two\npossibilities:\n\n1) [2] effectively says that y.s is invariant across all y belong to a given\n   time zone.  This isn't true if, for political reasons or continental drift,\n   a region decides to change its base offset from UTC.\n\n2) There may be versions of \"double daylight\" time where the tail end of\n   the analysis gives up a step too early.  I haven't thought about that\n   enough to say.\n\nIn any case, it's clear that the default fromutc() is strong enough to handle\n\"almost all\" time zones:  so long as the standard offset is invariant, it\ndoesn't matter if daylight time transition points change from year to year, or\nif daylight time is skipped in some years; it doesn't matter how large or\nsmall dst() may get within its bounds; and it doesn't even matter if some\nperverse time zone returns a negative dst()).  So a breaking case must be\npretty bizarre, and a tzinfo subclass can override fromutc() if it is.\n\"\"\"\n", 
    "difflib": "\"\"\"\nModule difflib -- helpers for computing deltas between objects.\n\nFunction get_close_matches(word, possibilities, n=3, cutoff=0.6):\n    Use SequenceMatcher to return list of the best \"good enough\" matches.\n\nFunction context_diff(a, b):\n    For two lists of strings, return a delta in context diff format.\n\nFunction ndiff(a, b):\n    Return a delta: the difference between `a` and `b` (lists of strings).\n\nFunction restore(delta, which):\n    Return one of the two sequences that generated an ndiff delta.\n\nFunction unified_diff(a, b):\n    For two lists of strings, return a delta in unified diff format.\n\nClass SequenceMatcher:\n    A flexible class for comparing pairs of sequences of any type.\n\nClass Differ:\n    For producing human-readable deltas from sequences of lines of text.\n\nClass HtmlDiff:\n    For producing HTML side by side comparison with change highlights.\n\"\"\"\n\n__all__ = ['get_close_matches', 'ndiff', 'restore', 'SequenceMatcher',\n           'Differ','IS_CHARACTER_JUNK', 'IS_LINE_JUNK', 'context_diff',\n           'unified_diff', 'HtmlDiff', 'Match']\n\nimport heapq\nfrom collections import namedtuple as _namedtuple\nfrom functools import reduce\n\nMatch = _namedtuple('Match', 'a b size')\n\ndef _calculate_ratio(matches, length):\n    if length:\n        return 2.0 * matches / length\n    return 1.0\n\nclass SequenceMatcher:\n\n    \"\"\"\n    SequenceMatcher is a flexible class for comparing pairs of sequences of\n    any type, so long as the sequence elements are hashable.  The basic\n    algorithm predates, and is a little fancier than, an algorithm\n    published in the late 1980's by Ratcliff and Obershelp under the\n    hyperbolic name \"gestalt pattern matching\".  The basic idea is to find\n    the longest contiguous matching subsequence that contains no \"junk\"\n    elements (R-O doesn't address junk).  The same idea is then applied\n    recursively to the pieces of the sequences to the left and to the right\n    of the matching subsequence.  This does not yield minimal edit\n    sequences, but does tend to yield matches that \"look right\" to people.\n\n    SequenceMatcher tries to compute a \"human-friendly diff\" between two\n    sequences.  Unlike e.g. UNIX(tm) diff, the fundamental notion is the\n    longest *contiguous* & junk-free matching subsequence.  That's what\n    catches peoples' eyes.  The Windows(tm) windiff has another interesting\n    notion, pairing up elements that appear uniquely in each sequence.\n    That, and the method here, appear to yield more intuitive difference\n    reports than does diff.  This method appears to be the least vulnerable\n    to synching up on blocks of \"junk lines\", though (like blank lines in\n    ordinary text files, or maybe \"<P>\" lines in HTML files).  That may be\n    because this is the only method of the 3 that has a *concept* of\n    \"junk\" <wink>.\n\n    Example, comparing two strings, and considering blanks to be \"junk\":\n\n    >>> s = SequenceMatcher(lambda x: x == \" \",\n    ...                     \"private Thread currentThread;\",\n    ...                     \"private volatile Thread currentThread;\")\n    >>>\n\n    .ratio() returns a float in [0, 1], measuring the \"similarity\" of the\n    sequences.  As a rule of thumb, a .ratio() value over 0.6 means the\n    sequences are close matches:\n\n    >>> print round(s.ratio(), 3)\n    0.866\n    >>>\n\n    If you're only interested in where the sequences match,\n    .get_matching_blocks() is handy:\n\n    >>> for block in s.get_matching_blocks():\n    ...     print \"a[%d] and b[%d] match for %d elements\" % block\n    a[0] and b[0] match for 8 elements\n    a[8] and b[17] match for 21 elements\n    a[29] and b[38] match for 0 elements\n\n    Note that the last tuple returned by .get_matching_blocks() is always a\n    dummy, (len(a), len(b), 0), and this is the only case in which the last\n    tuple element (number of elements matched) is 0.\n\n    If you want to know how to change the first sequence into the second,\n    use .get_opcodes():\n\n    >>> for opcode in s.get_opcodes():\n    ...     print \"%6s a[%d:%d] b[%d:%d]\" % opcode\n     equal a[0:8] b[0:8]\n    insert a[8:8] b[8:17]\n     equal a[8:29] b[17:38]\n\n    See the Differ class for a fancy human-friendly file differencer, which\n    uses SequenceMatcher both to compare sequences of lines, and to compare\n    sequences of characters within similar (near-matching) lines.\n\n    See also function get_close_matches() in this module, which shows how\n    simple code building on SequenceMatcher can be used to do useful work.\n\n    Timing:  Basic R-O is cubic time worst case and quadratic time expected\n    case.  SequenceMatcher is quadratic time for the worst case and has\n    expected-case behavior dependent in a complicated way on how many\n    elements the sequences have in common; best case time is linear.\n\n    Methods:\n\n    __init__(isjunk=None, a='', b='')\n        Construct a SequenceMatcher.\n\n    set_seqs(a, b)\n        Set the two sequences to be compared.\n\n    set_seq1(a)\n        Set the first sequence to be compared.\n\n    set_seq2(b)\n        Set the second sequence to be compared.\n\n    find_longest_match(alo, ahi, blo, bhi)\n        Find longest matching block in a[alo:ahi] and b[blo:bhi].\n\n    get_matching_blocks()\n        Return list of triples describing matching subsequences.\n\n    get_opcodes()\n        Return list of 5-tuples describing how to turn a into b.\n\n    ratio()\n        Return a measure of the sequences' similarity (float in [0,1]).\n\n    quick_ratio()\n        Return an upper bound on .ratio() relatively quickly.\n\n    real_quick_ratio()\n        Return an upper bound on ratio() very quickly.\n    \"\"\"\n\n    def __init__(self, isjunk=None, a='', b='', autojunk=True):\n        \"\"\"Construct a SequenceMatcher.\n\n        Optional arg isjunk is None (the default), or a one-argument\n        function that takes a sequence element and returns true iff the\n        element is junk.  None is equivalent to passing \"lambda x: 0\", i.e.\n        no elements are considered to be junk.  For example, pass\n            lambda x: x in \" \\\\t\"\n        if you're comparing lines as sequences of characters, and don't\n        want to synch up on blanks or hard tabs.\n\n        Optional arg a is the first of two sequences to be compared.  By\n        default, an empty string.  The elements of a must be hashable.  See\n        also .set_seqs() and .set_seq1().\n\n        Optional arg b is the second of two sequences to be compared.  By\n        default, an empty string.  The elements of b must be hashable. See\n        also .set_seqs() and .set_seq2().\n\n        Optional arg autojunk should be set to False to disable the\n        \"automatic junk heuristic\" that treats popular elements as junk\n        (see module documentation for more information).\n        \"\"\"\n\n        # Members:\n        # a\n        #      first sequence\n        # b\n        #      second sequence; differences are computed as \"what do\n        #      we need to do to 'a' to change it into 'b'?\"\n        # b2j\n        #      for x in b, b2j[x] is a list of the indices (into b)\n        #      at which x appears; junk elements do not appear\n        # fullbcount\n        #      for x in b, fullbcount[x] == the number of times x\n        #      appears in b; only materialized if really needed (used\n        #      only for computing quick_ratio())\n        # matching_blocks\n        #      a list of (i, j, k) triples, where a[i:i+k] == b[j:j+k];\n        #      ascending & non-overlapping in i and in j; terminated by\n        #      a dummy (len(a), len(b), 0) sentinel\n        # opcodes\n        #      a list of (tag, i1, i2, j1, j2) tuples, where tag is\n        #      one of\n        #          'replace'   a[i1:i2] should be replaced by b[j1:j2]\n        #          'delete'    a[i1:i2] should be deleted\n        #          'insert'    b[j1:j2] should be inserted\n        #          'equal'     a[i1:i2] == b[j1:j2]\n        # isjunk\n        #      a user-supplied function taking a sequence element and\n        #      returning true iff the element is \"junk\" -- this has\n        #      subtle but helpful effects on the algorithm, which I'll\n        #      get around to writing up someday <0.9 wink>.\n        #      DON'T USE!  Only __chain_b uses this.  Use isbjunk.\n        # isbjunk\n        #      for x in b, isbjunk(x) == isjunk(x) but much faster;\n        #      it's really the __contains__ method of a hidden dict.\n        #      DOES NOT WORK for x in a!\n        # isbpopular\n        #      for x in b, isbpopular(x) is true iff b is reasonably long\n        #      (at least 200 elements) and x accounts for more than 1 + 1% of\n        #      its elements (when autojunk is enabled).\n        #      DOES NOT WORK for x in a!\n\n        self.isjunk = isjunk\n        self.a = self.b = None\n        self.autojunk = autojunk\n        self.set_seqs(a, b)\n\n    def set_seqs(self, a, b):\n        \"\"\"Set the two sequences to be compared.\n\n        >>> s = SequenceMatcher()\n        >>> s.set_seqs(\"abcd\", \"bcde\")\n        >>> s.ratio()\n        0.75\n        \"\"\"\n\n        self.set_seq1(a)\n        self.set_seq2(b)\n\n    def set_seq1(self, a):\n        \"\"\"Set the first sequence to be compared.\n\n        The second sequence to be compared is not changed.\n\n        >>> s = SequenceMatcher(None, \"abcd\", \"bcde\")\n        >>> s.ratio()\n        0.75\n        >>> s.set_seq1(\"bcde\")\n        >>> s.ratio()\n        1.0\n        >>>\n\n        SequenceMatcher computes and caches detailed information about the\n        second sequence, so if you want to compare one sequence S against\n        many sequences, use .set_seq2(S) once and call .set_seq1(x)\n        repeatedly for each of the other sequences.\n\n        See also set_seqs() and set_seq2().\n        \"\"\"\n\n        if a is self.a:\n            return\n        self.a = a\n        self.matching_blocks = self.opcodes = None\n\n    def set_seq2(self, b):\n        \"\"\"Set the second sequence to be compared.\n\n        The first sequence to be compared is not changed.\n\n        >>> s = SequenceMatcher(None, \"abcd\", \"bcde\")\n        >>> s.ratio()\n        0.75\n        >>> s.set_seq2(\"abcd\")\n        >>> s.ratio()\n        1.0\n        >>>\n\n        SequenceMatcher computes and caches detailed information about the\n        second sequence, so if you want to compare one sequence S against\n        many sequences, use .set_seq2(S) once and call .set_seq1(x)\n        repeatedly for each of the other sequences.\n\n        See also set_seqs() and set_seq1().\n        \"\"\"\n\n        if b is self.b:\n            return\n        self.b = b\n        self.matching_blocks = self.opcodes = None\n        self.fullbcount = None\n        self.__chain_b()\n\n    # For each element x in b, set b2j[x] to a list of the indices in\n    # b where x appears; the indices are in increasing order; note that\n    # the number of times x appears in b is len(b2j[x]) ...\n    # when self.isjunk is defined, junk elements don't show up in this\n    # map at all, which stops the central find_longest_match method\n    # from starting any matching block at a junk element ...\n    # also creates the fast isbjunk function ...\n    # b2j also does not contain entries for \"popular\" elements, meaning\n    # elements that account for more than 1 + 1% of the total elements, and\n    # when the sequence is reasonably large (>= 200 elements); this can\n    # be viewed as an adaptive notion of semi-junk, and yields an enormous\n    # speedup when, e.g., comparing program files with hundreds of\n    # instances of \"return NULL;\" ...\n    # note that this is only called when b changes; so for cross-product\n    # kinds of matches, it's best to call set_seq2 once, then set_seq1\n    # repeatedly\n\n    def __chain_b(self):\n        # Because isjunk is a user-defined (not C) function, and we test\n        # for junk a LOT, it's important to minimize the number of calls.\n        # Before the tricks described here, __chain_b was by far the most\n        # time-consuming routine in the whole module!  If anyone sees\n        # Jim Roskind, thank him again for profile.py -- I never would\n        # have guessed that.\n        # The first trick is to build b2j ignoring the possibility\n        # of junk.  I.e., we don't call isjunk at all yet.  Throwing\n        # out the junk later is much cheaper than building b2j \"right\"\n        # from the start.\n        b = self.b\n        self.b2j = b2j = {}\n\n        for i, elt in enumerate(b):\n            indices = b2j.setdefault(elt, [])\n            indices.append(i)\n\n        # Purge junk elements\n        junk = set()\n        isjunk = self.isjunk\n        if isjunk:\n            for elt in list(b2j.keys()):  # using list() since b2j is modified\n                if isjunk(elt):\n                    junk.add(elt)\n                    del b2j[elt]\n\n        # Purge popular elements that are not junk\n        popular = set()\n        n = len(b)\n        if self.autojunk and n >= 200:\n            ntest = n // 100 + 1\n            for elt, idxs in list(b2j.items()):\n                if len(idxs) > ntest:\n                    popular.add(elt)\n                    del b2j[elt]\n\n        # Now for x in b, isjunk(x) == x in junk, but the latter is much faster.\n        # Sicne the number of *unique* junk elements is probably small, the\n        # memory burden of keeping this set alive is likely trivial compared to\n        # the size of b2j.\n        self.isbjunk = junk.__contains__\n        self.isbpopular = popular.__contains__\n\n    def find_longest_match(self, alo, ahi, blo, bhi):\n        \"\"\"Find longest matching block in a[alo:ahi] and b[blo:bhi].\n\n        If isjunk is not defined:\n\n        Return (i,j,k) such that a[i:i+k] is equal to b[j:j+k], where\n            alo <= i <= i+k <= ahi\n            blo <= j <= j+k <= bhi\n        and for all (i',j',k') meeting those conditions,\n            k >= k'\n            i <= i'\n            and if i == i', j <= j'\n\n        In other words, of all maximal matching blocks, return one that\n        starts earliest in a, and of all those maximal matching blocks that\n        start earliest in a, return the one that starts earliest in b.\n\n        >>> s = SequenceMatcher(None, \" abcd\", \"abcd abcd\")\n        >>> s.find_longest_match(0, 5, 0, 9)\n        Match(a=0, b=4, size=5)\n\n        If isjunk is defined, first the longest matching block is\n        determined as above, but with the additional restriction that no\n        junk element appears in the block.  Then that block is extended as\n        far as possible by matching (only) junk elements on both sides.  So\n        the resulting block never matches on junk except as identical junk\n        happens to be adjacent to an \"interesting\" match.\n\n        Here's the same example as before, but considering blanks to be\n        junk.  That prevents \" abcd\" from matching the \" abcd\" at the tail\n        end of the second sequence directly.  Instead only the \"abcd\" can\n        match, and matches the leftmost \"abcd\" in the second sequence:\n\n        >>> s = SequenceMatcher(lambda x: x==\" \", \" abcd\", \"abcd abcd\")\n        >>> s.find_longest_match(0, 5, 0, 9)\n        Match(a=1, b=0, size=4)\n\n        If no blocks match, return (alo, blo, 0).\n\n        >>> s = SequenceMatcher(None, \"ab\", \"c\")\n        >>> s.find_longest_match(0, 2, 0, 1)\n        Match(a=0, b=0, size=0)\n        \"\"\"\n\n        # CAUTION:  stripping common prefix or suffix would be incorrect.\n        # E.g.,\n        #    ab\n        #    acab\n        # Longest matching block is \"ab\", but if common prefix is\n        # stripped, it's \"a\" (tied with \"b\").  UNIX(tm) diff does so\n        # strip, so ends up claiming that ab is changed to acab by\n        # inserting \"ca\" in the middle.  That's minimal but unintuitive:\n        # \"it's obvious\" that someone inserted \"ac\" at the front.\n        # Windiff ends up at the same place as diff, but by pairing up\n        # the unique 'b's and then matching the first two 'a's.\n\n        a, b, b2j, isbjunk = self.a, self.b, self.b2j, self.isbjunk\n        besti, bestj, bestsize = alo, blo, 0\n        # find longest junk-free match\n        # during an iteration of the loop, j2len[j] = length of longest\n        # junk-free match ending with a[i-1] and b[j]\n        j2len = {}\n        nothing = []\n        for i in xrange(alo, ahi):\n            # look at all instances of a[i] in b; note that because\n            # b2j has no junk keys, the loop is skipped if a[i] is junk\n            j2lenget = j2len.get\n            newj2len = {}\n            for j in b2j.get(a[i], nothing):\n                # a[i] matches b[j]\n                if j < blo:\n                    continue\n                if j >= bhi:\n                    break\n                k = newj2len[j] = j2lenget(j-1, 0) + 1\n                if k > bestsize:\n                    besti, bestj, bestsize = i-k+1, j-k+1, k\n            j2len = newj2len\n\n        # Extend the best by non-junk elements on each end.  In particular,\n        # \"popular\" non-junk elements aren't in b2j, which greatly speeds\n        # the inner loop above, but also means \"the best\" match so far\n        # doesn't contain any junk *or* popular non-junk elements.\n        while besti > alo and bestj > blo and \\\n              not isbjunk(b[bestj-1]) and \\\n              a[besti-1] == b[bestj-1]:\n            besti, bestj, bestsize = besti-1, bestj-1, bestsize+1\n        while besti+bestsize < ahi and bestj+bestsize < bhi and \\\n              not isbjunk(b[bestj+bestsize]) and \\\n              a[besti+bestsize] == b[bestj+bestsize]:\n            bestsize += 1\n\n        # Now that we have a wholly interesting match (albeit possibly\n        # empty!), we may as well suck up the matching junk on each\n        # side of it too.  Can't think of a good reason not to, and it\n        # saves post-processing the (possibly considerable) expense of\n        # figuring out what to do with it.  In the case of an empty\n        # interesting match, this is clearly the right thing to do,\n        # because no other kind of match is possible in the regions.\n        while besti > alo and bestj > blo and \\\n              isbjunk(b[bestj-1]) and \\\n              a[besti-1] == b[bestj-1]:\n            besti, bestj, bestsize = besti-1, bestj-1, bestsize+1\n        while besti+bestsize < ahi and bestj+bestsize < bhi and \\\n              isbjunk(b[bestj+bestsize]) and \\\n              a[besti+bestsize] == b[bestj+bestsize]:\n            bestsize = bestsize + 1\n\n        return Match(besti, bestj, bestsize)\n\n    def get_matching_blocks(self):\n        \"\"\"Return list of triples describing matching subsequences.\n\n        Each triple is of the form (i, j, n), and means that\n        a[i:i+n] == b[j:j+n].  The triples are monotonically increasing in\n        i and in j.  New in Python 2.5, it's also guaranteed that if\n        (i, j, n) and (i', j', n') are adjacent triples in the list, and\n        the second is not the last triple in the list, then i+n != i' or\n        j+n != j'.  IOW, adjacent triples never describe adjacent equal\n        blocks.\n\n        The last triple is a dummy, (len(a), len(b), 0), and is the only\n        triple with n==0.\n\n        >>> s = SequenceMatcher(None, \"abxcd\", \"abcd\")\n        >>> s.get_matching_blocks()\n        [Match(a=0, b=0, size=2), Match(a=3, b=2, size=2), Match(a=5, b=4, size=0)]\n        \"\"\"\n\n        if self.matching_blocks is not None:\n            return self.matching_blocks\n        la, lb = len(self.a), len(self.b)\n\n        # This is most naturally expressed as a recursive algorithm, but\n        # at least one user bumped into extreme use cases that exceeded\n        # the recursion limit on their box.  So, now we maintain a list\n        # ('queue`) of blocks we still need to look at, and append partial\n        # results to `matching_blocks` in a loop; the matches are sorted\n        # at the end.\n        queue = [(0, la, 0, lb)]\n        matching_blocks = []\n        while queue:\n            alo, ahi, blo, bhi = queue.pop()\n            i, j, k = x = self.find_longest_match(alo, ahi, blo, bhi)\n            # a[alo:i] vs b[blo:j] unknown\n            # a[i:i+k] same as b[j:j+k]\n            # a[i+k:ahi] vs b[j+k:bhi] unknown\n            if k:   # if k is 0, there was no matching block\n                matching_blocks.append(x)\n                if alo < i and blo < j:\n                    queue.append((alo, i, blo, j))\n                if i+k < ahi and j+k < bhi:\n                    queue.append((i+k, ahi, j+k, bhi))\n        matching_blocks.sort()\n\n        # It's possible that we have adjacent equal blocks in the\n        # matching_blocks list now.  Starting with 2.5, this code was added\n        # to collapse them.\n        i1 = j1 = k1 = 0\n        non_adjacent = []\n        for i2, j2, k2 in matching_blocks:\n            # Is this block adjacent to i1, j1, k1?\n            if i1 + k1 == i2 and j1 + k1 == j2:\n                # Yes, so collapse them -- this just increases the length of\n                # the first block by the length of the second, and the first\n                # block so lengthened remains the block to compare against.\n                k1 += k2\n            else:\n                # Not adjacent.  Remember the first block (k1==0 means it's\n                # the dummy we started with), and make the second block the\n                # new block to compare against.\n                if k1:\n                    non_adjacent.append((i1, j1, k1))\n                i1, j1, k1 = i2, j2, k2\n        if k1:\n            non_adjacent.append((i1, j1, k1))\n\n        non_adjacent.append( (la, lb, 0) )\n        self.matching_blocks = map(Match._make, non_adjacent)\n        return self.matching_blocks\n\n    def get_opcodes(self):\n        \"\"\"Return list of 5-tuples describing how to turn a into b.\n\n        Each tuple is of the form (tag, i1, i2, j1, j2).  The first tuple\n        has i1 == j1 == 0, and remaining tuples have i1 == the i2 from the\n        tuple preceding it, and likewise for j1 == the previous j2.\n\n        The tags are strings, with these meanings:\n\n        'replace':  a[i1:i2] should be replaced by b[j1:j2]\n        'delete':   a[i1:i2] should be deleted.\n                    Note that j1==j2 in this case.\n        'insert':   b[j1:j2] should be inserted at a[i1:i1].\n                    Note that i1==i2 in this case.\n        'equal':    a[i1:i2] == b[j1:j2]\n\n        >>> a = \"qabxcd\"\n        >>> b = \"abycdf\"\n        >>> s = SequenceMatcher(None, a, b)\n        >>> for tag, i1, i2, j1, j2 in s.get_opcodes():\n        ...    print (\"%7s a[%d:%d] (%s) b[%d:%d] (%s)\" %\n        ...           (tag, i1, i2, a[i1:i2], j1, j2, b[j1:j2]))\n         delete a[0:1] (q) b[0:0] ()\n          equal a[1:3] (ab) b[0:2] (ab)\n        replace a[3:4] (x) b[2:3] (y)\n          equal a[4:6] (cd) b[3:5] (cd)\n         insert a[6:6] () b[5:6] (f)\n        \"\"\"\n\n        if self.opcodes is not None:\n            return self.opcodes\n        i = j = 0\n        self.opcodes = answer = []\n        for ai, bj, size in self.get_matching_blocks():\n            # invariant:  we've pumped out correct diffs to change\n            # a[:i] into b[:j], and the next matching block is\n            # a[ai:ai+size] == b[bj:bj+size].  So we need to pump\n            # out a diff to change a[i:ai] into b[j:bj], pump out\n            # the matching block, and move (i,j) beyond the match\n            tag = ''\n            if i < ai and j < bj:\n                tag = 'replace'\n            elif i < ai:\n                tag = 'delete'\n            elif j < bj:\n                tag = 'insert'\n            if tag:\n                answer.append( (tag, i, ai, j, bj) )\n            i, j = ai+size, bj+size\n            # the list of matching blocks is terminated by a\n            # sentinel with size 0\n            if size:\n                answer.append( ('equal', ai, i, bj, j) )\n        return answer\n\n    def get_grouped_opcodes(self, n=3):\n        \"\"\" Isolate change clusters by eliminating ranges with no changes.\n\n        Return a generator of groups with up to n lines of context.\n        Each group is in the same format as returned by get_opcodes().\n\n        >>> from pprint import pprint\n        >>> a = map(str, range(1,40))\n        >>> b = a[:]\n        >>> b[8:8] = ['i']     # Make an insertion\n        >>> b[20] += 'x'       # Make a replacement\n        >>> b[23:28] = []      # Make a deletion\n        >>> b[30] += 'y'       # Make another replacement\n        >>> pprint(list(SequenceMatcher(None,a,b).get_grouped_opcodes()))\n        [[('equal', 5, 8, 5, 8), ('insert', 8, 8, 8, 9), ('equal', 8, 11, 9, 12)],\n         [('equal', 16, 19, 17, 20),\n          ('replace', 19, 20, 20, 21),\n          ('equal', 20, 22, 21, 23),\n          ('delete', 22, 27, 23, 23),\n          ('equal', 27, 30, 23, 26)],\n         [('equal', 31, 34, 27, 30),\n          ('replace', 34, 35, 30, 31),\n          ('equal', 35, 38, 31, 34)]]\n        \"\"\"\n\n        codes = self.get_opcodes()\n        if not codes:\n            codes = [(\"equal\", 0, 1, 0, 1)]\n        # Fixup leading and trailing groups if they show no changes.\n        if codes[0][0] == 'equal':\n            tag, i1, i2, j1, j2 = codes[0]\n            codes[0] = tag, max(i1, i2-n), i2, max(j1, j2-n), j2\n        if codes[-1][0] == 'equal':\n            tag, i1, i2, j1, j2 = codes[-1]\n            codes[-1] = tag, i1, min(i2, i1+n), j1, min(j2, j1+n)\n\n        nn = n + n\n        group = []\n        for tag, i1, i2, j1, j2 in codes:\n            # End the current group and start a new one whenever\n            # there is a large range with no changes.\n            if tag == 'equal' and i2-i1 > nn:\n                group.append((tag, i1, min(i2, i1+n), j1, min(j2, j1+n)))\n                yield group\n                group = []\n                i1, j1 = max(i1, i2-n), max(j1, j2-n)\n            group.append((tag, i1, i2, j1 ,j2))\n        if group and not (len(group)==1 and group[0][0] == 'equal'):\n            yield group\n\n    def ratio(self):\n        \"\"\"Return a measure of the sequences' similarity (float in [0,1]).\n\n        Where T is the total number of elements in both sequences, and\n        M is the number of matches, this is 2.0*M / T.\n        Note that this is 1 if the sequences are identical, and 0 if\n        they have nothing in common.\n\n        .ratio() is expensive to compute if you haven't already computed\n        .get_matching_blocks() or .get_opcodes(), in which case you may\n        want to try .quick_ratio() or .real_quick_ratio() first to get an\n        upper bound.\n\n        >>> s = SequenceMatcher(None, \"abcd\", \"bcde\")\n        >>> s.ratio()\n        0.75\n        >>> s.quick_ratio()\n        0.75\n        >>> s.real_quick_ratio()\n        1.0\n        \"\"\"\n\n        matches = reduce(lambda sum, triple: sum + triple[-1],\n                         self.get_matching_blocks(), 0)\n        return _calculate_ratio(matches, len(self.a) + len(self.b))\n\n    def quick_ratio(self):\n        \"\"\"Return an upper bound on ratio() relatively quickly.\n\n        This isn't defined beyond that it is an upper bound on .ratio(), and\n        is faster to compute.\n        \"\"\"\n\n        # viewing a and b as multisets, set matches to the cardinality\n        # of their intersection; this counts the number of matches\n        # without regard to order, so is clearly an upper bound\n        if self.fullbcount is None:\n            self.fullbcount = fullbcount = {}\n            for elt in self.b:\n                fullbcount[elt] = fullbcount.get(elt, 0) + 1\n        fullbcount = self.fullbcount\n        # avail[x] is the number of times x appears in 'b' less the\n        # number of times we've seen it in 'a' so far ... kinda\n        avail = {}\n        availhas, matches = avail.__contains__, 0\n        for elt in self.a:\n            if availhas(elt):\n                numb = avail[elt]\n            else:\n                numb = fullbcount.get(elt, 0)\n            avail[elt] = numb - 1\n            if numb > 0:\n                matches = matches + 1\n        return _calculate_ratio(matches, len(self.a) + len(self.b))\n\n    def real_quick_ratio(self):\n        \"\"\"Return an upper bound on ratio() very quickly.\n\n        This isn't defined beyond that it is an upper bound on .ratio(), and\n        is faster to compute than either .ratio() or .quick_ratio().\n        \"\"\"\n\n        la, lb = len(self.a), len(self.b)\n        # can't have more matches than the number of elements in the\n        # shorter sequence\n        return _calculate_ratio(min(la, lb), la + lb)\n\ndef get_close_matches(word, possibilities, n=3, cutoff=0.6):\n    \"\"\"Use SequenceMatcher to return list of the best \"good enough\" matches.\n\n    word is a sequence for which close matches are desired (typically a\n    string).\n\n    possibilities is a list of sequences against which to match word\n    (typically a list of strings).\n\n    Optional arg n (default 3) is the maximum number of close matches to\n    return.  n must be > 0.\n\n    Optional arg cutoff (default 0.6) is a float in [0, 1].  Possibilities\n    that don't score at least that similar to word are ignored.\n\n    The best (no more than n) matches among the possibilities are returned\n    in a list, sorted by similarity score, most similar first.\n\n    >>> get_close_matches(\"appel\", [\"ape\", \"apple\", \"peach\", \"puppy\"])\n    ['apple', 'ape']\n    >>> import keyword as _keyword\n    >>> get_close_matches(\"wheel\", _keyword.kwlist)\n    ['while']\n    >>> get_close_matches(\"apple\", _keyword.kwlist)\n    []\n    >>> get_close_matches(\"accept\", _keyword.kwlist)\n    ['except']\n    \"\"\"\n\n    if not n >  0:\n        raise ValueError(\"n must be > 0: %r\" % (n,))\n    if not 0.0 <= cutoff <= 1.0:\n        raise ValueError(\"cutoff must be in [0.0, 1.0]: %r\" % (cutoff,))\n    result = []\n    s = SequenceMatcher()\n    s.set_seq2(word)\n    for x in possibilities:\n        s.set_seq1(x)\n        if s.real_quick_ratio() >= cutoff and \\\n           s.quick_ratio() >= cutoff and \\\n           s.ratio() >= cutoff:\n            result.append((s.ratio(), x))\n\n    # Move the best scorers to head of list\n    result = heapq.nlargest(n, result)\n    # Strip scores for the best n matches\n    return [x for score, x in result]\n\ndef _count_leading(line, ch):\n    \"\"\"\n    Return number of `ch` characters at the start of `line`.\n\n    Example:\n\n    >>> _count_leading('   abc', ' ')\n    3\n    \"\"\"\n\n    i, n = 0, len(line)\n    while i < n and line[i] == ch:\n        i += 1\n    return i\n\nclass Differ:\n    r\"\"\"\n    Differ is a class for comparing sequences of lines of text, and\n    producing human-readable differences or deltas.  Differ uses\n    SequenceMatcher both to compare sequences of lines, and to compare\n    sequences of characters within similar (near-matching) lines.\n\n    Each line of a Differ delta begins with a two-letter code:\n\n        '- '    line unique to sequence 1\n        '+ '    line unique to sequence 2\n        '  '    line common to both sequences\n        '? '    line not present in either input sequence\n\n    Lines beginning with '? ' attempt to guide the eye to intraline\n    differences, and were not present in either input sequence.  These lines\n    can be confusing if the sequences contain tab characters.\n\n    Note that Differ makes no claim to produce a *minimal* diff.  To the\n    contrary, minimal diffs are often counter-intuitive, because they synch\n    up anywhere possible, sometimes accidental matches 100 pages apart.\n    Restricting synch points to contiguous matches preserves some notion of\n    locality, at the occasional cost of producing a longer diff.\n\n    Example: Comparing two texts.\n\n    First we set up the texts, sequences of individual single-line strings\n    ending with newlines (such sequences can also be obtained from the\n    `readlines()` method of file-like objects):\n\n    >>> text1 = '''  1. Beautiful is better than ugly.\n    ...   2. Explicit is better than implicit.\n    ...   3. Simple is better than complex.\n    ...   4. Complex is better than complicated.\n    ... '''.splitlines(1)\n    >>> len(text1)\n    4\n    >>> text1[0][-1]\n    '\\n'\n    >>> text2 = '''  1. Beautiful is better than ugly.\n    ...   3.   Simple is better than complex.\n    ...   4. Complicated is better than complex.\n    ...   5. Flat is better than nested.\n    ... '''.splitlines(1)\n\n    Next we instantiate a Differ object:\n\n    >>> d = Differ()\n\n    Note that when instantiating a Differ object we may pass functions to\n    filter out line and character 'junk'.  See Differ.__init__ for details.\n\n    Finally, we compare the two:\n\n    >>> result = list(d.compare(text1, text2))\n\n    'result' is a list of strings, so let's pretty-print it:\n\n    >>> from pprint import pprint as _pprint\n    >>> _pprint(result)\n    ['    1. Beautiful is better than ugly.\\n',\n     '-   2. Explicit is better than implicit.\\n',\n     '-   3. Simple is better than complex.\\n',\n     '+   3.   Simple is better than complex.\\n',\n     '?     ++\\n',\n     '-   4. Complex is better than complicated.\\n',\n     '?            ^                     ---- ^\\n',\n     '+   4. Complicated is better than complex.\\n',\n     '?           ++++ ^                      ^\\n',\n     '+   5. Flat is better than nested.\\n']\n\n    As a single multi-line string it looks like this:\n\n    >>> print ''.join(result),\n        1. Beautiful is better than ugly.\n    -   2. Explicit is better than implicit.\n    -   3. Simple is better than complex.\n    +   3.   Simple is better than complex.\n    ?     ++\n    -   4. Complex is better than complicated.\n    ?            ^                     ---- ^\n    +   4. Complicated is better than complex.\n    ?           ++++ ^                      ^\n    +   5. Flat is better than nested.\n\n    Methods:\n\n    __init__(linejunk=None, charjunk=None)\n        Construct a text differencer, with optional filters.\n\n    compare(a, b)\n        Compare two sequences of lines; generate the resulting delta.\n    \"\"\"\n\n    def __init__(self, linejunk=None, charjunk=None):\n        \"\"\"\n        Construct a text differencer, with optional filters.\n\n        The two optional keyword parameters are for filter functions:\n\n        - `linejunk`: A function that should accept a single string argument,\n          and return true iff the string is junk. The module-level function\n          `IS_LINE_JUNK` may be used to filter out lines without visible\n          characters, except for at most one splat ('#').  It is recommended\n          to leave linejunk None; as of Python 2.3, the underlying\n          SequenceMatcher class has grown an adaptive notion of \"noise\" lines\n          that's better than any static definition the author has ever been\n          able to craft.\n\n        - `charjunk`: A function that should accept a string of length 1. The\n          module-level function `IS_CHARACTER_JUNK` may be used to filter out\n          whitespace characters (a blank or tab; **note**: bad idea to include\n          newline in this!).  Use of IS_CHARACTER_JUNK is recommended.\n        \"\"\"\n\n        self.linejunk = linejunk\n        self.charjunk = charjunk\n\n    def compare(self, a, b):\n        r\"\"\"\n        Compare two sequences of lines; generate the resulting delta.\n\n        Each sequence must contain individual single-line strings ending with\n        newlines. Such sequences can be obtained from the `readlines()` method\n        of file-like objects.  The delta generated also consists of newline-\n        terminated strings, ready to be printed as-is via the writeline()\n        method of a file-like object.\n\n        Example:\n\n        >>> print ''.join(Differ().compare('one\\ntwo\\nthree\\n'.splitlines(1),\n        ...                                'ore\\ntree\\nemu\\n'.splitlines(1))),\n        - one\n        ?  ^\n        + ore\n        ?  ^\n        - two\n        - three\n        ?  -\n        + tree\n        + emu\n        \"\"\"\n\n        cruncher = SequenceMatcher(self.linejunk, a, b)\n        for tag, alo, ahi, blo, bhi in cruncher.get_opcodes():\n            if tag == 'replace':\n                g = self._fancy_replace(a, alo, ahi, b, blo, bhi)\n            elif tag == 'delete':\n                g = self._dump('-', a, alo, ahi)\n            elif tag == 'insert':\n                g = self._dump('+', b, blo, bhi)\n            elif tag == 'equal':\n                g = self._dump(' ', a, alo, ahi)\n            else:\n                raise ValueError, 'unknown tag %r' % (tag,)\n\n            for line in g:\n                yield line\n\n    def _dump(self, tag, x, lo, hi):\n        \"\"\"Generate comparison results for a same-tagged range.\"\"\"\n        for i in xrange(lo, hi):\n            yield '%s %s' % (tag, x[i])\n\n    def _plain_replace(self, a, alo, ahi, b, blo, bhi):\n        assert alo < ahi and blo < bhi\n        # dump the shorter block first -- reduces the burden on short-term\n        # memory if the blocks are of very different sizes\n        if bhi - blo < ahi - alo:\n            first  = self._dump('+', b, blo, bhi)\n            second = self._dump('-', a, alo, ahi)\n        else:\n            first  = self._dump('-', a, alo, ahi)\n            second = self._dump('+', b, blo, bhi)\n\n        for g in first, second:\n            for line in g:\n                yield line\n\n    def _fancy_replace(self, a, alo, ahi, b, blo, bhi):\n        r\"\"\"\n        When replacing one block of lines with another, search the blocks\n        for *similar* lines; the best-matching pair (if any) is used as a\n        synch point, and intraline difference marking is done on the\n        similar pair. Lots of work, but often worth it.\n\n        Example:\n\n        >>> d = Differ()\n        >>> results = d._fancy_replace(['abcDefghiJkl\\n'], 0, 1,\n        ...                            ['abcdefGhijkl\\n'], 0, 1)\n        >>> print ''.join(results),\n        - abcDefghiJkl\n        ?    ^  ^  ^\n        + abcdefGhijkl\n        ?    ^  ^  ^\n        \"\"\"\n\n        # don't synch up unless the lines have a similarity score of at\n        # least cutoff; best_ratio tracks the best score seen so far\n        best_ratio, cutoff = 0.74, 0.75\n        cruncher = SequenceMatcher(self.charjunk)\n        eqi, eqj = None, None   # 1st indices of equal lines (if any)\n\n        # search for the pair that matches best without being identical\n        # (identical lines must be junk lines, & we don't want to synch up\n        # on junk -- unless we have to)\n        for j in xrange(blo, bhi):\n            bj = b[j]\n            cruncher.set_seq2(bj)\n            for i in xrange(alo, ahi):\n                ai = a[i]\n                if ai == bj:\n                    if eqi is None:\n                        eqi, eqj = i, j\n                    continue\n                cruncher.set_seq1(ai)\n                # computing similarity is expensive, so use the quick\n                # upper bounds first -- have seen this speed up messy\n                # compares by a factor of 3.\n                # note that ratio() is only expensive to compute the first\n                # time it's called on a sequence pair; the expensive part\n                # of the computation is cached by cruncher\n                if cruncher.real_quick_ratio() > best_ratio and \\\n                      cruncher.quick_ratio() > best_ratio and \\\n                      cruncher.ratio() > best_ratio:\n                    best_ratio, best_i, best_j = cruncher.ratio(), i, j\n        if best_ratio < cutoff:\n            # no non-identical \"pretty close\" pair\n            if eqi is None:\n                # no identical pair either -- treat it as a straight replace\n                for line in self._plain_replace(a, alo, ahi, b, blo, bhi):\n                    yield line\n                return\n            # no close pair, but an identical pair -- synch up on that\n            best_i, best_j, best_ratio = eqi, eqj, 1.0\n        else:\n            # there's a close pair, so forget the identical pair (if any)\n            eqi = None\n\n        # a[best_i] very similar to b[best_j]; eqi is None iff they're not\n        # identical\n\n        # pump out diffs from before the synch point\n        for line in self._fancy_helper(a, alo, best_i, b, blo, best_j):\n            yield line\n\n        # do intraline marking on the synch pair\n        aelt, belt = a[best_i], b[best_j]\n        if eqi is None:\n            # pump out a '-', '?', '+', '?' quad for the synched lines\n            atags = btags = \"\"\n            cruncher.set_seqs(aelt, belt)\n            for tag, ai1, ai2, bj1, bj2 in cruncher.get_opcodes():\n                la, lb = ai2 - ai1, bj2 - bj1\n                if tag == 'replace':\n                    atags += '^' * la\n                    btags += '^' * lb\n                elif tag == 'delete':\n                    atags += '-' * la\n                elif tag == 'insert':\n                    btags += '+' * lb\n                elif tag == 'equal':\n                    atags += ' ' * la\n                    btags += ' ' * lb\n                else:\n                    raise ValueError, 'unknown tag %r' % (tag,)\n            for line in self._qformat(aelt, belt, atags, btags):\n                yield line\n        else:\n            # the synch pair is identical\n            yield '  ' + aelt\n\n        # pump out diffs from after the synch point\n        for line in self._fancy_helper(a, best_i+1, ahi, b, best_j+1, bhi):\n            yield line\n\n    def _fancy_helper(self, a, alo, ahi, b, blo, bhi):\n        g = []\n        if alo < ahi:\n            if blo < bhi:\n                g = self._fancy_replace(a, alo, ahi, b, blo, bhi)\n            else:\n                g = self._dump('-', a, alo, ahi)\n        elif blo < bhi:\n            g = self._dump('+', b, blo, bhi)\n\n        for line in g:\n            yield line\n\n    def _qformat(self, aline, bline, atags, btags):\n        r\"\"\"\n        Format \"?\" output and deal with leading tabs.\n\n        Example:\n\n        >>> d = Differ()\n        >>> results = d._qformat('\\tabcDefghiJkl\\n', '\\tabcdefGhijkl\\n',\n        ...                      '  ^ ^  ^      ', '  ^ ^  ^      ')\n        >>> for line in results: print repr(line)\n        ...\n        '- \\tabcDefghiJkl\\n'\n        '? \\t ^ ^  ^\\n'\n        '+ \\tabcdefGhijkl\\n'\n        '? \\t ^ ^  ^\\n'\n        \"\"\"\n\n        # Can hurt, but will probably help most of the time.\n        common = min(_count_leading(aline, \"\\t\"),\n                     _count_leading(bline, \"\\t\"))\n        common = min(common, _count_leading(atags[:common], \" \"))\n        common = min(common, _count_leading(btags[:common], \" \"))\n        atags = atags[common:].rstrip()\n        btags = btags[common:].rstrip()\n\n        yield \"- \" + aline\n        if atags:\n            yield \"? %s%s\\n\" % (\"\\t\" * common, atags)\n\n        yield \"+ \" + bline\n        if btags:\n            yield \"? %s%s\\n\" % (\"\\t\" * common, btags)\n\n# With respect to junk, an earlier version of ndiff simply refused to\n# *start* a match with a junk element.  The result was cases like this:\n#     before: private Thread currentThread;\n#     after:  private volatile Thread currentThread;\n# If you consider whitespace to be junk, the longest contiguous match\n# not starting with junk is \"e Thread currentThread\".  So ndiff reported\n# that \"e volatil\" was inserted between the 't' and the 'e' in \"private\".\n# While an accurate view, to people that's absurd.  The current version\n# looks for matching blocks that are entirely junk-free, then extends the\n# longest one of those as far as possible but only with matching junk.\n# So now \"currentThread\" is matched, then extended to suck up the\n# preceding blank; then \"private\" is matched, and extended to suck up the\n# following blank; then \"Thread\" is matched; and finally ndiff reports\n# that \"volatile \" was inserted before \"Thread\".  The only quibble\n# remaining is that perhaps it was really the case that \" volatile\"\n# was inserted after \"private\".  I can live with that <wink>.\n\nimport re\n\ndef IS_LINE_JUNK(line, pat=re.compile(r\"\\s*#?\\s*$\").match):\n    r\"\"\"\n    Return 1 for ignorable line: iff `line` is blank or contains a single '#'.\n\n    Examples:\n\n    >>> IS_LINE_JUNK('\\n')\n    True\n    >>> IS_LINE_JUNK('  #   \\n')\n    True\n    >>> IS_LINE_JUNK('hello\\n')\n    False\n    \"\"\"\n\n    return pat(line) is not None\n\ndef IS_CHARACTER_JUNK(ch, ws=\" \\t\"):\n    r\"\"\"\n    Return 1 for ignorable character: iff `ch` is a space or tab.\n\n    Examples:\n\n    >>> IS_CHARACTER_JUNK(' ')\n    True\n    >>> IS_CHARACTER_JUNK('\\t')\n    True\n    >>> IS_CHARACTER_JUNK('\\n')\n    False\n    >>> IS_CHARACTER_JUNK('x')\n    False\n    \"\"\"\n\n    return ch in ws\n\n\n########################################################################\n###  Unified Diff\n########################################################################\n\ndef _format_range_unified(start, stop):\n    'Convert range to the \"ed\" format'\n    # Per the diff spec at http://www.unix.org/single_unix_specification/\n    beginning = start + 1     # lines start numbering with one\n    length = stop - start\n    if length == 1:\n        return '{}'.format(beginning)\n    if not length:\n        beginning -= 1        # empty ranges begin at line just before the range\n    return '{},{}'.format(beginning, length)\n\ndef unified_diff(a, b, fromfile='', tofile='', fromfiledate='',\n                 tofiledate='', n=3, lineterm='\\n'):\n    r\"\"\"\n    Compare two sequences of lines; generate the delta as a unified diff.\n\n    Unified diffs are a compact way of showing line changes and a few\n    lines of context.  The number of context lines is set by 'n' which\n    defaults to three.\n\n    By default, the diff control lines (those with ---, +++, or @@) are\n    created with a trailing newline.  This is helpful so that inputs\n    created from file.readlines() result in diffs that are suitable for\n    file.writelines() since both the inputs and outputs have trailing\n    newlines.\n\n    For inputs that do not have trailing newlines, set the lineterm\n    argument to \"\" so that the output will be uniformly newline free.\n\n    The unidiff format normally has a header for filenames and modification\n    times.  Any or all of these may be specified using strings for\n    'fromfile', 'tofile', 'fromfiledate', and 'tofiledate'.\n    The modification times are normally expressed in the ISO 8601 format.\n\n    Example:\n\n    >>> for line in unified_diff('one two three four'.split(),\n    ...             'zero one tree four'.split(), 'Original', 'Current',\n    ...             '2005-01-26 23:30:50', '2010-04-02 10:20:52',\n    ...             lineterm=''):\n    ...     print line                  # doctest: +NORMALIZE_WHITESPACE\n    --- Original        2005-01-26 23:30:50\n    +++ Current         2010-04-02 10:20:52\n    @@ -1,4 +1,4 @@\n    +zero\n     one\n    -two\n    -three\n    +tree\n     four\n    \"\"\"\n\n    started = False\n    for group in SequenceMatcher(None,a,b).get_grouped_opcodes(n):\n        if not started:\n            started = True\n            fromdate = '\\t{}'.format(fromfiledate) if fromfiledate else ''\n            todate = '\\t{}'.format(tofiledate) if tofiledate else ''\n            yield '--- {}{}{}'.format(fromfile, fromdate, lineterm)\n            yield '+++ {}{}{}'.format(tofile, todate, lineterm)\n\n        first, last = group[0], group[-1]\n        file1_range = _format_range_unified(first[1], last[2])\n        file2_range = _format_range_unified(first[3], last[4])\n        yield '@@ -{} +{} @@{}'.format(file1_range, file2_range, lineterm)\n\n        for tag, i1, i2, j1, j2 in group:\n            if tag == 'equal':\n                for line in a[i1:i2]:\n                    yield ' ' + line\n                continue\n            if tag in ('replace', 'delete'):\n                for line in a[i1:i2]:\n                    yield '-' + line\n            if tag in ('replace', 'insert'):\n                for line in b[j1:j2]:\n                    yield '+' + line\n\n\n########################################################################\n###  Context Diff\n########################################################################\n\ndef _format_range_context(start, stop):\n    'Convert range to the \"ed\" format'\n    # Per the diff spec at http://www.unix.org/single_unix_specification/\n    beginning = start + 1     # lines start numbering with one\n    length = stop - start\n    if not length:\n        beginning -= 1        # empty ranges begin at line just before the range\n    if length <= 1:\n        return '{}'.format(beginning)\n    return '{},{}'.format(beginning, beginning + length - 1)\n\n# See http://www.unix.org/single_unix_specification/\ndef context_diff(a, b, fromfile='', tofile='',\n                 fromfiledate='', tofiledate='', n=3, lineterm='\\n'):\n    r\"\"\"\n    Compare two sequences of lines; generate the delta as a context diff.\n\n    Context diffs are a compact way of showing line changes and a few\n    lines of context.  The number of context lines is set by 'n' which\n    defaults to three.\n\n    By default, the diff control lines (those with *** or ---) are\n    created with a trailing newline.  This is helpful so that inputs\n    created from file.readlines() result in diffs that are suitable for\n    file.writelines() since both the inputs and outputs have trailing\n    newlines.\n\n    For inputs that do not have trailing newlines, set the lineterm\n    argument to \"\" so that the output will be uniformly newline free.\n\n    The context diff format normally has a header for filenames and\n    modification times.  Any or all of these may be specified using\n    strings for 'fromfile', 'tofile', 'fromfiledate', and 'tofiledate'.\n    The modification times are normally expressed in the ISO 8601 format.\n    If not specified, the strings default to blanks.\n\n    Example:\n\n    >>> print ''.join(context_diff('one\\ntwo\\nthree\\nfour\\n'.splitlines(1),\n    ...       'zero\\none\\ntree\\nfour\\n'.splitlines(1), 'Original', 'Current')),\n    *** Original\n    --- Current\n    ***************\n    *** 1,4 ****\n      one\n    ! two\n    ! three\n      four\n    --- 1,4 ----\n    + zero\n      one\n    ! tree\n      four\n    \"\"\"\n\n    prefix = dict(insert='+ ', delete='- ', replace='! ', equal='  ')\n    started = False\n    for group in SequenceMatcher(None,a,b).get_grouped_opcodes(n):\n        if not started:\n            started = True\n            fromdate = '\\t{}'.format(fromfiledate) if fromfiledate else ''\n            todate = '\\t{}'.format(tofiledate) if tofiledate else ''\n            yield '*** {}{}{}'.format(fromfile, fromdate, lineterm)\n            yield '--- {}{}{}'.format(tofile, todate, lineterm)\n\n        first, last = group[0], group[-1]\n        yield '***************' + lineterm\n\n        file1_range = _format_range_context(first[1], last[2])\n        yield '*** {} ****{}'.format(file1_range, lineterm)\n\n        if any(tag in ('replace', 'delete') for tag, _, _, _, _ in group):\n            for tag, i1, i2, _, _ in group:\n                if tag != 'insert':\n                    for line in a[i1:i2]:\n                        yield prefix[tag] + line\n\n        file2_range = _format_range_context(first[3], last[4])\n        yield '--- {} ----{}'.format(file2_range, lineterm)\n\n        if any(tag in ('replace', 'insert') for tag, _, _, _, _ in group):\n            for tag, _, _, j1, j2 in group:\n                if tag != 'delete':\n                    for line in b[j1:j2]:\n                        yield prefix[tag] + line\n\ndef ndiff(a, b, linejunk=None, charjunk=IS_CHARACTER_JUNK):\n    r\"\"\"\n    Compare `a` and `b` (lists of strings); return a `Differ`-style delta.\n\n    Optional keyword parameters `linejunk` and `charjunk` are for filter\n    functions (or None):\n\n    - linejunk: A function that should accept a single string argument, and\n      return true iff the string is junk.  The default is None, and is\n      recommended; as of Python 2.3, an adaptive notion of \"noise\" lines is\n      used that does a good job on its own.\n\n    - charjunk: A function that should accept a string of length 1. The\n      default is module-level function IS_CHARACTER_JUNK, which filters out\n      whitespace characters (a blank or tab; note: bad idea to include newline\n      in this!).\n\n    Tools/scripts/ndiff.py is a command-line front-end to this function.\n\n    Example:\n\n    >>> diff = ndiff('one\\ntwo\\nthree\\n'.splitlines(1),\n    ...              'ore\\ntree\\nemu\\n'.splitlines(1))\n    >>> print ''.join(diff),\n    - one\n    ?  ^\n    + ore\n    ?  ^\n    - two\n    - three\n    ?  -\n    + tree\n    + emu\n    \"\"\"\n    return Differ(linejunk, charjunk).compare(a, b)\n\ndef _mdiff(fromlines, tolines, context=None, linejunk=None,\n           charjunk=IS_CHARACTER_JUNK):\n    r\"\"\"Returns generator yielding marked up from/to side by side differences.\n\n    Arguments:\n    fromlines -- list of text lines to compared to tolines\n    tolines -- list of text lines to be compared to fromlines\n    context -- number of context lines to display on each side of difference,\n               if None, all from/to text lines will be generated.\n    linejunk -- passed on to ndiff (see ndiff documentation)\n    charjunk -- passed on to ndiff (see ndiff documentation)\n\n    This function returns an iterator which returns a tuple:\n    (from line tuple, to line tuple, boolean flag)\n\n    from/to line tuple -- (line num, line text)\n        line num -- integer or None (to indicate a context separation)\n        line text -- original line text with following markers inserted:\n            '\\0+' -- marks start of added text\n            '\\0-' -- marks start of deleted text\n            '\\0^' -- marks start of changed text\n            '\\1' -- marks end of added/deleted/changed text\n\n    boolean flag -- None indicates context separation, True indicates\n        either \"from\" or \"to\" line contains a change, otherwise False.\n\n    This function/iterator was originally developed to generate side by side\n    file difference for making HTML pages (see HtmlDiff class for example\n    usage).\n\n    Note, this function utilizes the ndiff function to generate the side by\n    side difference markup.  Optional ndiff arguments may be passed to this\n    function and they in turn will be passed to ndiff.\n    \"\"\"\n    import re\n\n    # regular expression for finding intraline change indices\n    change_re = re.compile('(\\++|\\-+|\\^+)')\n\n    # create the difference iterator to generate the differences\n    diff_lines_iterator = ndiff(fromlines,tolines,linejunk,charjunk)\n\n    def _make_line(lines, format_key, side, num_lines=[0,0]):\n        \"\"\"Returns line of text with user's change markup and line formatting.\n\n        lines -- list of lines from the ndiff generator to produce a line of\n                 text from.  When producing the line of text to return, the\n                 lines used are removed from this list.\n        format_key -- '+' return first line in list with \"add\" markup around\n                          the entire line.\n                      '-' return first line in list with \"delete\" markup around\n                          the entire line.\n                      '?' return first line in list with add/delete/change\n                          intraline markup (indices obtained from second line)\n                      None return first line in list with no markup\n        side -- indice into the num_lines list (0=from,1=to)\n        num_lines -- from/to current line number.  This is NOT intended to be a\n                     passed parameter.  It is present as a keyword argument to\n                     maintain memory of the current line numbers between calls\n                     of this function.\n\n        Note, this function is purposefully not defined at the module scope so\n        that data it needs from its parent function (within whose context it\n        is defined) does not need to be of module scope.\n        \"\"\"\n        num_lines[side] += 1\n        # Handle case where no user markup is to be added, just return line of\n        # text with user's line format to allow for usage of the line number.\n        if format_key is None:\n            return (num_lines[side],lines.pop(0)[2:])\n        # Handle case of intraline changes\n        if format_key == '?':\n            text, markers = lines.pop(0), lines.pop(0)\n            # find intraline changes (store change type and indices in tuples)\n            sub_info = []\n            def record_sub_info(match_object,sub_info=sub_info):\n                sub_info.append([match_object.group(1)[0],match_object.span()])\n                return match_object.group(1)\n            change_re.sub(record_sub_info,markers)\n            # process each tuple inserting our special marks that won't be\n            # noticed by an xml/html escaper.\n            for key,(begin,end) in sub_info[::-1]:\n                text = text[0:begin]+'\\0'+key+text[begin:end]+'\\1'+text[end:]\n            text = text[2:]\n        # Handle case of add/delete entire line\n        else:\n            text = lines.pop(0)[2:]\n            # if line of text is just a newline, insert a space so there is\n            # something for the user to highlight and see.\n            if not text:\n                text = ' '\n            # insert marks that won't be noticed by an xml/html escaper.\n            text = '\\0' + format_key + text + '\\1'\n        # Return line of text, first allow user's line formatter to do its\n        # thing (such as adding the line number) then replace the special\n        # marks with what the user's change markup.\n        return (num_lines[side],text)\n\n    def _line_iterator():\n        \"\"\"Yields from/to lines of text with a change indication.\n\n        This function is an iterator.  It itself pulls lines from a\n        differencing iterator, processes them and yields them.  When it can\n        it yields both a \"from\" and a \"to\" line, otherwise it will yield one\n        or the other.  In addition to yielding the lines of from/to text, a\n        boolean flag is yielded to indicate if the text line(s) have\n        differences in them.\n\n        Note, this function is purposefully not defined at the module scope so\n        that data it needs from its parent function (within whose context it\n        is defined) does not need to be of module scope.\n        \"\"\"\n        lines = []\n        num_blanks_pending, num_blanks_to_yield = 0, 0\n        while True:\n            # Load up next 4 lines so we can look ahead, create strings which\n            # are a concatenation of the first character of each of the 4 lines\n            # so we can do some very readable comparisons.\n            while len(lines) < 4:\n                try:\n                    lines.append(diff_lines_iterator.next())\n                except StopIteration:\n                    lines.append('X')\n            s = ''.join([line[0] for line in lines])\n            if s.startswith('X'):\n                # When no more lines, pump out any remaining blank lines so the\n                # corresponding add/delete lines get a matching blank line so\n                # all line pairs get yielded at the next level.\n                num_blanks_to_yield = num_blanks_pending\n            elif s.startswith('-?+?'):\n                # simple intraline change\n                yield _make_line(lines,'?',0), _make_line(lines,'?',1), True\n                continue\n            elif s.startswith('--++'):\n                # in delete block, add block coming: we do NOT want to get\n                # caught up on blank lines yet, just process the delete line\n                num_blanks_pending -= 1\n                yield _make_line(lines,'-',0), None, True\n                continue\n            elif s.startswith(('--?+', '--+', '- ')):\n                # in delete block and see a intraline change or unchanged line\n                # coming: yield the delete line and then blanks\n                from_line,to_line = _make_line(lines,'-',0), None\n                num_blanks_to_yield,num_blanks_pending = num_blanks_pending-1,0\n            elif s.startswith('-+?'):\n                # intraline change\n                yield _make_line(lines,None,0), _make_line(lines,'?',1), True\n                continue\n            elif s.startswith('-?+'):\n                # intraline change\n                yield _make_line(lines,'?',0), _make_line(lines,None,1), True\n                continue\n            elif s.startswith('-'):\n                # delete FROM line\n                num_blanks_pending -= 1\n                yield _make_line(lines,'-',0), None, True\n                continue\n            elif s.startswith('+--'):\n                # in add block, delete block coming: we do NOT want to get\n                # caught up on blank lines yet, just process the add line\n                num_blanks_pending += 1\n                yield None, _make_line(lines,'+',1), True\n                continue\n            elif s.startswith(('+ ', '+-')):\n                # will be leaving an add block: yield blanks then add line\n                from_line, to_line = None, _make_line(lines,'+',1)\n                num_blanks_to_yield,num_blanks_pending = num_blanks_pending+1,0\n            elif s.startswith('+'):\n                # inside an add block, yield the add line\n                num_blanks_pending += 1\n                yield None, _make_line(lines,'+',1), True\n                continue\n            elif s.startswith(' '):\n                # unchanged text, yield it to both sides\n                yield _make_line(lines[:],None,0),_make_line(lines,None,1),False\n                continue\n            # Catch up on the blank lines so when we yield the next from/to\n            # pair, they are lined up.\n            while(num_blanks_to_yield < 0):\n                num_blanks_to_yield += 1\n                yield None,('','\\n'),True\n            while(num_blanks_to_yield > 0):\n                num_blanks_to_yield -= 1\n                yield ('','\\n'),None,True\n            if s.startswith('X'):\n                raise StopIteration\n            else:\n                yield from_line,to_line,True\n\n    def _line_pair_iterator():\n        \"\"\"Yields from/to lines of text with a change indication.\n\n        This function is an iterator.  It itself pulls lines from the line\n        iterator.  Its difference from that iterator is that this function\n        always yields a pair of from/to text lines (with the change\n        indication).  If necessary it will collect single from/to lines\n        until it has a matching pair from/to pair to yield.\n\n        Note, this function is purposefully not defined at the module scope so\n        that data it needs from its parent function (within whose context it\n        is defined) does not need to be of module scope.\n        \"\"\"\n        line_iterator = _line_iterator()\n        fromlines,tolines=[],[]\n        while True:\n            # Collecting lines of text until we have a from/to pair\n            while (len(fromlines)==0 or len(tolines)==0):\n                from_line, to_line, found_diff =line_iterator.next()\n                if from_line is not None:\n                    fromlines.append((from_line,found_diff))\n                if to_line is not None:\n                    tolines.append((to_line,found_diff))\n            # Once we have a pair, remove them from the collection and yield it\n            from_line, fromDiff = fromlines.pop(0)\n            to_line, to_diff = tolines.pop(0)\n            yield (from_line,to_line,fromDiff or to_diff)\n\n    # Handle case where user does not want context differencing, just yield\n    # them up without doing anything else with them.\n    line_pair_iterator = _line_pair_iterator()\n    if context is None:\n        while True:\n            yield line_pair_iterator.next()\n    # Handle case where user wants context differencing.  We must do some\n    # storage of lines until we know for sure that they are to be yielded.\n    else:\n        context += 1\n        lines_to_write = 0\n        while True:\n            # Store lines up until we find a difference, note use of a\n            # circular queue because we only need to keep around what\n            # we need for context.\n            index, contextLines = 0, [None]*(context)\n            found_diff = False\n            while(found_diff is False):\n                from_line, to_line, found_diff = line_pair_iterator.next()\n                i = index % context\n                contextLines[i] = (from_line, to_line, found_diff)\n                index += 1\n            # Yield lines that we have collected so far, but first yield\n            # the user's separator.\n            if index > context:\n                yield None, None, None\n                lines_to_write = context\n            else:\n                lines_to_write = index\n                index = 0\n            while(lines_to_write):\n                i = index % context\n                index += 1\n                yield contextLines[i]\n                lines_to_write -= 1\n            # Now yield the context lines after the change\n            lines_to_write = context-1\n            while(lines_to_write):\n                from_line, to_line, found_diff = line_pair_iterator.next()\n                # If another change within the context, extend the context\n                if found_diff:\n                    lines_to_write = context-1\n                else:\n                    lines_to_write -= 1\n                yield from_line, to_line, found_diff\n\n\n_file_template = \"\"\"\n<!DOCTYPE html PUBLIC \"-//W3C//DTD XHTML 1.0 Transitional//EN\"\n          \"http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd\">\n\n<html>\n\n<head>\n    <meta http-equiv=\"Content-Type\"\n          content=\"text/html; charset=ISO-8859-1\" />\n    <title></title>\n    <style type=\"text/css\">%(styles)s\n    </style>\n</head>\n\n<body>\n    %(table)s%(legend)s\n</body>\n\n</html>\"\"\"\n\n_styles = \"\"\"\n        table.diff {font-family:Courier; border:medium;}\n        .diff_header {background-color:#e0e0e0}\n        td.diff_header {text-align:right}\n        .diff_next {background-color:#c0c0c0}\n        .diff_add {background-color:#aaffaa}\n        .diff_chg {background-color:#ffff77}\n        .diff_sub {background-color:#ffaaaa}\"\"\"\n\n_table_template = \"\"\"\n    <table class=\"diff\" id=\"difflib_chg_%(prefix)s_top\"\n           cellspacing=\"0\" cellpadding=\"0\" rules=\"groups\" >\n        <colgroup></colgroup> <colgroup></colgroup> <colgroup></colgroup>\n        <colgroup></colgroup> <colgroup></colgroup> <colgroup></colgroup>\n        %(header_row)s\n        <tbody>\n%(data_rows)s        </tbody>\n    </table>\"\"\"\n\n_legend = \"\"\"\n    <table class=\"diff\" summary=\"Legends\">\n        <tr> <th colspan=\"2\"> Legends </th> </tr>\n        <tr> <td> <table border=\"\" summary=\"Colors\">\n                      <tr><th> Colors </th> </tr>\n                      <tr><td class=\"diff_add\">&nbsp;Added&nbsp;</td></tr>\n                      <tr><td class=\"diff_chg\">Changed</td> </tr>\n                      <tr><td class=\"diff_sub\">Deleted</td> </tr>\n                  </table></td>\n             <td> <table border=\"\" summary=\"Links\">\n                      <tr><th colspan=\"2\"> Links </th> </tr>\n                      <tr><td>(f)irst change</td> </tr>\n                      <tr><td>(n)ext change</td> </tr>\n                      <tr><td>(t)op</td> </tr>\n                  </table></td> </tr>\n    </table>\"\"\"\n\nclass HtmlDiff(object):\n    \"\"\"For producing HTML side by side comparison with change highlights.\n\n    This class can be used to create an HTML table (or a complete HTML file\n    containing the table) showing a side by side, line by line comparison\n    of text with inter-line and intra-line change highlights.  The table can\n    be generated in either full or contextual difference mode.\n\n    The following methods are provided for HTML generation:\n\n    make_table -- generates HTML for a single side by side table\n    make_file -- generates complete HTML file with a single side by side table\n\n    See tools/scripts/diff.py for an example usage of this class.\n    \"\"\"\n\n    _file_template = _file_template\n    _styles = _styles\n    _table_template = _table_template\n    _legend = _legend\n    _default_prefix = 0\n\n    def __init__(self,tabsize=8,wrapcolumn=None,linejunk=None,\n                 charjunk=IS_CHARACTER_JUNK):\n        \"\"\"HtmlDiff instance initializer\n\n        Arguments:\n        tabsize -- tab stop spacing, defaults to 8.\n        wrapcolumn -- column number where lines are broken and wrapped,\n            defaults to None where lines are not wrapped.\n        linejunk,charjunk -- keyword arguments passed into ndiff() (used to by\n            HtmlDiff() to generate the side by side HTML differences).  See\n            ndiff() documentation for argument default values and descriptions.\n        \"\"\"\n        self._tabsize = tabsize\n        self._wrapcolumn = wrapcolumn\n        self._linejunk = linejunk\n        self._charjunk = charjunk\n\n    def make_file(self,fromlines,tolines,fromdesc='',todesc='',context=False,\n                  numlines=5):\n        \"\"\"Returns HTML file of side by side comparison with change highlights\n\n        Arguments:\n        fromlines -- list of \"from\" lines\n        tolines -- list of \"to\" lines\n        fromdesc -- \"from\" file column header string\n        todesc -- \"to\" file column header string\n        context -- set to True for contextual differences (defaults to False\n            which shows full differences).\n        numlines -- number of context lines.  When context is set True,\n            controls number of lines displayed before and after the change.\n            When context is False, controls the number of lines to place\n            the \"next\" link anchors before the next change (so click of\n            \"next\" link jumps to just before the change).\n        \"\"\"\n\n        return self._file_template % dict(\n            styles = self._styles,\n            legend = self._legend,\n            table = self.make_table(fromlines,tolines,fromdesc,todesc,\n                                    context=context,numlines=numlines))\n\n    def _tab_newline_replace(self,fromlines,tolines):\n        \"\"\"Returns from/to line lists with tabs expanded and newlines removed.\n\n        Instead of tab characters being replaced by the number of spaces\n        needed to fill in to the next tab stop, this function will fill\n        the space with tab characters.  This is done so that the difference\n        algorithms can identify changes in a file when tabs are replaced by\n        spaces and vice versa.  At the end of the HTML generation, the tab\n        characters will be replaced with a nonbreakable space.\n        \"\"\"\n        def expand_tabs(line):\n            # hide real spaces\n            line = line.replace(' ','\\0')\n            # expand tabs into spaces\n            line = line.expandtabs(self._tabsize)\n            # replace spaces from expanded tabs back into tab characters\n            # (we'll replace them with markup after we do differencing)\n            line = line.replace(' ','\\t')\n            return line.replace('\\0',' ').rstrip('\\n')\n        fromlines = [expand_tabs(line) for line in fromlines]\n        tolines = [expand_tabs(line) for line in tolines]\n        return fromlines,tolines\n\n    def _split_line(self,data_list,line_num,text):\n        \"\"\"Builds list of text lines by splitting text lines at wrap point\n\n        This function will determine if the input text line needs to be\n        wrapped (split) into separate lines.  If so, the first wrap point\n        will be determined and the first line appended to the output\n        text line list.  This function is used recursively to handle\n        the second part of the split line to further split it.\n        \"\"\"\n        # if blank line or context separator, just add it to the output list\n        if not line_num:\n            data_list.append((line_num,text))\n            return\n\n        # if line text doesn't need wrapping, just add it to the output list\n        size = len(text)\n        max = self._wrapcolumn\n        if (size <= max) or ((size -(text.count('\\0')*3)) <= max):\n            data_list.append((line_num,text))\n            return\n\n        # scan text looking for the wrap point, keeping track if the wrap\n        # point is inside markers\n        i = 0\n        n = 0\n        mark = ''\n        while n < max and i < size:\n            if text[i] == '\\0':\n                i += 1\n                mark = text[i]\n                i += 1\n            elif text[i] == '\\1':\n                i += 1\n                mark = ''\n            else:\n                i += 1\n                n += 1\n\n        # wrap point is inside text, break it up into separate lines\n        line1 = text[:i]\n        line2 = text[i:]\n\n        # if wrap point is inside markers, place end marker at end of first\n        # line and start marker at beginning of second line because each\n        # line will have its own table tag markup around it.\n        if mark:\n            line1 = line1 + '\\1'\n            line2 = '\\0' + mark + line2\n\n        # tack on first line onto the output list\n        data_list.append((line_num,line1))\n\n        # use this routine again to wrap the remaining text\n        self._split_line(data_list,'>',line2)\n\n    def _line_wrapper(self,diffs):\n        \"\"\"Returns iterator that splits (wraps) mdiff text lines\"\"\"\n\n        # pull from/to data and flags from mdiff iterator\n        for fromdata,todata,flag in diffs:\n            # check for context separators and pass them through\n            if flag is None:\n                yield fromdata,todata,flag\n                continue\n            (fromline,fromtext),(toline,totext) = fromdata,todata\n            # for each from/to line split it at the wrap column to form\n            # list of text lines.\n            fromlist,tolist = [],[]\n            self._split_line(fromlist,fromline,fromtext)\n            self._split_line(tolist,toline,totext)\n            # yield from/to line in pairs inserting blank lines as\n            # necessary when one side has more wrapped lines\n            while fromlist or tolist:\n                if fromlist:\n                    fromdata = fromlist.pop(0)\n                else:\n                    fromdata = ('',' ')\n                if tolist:\n                    todata = tolist.pop(0)\n                else:\n                    todata = ('',' ')\n                yield fromdata,todata,flag\n\n    def _collect_lines(self,diffs):\n        \"\"\"Collects mdiff output into separate lists\n\n        Before storing the mdiff from/to data into a list, it is converted\n        into a single line of text with HTML markup.\n        \"\"\"\n\n        fromlist,tolist,flaglist = [],[],[]\n        # pull from/to data and flags from mdiff style iterator\n        for fromdata,todata,flag in diffs:\n            try:\n                # store HTML markup of the lines into the lists\n                fromlist.append(self._format_line(0,flag,*fromdata))\n                tolist.append(self._format_line(1,flag,*todata))\n            except TypeError:\n                # exceptions occur for lines where context separators go\n                fromlist.append(None)\n                tolist.append(None)\n            flaglist.append(flag)\n        return fromlist,tolist,flaglist\n\n    def _format_line(self,side,flag,linenum,text):\n        \"\"\"Returns HTML markup of \"from\" / \"to\" text lines\n\n        side -- 0 or 1 indicating \"from\" or \"to\" text\n        flag -- indicates if difference on line\n        linenum -- line number (used for line number column)\n        text -- line text to be marked up\n        \"\"\"\n        try:\n            linenum = '%d' % linenum\n            id = ' id=\"%s%s\"' % (self._prefix[side],linenum)\n        except TypeError:\n            # handle blank lines where linenum is '>' or ''\n            id = ''\n        # replace those things that would get confused with HTML symbols\n        text=text.replace(\"&\",\"&amp;\").replace(\">\",\"&gt;\").replace(\"<\",\"&lt;\")\n\n        # make space non-breakable so they don't get compressed or line wrapped\n        text = text.replace(' ','&nbsp;').rstrip()\n\n        return '<td class=\"diff_header\"%s>%s</td><td nowrap=\"nowrap\">%s</td>' \\\n               % (id,linenum,text)\n\n    def _make_prefix(self):\n        \"\"\"Create unique anchor prefixes\"\"\"\n\n        # Generate a unique anchor prefix so multiple tables\n        # can exist on the same HTML page without conflicts.\n        fromprefix = \"from%d_\" % HtmlDiff._default_prefix\n        toprefix = \"to%d_\" % HtmlDiff._default_prefix\n        HtmlDiff._default_prefix += 1\n        # store prefixes so line format method has access\n        self._prefix = [fromprefix,toprefix]\n\n    def _convert_flags(self,fromlist,tolist,flaglist,context,numlines):\n        \"\"\"Makes list of \"next\" links\"\"\"\n\n        # all anchor names will be generated using the unique \"to\" prefix\n        toprefix = self._prefix[1]\n\n        # process change flags, generating middle column of next anchors/links\n        next_id = ['']*len(flaglist)\n        next_href = ['']*len(flaglist)\n        num_chg, in_change = 0, False\n        last = 0\n        for i,flag in enumerate(flaglist):\n            if flag:\n                if not in_change:\n                    in_change = True\n                    last = i\n                    # at the beginning of a change, drop an anchor a few lines\n                    # (the context lines) before the change for the previous\n                    # link\n                    i = max([0,i-numlines])\n                    next_id[i] = ' id=\"difflib_chg_%s_%d\"' % (toprefix,num_chg)\n                    # at the beginning of a change, drop a link to the next\n                    # change\n                    num_chg += 1\n                    next_href[last] = '<a href=\"#difflib_chg_%s_%d\">n</a>' % (\n                         toprefix,num_chg)\n            else:\n                in_change = False\n        # check for cases where there is no content to avoid exceptions\n        if not flaglist:\n            flaglist = [False]\n            next_id = ['']\n            next_href = ['']\n            last = 0\n            if context:\n                fromlist = ['<td></td><td>&nbsp;No Differences Found&nbsp;</td>']\n                tolist = fromlist\n            else:\n                fromlist = tolist = ['<td></td><td>&nbsp;Empty File&nbsp;</td>']\n        # if not a change on first line, drop a link\n        if not flaglist[0]:\n            next_href[0] = '<a href=\"#difflib_chg_%s_0\">f</a>' % toprefix\n        # redo the last link to link to the top\n        next_href[last] = '<a href=\"#difflib_chg_%s_top\">t</a>' % (toprefix)\n\n        return fromlist,tolist,flaglist,next_href,next_id\n\n    def make_table(self,fromlines,tolines,fromdesc='',todesc='',context=False,\n                   numlines=5):\n        \"\"\"Returns HTML table of side by side comparison with change highlights\n\n        Arguments:\n        fromlines -- list of \"from\" lines\n        tolines -- list of \"to\" lines\n        fromdesc -- \"from\" file column header string\n        todesc -- \"to\" file column header string\n        context -- set to True for contextual differences (defaults to False\n            which shows full differences).\n        numlines -- number of context lines.  When context is set True,\n            controls number of lines displayed before and after the change.\n            When context is False, controls the number of lines to place\n            the \"next\" link anchors before the next change (so click of\n            \"next\" link jumps to just before the change).\n        \"\"\"\n\n        # make unique anchor prefixes so that multiple tables may exist\n        # on the same page without conflict.\n        self._make_prefix()\n\n        # change tabs to spaces before it gets more difficult after we insert\n        # markup\n        fromlines,tolines = self._tab_newline_replace(fromlines,tolines)\n\n        # create diffs iterator which generates side by side from/to data\n        if context:\n            context_lines = numlines\n        else:\n            context_lines = None\n        diffs = _mdiff(fromlines,tolines,context_lines,linejunk=self._linejunk,\n                      charjunk=self._charjunk)\n\n        # set up iterator to wrap lines that exceed desired width\n        if self._wrapcolumn:\n            diffs = self._line_wrapper(diffs)\n\n        # collect up from/to lines and flags into lists (also format the lines)\n        fromlist,tolist,flaglist = self._collect_lines(diffs)\n\n        # process change flags, generating middle column of next anchors/links\n        fromlist,tolist,flaglist,next_href,next_id = self._convert_flags(\n            fromlist,tolist,flaglist,context,numlines)\n\n        s = []\n        fmt = '            <tr><td class=\"diff_next\"%s>%s</td>%s' + \\\n              '<td class=\"diff_next\">%s</td>%s</tr>\\n'\n        for i in range(len(flaglist)):\n            if flaglist[i] is None:\n                # mdiff yields None on separator lines skip the bogus ones\n                # generated for the first line\n                if i > 0:\n                    s.append('        </tbody>        \\n        <tbody>\\n')\n            else:\n                s.append( fmt % (next_id[i],next_href[i],fromlist[i],\n                                           next_href[i],tolist[i]))\n        if fromdesc or todesc:\n            header_row = '<thead><tr>%s%s%s%s</tr></thead>' % (\n                '<th class=\"diff_next\"><br /></th>',\n                '<th colspan=\"2\" class=\"diff_header\">%s</th>' % fromdesc,\n                '<th class=\"diff_next\"><br /></th>',\n                '<th colspan=\"2\" class=\"diff_header\">%s</th>' % todesc)\n        else:\n            header_row = ''\n\n        table = self._table_template % dict(\n            data_rows=''.join(s),\n            header_row=header_row,\n            prefix=self._prefix[1])\n\n        return table.replace('\\0+','<span class=\"diff_add\">'). \\\n                     replace('\\0-','<span class=\"diff_sub\">'). \\\n                     replace('\\0^','<span class=\"diff_chg\">'). \\\n                     replace('\\1','</span>'). \\\n                     replace('\\t','&nbsp;')\n\ndel re\n\ndef restore(delta, which):\n    r\"\"\"\n    Generate one of the two sequences that generated a delta.\n\n    Given a `delta` produced by `Differ.compare()` or `ndiff()`, extract\n    lines originating from file 1 or 2 (parameter `which`), stripping off line\n    prefixes.\n\n    Examples:\n\n    >>> diff = ndiff('one\\ntwo\\nthree\\n'.splitlines(1),\n    ...              'ore\\ntree\\nemu\\n'.splitlines(1))\n    >>> diff = list(diff)\n    >>> print ''.join(restore(diff, 1)),\n    one\n    two\n    three\n    >>> print ''.join(restore(diff, 2)),\n    ore\n    tree\n    emu\n    \"\"\"\n    try:\n        tag = {1: \"- \", 2: \"+ \"}[int(which)]\n    except KeyError:\n        raise ValueError, ('unknown delta choice (must be 1 or 2): %r'\n                           % which)\n    prefixes = (\"  \", tag)\n    for line in delta:\n        if line[:2] in prefixes:\n            yield line[2:]\n\ndef _test():\n    import doctest, difflib\n    return doctest.testmod(difflib)\n\nif __name__ == \"__main__\":\n    _test()\n", 
    "dis": "\"\"\"Disassembler of Python byte code into mnemonics.\"\"\"\n\nimport sys\nimport types\n\nfrom opcode import *\nfrom opcode import __all__ as _opcodes_all\n\n__all__ = [\"dis\", \"disassemble\", \"distb\", \"disco\",\n           \"findlinestarts\", \"findlabels\"] + _opcodes_all\ndel _opcodes_all\n\n_have_code = (types.MethodType, types.FunctionType, types.CodeType,\n              types.ClassType, type)\n\ndef dis(x=None):\n    \"\"\"Disassemble classes, methods, functions, or code.\n\n    With no argument, disassemble the last traceback.\n\n    \"\"\"\n    if x is None:\n        distb()\n        return\n    if isinstance(x, types.InstanceType):\n        x = x.__class__\n    if hasattr(x, 'im_func'):\n        x = x.im_func\n    if hasattr(x, 'func_code'):\n        x = x.func_code\n    if hasattr(x, '__dict__'):\n        items = x.__dict__.items()\n        items.sort()\n        for name, x1 in items:\n            if isinstance(x1, _have_code):\n                print \"Disassembly of %s:\" % name\n                try:\n                    dis(x1)\n                except TypeError, msg:\n                    print \"Sorry:\", msg\n                print\n    elif hasattr(x, 'co_code'):\n        disassemble(x)\n    elif isinstance(x, str):\n        disassemble_string(x)\n    else:\n        raise TypeError, \\\n              \"don't know how to disassemble %s objects\" % \\\n              type(x).__name__\n\ndef distb(tb=None):\n    \"\"\"Disassemble a traceback (default: last traceback).\"\"\"\n    if tb is None:\n        try:\n            tb = sys.last_traceback\n        except AttributeError:\n            raise RuntimeError, \"no last traceback to disassemble\"\n        while tb.tb_next: tb = tb.tb_next\n    disassemble(tb.tb_frame.f_code, tb.tb_lasti)\n\ndef disassemble(co, lasti=-1):\n    \"\"\"Disassemble a code object.\"\"\"\n    code = co.co_code\n    labels = findlabels(code)\n    linestarts = dict(findlinestarts(co))\n    n = len(code)\n    i = 0\n    extended_arg = 0\n    free = None\n    while i < n:\n        c = code[i]\n        op = ord(c)\n        if i in linestarts:\n            if i > 0:\n                print\n            print \"%3d\" % linestarts[i],\n        else:\n            print '   ',\n\n        if i == lasti: print '-->',\n        else: print '   ',\n        if i in labels: print '>>',\n        else: print '  ',\n        print repr(i).rjust(4),\n        print opname[op].ljust(20),\n        i = i+1\n        if op >= HAVE_ARGUMENT:\n            oparg = ord(code[i]) + ord(code[i+1])*256 + extended_arg\n            extended_arg = 0\n            i = i+2\n            if op == EXTENDED_ARG:\n                extended_arg = oparg*65536L\n            print repr(oparg).rjust(5),\n            if op in hasconst:\n                print '(' + repr(co.co_consts[oparg]) + ')',\n            elif op in hasname:\n                print '(' + co.co_names[oparg] + ')',\n            elif op in hasjrel:\n                print '(to ' + repr(i + oparg) + ')',\n            elif op in haslocal:\n                print '(' + co.co_varnames[oparg] + ')',\n            elif op in hascompare:\n                print '(' + cmp_op[oparg] + ')',\n            elif op in hasfree:\n                if free is None:\n                    free = co.co_cellvars + co.co_freevars\n                print '(' + free[oparg] + ')',\n        print\n\ndef disassemble_string(code, lasti=-1, varnames=None, names=None,\n                       constants=None):\n    labels = findlabels(code)\n    n = len(code)\n    i = 0\n    while i < n:\n        c = code[i]\n        op = ord(c)\n        if i == lasti: print '-->',\n        else: print '   ',\n        if i in labels: print '>>',\n        else: print '  ',\n        print repr(i).rjust(4),\n        print opname[op].ljust(15),\n        i = i+1\n        if op >= HAVE_ARGUMENT:\n            oparg = ord(code[i]) + ord(code[i+1])*256\n            i = i+2\n            print repr(oparg).rjust(5),\n            if op in hasconst:\n                if constants:\n                    print '(' + repr(constants[oparg]) + ')',\n                else:\n                    print '(%d)'%oparg,\n            elif op in hasname:\n                if names is not None:\n                    print '(' + names[oparg] + ')',\n                else:\n                    print '(%d)'%oparg,\n            elif op in hasjrel:\n                print '(to ' + repr(i + oparg) + ')',\n            elif op in haslocal:\n                if varnames:\n                    print '(' + varnames[oparg] + ')',\n                else:\n                    print '(%d)' % oparg,\n            elif op in hascompare:\n                print '(' + cmp_op[oparg] + ')',\n        print\n\ndisco = disassemble                     # XXX For backwards compatibility\n\ndef findlabels(code):\n    \"\"\"Detect all offsets in a byte code which are jump targets.\n\n    Return the list of offsets.\n\n    \"\"\"\n    labels = []\n    n = len(code)\n    i = 0\n    while i < n:\n        c = code[i]\n        op = ord(c)\n        i = i+1\n        if op >= HAVE_ARGUMENT:\n            oparg = ord(code[i]) + ord(code[i+1])*256\n            i = i+2\n            label = -1\n            if op in hasjrel:\n                label = i+oparg\n            elif op in hasjabs:\n                label = oparg\n            if label >= 0:\n                if label not in labels:\n                    labels.append(label)\n    return labels\n\ndef findlinestarts(code):\n    \"\"\"Find the offsets in a byte code which are start of lines in the source.\n\n    Generate pairs (offset, lineno) as described in Python/compile.c.\n\n    \"\"\"\n    byte_increments = [ord(c) for c in code.co_lnotab[0::2]]\n    line_increments = [ord(c) for c in code.co_lnotab[1::2]]\n\n    lastlineno = None\n    lineno = code.co_firstlineno\n    addr = 0\n    for byte_incr, line_incr in zip(byte_increments, line_increments):\n        if byte_incr:\n            if lineno != lastlineno:\n                yield (addr, lineno)\n                lastlineno = lineno\n            addr += byte_incr\n        lineno += line_incr\n    if lineno != lastlineno:\n        yield (addr, lineno)\n\ndef _test():\n    \"\"\"Simple test program to disassemble a file.\"\"\"\n    if sys.argv[1:]:\n        if sys.argv[2:]:\n            sys.stderr.write(\"usage: python dis.py [-|file]\\n\")\n            sys.exit(2)\n        fn = sys.argv[1]\n        if not fn or fn == \"-\":\n            fn = None\n    else:\n        fn = None\n    if fn is None:\n        f = sys.stdin\n    else:\n        f = open(fn)\n    source = f.read()\n    if fn is not None:\n        f.close()\n    else:\n        fn = \"<stdin>\"\n    code = compile(source, fn, \"exec\")\n    dis(code)\n\nif __name__ == \"__main__\":\n    _test()\n", 
    "doctest": "# Module doctest.\n# Released to the public domain 16-Jan-2001, by Tim Peters (tim@python.org).\n# Major enhancements and refactoring by:\n#     Jim Fulton\n#     Edward Loper\n\n# Provided as-is; use at your own risk; no warranty; no promises; enjoy!\n\nr\"\"\"Module doctest -- a framework for running examples in docstrings.\n\nIn simplest use, end each module M to be tested with:\n\ndef _test():\n    import doctest\n    doctest.testmod()\n\nif __name__ == \"__main__\":\n    _test()\n\nThen running the module as a script will cause the examples in the\ndocstrings to get executed and verified:\n\npython M.py\n\nThis won't display anything unless an example fails, in which case the\nfailing example(s) and the cause(s) of the failure(s) are printed to stdout\n(why not stderr? because stderr is a lame hack <0.2 wink>), and the final\nline of output is \"Test failed.\".\n\nRun it with the -v switch instead:\n\npython M.py -v\n\nand a detailed report of all examples tried is printed to stdout, along\nwith assorted summaries at the end.\n\nYou can force verbose mode by passing \"verbose=True\" to testmod, or prohibit\nit by passing \"verbose=False\".  In either of those cases, sys.argv is not\nexamined by testmod.\n\nThere are a variety of other ways to run doctests, including integration\nwith the unittest framework, and support for running non-Python text\nfiles containing doctests.  There are also many ways to override parts\nof doctest's default behaviors.  See the Library Reference Manual for\ndetails.\n\"\"\"\n\n__docformat__ = 'reStructuredText en'\n\n__all__ = [\n    # 0, Option Flags\n    'register_optionflag',\n    'DONT_ACCEPT_TRUE_FOR_1',\n    'DONT_ACCEPT_BLANKLINE',\n    'NORMALIZE_WHITESPACE',\n    'ELLIPSIS',\n    'SKIP',\n    'IGNORE_EXCEPTION_DETAIL',\n    'COMPARISON_FLAGS',\n    'REPORT_UDIFF',\n    'REPORT_CDIFF',\n    'REPORT_NDIFF',\n    'REPORT_ONLY_FIRST_FAILURE',\n    'REPORTING_FLAGS',\n    # 1. Utility Functions\n    # 2. Example & DocTest\n    'Example',\n    'DocTest',\n    # 3. Doctest Parser\n    'DocTestParser',\n    # 4. Doctest Finder\n    'DocTestFinder',\n    # 5. Doctest Runner\n    'DocTestRunner',\n    'OutputChecker',\n    'DocTestFailure',\n    'UnexpectedException',\n    'DebugRunner',\n    # 6. Test Functions\n    'testmod',\n    'testfile',\n    'run_docstring_examples',\n    # 7. Tester\n    'Tester',\n    # 8. Unittest Support\n    'DocTestSuite',\n    'DocFileSuite',\n    'set_unittest_reportflags',\n    # 9. Debugging Support\n    'script_from_examples',\n    'testsource',\n    'debug_src',\n    'debug',\n]\n\nimport __future__\n\nimport sys, traceback, inspect, linecache, os, re\nimport unittest, difflib, pdb, tempfile\nimport warnings\nfrom StringIO import StringIO\nfrom collections import namedtuple\n\nTestResults = namedtuple('TestResults', 'failed attempted')\n\n# There are 4 basic classes:\n#  - Example: a <source, want> pair, plus an intra-docstring line number.\n#  - DocTest: a collection of examples, parsed from a docstring, plus\n#    info about where the docstring came from (name, filename, lineno).\n#  - DocTestFinder: extracts DocTests from a given object's docstring and\n#    its contained objects' docstrings.\n#  - DocTestRunner: runs DocTest cases, and accumulates statistics.\n#\n# So the basic picture is:\n#\n#                             list of:\n# +------+                   +---------+                   +-------+\n# |object| --DocTestFinder-> | DocTest | --DocTestRunner-> |results|\n# +------+                   +---------+                   +-------+\n#                            | Example |\n#                            |   ...   |\n#                            | Example |\n#                            +---------+\n\n# Option constants.\n\nOPTIONFLAGS_BY_NAME = {}\ndef register_optionflag(name):\n    # Create a new flag unless `name` is already known.\n    return OPTIONFLAGS_BY_NAME.setdefault(name, 1 << len(OPTIONFLAGS_BY_NAME))\n\nDONT_ACCEPT_TRUE_FOR_1 = register_optionflag('DONT_ACCEPT_TRUE_FOR_1')\nDONT_ACCEPT_BLANKLINE = register_optionflag('DONT_ACCEPT_BLANKLINE')\nNORMALIZE_WHITESPACE = register_optionflag('NORMALIZE_WHITESPACE')\nELLIPSIS = register_optionflag('ELLIPSIS')\nSKIP = register_optionflag('SKIP')\nIGNORE_EXCEPTION_DETAIL = register_optionflag('IGNORE_EXCEPTION_DETAIL')\n\nCOMPARISON_FLAGS = (DONT_ACCEPT_TRUE_FOR_1 |\n                    DONT_ACCEPT_BLANKLINE |\n                    NORMALIZE_WHITESPACE |\n                    ELLIPSIS |\n                    SKIP |\n                    IGNORE_EXCEPTION_DETAIL)\n\nREPORT_UDIFF = register_optionflag('REPORT_UDIFF')\nREPORT_CDIFF = register_optionflag('REPORT_CDIFF')\nREPORT_NDIFF = register_optionflag('REPORT_NDIFF')\nREPORT_ONLY_FIRST_FAILURE = register_optionflag('REPORT_ONLY_FIRST_FAILURE')\n\nREPORTING_FLAGS = (REPORT_UDIFF |\n                   REPORT_CDIFF |\n                   REPORT_NDIFF |\n                   REPORT_ONLY_FIRST_FAILURE)\n\n# Special string markers for use in `want` strings:\nBLANKLINE_MARKER = '<BLANKLINE>'\nELLIPSIS_MARKER = '...'\n\n######################################################################\n## Table of Contents\n######################################################################\n#  1. Utility Functions\n#  2. Example & DocTest -- store test cases\n#  3. DocTest Parser -- extracts examples from strings\n#  4. DocTest Finder -- extracts test cases from objects\n#  5. DocTest Runner -- runs test cases\n#  6. Test Functions -- convenient wrappers for testing\n#  7. Tester Class -- for backwards compatibility\n#  8. Unittest Support\n#  9. Debugging Support\n# 10. Example Usage\n\n######################################################################\n## 1. Utility Functions\n######################################################################\n\ndef _extract_future_flags(globs):\n    \"\"\"\n    Return the compiler-flags associated with the future features that\n    have been imported into the given namespace (globs).\n    \"\"\"\n    flags = 0\n    for fname in __future__.all_feature_names:\n        feature = globs.get(fname, None)\n        if feature is getattr(__future__, fname):\n            flags |= feature.compiler_flag\n    return flags\n\ndef _normalize_module(module, depth=2):\n    \"\"\"\n    Return the module specified by `module`.  In particular:\n      - If `module` is a module, then return module.\n      - If `module` is a string, then import and return the\n        module with that name.\n      - If `module` is None, then return the calling module.\n        The calling module is assumed to be the module of\n        the stack frame at the given depth in the call stack.\n    \"\"\"\n    if inspect.ismodule(module):\n        return module\n    elif isinstance(module, (str, unicode)):\n        return __import__(module, globals(), locals(), [\"*\"])\n    elif module is None:\n        return sys.modules[sys._getframe(depth).f_globals['__name__']]\n    else:\n        raise TypeError(\"Expected a module, string, or None\")\n\ndef _load_testfile(filename, package, module_relative):\n    if module_relative:\n        package = _normalize_module(package, 3)\n        filename = _module_relative_path(package, filename)\n        if hasattr(package, '__loader__'):\n            if hasattr(package.__loader__, 'get_data'):\n                file_contents = package.__loader__.get_data(filename)\n                # get_data() opens files as 'rb', so one must do the equivalent\n                # conversion as universal newlines would do.\n                return file_contents.replace(os.linesep, '\\n'), filename\n    with open(filename, 'U') as f:\n        return f.read(), filename\n\n# Use sys.stdout encoding for ouput.\n_encoding = getattr(sys.__stdout__, 'encoding', None) or 'utf-8'\n\ndef _indent(s, indent=4):\n    \"\"\"\n    Add the given number of space characters to the beginning of\n    every non-blank line in `s`, and return the result.\n    If the string `s` is Unicode, it is encoded using the stdout\n    encoding and the `backslashreplace` error handler.\n    \"\"\"\n    if isinstance(s, unicode):\n        s = s.encode(_encoding, 'backslashreplace')\n    # This regexp matches the start of non-blank lines:\n    return re.sub('(?m)^(?!$)', indent*' ', s)\n\ndef _exception_traceback(exc_info):\n    \"\"\"\n    Return a string containing a traceback message for the given\n    exc_info tuple (as returned by sys.exc_info()).\n    \"\"\"\n    # Get a traceback message.\n    excout = StringIO()\n    exc_type, exc_val, exc_tb = exc_info\n    traceback.print_exception(exc_type, exc_val, exc_tb, file=excout)\n    return excout.getvalue()\n\n# Override some StringIO methods.\nclass _SpoofOut(StringIO):\n    def getvalue(self):\n        result = StringIO.getvalue(self)\n        # If anything at all was written, make sure there's a trailing\n        # newline.  There's no way for the expected output to indicate\n        # that a trailing newline is missing.\n        if result and not result.endswith(\"\\n\"):\n            result += \"\\n\"\n        # Prevent softspace from screwing up the next test case, in\n        # case they used print with a trailing comma in an example.\n        if hasattr(self, \"softspace\"):\n            del self.softspace\n        return result\n\n    def truncate(self,   size=None):\n        StringIO.truncate(self, size)\n        if hasattr(self, \"softspace\"):\n            del self.softspace\n        if not self.buf:\n            # Reset it to an empty string, to make sure it's not unicode.\n            self.buf = ''\n\n# Worst-case linear-time ellipsis matching.\ndef _ellipsis_match(want, got):\n    \"\"\"\n    Essentially the only subtle case:\n    >>> _ellipsis_match('aa...aa', 'aaa')\n    False\n    \"\"\"\n    if ELLIPSIS_MARKER not in want:\n        return want == got\n\n    # Find \"the real\" strings.\n    ws = want.split(ELLIPSIS_MARKER)\n    assert len(ws) >= 2\n\n    # Deal with exact matches possibly needed at one or both ends.\n    startpos, endpos = 0, len(got)\n    w = ws[0]\n    if w:   # starts with exact match\n        if got.startswith(w):\n            startpos = len(w)\n            del ws[0]\n        else:\n            return False\n    w = ws[-1]\n    if w:   # ends with exact match\n        if got.endswith(w):\n            endpos -= len(w)\n            del ws[-1]\n        else:\n            return False\n\n    if startpos > endpos:\n        # Exact end matches required more characters than we have, as in\n        # _ellipsis_match('aa...aa', 'aaa')\n        return False\n\n    # For the rest, we only need to find the leftmost non-overlapping\n    # match for each piece.  If there's no overall match that way alone,\n    # there's no overall match period.\n    for w in ws:\n        # w may be '' at times, if there are consecutive ellipses, or\n        # due to an ellipsis at the start or end of `want`.  That's OK.\n        # Search for an empty string succeeds, and doesn't change startpos.\n        startpos = got.find(w, startpos, endpos)\n        if startpos < 0:\n            return False\n        startpos += len(w)\n\n    return True\n\ndef _comment_line(line):\n    \"Return a commented form of the given line\"\n    line = line.rstrip()\n    if line:\n        return '# '+line\n    else:\n        return '#'\n\ndef _strip_exception_details(msg):\n    # Support for IGNORE_EXCEPTION_DETAIL.\n    # Get rid of everything except the exception name; in particular, drop\n    # the possibly dotted module path (if any) and the exception message (if\n    # any).  We assume that a colon is never part of a dotted name, or of an\n    # exception name.\n    # E.g., given\n    #    \"foo.bar.MyError: la di da\"\n    # return \"MyError\"\n    # Or for \"abc.def\" or \"abc.def:\\n\" return \"def\".\n\n    start, end = 0, len(msg)\n    # The exception name must appear on the first line.\n    i = msg.find(\"\\n\")\n    if i >= 0:\n        end = i\n    # retain up to the first colon (if any)\n    i = msg.find(':', 0, end)\n    if i >= 0:\n        end = i\n    # retain just the exception name\n    i = msg.rfind('.', 0, end)\n    if i >= 0:\n        start = i+1\n    return msg[start: end]\n\nclass _OutputRedirectingPdb(pdb.Pdb):\n    \"\"\"\n    A specialized version of the python debugger that redirects stdout\n    to a given stream when interacting with the user.  Stdout is *not*\n    redirected when traced code is executed.\n    \"\"\"\n    def __init__(self, out):\n        self.__out = out\n        self.__debugger_used = False\n        pdb.Pdb.__init__(self, stdout=out)\n        # still use input() to get user input\n        self.use_rawinput = 1\n\n    def set_trace(self, frame=None):\n        self.__debugger_used = True\n        if frame is None:\n            frame = sys._getframe().f_back\n        pdb.Pdb.set_trace(self, frame)\n\n    def set_continue(self):\n        # Calling set_continue unconditionally would break unit test\n        # coverage reporting, as Bdb.set_continue calls sys.settrace(None).\n        if self.__debugger_used:\n            pdb.Pdb.set_continue(self)\n\n    def trace_dispatch(self, *args):\n        # Redirect stdout to the given stream.\n        save_stdout = sys.stdout\n        sys.stdout = self.__out\n        # Call Pdb's trace dispatch method.\n        try:\n            return pdb.Pdb.trace_dispatch(self, *args)\n        finally:\n            sys.stdout = save_stdout\n\n# [XX] Normalize with respect to os.path.pardir?\ndef _module_relative_path(module, path):\n    if not inspect.ismodule(module):\n        raise TypeError, 'Expected a module: %r' % module\n    if path.startswith('/'):\n        raise ValueError, 'Module-relative files may not have absolute paths'\n\n    # Find the base directory for the path.\n    if hasattr(module, '__file__'):\n        # A normal module/package\n        basedir = os.path.split(module.__file__)[0]\n    elif module.__name__ == '__main__':\n        # An interactive session.\n        if len(sys.argv)>0 and sys.argv[0] != '':\n            basedir = os.path.split(sys.argv[0])[0]\n        else:\n            basedir = os.curdir\n    else:\n        # A module w/o __file__ (this includes builtins)\n        raise ValueError(\"Can't resolve paths relative to the module \" +\n                         module + \" (it has no __file__)\")\n\n    # Combine the base directory and the path.\n    return os.path.join(basedir, *(path.split('/')))\n\n######################################################################\n## 2. Example & DocTest\n######################################################################\n## - An \"example\" is a <source, want> pair, where \"source\" is a\n##   fragment of source code, and \"want\" is the expected output for\n##   \"source.\"  The Example class also includes information about\n##   where the example was extracted from.\n##\n## - A \"doctest\" is a collection of examples, typically extracted from\n##   a string (such as an object's docstring).  The DocTest class also\n##   includes information about where the string was extracted from.\n\nclass Example:\n    \"\"\"\n    A single doctest example, consisting of source code and expected\n    output.  `Example` defines the following attributes:\n\n      - source: A single Python statement, always ending with a newline.\n        The constructor adds a newline if needed.\n\n      - want: The expected output from running the source code (either\n        from stdout, or a traceback in case of exception).  `want` ends\n        with a newline unless it's empty, in which case it's an empty\n        string.  The constructor adds a newline if needed.\n\n      - exc_msg: The exception message generated by the example, if\n        the example is expected to generate an exception; or `None` if\n        it is not expected to generate an exception.  This exception\n        message is compared against the return value of\n        `traceback.format_exception_only()`.  `exc_msg` ends with a\n        newline unless it's `None`.  The constructor adds a newline\n        if needed.\n\n      - lineno: The line number within the DocTest string containing\n        this Example where the Example begins.  This line number is\n        zero-based, with respect to the beginning of the DocTest.\n\n      - indent: The example's indentation in the DocTest string.\n        I.e., the number of space characters that precede the\n        example's first prompt.\n\n      - options: A dictionary mapping from option flags to True or\n        False, which is used to override default options for this\n        example.  Any option flags not contained in this dictionary\n        are left at their default value (as specified by the\n        DocTestRunner's optionflags).  By default, no options are set.\n    \"\"\"\n    def __init__(self, source, want, exc_msg=None, lineno=0, indent=0,\n                 options=None):\n        # Normalize inputs.\n        if not source.endswith('\\n'):\n            source += '\\n'\n        if want and not want.endswith('\\n'):\n            want += '\\n'\n        if exc_msg is not None and not exc_msg.endswith('\\n'):\n            exc_msg += '\\n'\n        # Store properties.\n        self.source = source\n        self.want = want\n        self.lineno = lineno\n        self.indent = indent\n        if options is None: options = {}\n        self.options = options\n        self.exc_msg = exc_msg\n\n    def __eq__(self, other):\n        if type(self) is not type(other):\n            return NotImplemented\n\n        return self.source == other.source and \\\n               self.want == other.want and \\\n               self.lineno == other.lineno and \\\n               self.indent == other.indent and \\\n               self.options == other.options and \\\n               self.exc_msg == other.exc_msg\n\n    def __ne__(self, other):\n        return not self == other\n\n    def __hash__(self):\n        return hash((self.source, self.want, self.lineno, self.indent,\n                     self.exc_msg))\n\n\nclass DocTest:\n    \"\"\"\n    A collection of doctest examples that should be run in a single\n    namespace.  Each `DocTest` defines the following attributes:\n\n      - examples: the list of examples.\n\n      - globs: The namespace (aka globals) that the examples should\n        be run in.\n\n      - name: A name identifying the DocTest (typically, the name of\n        the object whose docstring this DocTest was extracted from).\n\n      - filename: The name of the file that this DocTest was extracted\n        from, or `None` if the filename is unknown.\n\n      - lineno: The line number within filename where this DocTest\n        begins, or `None` if the line number is unavailable.  This\n        line number is zero-based, with respect to the beginning of\n        the file.\n\n      - docstring: The string that the examples were extracted from,\n        or `None` if the string is unavailable.\n    \"\"\"\n    def __init__(self, examples, globs, name, filename, lineno, docstring):\n        \"\"\"\n        Create a new DocTest containing the given examples.  The\n        DocTest's globals are initialized with a copy of `globs`.\n        \"\"\"\n        assert not isinstance(examples, basestring), \\\n               \"DocTest no longer accepts str; use DocTestParser instead\"\n        self.examples = examples\n        self.docstring = docstring\n        self.globs = globs.copy()\n        self.name = name\n        self.filename = filename\n        self.lineno = lineno\n\n    def __repr__(self):\n        if len(self.examples) == 0:\n            examples = 'no examples'\n        elif len(self.examples) == 1:\n            examples = '1 example'\n        else:\n            examples = '%d examples' % len(self.examples)\n        return ('<DocTest %s from %s:%s (%s)>' %\n                (self.name, self.filename, self.lineno, examples))\n\n    def __eq__(self, other):\n        if type(self) is not type(other):\n            return NotImplemented\n\n        return self.examples == other.examples and \\\n               self.docstring == other.docstring and \\\n               self.globs == other.globs and \\\n               self.name == other.name and \\\n               self.filename == other.filename and \\\n               self.lineno == other.lineno\n\n    def __ne__(self, other):\n        return not self == other\n\n    def __hash__(self):\n        return hash((self.docstring, self.name, self.filename, self.lineno))\n\n    # This lets us sort tests by name:\n    def __cmp__(self, other):\n        if not isinstance(other, DocTest):\n            return -1\n        return cmp((self.name, self.filename, self.lineno, id(self)),\n                   (other.name, other.filename, other.lineno, id(other)))\n\n######################################################################\n## 3. DocTestParser\n######################################################################\n\nclass DocTestParser:\n    \"\"\"\n    A class used to parse strings containing doctest examples.\n    \"\"\"\n    # This regular expression is used to find doctest examples in a\n    # string.  It defines three groups: `source` is the source code\n    # (including leading indentation and prompts); `indent` is the\n    # indentation of the first (PS1) line of the source code; and\n    # `want` is the expected output (including leading indentation).\n    _EXAMPLE_RE = re.compile(r'''\n        # Source consists of a PS1 line followed by zero or more PS2 lines.\n        (?P<source>\n            (?:^(?P<indent> [ ]*) >>>    .*)    # PS1 line\n            (?:\\n           [ ]*  \\.\\.\\. .*)*)  # PS2 lines\n        \\n?\n        # Want consists of any non-blank lines that do not start with PS1.\n        (?P<want> (?:(?![ ]*$)    # Not a blank line\n                     (?![ ]*>>>)  # Not a line starting with PS1\n                     .+$\\n?       # But any other line\n                  )*)\n        ''', re.MULTILINE | re.VERBOSE)\n\n    # A regular expression for handling `want` strings that contain\n    # expected exceptions.  It divides `want` into three pieces:\n    #    - the traceback header line (`hdr`)\n    #    - the traceback stack (`stack`)\n    #    - the exception message (`msg`), as generated by\n    #      traceback.format_exception_only()\n    # `msg` may have multiple lines.  We assume/require that the\n    # exception message is the first non-indented line starting with a word\n    # character following the traceback header line.\n    _EXCEPTION_RE = re.compile(r\"\"\"\n        # Grab the traceback header.  Different versions of Python have\n        # said different things on the first traceback line.\n        ^(?P<hdr> Traceback\\ \\(\n            (?: most\\ recent\\ call\\ last\n            |   innermost\\ last\n            ) \\) :\n        )\n        \\s* $                # toss trailing whitespace on the header.\n        (?P<stack> .*?)      # don't blink: absorb stuff until...\n        ^ (?P<msg> \\w+ .*)   #     a line *starts* with alphanum.\n        \"\"\", re.VERBOSE | re.MULTILINE | re.DOTALL)\n\n    # A callable returning a true value iff its argument is a blank line\n    # or contains a single comment.\n    _IS_BLANK_OR_COMMENT = re.compile(r'^[ ]*(#.*)?$').match\n\n    def parse(self, string, name='<string>'):\n        \"\"\"\n        Divide the given string into examples and intervening text,\n        and return them as a list of alternating Examples and strings.\n        Line numbers for the Examples are 0-based.  The optional\n        argument `name` is a name identifying this string, and is only\n        used for error messages.\n        \"\"\"\n        string = string.expandtabs()\n        # If all lines begin with the same indentation, then strip it.\n        min_indent = self._min_indent(string)\n        if min_indent > 0:\n            string = '\\n'.join([l[min_indent:] for l in string.split('\\n')])\n\n        output = []\n        charno, lineno = 0, 0\n        # Find all doctest examples in the string:\n        for m in self._EXAMPLE_RE.finditer(string):\n            # Add the pre-example text to `output`.\n            output.append(string[charno:m.start()])\n            # Update lineno (lines before this example)\n            lineno += string.count('\\n', charno, m.start())\n            # Extract info from the regexp match.\n            (source, options, want, exc_msg) = \\\n                     self._parse_example(m, name, lineno)\n            # Create an Example, and add it to the list.\n            if not self._IS_BLANK_OR_COMMENT(source):\n                output.append( Example(source, want, exc_msg,\n                                    lineno=lineno,\n                                    indent=min_indent+len(m.group('indent')),\n                                    options=options) )\n            # Update lineno (lines inside this example)\n            lineno += string.count('\\n', m.start(), m.end())\n            # Update charno.\n            charno = m.end()\n        # Add any remaining post-example text to `output`.\n        output.append(string[charno:])\n        return output\n\n    def get_doctest(self, string, globs, name, filename, lineno):\n        \"\"\"\n        Extract all doctest examples from the given string, and\n        collect them into a `DocTest` object.\n\n        `globs`, `name`, `filename`, and `lineno` are attributes for\n        the new `DocTest` object.  See the documentation for `DocTest`\n        for more information.\n        \"\"\"\n        return DocTest(self.get_examples(string, name), globs,\n                       name, filename, lineno, string)\n\n    def get_examples(self, string, name='<string>'):\n        \"\"\"\n        Extract all doctest examples from the given string, and return\n        them as a list of `Example` objects.  Line numbers are\n        0-based, because it's most common in doctests that nothing\n        interesting appears on the same line as opening triple-quote,\n        and so the first interesting line is called \\\"line 1\\\" then.\n\n        The optional argument `name` is a name identifying this\n        string, and is only used for error messages.\n        \"\"\"\n        return [x for x in self.parse(string, name)\n                if isinstance(x, Example)]\n\n    def _parse_example(self, m, name, lineno):\n        \"\"\"\n        Given a regular expression match from `_EXAMPLE_RE` (`m`),\n        return a pair `(source, want)`, where `source` is the matched\n        example's source code (with prompts and indentation stripped);\n        and `want` is the example's expected output (with indentation\n        stripped).\n\n        `name` is the string's name, and `lineno` is the line number\n        where the example starts; both are used for error messages.\n        \"\"\"\n        # Get the example's indentation level.\n        indent = len(m.group('indent'))\n\n        # Divide source into lines; check that they're properly\n        # indented; and then strip their indentation & prompts.\n        source_lines = m.group('source').split('\\n')\n        self._check_prompt_blank(source_lines, indent, name, lineno)\n        self._check_prefix(source_lines[1:], ' '*indent + '.', name, lineno)\n        source = '\\n'.join([sl[indent+4:] for sl in source_lines])\n\n        # Divide want into lines; check that it's properly indented; and\n        # then strip the indentation.  Spaces before the last newline should\n        # be preserved, so plain rstrip() isn't good enough.\n        want = m.group('want')\n        want_lines = want.split('\\n')\n        if len(want_lines) > 1 and re.match(r' *$', want_lines[-1]):\n            del want_lines[-1]  # forget final newline & spaces after it\n        self._check_prefix(want_lines, ' '*indent, name,\n                           lineno + len(source_lines))\n        want = '\\n'.join([wl[indent:] for wl in want_lines])\n\n        # If `want` contains a traceback message, then extract it.\n        m = self._EXCEPTION_RE.match(want)\n        if m:\n            exc_msg = m.group('msg')\n        else:\n            exc_msg = None\n\n        # Extract options from the source.\n        options = self._find_options(source, name, lineno)\n\n        return source, options, want, exc_msg\n\n    # This regular expression looks for option directives in the\n    # source code of an example.  Option directives are comments\n    # starting with \"doctest:\".  Warning: this may give false\n    # positives for string-literals that contain the string\n    # \"#doctest:\".  Eliminating these false positives would require\n    # actually parsing the string; but we limit them by ignoring any\n    # line containing \"#doctest:\" that is *followed* by a quote mark.\n    _OPTION_DIRECTIVE_RE = re.compile(r'#\\s*doctest:\\s*([^\\n\\'\"]*)$',\n                                      re.MULTILINE)\n\n    def _find_options(self, source, name, lineno):\n        \"\"\"\n        Return a dictionary containing option overrides extracted from\n        option directives in the given source string.\n\n        `name` is the string's name, and `lineno` is the line number\n        where the example starts; both are used for error messages.\n        \"\"\"\n        options = {}\n        # (note: with the current regexp, this will match at most once:)\n        for m in self._OPTION_DIRECTIVE_RE.finditer(source):\n            option_strings = m.group(1).replace(',', ' ').split()\n            for option in option_strings:\n                if (option[0] not in '+-' or\n                    option[1:] not in OPTIONFLAGS_BY_NAME):\n                    raise ValueError('line %r of the doctest for %s '\n                                     'has an invalid option: %r' %\n                                     (lineno+1, name, option))\n                flag = OPTIONFLAGS_BY_NAME[option[1:]]\n                options[flag] = (option[0] == '+')\n        if options and self._IS_BLANK_OR_COMMENT(source):\n            raise ValueError('line %r of the doctest for %s has an option '\n                             'directive on a line with no example: %r' %\n                             (lineno, name, source))\n        return options\n\n    # This regular expression finds the indentation of every non-blank\n    # line in a string.\n    _INDENT_RE = re.compile('^([ ]*)(?=\\S)', re.MULTILINE)\n\n    def _min_indent(self, s):\n        \"Return the minimum indentation of any non-blank line in `s`\"\n        indents = [len(indent) for indent in self._INDENT_RE.findall(s)]\n        if len(indents) > 0:\n            return min(indents)\n        else:\n            return 0\n\n    def _check_prompt_blank(self, lines, indent, name, lineno):\n        \"\"\"\n        Given the lines of a source string (including prompts and\n        leading indentation), check to make sure that every prompt is\n        followed by a space character.  If any line is not followed by\n        a space character, then raise ValueError.\n        \"\"\"\n        for i, line in enumerate(lines):\n            if len(line) >= indent+4 and line[indent+3] != ' ':\n                raise ValueError('line %r of the docstring for %s '\n                                 'lacks blank after %s: %r' %\n                                 (lineno+i+1, name,\n                                  line[indent:indent+3], line))\n\n    def _check_prefix(self, lines, prefix, name, lineno):\n        \"\"\"\n        Check that every line in the given list starts with the given\n        prefix; if any line does not, then raise a ValueError.\n        \"\"\"\n        for i, line in enumerate(lines):\n            if line and not line.startswith(prefix):\n                raise ValueError('line %r of the docstring for %s has '\n                                 'inconsistent leading whitespace: %r' %\n                                 (lineno+i+1, name, line))\n\n\n######################################################################\n## 4. DocTest Finder\n######################################################################\n\nclass DocTestFinder:\n    \"\"\"\n    A class used to extract the DocTests that are relevant to a given\n    object, from its docstring and the docstrings of its contained\n    objects.  Doctests can currently be extracted from the following\n    object types: modules, functions, classes, methods, staticmethods,\n    classmethods, and properties.\n    \"\"\"\n\n    def __init__(self, verbose=False, parser=DocTestParser(),\n                 recurse=True, exclude_empty=True):\n        \"\"\"\n        Create a new doctest finder.\n\n        The optional argument `parser` specifies a class or\n        function that should be used to create new DocTest objects (or\n        objects that implement the same interface as DocTest).  The\n        signature for this factory function should match the signature\n        of the DocTest constructor.\n\n        If the optional argument `recurse` is false, then `find` will\n        only examine the given object, and not any contained objects.\n\n        If the optional argument `exclude_empty` is false, then `find`\n        will include tests for objects with empty docstrings.\n        \"\"\"\n        self._parser = parser\n        self._verbose = verbose\n        self._recurse = recurse\n        self._exclude_empty = exclude_empty\n\n    def find(self, obj, name=None, module=None, globs=None, extraglobs=None):\n        \"\"\"\n        Return a list of the DocTests that are defined by the given\n        object's docstring, or by any of its contained objects'\n        docstrings.\n\n        The optional parameter `module` is the module that contains\n        the given object.  If the module is not specified or is None, then\n        the test finder will attempt to automatically determine the\n        correct module.  The object's module is used:\n\n            - As a default namespace, if `globs` is not specified.\n            - To prevent the DocTestFinder from extracting DocTests\n              from objects that are imported from other modules.\n            - To find the name of the file containing the object.\n            - To help find the line number of the object within its\n              file.\n\n        Contained objects whose module does not match `module` are ignored.\n\n        If `module` is False, no attempt to find the module will be made.\n        This is obscure, of use mostly in tests:  if `module` is False, or\n        is None but cannot be found automatically, then all objects are\n        considered to belong to the (non-existent) module, so all contained\n        objects will (recursively) be searched for doctests.\n\n        The globals for each DocTest is formed by combining `globs`\n        and `extraglobs` (bindings in `extraglobs` override bindings\n        in `globs`).  A new copy of the globals dictionary is created\n        for each DocTest.  If `globs` is not specified, then it\n        defaults to the module's `__dict__`, if specified, or {}\n        otherwise.  If `extraglobs` is not specified, then it defaults\n        to {}.\n\n        \"\"\"\n        # If name was not specified, then extract it from the object.\n        if name is None:\n            name = getattr(obj, '__name__', None)\n            if name is None:\n                raise ValueError(\"DocTestFinder.find: name must be given \"\n                        \"when obj.__name__ doesn't exist: %r\" %\n                                 (type(obj),))\n\n        # Find the module that contains the given object (if obj is\n        # a module, then module=obj.).  Note: this may fail, in which\n        # case module will be None.\n        if module is False:\n            module = None\n        elif module is None:\n            module = inspect.getmodule(obj)\n\n        # Read the module's source code.  This is used by\n        # DocTestFinder._find_lineno to find the line number for a\n        # given object's docstring.\n        try:\n            file = inspect.getsourcefile(obj) or inspect.getfile(obj)\n            if module is not None:\n                # Supply the module globals in case the module was\n                # originally loaded via a PEP 302 loader and\n                # file is not a valid filesystem path\n                source_lines = linecache.getlines(file, module.__dict__)\n            else:\n                # No access to a loader, so assume it's a normal\n                # filesystem path\n                source_lines = linecache.getlines(file)\n            if not source_lines:\n                source_lines = None\n        except TypeError:\n            source_lines = None\n\n        # Initialize globals, and merge in extraglobs.\n        if globs is None:\n            if module is None:\n                globs = {}\n            else:\n                globs = module.__dict__.copy()\n        else:\n            globs = globs.copy()\n        if extraglobs is not None:\n            globs.update(extraglobs)\n        if '__name__' not in globs:\n            globs['__name__'] = '__main__'  # provide a default module name\n\n        # Recursively explore `obj`, extracting DocTests.\n        tests = []\n        self._find(tests, obj, name, module, source_lines, globs, {})\n        # Sort the tests by alpha order of names, for consistency in\n        # verbose-mode output.  This was a feature of doctest in Pythons\n        # <= 2.3 that got lost by accident in 2.4.  It was repaired in\n        # 2.4.4 and 2.5.\n        tests.sort()\n        return tests\n\n    def _from_module(self, module, object):\n        \"\"\"\n        Return true if the given object is defined in the given\n        module.\n        \"\"\"\n        if module is None:\n            return True\n        elif inspect.getmodule(object) is not None:\n            return module is inspect.getmodule(object)\n        elif inspect.isfunction(object):\n            return module.__dict__ is object.func_globals\n        elif inspect.isclass(object):\n            return module.__name__ == object.__module__\n        elif hasattr(object, '__module__'):\n            return module.__name__ == object.__module__\n        elif isinstance(object, property):\n            return True # [XX] no way not be sure.\n        else:\n            raise ValueError(\"object must be a class or function\")\n\n    def _find(self, tests, obj, name, module, source_lines, globs, seen):\n        \"\"\"\n        Find tests for the given object and any contained objects, and\n        add them to `tests`.\n        \"\"\"\n        if self._verbose:\n            print 'Finding tests in %s' % name\n\n        # If we've already processed this object, then ignore it.\n        if id(obj) in seen:\n            return\n        seen[id(obj)] = 1\n\n        # Find a test for this object, and add it to the list of tests.\n        test = self._get_test(obj, name, module, globs, source_lines)\n        if test is not None:\n            tests.append(test)\n\n        # Look for tests in a module's contained objects.\n        if inspect.ismodule(obj) and self._recurse:\n            for valname, val in obj.__dict__.items():\n                valname = '%s.%s' % (name, valname)\n                # Recurse to functions & classes.\n                if ((inspect.isfunction(val) or inspect.isclass(val)) and\n                    self._from_module(module, val)):\n                    self._find(tests, val, valname, module, source_lines,\n                               globs, seen)\n\n        # Look for tests in a module's __test__ dictionary.\n        if inspect.ismodule(obj) and self._recurse:\n            for valname, val in getattr(obj, '__test__', {}).items():\n                if not isinstance(valname, basestring):\n                    raise ValueError(\"DocTestFinder.find: __test__ keys \"\n                                     \"must be strings: %r\" %\n                                     (type(valname),))\n                if not (inspect.isfunction(val) or inspect.isclass(val) or\n                        inspect.ismethod(val) or inspect.ismodule(val) or\n                        isinstance(val, basestring)):\n                    raise ValueError(\"DocTestFinder.find: __test__ values \"\n                                     \"must be strings, functions, methods, \"\n                                     \"classes, or modules: %r\" %\n                                     (type(val),))\n                valname = '%s.__test__.%s' % (name, valname)\n                self._find(tests, val, valname, module, source_lines,\n                           globs, seen)\n\n        # Look for tests in a class's contained objects.\n        if inspect.isclass(obj) and self._recurse:\n            for valname, val in obj.__dict__.items():\n                # Special handling for staticmethod/classmethod.\n                if isinstance(val, staticmethod):\n                    val = getattr(obj, valname)\n                if isinstance(val, classmethod):\n                    val = getattr(obj, valname).im_func\n\n                # Recurse to methods, properties, and nested classes.\n                if ((inspect.isfunction(val) or inspect.isclass(val) or\n                      isinstance(val, property)) and\n                      self._from_module(module, val)):\n                    valname = '%s.%s' % (name, valname)\n                    self._find(tests, val, valname, module, source_lines,\n                               globs, seen)\n\n    def _get_test(self, obj, name, module, globs, source_lines):\n        \"\"\"\n        Return a DocTest for the given object, if it defines a docstring;\n        otherwise, return None.\n        \"\"\"\n        # Extract the object's docstring.  If it doesn't have one,\n        # then return None (no test for this object).\n        if isinstance(obj, basestring):\n            docstring = obj\n        else:\n            try:\n                if obj.__doc__ is None:\n                    docstring = ''\n                else:\n                    docstring = obj.__doc__\n                    if not isinstance(docstring, basestring):\n                        docstring = str(docstring)\n            except (TypeError, AttributeError):\n                docstring = ''\n\n        # Find the docstring's location in the file.\n        lineno = self._find_lineno(obj, source_lines)\n\n        # Don't bother if the docstring is empty.\n        if self._exclude_empty and not docstring:\n            return None\n\n        # Return a DocTest for this object.\n        if module is None:\n            filename = None\n        else:\n            filename = getattr(module, '__file__', module.__name__)\n            if filename[-4:] in (\".pyc\", \".pyo\"):\n                filename = filename[:-1]\n        return self._parser.get_doctest(docstring, globs, name,\n                                        filename, lineno)\n\n    def _find_lineno(self, obj, source_lines):\n        \"\"\"\n        Return a line number of the given object's docstring.  Note:\n        this method assumes that the object has a docstring.\n        \"\"\"\n        lineno = None\n\n        # Find the line number for modules.\n        if inspect.ismodule(obj):\n            lineno = 0\n\n        # Find the line number for classes.\n        # Note: this could be fooled if a class is defined multiple\n        # times in a single file.\n        if inspect.isclass(obj):\n            if source_lines is None:\n                return None\n            pat = re.compile(r'^\\s*class\\s*%s\\b' %\n                             getattr(obj, '__name__', '-'))\n            for i, line in enumerate(source_lines):\n                if pat.match(line):\n                    lineno = i\n                    break\n\n        # Find the line number for functions & methods.\n        if inspect.ismethod(obj): obj = obj.im_func\n        if inspect.isfunction(obj): obj = obj.func_code\n        if inspect.istraceback(obj): obj = obj.tb_frame\n        if inspect.isframe(obj): obj = obj.f_code\n        if inspect.iscode(obj):\n            lineno = getattr(obj, 'co_firstlineno', None)-1\n\n        # Find the line number where the docstring starts.  Assume\n        # that it's the first line that begins with a quote mark.\n        # Note: this could be fooled by a multiline function\n        # signature, where a continuation line begins with a quote\n        # mark.\n        if lineno is not None:\n            if source_lines is None:\n                return lineno+1\n            pat = re.compile('(^|.*:)\\s*\\w*(\"|\\')')\n            for lineno in range(lineno, len(source_lines)):\n                if pat.match(source_lines[lineno]):\n                    return lineno\n\n        # We couldn't find the line number.\n        return None\n\n######################################################################\n## 5. DocTest Runner\n######################################################################\n\nclass DocTestRunner:\n    \"\"\"\n    A class used to run DocTest test cases, and accumulate statistics.\n    The `run` method is used to process a single DocTest case.  It\n    returns a tuple `(f, t)`, where `t` is the number of test cases\n    tried, and `f` is the number of test cases that failed.\n\n        >>> tests = DocTestFinder().find(_TestClass)\n        >>> runner = DocTestRunner(verbose=False)\n        >>> tests.sort(key = lambda test: test.name)\n        >>> for test in tests:\n        ...     print test.name, '->', runner.run(test)\n        _TestClass -> TestResults(failed=0, attempted=2)\n        _TestClass.__init__ -> TestResults(failed=0, attempted=2)\n        _TestClass.get -> TestResults(failed=0, attempted=2)\n        _TestClass.square -> TestResults(failed=0, attempted=1)\n\n    The `summarize` method prints a summary of all the test cases that\n    have been run by the runner, and returns an aggregated `(f, t)`\n    tuple:\n\n        >>> runner.summarize(verbose=1)\n        4 items passed all tests:\n           2 tests in _TestClass\n           2 tests in _TestClass.__init__\n           2 tests in _TestClass.get\n           1 tests in _TestClass.square\n        7 tests in 4 items.\n        7 passed and 0 failed.\n        Test passed.\n        TestResults(failed=0, attempted=7)\n\n    The aggregated number of tried examples and failed examples is\n    also available via the `tries` and `failures` attributes:\n\n        >>> runner.tries\n        7\n        >>> runner.failures\n        0\n\n    The comparison between expected outputs and actual outputs is done\n    by an `OutputChecker`.  This comparison may be customized with a\n    number of option flags; see the documentation for `testmod` for\n    more information.  If the option flags are insufficient, then the\n    comparison may also be customized by passing a subclass of\n    `OutputChecker` to the constructor.\n\n    The test runner's display output can be controlled in two ways.\n    First, an output function (`out) can be passed to\n    `TestRunner.run`; this function will be called with strings that\n    should be displayed.  It defaults to `sys.stdout.write`.  If\n    capturing the output is not sufficient, then the display output\n    can be also customized by subclassing DocTestRunner, and\n    overriding the methods `report_start`, `report_success`,\n    `report_unexpected_exception`, and `report_failure`.\n    \"\"\"\n    # This divider string is used to separate failure messages, and to\n    # separate sections of the summary.\n    DIVIDER = \"*\" * 70\n\n    def __init__(self, checker=None, verbose=None, optionflags=0):\n        \"\"\"\n        Create a new test runner.\n\n        Optional keyword arg `checker` is the `OutputChecker` that\n        should be used to compare the expected outputs and actual\n        outputs of doctest examples.\n\n        Optional keyword arg 'verbose' prints lots of stuff if true,\n        only failures if false; by default, it's true iff '-v' is in\n        sys.argv.\n\n        Optional argument `optionflags` can be used to control how the\n        test runner compares expected output to actual output, and how\n        it displays failures.  See the documentation for `testmod` for\n        more information.\n        \"\"\"\n        self._checker = checker or OutputChecker()\n        if verbose is None:\n            verbose = '-v' in sys.argv\n        self._verbose = verbose\n        self.optionflags = optionflags\n        self.original_optionflags = optionflags\n\n        # Keep track of the examples we've run.\n        self.tries = 0\n        self.failures = 0\n        self._name2ft = {}\n\n        # Create a fake output target for capturing doctest output.\n        self._fakeout = _SpoofOut()\n\n    #/////////////////////////////////////////////////////////////////\n    # Reporting methods\n    #/////////////////////////////////////////////////////////////////\n\n    def report_start(self, out, test, example):\n        \"\"\"\n        Report that the test runner is about to process the given\n        example.  (Only displays a message if verbose=True)\n        \"\"\"\n        if self._verbose:\n            if example.want:\n                out('Trying:\\n' + _indent(example.source) +\n                    'Expecting:\\n' + _indent(example.want))\n            else:\n                out('Trying:\\n' + _indent(example.source) +\n                    'Expecting nothing\\n')\n\n    def report_success(self, out, test, example, got):\n        \"\"\"\n        Report that the given example ran successfully.  (Only\n        displays a message if verbose=True)\n        \"\"\"\n        if self._verbose:\n            out(\"ok\\n\")\n\n    def report_failure(self, out, test, example, got):\n        \"\"\"\n        Report that the given example failed.\n        \"\"\"\n        out(self._failure_header(test, example) +\n            self._checker.output_difference(example, got, self.optionflags))\n\n    def report_unexpected_exception(self, out, test, example, exc_info):\n        \"\"\"\n        Report that the given example raised an unexpected exception.\n        \"\"\"\n        out(self._failure_header(test, example) +\n            'Exception raised:\\n' + _indent(_exception_traceback(exc_info)))\n\n    def _failure_header(self, test, example):\n        out = [self.DIVIDER]\n        if test.filename:\n            if test.lineno is not None and example.lineno is not None:\n                lineno = test.lineno + example.lineno + 1\n            else:\n                lineno = '?'\n            out.append('File \"%s\", line %s, in %s' %\n                       (test.filename, lineno, test.name))\n        else:\n            out.append('Line %s, in %s' % (example.lineno+1, test.name))\n        out.append('Failed example:')\n        source = example.source\n        out.append(_indent(source))\n        return '\\n'.join(out)\n\n    #/////////////////////////////////////////////////////////////////\n    # DocTest Running\n    #/////////////////////////////////////////////////////////////////\n\n    def __run(self, test, compileflags, out):\n        \"\"\"\n        Run the examples in `test`.  Write the outcome of each example\n        with one of the `DocTestRunner.report_*` methods, using the\n        writer function `out`.  `compileflags` is the set of compiler\n        flags that should be used to execute examples.  Return a tuple\n        `(f, t)`, where `t` is the number of examples tried, and `f`\n        is the number of examples that failed.  The examples are run\n        in the namespace `test.globs`.\n        \"\"\"\n        # Keep track of the number of failures and tries.\n        failures = tries = 0\n\n        # Save the option flags (since option directives can be used\n        # to modify them).\n        original_optionflags = self.optionflags\n\n        SUCCESS, FAILURE, BOOM = range(3) # `outcome` state\n\n        check = self._checker.check_output\n\n        # Process each example.\n        for examplenum, example in enumerate(test.examples):\n\n            # If REPORT_ONLY_FIRST_FAILURE is set, then suppress\n            # reporting after the first failure.\n            quiet = (self.optionflags & REPORT_ONLY_FIRST_FAILURE and\n                     failures > 0)\n\n            # Merge in the example's options.\n            self.optionflags = original_optionflags\n            if example.options:\n                for (optionflag, val) in example.options.items():\n                    if val:\n                        self.optionflags |= optionflag\n                    else:\n                        self.optionflags &= ~optionflag\n\n            # If 'SKIP' is set, then skip this example.\n            if self.optionflags & SKIP:\n                continue\n\n            # Record that we started this example.\n            tries += 1\n            if not quiet:\n                self.report_start(out, test, example)\n\n            # Use a special filename for compile(), so we can retrieve\n            # the source code during interactive debugging (see\n            # __patched_linecache_getlines).\n            filename = '<doctest %s[%d]>' % (test.name, examplenum)\n\n            # Run the example in the given context (globs), and record\n            # any exception that gets raised.  (But don't intercept\n            # keyboard interrupts.)\n            try:\n                # Don't blink!  This is where the user's code gets run.\n                exec compile(example.source, filename, \"single\",\n                             compileflags, 1) in test.globs\n                self.debugger.set_continue() # ==== Example Finished ====\n                exception = None\n            except KeyboardInterrupt:\n                raise\n            except:\n                exception = sys.exc_info()\n                self.debugger.set_continue() # ==== Example Finished ====\n\n            got = self._fakeout.getvalue()  # the actual output\n            self._fakeout.truncate(0)\n            outcome = FAILURE   # guilty until proved innocent or insane\n\n            # If the example executed without raising any exceptions,\n            # verify its output.\n            if exception is None:\n                if check(example.want, got, self.optionflags):\n                    outcome = SUCCESS\n\n            # The example raised an exception:  check if it was expected.\n            else:\n                exc_info = sys.exc_info()\n                exc_msg = traceback.format_exception_only(*exc_info[:2])[-1]\n                if not quiet:\n                    got += _exception_traceback(exc_info)\n\n                # If `example.exc_msg` is None, then we weren't expecting\n                # an exception.\n                if example.exc_msg is None:\n                    outcome = BOOM\n\n                # We expected an exception:  see whether it matches.\n                elif check(example.exc_msg, exc_msg, self.optionflags):\n                    outcome = SUCCESS\n\n                # Another chance if they didn't care about the detail.\n                elif self.optionflags & IGNORE_EXCEPTION_DETAIL:\n                    if check(_strip_exception_details(example.exc_msg),\n                             _strip_exception_details(exc_msg),\n                             self.optionflags):\n                        outcome = SUCCESS\n\n            # Report the outcome.\n            if outcome is SUCCESS:\n                if not quiet:\n                    self.report_success(out, test, example, got)\n            elif outcome is FAILURE:\n                if not quiet:\n                    self.report_failure(out, test, example, got)\n                failures += 1\n            elif outcome is BOOM:\n                if not quiet:\n                    self.report_unexpected_exception(out, test, example,\n                                                     exc_info)\n                failures += 1\n            else:\n                assert False, (\"unknown outcome\", outcome)\n\n        # Restore the option flags (in case they were modified)\n        self.optionflags = original_optionflags\n\n        # Record and return the number of failures and tries.\n        self.__record_outcome(test, failures, tries)\n        return TestResults(failures, tries)\n\n    def __record_outcome(self, test, f, t):\n        \"\"\"\n        Record the fact that the given DocTest (`test`) generated `f`\n        failures out of `t` tried examples.\n        \"\"\"\n        f2, t2 = self._name2ft.get(test.name, (0,0))\n        self._name2ft[test.name] = (f+f2, t+t2)\n        self.failures += f\n        self.tries += t\n\n    __LINECACHE_FILENAME_RE = re.compile(r'<doctest '\n                                         r'(?P<name>.+)'\n                                         r'\\[(?P<examplenum>\\d+)\\]>$')\n    def __patched_linecache_getlines(self, filename, module_globals=None):\n        m = self.__LINECACHE_FILENAME_RE.match(filename)\n        if m and m.group('name') == self.test.name:\n            example = self.test.examples[int(m.group('examplenum'))]\n            source = example.source\n            if isinstance(source, unicode):\n                source = source.encode('ascii', 'backslashreplace')\n            return source.splitlines(True)\n        else:\n            return self.save_linecache_getlines(filename, module_globals)\n\n    def run(self, test, compileflags=None, out=None, clear_globs=True):\n        \"\"\"\n        Run the examples in `test`, and display the results using the\n        writer function `out`.\n\n        The examples are run in the namespace `test.globs`.  If\n        `clear_globs` is true (the default), then this namespace will\n        be cleared after the test runs, to help with garbage\n        collection.  If you would like to examine the namespace after\n        the test completes, then use `clear_globs=False`.\n\n        `compileflags` gives the set of flags that should be used by\n        the Python compiler when running the examples.  If not\n        specified, then it will default to the set of future-import\n        flags that apply to `globs`.\n\n        The output of each example is checked using\n        `DocTestRunner.check_output`, and the results are formatted by\n        the `DocTestRunner.report_*` methods.\n        \"\"\"\n        self.test = test\n\n        if compileflags is None:\n            compileflags = _extract_future_flags(test.globs)\n\n        save_stdout = sys.stdout\n        if out is None:\n            out = save_stdout.write\n        sys.stdout = self._fakeout\n\n        # Patch pdb.set_trace to restore sys.stdout during interactive\n        # debugging (so it's not still redirected to self._fakeout).\n        # Note that the interactive output will go to *our*\n        # save_stdout, even if that's not the real sys.stdout; this\n        # allows us to write test cases for the set_trace behavior.\n        save_set_trace = pdb.set_trace\n        self.debugger = _OutputRedirectingPdb(save_stdout)\n        self.debugger.reset()\n        pdb.set_trace = self.debugger.set_trace\n\n        # Patch linecache.getlines, so we can see the example's source\n        # when we're inside the debugger.\n        self.save_linecache_getlines = linecache.getlines\n        linecache.getlines = self.__patched_linecache_getlines\n\n        # Make sure sys.displayhook just prints the value to stdout\n        save_displayhook = sys.displayhook\n        sys.displayhook = sys.__displayhook__\n\n        try:\n            return self.__run(test, compileflags, out)\n        finally:\n            sys.stdout = save_stdout\n            pdb.set_trace = save_set_trace\n            linecache.getlines = self.save_linecache_getlines\n            sys.displayhook = save_displayhook\n            if clear_globs:\n                test.globs.clear()\n\n    #/////////////////////////////////////////////////////////////////\n    # Summarization\n    #/////////////////////////////////////////////////////////////////\n    def summarize(self, verbose=None):\n        \"\"\"\n        Print a summary of all the test cases that have been run by\n        this DocTestRunner, and return a tuple `(f, t)`, where `f` is\n        the total number of failed examples, and `t` is the total\n        number of tried examples.\n\n        The optional `verbose` argument controls how detailed the\n        summary is.  If the verbosity is not specified, then the\n        DocTestRunner's verbosity is used.\n        \"\"\"\n        if verbose is None:\n            verbose = self._verbose\n        notests = []\n        passed = []\n        failed = []\n        totalt = totalf = 0\n        for x in self._name2ft.items():\n            name, (f, t) = x\n            assert f <= t\n            totalt += t\n            totalf += f\n            if t == 0:\n                notests.append(name)\n            elif f == 0:\n                passed.append( (name, t) )\n            else:\n                failed.append(x)\n        if verbose:\n            if notests:\n                print len(notests), \"items had no tests:\"\n                notests.sort()\n                for thing in notests:\n                    print \"   \", thing\n            if passed:\n                print len(passed), \"items passed all tests:\"\n                passed.sort()\n                for thing, count in passed:\n                    print \" %3d tests in %s\" % (count, thing)\n        if failed:\n            print self.DIVIDER\n            print len(failed), \"items had failures:\"\n            failed.sort()\n            for thing, (f, t) in failed:\n                print \" %3d of %3d in %s\" % (f, t, thing)\n        if verbose:\n            print totalt, \"tests in\", len(self._name2ft), \"items.\"\n            print totalt - totalf, \"passed and\", totalf, \"failed.\"\n        if totalf:\n            print \"***Test Failed***\", totalf, \"failures.\"\n        elif verbose:\n            print \"Test passed.\"\n        return TestResults(totalf, totalt)\n\n    #/////////////////////////////////////////////////////////////////\n    # Backward compatibility cruft to maintain doctest.master.\n    #/////////////////////////////////////////////////////////////////\n    def merge(self, other):\n        d = self._name2ft\n        for name, (f, t) in other._name2ft.items():\n            if name in d:\n                # Don't print here by default, since doing\n                #     so breaks some of the buildbots\n                #print \"*** DocTestRunner.merge: '\" + name + \"' in both\" \\\n                #    \" testers; summing outcomes.\"\n                f2, t2 = d[name]\n                f = f + f2\n                t = t + t2\n            d[name] = f, t\n\nclass OutputChecker:\n    \"\"\"\n    A class used to check the whether the actual output from a doctest\n    example matches the expected output.  `OutputChecker` defines two\n    methods: `check_output`, which compares a given pair of outputs,\n    and returns true if they match; and `output_difference`, which\n    returns a string describing the differences between two outputs.\n    \"\"\"\n    def check_output(self, want, got, optionflags):\n        \"\"\"\n        Return True iff the actual output from an example (`got`)\n        matches the expected output (`want`).  These strings are\n        always considered to match if they are identical; but\n        depending on what option flags the test runner is using,\n        several non-exact match types are also possible.  See the\n        documentation for `TestRunner` for more information about\n        option flags.\n        \"\"\"\n        # Handle the common case first, for efficiency:\n        # if they're string-identical, always return true.\n        if got == want:\n            return True\n\n        # The values True and False replaced 1 and 0 as the return\n        # value for boolean comparisons in Python 2.3.\n        if not (optionflags & DONT_ACCEPT_TRUE_FOR_1):\n            if (got,want) == (\"True\\n\", \"1\\n\"):\n                return True\n            if (got,want) == (\"False\\n\", \"0\\n\"):\n                return True\n\n        # <BLANKLINE> can be used as a special sequence to signify a\n        # blank line, unless the DONT_ACCEPT_BLANKLINE flag is used.\n        if not (optionflags & DONT_ACCEPT_BLANKLINE):\n            # Replace <BLANKLINE> in want with a blank line.\n            want = re.sub('(?m)^%s\\s*?$' % re.escape(BLANKLINE_MARKER),\n                          '', want)\n            # If a line in got contains only spaces, then remove the\n            # spaces.\n            got = re.sub('(?m)^\\s*?$', '', got)\n            if got == want:\n                return True\n\n        # This flag causes doctest to ignore any differences in the\n        # contents of whitespace strings.  Note that this can be used\n        # in conjunction with the ELLIPSIS flag.\n        if optionflags & NORMALIZE_WHITESPACE:\n            got = ' '.join(got.split())\n            want = ' '.join(want.split())\n            if got == want:\n                return True\n\n        # The ELLIPSIS flag says to let the sequence \"...\" in `want`\n        # match any substring in `got`.\n        if optionflags & ELLIPSIS:\n            if _ellipsis_match(want, got):\n                return True\n\n        # We didn't find any match; return false.\n        return False\n\n    # Should we do a fancy diff?\n    def _do_a_fancy_diff(self, want, got, optionflags):\n        # Not unless they asked for a fancy diff.\n        if not optionflags & (REPORT_UDIFF |\n                              REPORT_CDIFF |\n                              REPORT_NDIFF):\n            return False\n\n        # If expected output uses ellipsis, a meaningful fancy diff is\n        # too hard ... or maybe not.  In two real-life failures Tim saw,\n        # a diff was a major help anyway, so this is commented out.\n        # [todo] _ellipsis_match() knows which pieces do and don't match,\n        # and could be the basis for a kick-ass diff in this case.\n        ##if optionflags & ELLIPSIS and ELLIPSIS_MARKER in want:\n        ##    return False\n\n        # ndiff does intraline difference marking, so can be useful even\n        # for 1-line differences.\n        if optionflags & REPORT_NDIFF:\n            return True\n\n        # The other diff types need at least a few lines to be helpful.\n        return want.count('\\n') > 2 and got.count('\\n') > 2\n\n    def output_difference(self, example, got, optionflags):\n        \"\"\"\n        Return a string describing the differences between the\n        expected output for a given example (`example`) and the actual\n        output (`got`).  `optionflags` is the set of option flags used\n        to compare `want` and `got`.\n        \"\"\"\n        want = example.want\n        # If <BLANKLINE>s are being used, then replace blank lines\n        # with <BLANKLINE> in the actual output string.\n        if not (optionflags & DONT_ACCEPT_BLANKLINE):\n            got = re.sub('(?m)^[ ]*(?=\\n)', BLANKLINE_MARKER, got)\n\n        # Check if we should use diff.\n        if self._do_a_fancy_diff(want, got, optionflags):\n            # Split want & got into lines.\n            want_lines = want.splitlines(True)  # True == keep line ends\n            got_lines = got.splitlines(True)\n            # Use difflib to find their differences.\n            if optionflags & REPORT_UDIFF:\n                diff = difflib.unified_diff(want_lines, got_lines, n=2)\n                diff = list(diff)[2:] # strip the diff header\n                kind = 'unified diff with -expected +actual'\n            elif optionflags & REPORT_CDIFF:\n                diff = difflib.context_diff(want_lines, got_lines, n=2)\n                diff = list(diff)[2:] # strip the diff header\n                kind = 'context diff with expected followed by actual'\n            elif optionflags & REPORT_NDIFF:\n                engine = difflib.Differ(charjunk=difflib.IS_CHARACTER_JUNK)\n                diff = list(engine.compare(want_lines, got_lines))\n                kind = 'ndiff with -expected +actual'\n            else:\n                assert 0, 'Bad diff option'\n            # Remove trailing whitespace on diff output.\n            diff = [line.rstrip() + '\\n' for line in diff]\n            return 'Differences (%s):\\n' % kind + _indent(''.join(diff))\n\n        # If we're not using diff, then simply list the expected\n        # output followed by the actual output.\n        if want and got:\n            return 'Expected:\\n%sGot:\\n%s' % (_indent(want), _indent(got))\n        elif want:\n            return 'Expected:\\n%sGot nothing\\n' % _indent(want)\n        elif got:\n            return 'Expected nothing\\nGot:\\n%s' % _indent(got)\n        else:\n            return 'Expected nothing\\nGot nothing\\n'\n\nclass DocTestFailure(Exception):\n    \"\"\"A DocTest example has failed in debugging mode.\n\n    The exception instance has variables:\n\n    - test: the DocTest object being run\n\n    - example: the Example object that failed\n\n    - got: the actual output\n    \"\"\"\n    def __init__(self, test, example, got):\n        self.test = test\n        self.example = example\n        self.got = got\n\n    def __str__(self):\n        return str(self.test)\n\nclass UnexpectedException(Exception):\n    \"\"\"A DocTest example has encountered an unexpected exception\n\n    The exception instance has variables:\n\n    - test: the DocTest object being run\n\n    - example: the Example object that failed\n\n    - exc_info: the exception info\n    \"\"\"\n    def __init__(self, test, example, exc_info):\n        self.test = test\n        self.example = example\n        self.exc_info = exc_info\n\n    def __str__(self):\n        return str(self.test)\n\nclass DebugRunner(DocTestRunner):\n    r\"\"\"Run doc tests but raise an exception as soon as there is a failure.\n\n       If an unexpected exception occurs, an UnexpectedException is raised.\n       It contains the test, the example, and the original exception:\n\n         >>> runner = DebugRunner(verbose=False)\n         >>> test = DocTestParser().get_doctest('>>> raise KeyError\\n42',\n         ...                                    {}, 'foo', 'foo.py', 0)\n         >>> try:\n         ...     runner.run(test)\n         ... except UnexpectedException, failure:\n         ...     pass\n\n         >>> failure.test is test\n         True\n\n         >>> failure.example.want\n         '42\\n'\n\n         >>> exc_info = failure.exc_info\n         >>> raise exc_info[0], exc_info[1], exc_info[2]\n         Traceback (most recent call last):\n         ...\n         KeyError\n\n       We wrap the original exception to give the calling application\n       access to the test and example information.\n\n       If the output doesn't match, then a DocTestFailure is raised:\n\n         >>> test = DocTestParser().get_doctest('''\n         ...      >>> x = 1\n         ...      >>> x\n         ...      2\n         ...      ''', {}, 'foo', 'foo.py', 0)\n\n         >>> try:\n         ...    runner.run(test)\n         ... except DocTestFailure, failure:\n         ...    pass\n\n       DocTestFailure objects provide access to the test:\n\n         >>> failure.test is test\n         True\n\n       As well as to the example:\n\n         >>> failure.example.want\n         '2\\n'\n\n       and the actual output:\n\n         >>> failure.got\n         '1\\n'\n\n       If a failure or error occurs, the globals are left intact:\n\n         >>> del test.globs['__builtins__']\n         >>> test.globs\n         {'x': 1}\n\n         >>> test = DocTestParser().get_doctest('''\n         ...      >>> x = 2\n         ...      >>> raise KeyError\n         ...      ''', {}, 'foo', 'foo.py', 0)\n\n         >>> runner.run(test)\n         Traceback (most recent call last):\n         ...\n         UnexpectedException: <DocTest foo from foo.py:0 (2 examples)>\n\n         >>> del test.globs['__builtins__']\n         >>> test.globs\n         {'x': 2}\n\n       But the globals are cleared if there is no error:\n\n         >>> test = DocTestParser().get_doctest('''\n         ...      >>> x = 2\n         ...      ''', {}, 'foo', 'foo.py', 0)\n\n         >>> runner.run(test)\n         TestResults(failed=0, attempted=1)\n\n         >>> test.globs\n         {}\n\n       \"\"\"\n\n    def run(self, test, compileflags=None, out=None, clear_globs=True):\n        r = DocTestRunner.run(self, test, compileflags, out, False)\n        if clear_globs:\n            test.globs.clear()\n        return r\n\n    def report_unexpected_exception(self, out, test, example, exc_info):\n        raise UnexpectedException(test, example, exc_info)\n\n    def report_failure(self, out, test, example, got):\n        raise DocTestFailure(test, example, got)\n\n######################################################################\n## 6. Test Functions\n######################################################################\n# These should be backwards compatible.\n\n# For backward compatibility, a global instance of a DocTestRunner\n# class, updated by testmod.\nmaster = None\n\ndef testmod(m=None, name=None, globs=None, verbose=None,\n            report=True, optionflags=0, extraglobs=None,\n            raise_on_error=False, exclude_empty=False):\n    \"\"\"m=None, name=None, globs=None, verbose=None, report=True,\n       optionflags=0, extraglobs=None, raise_on_error=False,\n       exclude_empty=False\n\n    Test examples in docstrings in functions and classes reachable\n    from module m (or the current module if m is not supplied), starting\n    with m.__doc__.\n\n    Also test examples reachable from dict m.__test__ if it exists and is\n    not None.  m.__test__ maps names to functions, classes and strings;\n    function and class docstrings are tested even if the name is private;\n    strings are tested directly, as if they were docstrings.\n\n    Return (#failures, #tests).\n\n    See help(doctest) for an overview.\n\n    Optional keyword arg \"name\" gives the name of the module; by default\n    use m.__name__.\n\n    Optional keyword arg \"globs\" gives a dict to be used as the globals\n    when executing examples; by default, use m.__dict__.  A copy of this\n    dict is actually used for each docstring, so that each docstring's\n    examples start with a clean slate.\n\n    Optional keyword arg \"extraglobs\" gives a dictionary that should be\n    merged into the globals that are used to execute examples.  By\n    default, no extra globals are used.  This is new in 2.4.\n\n    Optional keyword arg \"verbose\" prints lots of stuff if true, prints\n    only failures if false; by default, it's true iff \"-v\" is in sys.argv.\n\n    Optional keyword arg \"report\" prints a summary at the end when true,\n    else prints nothing at the end.  In verbose mode, the summary is\n    detailed, else very brief (in fact, empty if all tests passed).\n\n    Optional keyword arg \"optionflags\" or's together module constants,\n    and defaults to 0.  This is new in 2.3.  Possible values (see the\n    docs for details):\n\n        DONT_ACCEPT_TRUE_FOR_1\n        DONT_ACCEPT_BLANKLINE\n        NORMALIZE_WHITESPACE\n        ELLIPSIS\n        SKIP\n        IGNORE_EXCEPTION_DETAIL\n        REPORT_UDIFF\n        REPORT_CDIFF\n        REPORT_NDIFF\n        REPORT_ONLY_FIRST_FAILURE\n\n    Optional keyword arg \"raise_on_error\" raises an exception on the\n    first unexpected exception or failure. This allows failures to be\n    post-mortem debugged.\n\n    Advanced tomfoolery:  testmod runs methods of a local instance of\n    class doctest.Tester, then merges the results into (or creates)\n    global Tester instance doctest.master.  Methods of doctest.master\n    can be called directly too, if you want to do something unusual.\n    Passing report=0 to testmod is especially useful then, to delay\n    displaying a summary.  Invoke doctest.master.summarize(verbose)\n    when you're done fiddling.\n    \"\"\"\n    global master\n\n    # If no module was given, then use __main__.\n    if m is None:\n        # DWA - m will still be None if this wasn't invoked from the command\n        # line, in which case the following TypeError is about as good an error\n        # as we should expect\n        m = sys.modules.get('__main__')\n\n    # Check that we were actually given a module.\n    if not inspect.ismodule(m):\n        raise TypeError(\"testmod: module required; %r\" % (m,))\n\n    # If no name was given, then use the module's name.\n    if name is None:\n        name = m.__name__\n\n    # Find, parse, and run all tests in the given module.\n    finder = DocTestFinder(exclude_empty=exclude_empty)\n\n    if raise_on_error:\n        runner = DebugRunner(verbose=verbose, optionflags=optionflags)\n    else:\n        runner = DocTestRunner(verbose=verbose, optionflags=optionflags)\n\n    for test in finder.find(m, name, globs=globs, extraglobs=extraglobs):\n        runner.run(test)\n\n    if report:\n        runner.summarize()\n\n    if master is None:\n        master = runner\n    else:\n        master.merge(runner)\n\n    return TestResults(runner.failures, runner.tries)\n\ndef testfile(filename, module_relative=True, name=None, package=None,\n             globs=None, verbose=None, report=True, optionflags=0,\n             extraglobs=None, raise_on_error=False, parser=DocTestParser(),\n             encoding=None):\n    \"\"\"\n    Test examples in the given file.  Return (#failures, #tests).\n\n    Optional keyword arg \"module_relative\" specifies how filenames\n    should be interpreted:\n\n      - If \"module_relative\" is True (the default), then \"filename\"\n         specifies a module-relative path.  By default, this path is\n         relative to the calling module's directory; but if the\n         \"package\" argument is specified, then it is relative to that\n         package.  To ensure os-independence, \"filename\" should use\n         \"/\" characters to separate path segments, and should not\n         be an absolute path (i.e., it may not begin with \"/\").\n\n      - If \"module_relative\" is False, then \"filename\" specifies an\n        os-specific path.  The path may be absolute or relative (to\n        the current working directory).\n\n    Optional keyword arg \"name\" gives the name of the test; by default\n    use the file's basename.\n\n    Optional keyword argument \"package\" is a Python package or the\n    name of a Python package whose directory should be used as the\n    base directory for a module relative filename.  If no package is\n    specified, then the calling module's directory is used as the base\n    directory for module relative filenames.  It is an error to\n    specify \"package\" if \"module_relative\" is False.\n\n    Optional keyword arg \"globs\" gives a dict to be used as the globals\n    when executing examples; by default, use {}.  A copy of this dict\n    is actually used for each docstring, so that each docstring's\n    examples start with a clean slate.\n\n    Optional keyword arg \"extraglobs\" gives a dictionary that should be\n    merged into the globals that are used to execute examples.  By\n    default, no extra globals are used.\n\n    Optional keyword arg \"verbose\" prints lots of stuff if true, prints\n    only failures if false; by default, it's true iff \"-v\" is in sys.argv.\n\n    Optional keyword arg \"report\" prints a summary at the end when true,\n    else prints nothing at the end.  In verbose mode, the summary is\n    detailed, else very brief (in fact, empty if all tests passed).\n\n    Optional keyword arg \"optionflags\" or's together module constants,\n    and defaults to 0.  Possible values (see the docs for details):\n\n        DONT_ACCEPT_TRUE_FOR_1\n        DONT_ACCEPT_BLANKLINE\n        NORMALIZE_WHITESPACE\n        ELLIPSIS\n        SKIP\n        IGNORE_EXCEPTION_DETAIL\n        REPORT_UDIFF\n        REPORT_CDIFF\n        REPORT_NDIFF\n        REPORT_ONLY_FIRST_FAILURE\n\n    Optional keyword arg \"raise_on_error\" raises an exception on the\n    first unexpected exception or failure. This allows failures to be\n    post-mortem debugged.\n\n    Optional keyword arg \"parser\" specifies a DocTestParser (or\n    subclass) that should be used to extract tests from the files.\n\n    Optional keyword arg \"encoding\" specifies an encoding that should\n    be used to convert the file to unicode.\n\n    Advanced tomfoolery:  testmod runs methods of a local instance of\n    class doctest.Tester, then merges the results into (or creates)\n    global Tester instance doctest.master.  Methods of doctest.master\n    can be called directly too, if you want to do something unusual.\n    Passing report=0 to testmod is especially useful then, to delay\n    displaying a summary.  Invoke doctest.master.summarize(verbose)\n    when you're done fiddling.\n    \"\"\"\n    global master\n\n    if package and not module_relative:\n        raise ValueError(\"Package may only be specified for module-\"\n                         \"relative paths.\")\n\n    # Relativize the path\n    text, filename = _load_testfile(filename, package, module_relative)\n\n    # If no name was given, then use the file's name.\n    if name is None:\n        name = os.path.basename(filename)\n\n    # Assemble the globals.\n    if globs is None:\n        globs = {}\n    else:\n        globs = globs.copy()\n    if extraglobs is not None:\n        globs.update(extraglobs)\n    if '__name__' not in globs:\n        globs['__name__'] = '__main__'\n\n    if raise_on_error:\n        runner = DebugRunner(verbose=verbose, optionflags=optionflags)\n    else:\n        runner = DocTestRunner(verbose=verbose, optionflags=optionflags)\n\n    if encoding is not None:\n        text = text.decode(encoding)\n\n    # Read the file, convert it to a test, and run it.\n    test = parser.get_doctest(text, globs, name, filename, 0)\n    runner.run(test)\n\n    if report:\n        runner.summarize()\n\n    if master is None:\n        master = runner\n    else:\n        master.merge(runner)\n\n    return TestResults(runner.failures, runner.tries)\n\ndef run_docstring_examples(f, globs, verbose=False, name=\"NoName\",\n                           compileflags=None, optionflags=0):\n    \"\"\"\n    Test examples in the given object's docstring (`f`), using `globs`\n    as globals.  Optional argument `name` is used in failure messages.\n    If the optional argument `verbose` is true, then generate output\n    even if there are no failures.\n\n    `compileflags` gives the set of flags that should be used by the\n    Python compiler when running the examples.  If not specified, then\n    it will default to the set of future-import flags that apply to\n    `globs`.\n\n    Optional keyword arg `optionflags` specifies options for the\n    testing and output.  See the documentation for `testmod` for more\n    information.\n    \"\"\"\n    # Find, parse, and run all tests in the given module.\n    finder = DocTestFinder(verbose=verbose, recurse=False)\n    runner = DocTestRunner(verbose=verbose, optionflags=optionflags)\n    for test in finder.find(f, name, globs=globs):\n        runner.run(test, compileflags=compileflags)\n\n######################################################################\n## 7. Tester\n######################################################################\n# This is provided only for backwards compatibility.  It's not\n# actually used in any way.\n\nclass Tester:\n    def __init__(self, mod=None, globs=None, verbose=None, optionflags=0):\n\n        warnings.warn(\"class Tester is deprecated; \"\n                      \"use class doctest.DocTestRunner instead\",\n                      DeprecationWarning, stacklevel=2)\n        if mod is None and globs is None:\n            raise TypeError(\"Tester.__init__: must specify mod or globs\")\n        if mod is not None and not inspect.ismodule(mod):\n            raise TypeError(\"Tester.__init__: mod must be a module; %r\" %\n                            (mod,))\n        if globs is None:\n            globs = mod.__dict__\n        self.globs = globs\n\n        self.verbose = verbose\n        self.optionflags = optionflags\n        self.testfinder = DocTestFinder()\n        self.testrunner = DocTestRunner(verbose=verbose,\n                                        optionflags=optionflags)\n\n    def runstring(self, s, name):\n        test = DocTestParser().get_doctest(s, self.globs, name, None, None)\n        if self.verbose:\n            print \"Running string\", name\n        (f,t) = self.testrunner.run(test)\n        if self.verbose:\n            print f, \"of\", t, \"examples failed in string\", name\n        return TestResults(f,t)\n\n    def rundoc(self, object, name=None, module=None):\n        f = t = 0\n        tests = self.testfinder.find(object, name, module=module,\n                                     globs=self.globs)\n        for test in tests:\n            (f2, t2) = self.testrunner.run(test)\n            (f,t) = (f+f2, t+t2)\n        return TestResults(f,t)\n\n    def rundict(self, d, name, module=None):\n        import types\n        m = types.ModuleType(name)\n        m.__dict__.update(d)\n        if module is None:\n            module = False\n        return self.rundoc(m, name, module)\n\n    def run__test__(self, d, name):\n        import types\n        m = types.ModuleType(name)\n        m.__test__ = d\n        return self.rundoc(m, name)\n\n    def summarize(self, verbose=None):\n        return self.testrunner.summarize(verbose)\n\n    def merge(self, other):\n        self.testrunner.merge(other.testrunner)\n\n######################################################################\n## 8. Unittest Support\n######################################################################\n\n_unittest_reportflags = 0\n\ndef set_unittest_reportflags(flags):\n    \"\"\"Sets the unittest option flags.\n\n    The old flag is returned so that a runner could restore the old\n    value if it wished to:\n\n      >>> import doctest\n      >>> old = doctest._unittest_reportflags\n      >>> doctest.set_unittest_reportflags(REPORT_NDIFF |\n      ...                          REPORT_ONLY_FIRST_FAILURE) == old\n      True\n\n      >>> doctest._unittest_reportflags == (REPORT_NDIFF |\n      ...                                   REPORT_ONLY_FIRST_FAILURE)\n      True\n\n    Only reporting flags can be set:\n\n      >>> doctest.set_unittest_reportflags(ELLIPSIS)\n      Traceback (most recent call last):\n      ...\n      ValueError: ('Only reporting flags allowed', 8)\n\n      >>> doctest.set_unittest_reportflags(old) == (REPORT_NDIFF |\n      ...                                   REPORT_ONLY_FIRST_FAILURE)\n      True\n    \"\"\"\n    global _unittest_reportflags\n\n    if (flags & REPORTING_FLAGS) != flags:\n        raise ValueError(\"Only reporting flags allowed\", flags)\n    old = _unittest_reportflags\n    _unittest_reportflags = flags\n    return old\n\n\nclass DocTestCase(unittest.TestCase):\n\n    def __init__(self, test, optionflags=0, setUp=None, tearDown=None,\n                 checker=None):\n\n        unittest.TestCase.__init__(self)\n        self._dt_optionflags = optionflags\n        self._dt_checker = checker\n        self._dt_test = test\n        self._dt_setUp = setUp\n        self._dt_tearDown = tearDown\n\n    def setUp(self):\n        test = self._dt_test\n\n        if self._dt_setUp is not None:\n            self._dt_setUp(test)\n\n    def tearDown(self):\n        test = self._dt_test\n\n        if self._dt_tearDown is not None:\n            self._dt_tearDown(test)\n\n        test.globs.clear()\n\n    def runTest(self):\n        test = self._dt_test\n        old = sys.stdout\n        new = StringIO()\n        optionflags = self._dt_optionflags\n\n        if not (optionflags & REPORTING_FLAGS):\n            # The option flags don't include any reporting flags,\n            # so add the default reporting flags\n            optionflags |= _unittest_reportflags\n\n        runner = DocTestRunner(optionflags=optionflags,\n                               checker=self._dt_checker, verbose=False)\n\n        try:\n            runner.DIVIDER = \"-\"*70\n            failures, tries = runner.run(\n                test, out=new.write, clear_globs=False)\n        finally:\n            sys.stdout = old\n\n        if failures:\n            raise self.failureException(self.format_failure(new.getvalue()))\n\n    def format_failure(self, err):\n        test = self._dt_test\n        if test.lineno is None:\n            lineno = 'unknown line number'\n        else:\n            lineno = '%s' % test.lineno\n        lname = '.'.join(test.name.split('.')[-1:])\n        return ('Failed doctest test for %s\\n'\n                '  File \"%s\", line %s, in %s\\n\\n%s'\n                % (test.name, test.filename, lineno, lname, err)\n                )\n\n    def debug(self):\n        r\"\"\"Run the test case without results and without catching exceptions\n\n           The unit test framework includes a debug method on test cases\n           and test suites to support post-mortem debugging.  The test code\n           is run in such a way that errors are not caught.  This way a\n           caller can catch the errors and initiate post-mortem debugging.\n\n           The DocTestCase provides a debug method that raises\n           UnexpectedException errors if there is an unexpected\n           exception:\n\n             >>> test = DocTestParser().get_doctest('>>> raise KeyError\\n42',\n             ...                {}, 'foo', 'foo.py', 0)\n             >>> case = DocTestCase(test)\n             >>> try:\n             ...     case.debug()\n             ... except UnexpectedException, failure:\n             ...     pass\n\n           The UnexpectedException contains the test, the example, and\n           the original exception:\n\n             >>> failure.test is test\n             True\n\n             >>> failure.example.want\n             '42\\n'\n\n             >>> exc_info = failure.exc_info\n             >>> raise exc_info[0], exc_info[1], exc_info[2]\n             Traceback (most recent call last):\n             ...\n             KeyError\n\n           If the output doesn't match, then a DocTestFailure is raised:\n\n             >>> test = DocTestParser().get_doctest('''\n             ...      >>> x = 1\n             ...      >>> x\n             ...      2\n             ...      ''', {}, 'foo', 'foo.py', 0)\n             >>> case = DocTestCase(test)\n\n             >>> try:\n             ...    case.debug()\n             ... except DocTestFailure, failure:\n             ...    pass\n\n           DocTestFailure objects provide access to the test:\n\n             >>> failure.test is test\n             True\n\n           As well as to the example:\n\n             >>> failure.example.want\n             '2\\n'\n\n           and the actual output:\n\n             >>> failure.got\n             '1\\n'\n\n           \"\"\"\n\n        self.setUp()\n        runner = DebugRunner(optionflags=self._dt_optionflags,\n                             checker=self._dt_checker, verbose=False)\n        runner.run(self._dt_test, clear_globs=False)\n        self.tearDown()\n\n    def id(self):\n        return self._dt_test.name\n\n    def __eq__(self, other):\n        if type(self) is not type(other):\n            return NotImplemented\n\n        return self._dt_test == other._dt_test and \\\n               self._dt_optionflags == other._dt_optionflags and \\\n               self._dt_setUp == other._dt_setUp and \\\n               self._dt_tearDown == other._dt_tearDown and \\\n               self._dt_checker == other._dt_checker\n\n    def __ne__(self, other):\n        return not self == other\n\n    def __hash__(self):\n        return hash((self._dt_optionflags, self._dt_setUp, self._dt_tearDown,\n                     self._dt_checker))\n\n    def __repr__(self):\n        name = self._dt_test.name.split('.')\n        return \"%s (%s)\" % (name[-1], '.'.join(name[:-1]))\n\n    __str__ = __repr__\n\n    def shortDescription(self):\n        return \"Doctest: \" + self._dt_test.name\n\nclass SkipDocTestCase(DocTestCase):\n    def __init__(self, module):\n        self.module = module\n        DocTestCase.__init__(self, None)\n\n    def setUp(self):\n        self.skipTest(\"DocTestSuite will not work with -O2 and above\")\n\n    def test_skip(self):\n        pass\n\n    def shortDescription(self):\n        return \"Skipping tests from %s\" % self.module.__name__\n\n    __str__ = shortDescription\n\n\ndef DocTestSuite(module=None, globs=None, extraglobs=None, test_finder=None,\n                 **options):\n    \"\"\"\n    Convert doctest tests for a module to a unittest test suite.\n\n    This converts each documentation string in a module that\n    contains doctest tests to a unittest test case.  If any of the\n    tests in a doc string fail, then the test case fails.  An exception\n    is raised showing the name of the file containing the test and a\n    (sometimes approximate) line number.\n\n    The `module` argument provides the module to be tested.  The argument\n    can be either a module or a module name.\n\n    If no argument is given, the calling module is used.\n\n    A number of options may be provided as keyword arguments:\n\n    setUp\n      A set-up function.  This is called before running the\n      tests in each file. The setUp function will be passed a DocTest\n      object.  The setUp function can access the test globals as the\n      globs attribute of the test passed.\n\n    tearDown\n      A tear-down function.  This is called after running the\n      tests in each file.  The tearDown function will be passed a DocTest\n      object.  The tearDown function can access the test globals as the\n      globs attribute of the test passed.\n\n    globs\n      A dictionary containing initial global variables for the tests.\n\n    optionflags\n       A set of doctest option flags expressed as an integer.\n    \"\"\"\n\n    if test_finder is None:\n        test_finder = DocTestFinder()\n\n    module = _normalize_module(module)\n    tests = test_finder.find(module, globs=globs, extraglobs=extraglobs)\n\n    if not tests and sys.flags.optimize >=2:\n        # Skip doctests when running with -O2\n        suite = unittest.TestSuite()\n        suite.addTest(SkipDocTestCase(module))\n        return suite\n    elif not tests:\n        # Why do we want to do this? Because it reveals a bug that might\n        # otherwise be hidden.\n        # It is probably a bug that this exception is not also raised if the\n        # number of doctest examples in tests is zero (i.e. if no doctest\n        # examples were found).  However, we should probably not be raising\n        # an exception at all here, though it is too late to make this change\n        # for a maintenance release.  See also issue #14649.\n        raise ValueError(module, \"has no docstrings\")\n\n    tests.sort()\n    suite = unittest.TestSuite()\n\n    for test in tests:\n        if len(test.examples) == 0:\n            continue\n        if not test.filename:\n            filename = module.__file__\n            if filename[-4:] in (\".pyc\", \".pyo\"):\n                filename = filename[:-1]\n            test.filename = filename\n        suite.addTest(DocTestCase(test, **options))\n\n    return suite\n\nclass DocFileCase(DocTestCase):\n\n    def id(self):\n        return '_'.join(self._dt_test.name.split('.'))\n\n    def __repr__(self):\n        return self._dt_test.filename\n    __str__ = __repr__\n\n    def format_failure(self, err):\n        return ('Failed doctest test for %s\\n  File \"%s\", line 0\\n\\n%s'\n                % (self._dt_test.name, self._dt_test.filename, err)\n                )\n\ndef DocFileTest(path, module_relative=True, package=None,\n                globs=None, parser=DocTestParser(),\n                encoding=None, **options):\n    if globs is None:\n        globs = {}\n    else:\n        globs = globs.copy()\n\n    if package and not module_relative:\n        raise ValueError(\"Package may only be specified for module-\"\n                         \"relative paths.\")\n\n    # Relativize the path.\n    doc, path = _load_testfile(path, package, module_relative)\n\n    if \"__file__\" not in globs:\n        globs[\"__file__\"] = path\n\n    # Find the file and read it.\n    name = os.path.basename(path)\n\n    # If an encoding is specified, use it to convert the file to unicode\n    if encoding is not None:\n        doc = doc.decode(encoding)\n\n    # Convert it to a test, and wrap it in a DocFileCase.\n    test = parser.get_doctest(doc, globs, name, path, 0)\n    return DocFileCase(test, **options)\n\ndef DocFileSuite(*paths, **kw):\n    \"\"\"A unittest suite for one or more doctest files.\n\n    The path to each doctest file is given as a string; the\n    interpretation of that string depends on the keyword argument\n    \"module_relative\".\n\n    A number of options may be provided as keyword arguments:\n\n    module_relative\n      If \"module_relative\" is True, then the given file paths are\n      interpreted as os-independent module-relative paths.  By\n      default, these paths are relative to the calling module's\n      directory; but if the \"package\" argument is specified, then\n      they are relative to that package.  To ensure os-independence,\n      \"filename\" should use \"/\" characters to separate path\n      segments, and may not be an absolute path (i.e., it may not\n      begin with \"/\").\n\n      If \"module_relative\" is False, then the given file paths are\n      interpreted as os-specific paths.  These paths may be absolute\n      or relative (to the current working directory).\n\n    package\n      A Python package or the name of a Python package whose directory\n      should be used as the base directory for module relative paths.\n      If \"package\" is not specified, then the calling module's\n      directory is used as the base directory for module relative\n      filenames.  It is an error to specify \"package\" if\n      \"module_relative\" is False.\n\n    setUp\n      A set-up function.  This is called before running the\n      tests in each file. The setUp function will be passed a DocTest\n      object.  The setUp function can access the test globals as the\n      globs attribute of the test passed.\n\n    tearDown\n      A tear-down function.  This is called after running the\n      tests in each file.  The tearDown function will be passed a DocTest\n      object.  The tearDown function can access the test globals as the\n      globs attribute of the test passed.\n\n    globs\n      A dictionary containing initial global variables for the tests.\n\n    optionflags\n      A set of doctest option flags expressed as an integer.\n\n    parser\n      A DocTestParser (or subclass) that should be used to extract\n      tests from the files.\n\n    encoding\n      An encoding that will be used to convert the files to unicode.\n    \"\"\"\n    suite = unittest.TestSuite()\n\n    # We do this here so that _normalize_module is called at the right\n    # level.  If it were called in DocFileTest, then this function\n    # would be the caller and we might guess the package incorrectly.\n    if kw.get('module_relative', True):\n        kw['package'] = _normalize_module(kw.get('package'))\n\n    for path in paths:\n        suite.addTest(DocFileTest(path, **kw))\n\n    return suite\n\n######################################################################\n## 9. Debugging Support\n######################################################################\n\ndef script_from_examples(s):\n    r\"\"\"Extract script from text with examples.\n\n       Converts text with examples to a Python script.  Example input is\n       converted to regular code.  Example output and all other words\n       are converted to comments:\n\n       >>> text = '''\n       ...       Here are examples of simple math.\n       ...\n       ...           Python has super accurate integer addition\n       ...\n       ...           >>> 2 + 2\n       ...           5\n       ...\n       ...           And very friendly error messages:\n       ...\n       ...           >>> 1/0\n       ...           To Infinity\n       ...           And\n       ...           Beyond\n       ...\n       ...           You can use logic if you want:\n       ...\n       ...           >>> if 0:\n       ...           ...    blah\n       ...           ...    blah\n       ...           ...\n       ...\n       ...           Ho hum\n       ...           '''\n\n       >>> print script_from_examples(text)\n       # Here are examples of simple math.\n       #\n       #     Python has super accurate integer addition\n       #\n       2 + 2\n       # Expected:\n       ## 5\n       #\n       #     And very friendly error messages:\n       #\n       1/0\n       # Expected:\n       ## To Infinity\n       ## And\n       ## Beyond\n       #\n       #     You can use logic if you want:\n       #\n       if 0:\n          blah\n          blah\n       #\n       #     Ho hum\n       <BLANKLINE>\n       \"\"\"\n    output = []\n    for piece in DocTestParser().parse(s):\n        if isinstance(piece, Example):\n            # Add the example's source code (strip trailing NL)\n            output.append(piece.source[:-1])\n            # Add the expected output:\n            want = piece.want\n            if want:\n                output.append('# Expected:')\n                output += ['## '+l for l in want.split('\\n')[:-1]]\n        else:\n            # Add non-example text.\n            output += [_comment_line(l)\n                       for l in piece.split('\\n')[:-1]]\n\n    # Trim junk on both ends.\n    while output and output[-1] == '#':\n        output.pop()\n    while output and output[0] == '#':\n        output.pop(0)\n    # Combine the output, and return it.\n    # Add a courtesy newline to prevent exec from choking (see bug #1172785)\n    return '\\n'.join(output) + '\\n'\n\ndef testsource(module, name):\n    \"\"\"Extract the test sources from a doctest docstring as a script.\n\n    Provide the module (or dotted name of the module) containing the\n    test to be debugged and the name (within the module) of the object\n    with the doc string with tests to be debugged.\n    \"\"\"\n    module = _normalize_module(module)\n    tests = DocTestFinder().find(module)\n    test = [t for t in tests if t.name == name]\n    if not test:\n        raise ValueError(name, \"not found in tests\")\n    test = test[0]\n    testsrc = script_from_examples(test.docstring)\n    return testsrc\n\ndef debug_src(src, pm=False, globs=None):\n    \"\"\"Debug a single doctest docstring, in argument `src`'\"\"\"\n    testsrc = script_from_examples(src)\n    debug_script(testsrc, pm, globs)\n\ndef debug_script(src, pm=False, globs=None):\n    \"Debug a test script.  `src` is the script, as a string.\"\n    import pdb\n\n    # Note that tempfile.NameTemporaryFile() cannot be used.  As the\n    # docs say, a file so created cannot be opened by name a second time\n    # on modern Windows boxes, and execfile() needs to open it.\n    srcfilename = tempfile.mktemp(\".py\", \"doctestdebug\")\n    f = open(srcfilename, 'w')\n    f.write(src)\n    f.close()\n\n    try:\n        if globs:\n            globs = globs.copy()\n        else:\n            globs = {}\n\n        if pm:\n            try:\n                execfile(srcfilename, globs, globs)\n            except:\n                print sys.exc_info()[1]\n                pdb.post_mortem(sys.exc_info()[2])\n        else:\n            # Note that %r is vital here.  '%s' instead can, e.g., cause\n            # backslashes to get treated as metacharacters on Windows.\n            pdb.run(\"execfile(%r)\" % srcfilename, globs, globs)\n\n    finally:\n        os.remove(srcfilename)\n\ndef debug(module, name, pm=False):\n    \"\"\"Debug a single doctest docstring.\n\n    Provide the module (or dotted name of the module) containing the\n    test to be debugged and the name (within the module) of the object\n    with the docstring with tests to be debugged.\n    \"\"\"\n    module = _normalize_module(module)\n    testsrc = testsource(module, name)\n    debug_script(testsrc, pm, module.__dict__)\n\n######################################################################\n## 10. Example Usage\n######################################################################\nclass _TestClass:\n    \"\"\"\n    A pointless class, for sanity-checking of docstring testing.\n\n    Methods:\n        square()\n        get()\n\n    >>> _TestClass(13).get() + _TestClass(-12).get()\n    1\n    >>> hex(_TestClass(13).square().get())\n    '0xa9'\n    \"\"\"\n\n    def __init__(self, val):\n        \"\"\"val -> _TestClass object with associated value val.\n\n        >>> t = _TestClass(123)\n        >>> print t.get()\n        123\n        \"\"\"\n\n        self.val = val\n\n    def square(self):\n        \"\"\"square() -> square TestClass's associated value\n\n        >>> _TestClass(13).square().get()\n        169\n        \"\"\"\n\n        self.val = self.val ** 2\n        return self\n\n    def get(self):\n        \"\"\"get() -> return TestClass's associated value.\n\n        >>> x = _TestClass(-42)\n        >>> print x.get()\n        -42\n        \"\"\"\n\n        return self.val\n\n__test__ = {\"_TestClass\": _TestClass,\n            \"string\": r\"\"\"\n                      Example of a string object, searched as-is.\n                      >>> x = 1; y = 2\n                      >>> x + y, x * y\n                      (3, 2)\n                      \"\"\",\n\n            \"bool-int equivalence\": r\"\"\"\n                                    In 2.2, boolean expressions displayed\n                                    0 or 1.  By default, we still accept\n                                    them.  This can be disabled by passing\n                                    DONT_ACCEPT_TRUE_FOR_1 to the new\n                                    optionflags argument.\n                                    >>> 4 == 4\n                                    1\n                                    >>> 4 == 4\n                                    True\n                                    >>> 4 > 4\n                                    0\n                                    >>> 4 > 4\n                                    False\n                                    \"\"\",\n\n            \"blank lines\": r\"\"\"\n                Blank lines can be marked with <BLANKLINE>:\n                    >>> print 'foo\\n\\nbar\\n'\n                    foo\n                    <BLANKLINE>\n                    bar\n                    <BLANKLINE>\n            \"\"\",\n\n            \"ellipsis\": r\"\"\"\n                If the ellipsis flag is used, then '...' can be used to\n                elide substrings in the desired output:\n                    >>> print range(1000) #doctest: +ELLIPSIS\n                    [0, 1, 2, ..., 999]\n            \"\"\",\n\n            \"whitespace normalization\": r\"\"\"\n                If the whitespace normalization flag is used, then\n                differences in whitespace are ignored.\n                    >>> print range(30) #doctest: +NORMALIZE_WHITESPACE\n                    [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14,\n                     15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26,\n                     27, 28, 29]\n            \"\"\",\n           }\n\n\ndef _test():\n    testfiles = [arg for arg in sys.argv[1:] if arg and arg[0] != '-']\n    if not testfiles:\n        name = os.path.basename(sys.argv[0])\n        if '__loader__' in globals():          # python -m\n            name, _ = os.path.splitext(name)\n        print(\"usage: {0} [-v] file ...\".format(name))\n        return 2\n    for filename in testfiles:\n        if filename.endswith(\".py\"):\n            # It is a module -- insert its dir into sys.path and try to\n            # import it. If it is part of a package, that possibly\n            # won't work because of package imports.\n            dirname, filename = os.path.split(filename)\n            sys.path.insert(0, dirname)\n            m = __import__(filename[:-3])\n            del sys.path[0]\n            failures, _ = testmod(m)\n        else:\n            failures, _ = testfile(filename, module_relative=False)\n        if failures:\n            return 1\n    return 0\n\n\nif __name__ == \"__main__\":\n    sys.exit(_test())\n", 
    "dummy_thread": "\"\"\"Drop-in replacement for the thread module.\n\nMeant to be used as a brain-dead substitute so that threaded code does\nnot need to be rewritten for when the thread module is not present.\n\nSuggested usage is::\n\n    try:\n        import thread\n    except ImportError:\n        import dummy_thread as thread\n\n\"\"\"\n# Exports only things specified by thread documentation;\n# skipping obsolete synonyms allocate(), start_new(), exit_thread().\n__all__ = ['error', 'start_new_thread', 'exit', 'get_ident', 'allocate_lock',\n           'interrupt_main', 'LockType']\n\nimport traceback as _traceback\n\nclass error(Exception):\n    \"\"\"Dummy implementation of thread.error.\"\"\"\n\n    def __init__(self, *args):\n        self.args = args\n\ndef start_new_thread(function, args, kwargs={}):\n    \"\"\"Dummy implementation of thread.start_new_thread().\n\n    Compatibility is maintained by making sure that ``args`` is a\n    tuple and ``kwargs`` is a dictionary.  If an exception is raised\n    and it is SystemExit (which can be done by thread.exit()) it is\n    caught and nothing is done; all other exceptions are printed out\n    by using traceback.print_exc().\n\n    If the executed function calls interrupt_main the KeyboardInterrupt will be\n    raised when the function returns.\n\n    \"\"\"\n    if type(args) != type(tuple()):\n        raise TypeError(\"2nd arg must be a tuple\")\n    if type(kwargs) != type(dict()):\n        raise TypeError(\"3rd arg must be a dict\")\n    global _main\n    _main = False\n    try:\n        function(*args, **kwargs)\n    except SystemExit:\n        pass\n    except:\n        _traceback.print_exc()\n    _main = True\n    global _interrupt\n    if _interrupt:\n        _interrupt = False\n        raise KeyboardInterrupt\n\ndef exit():\n    \"\"\"Dummy implementation of thread.exit().\"\"\"\n    raise SystemExit\n\ndef get_ident():\n    \"\"\"Dummy implementation of thread.get_ident().\n\n    Since this module should only be used when threadmodule is not\n    available, it is safe to assume that the current process is the\n    only thread.  Thus a constant can be safely returned.\n    \"\"\"\n    return -1\n\ndef allocate_lock():\n    \"\"\"Dummy implementation of thread.allocate_lock().\"\"\"\n    return LockType()\n\ndef stack_size(size=None):\n    \"\"\"Dummy implementation of thread.stack_size().\"\"\"\n    if size is not None:\n        raise error(\"setting thread stack size not supported\")\n    return 0\n\nclass LockType(object):\n    \"\"\"Class implementing dummy implementation of thread.LockType.\n\n    Compatibility is maintained by maintaining self.locked_status\n    which is a boolean that stores the state of the lock.  Pickling of\n    the lock, though, should not be done since if the thread module is\n    then used with an unpickled ``lock()`` from here problems could\n    occur from this class not having atomic methods.\n\n    \"\"\"\n\n    def __init__(self):\n        self.locked_status = False\n\n    def acquire(self, waitflag=None):\n        \"\"\"Dummy implementation of acquire().\n\n        For blocking calls, self.locked_status is automatically set to\n        True and returned appropriately based on value of\n        ``waitflag``.  If it is non-blocking, then the value is\n        actually checked and not set if it is already acquired.  This\n        is all done so that threading.Condition's assert statements\n        aren't triggered and throw a little fit.\n\n        \"\"\"\n        if waitflag is None or waitflag:\n            self.locked_status = True\n            return True\n        else:\n            if not self.locked_status:\n                self.locked_status = True\n                return True\n            else:\n                return False\n\n    __enter__ = acquire\n\n    def __exit__(self, typ, val, tb):\n        self.release()\n\n    def release(self):\n        \"\"\"Release the dummy lock.\"\"\"\n        # XXX Perhaps shouldn't actually bother to test?  Could lead\n        #     to problems for complex, threaded code.\n        if not self.locked_status:\n            raise error\n        self.locked_status = False\n        return True\n\n    def locked(self):\n        return self.locked_status\n\n# Used to signal that interrupt_main was called in a \"thread\"\n_interrupt = False\n# True when not executing in a \"thread\"\n_main = True\n\ndef interrupt_main():\n    \"\"\"Set _interrupt flag to True to have start_new_thread raise\n    KeyboardInterrupt upon exiting.\"\"\"\n    if _main:\n        raise KeyboardInterrupt\n    else:\n        global _interrupt\n        _interrupt = True\n", 
    "encodings.__init__": "\"\"\" Standard \"encodings\" Package\n\n    Standard Python encoding modules are stored in this package\n    directory.\n\n    Codec modules must have names corresponding to normalized encoding\n    names as defined in the normalize_encoding() function below, e.g.\n    'utf-8' must be implemented by the module 'utf_8.py'.\n\n    Each codec module must export the following interface:\n\n    * getregentry() -> codecs.CodecInfo object\n    The getregentry() API must a CodecInfo object with encoder, decoder,\n    incrementalencoder, incrementaldecoder, streamwriter and streamreader\n    atttributes which adhere to the Python Codec Interface Standard.\n\n    In addition, a module may optionally also define the following\n    APIs which are then used by the package's codec search function:\n\n    * getaliases() -> sequence of encoding name strings to use as aliases\n\n    Alias names returned by getaliases() must be normalized encoding\n    names as defined by normalize_encoding().\n\nWritten by Marc-Andre Lemburg (mal@lemburg.com).\n\n(c) Copyright CNRI, All Rights Reserved. NO WARRANTY.\n\n\"\"\"#\"\n\nimport codecs\nfrom encodings import aliases\nimport __builtin__\n\n_cache = {}\n_unknown = '--unknown--'\n_import_tail = ['*']\n_norm_encoding_map = ('                                              . '\n                      '0123456789       ABCDEFGHIJKLMNOPQRSTUVWXYZ     '\n                      ' abcdefghijklmnopqrstuvwxyz                     '\n                      '                                                '\n                      '                                                '\n                      '                ')\n_aliases = aliases.aliases\n\nclass CodecRegistryError(LookupError, SystemError):\n    pass\n\ndef normalize_encoding(encoding):\n\n    \"\"\" Normalize an encoding name.\n\n        Normalization works as follows: all non-alphanumeric\n        characters except the dot used for Python package names are\n        collapsed and replaced with a single underscore, e.g. '  -;#'\n        becomes '_'. Leading and trailing underscores are removed.\n\n        Note that encoding names should be ASCII only; if they do use\n        non-ASCII characters, these must be Latin-1 compatible.\n\n    \"\"\"\n    # Make sure we have an 8-bit string, because .translate() works\n    # differently for Unicode strings.\n    if hasattr(__builtin__, \"unicode\") and isinstance(encoding, unicode):\n        # Note that .encode('latin-1') does *not* use the codec\n        # registry, so this call doesn't recurse. (See unicodeobject.c\n        # PyUnicode_AsEncodedString() for details)\n        encoding = encoding.encode('latin-1')\n    return '_'.join(encoding.translate(_norm_encoding_map).split())\n\ndef search_function(encoding):\n\n    # Cache lookup\n    entry = _cache.get(encoding, _unknown)\n    if entry is not _unknown:\n        return entry\n\n    # Import the module:\n    #\n    # First try to find an alias for the normalized encoding\n    # name and lookup the module using the aliased name, then try to\n    # lookup the module using the standard import scheme, i.e. first\n    # try in the encodings package, then at top-level.\n    #\n    norm_encoding = normalize_encoding(encoding)\n    aliased_encoding = _aliases.get(norm_encoding) or \\\n                       _aliases.get(norm_encoding.replace('.', '_'))\n    if aliased_encoding is not None:\n        modnames = [aliased_encoding,\n                    norm_encoding]\n    else:\n        modnames = [norm_encoding]\n    for modname in modnames:\n        if not modname or '.' in modname:\n            continue\n        try:\n            # Import is absolute to prevent the possibly malicious import of a\n            # module with side-effects that is not in the 'encodings' package.\n            mod = __import__('encodings.' + modname, fromlist=_import_tail,\n                             level=0)\n        except ImportError:\n            pass\n        else:\n            break\n    else:\n        mod = None\n\n    try:\n        getregentry = mod.getregentry\n    except AttributeError:\n        # Not a codec module\n        mod = None\n\n    if mod is None:\n        # Cache misses\n        _cache[encoding] = None\n        return None\n\n    # Now ask the module for the registry entry\n    entry = getregentry()\n    if not isinstance(entry, codecs.CodecInfo):\n        if not 4 <= len(entry) <= 7:\n            raise CodecRegistryError,\\\n                 'module \"%s\" (%s) failed to register' % \\\n                  (mod.__name__, mod.__file__)\n        if not hasattr(entry[0], '__call__') or \\\n           not hasattr(entry[1], '__call__') or \\\n           (entry[2] is not None and not hasattr(entry[2], '__call__')) or \\\n           (entry[3] is not None and not hasattr(entry[3], '__call__')) or \\\n           (len(entry) > 4 and entry[4] is not None and not hasattr(entry[4], '__call__')) or \\\n           (len(entry) > 5 and entry[5] is not None and not hasattr(entry[5], '__call__')):\n            raise CodecRegistryError,\\\n                'incompatible codecs in module \"%s\" (%s)' % \\\n                (mod.__name__, mod.__file__)\n        if len(entry)<7 or entry[6] is None:\n            entry += (None,)*(6-len(entry)) + (mod.__name__.split(\".\", 1)[1],)\n        entry = codecs.CodecInfo(*entry)\n\n    # Cache the codec registry entry\n    _cache[encoding] = entry\n\n    # Register its aliases (without overwriting previously registered\n    # aliases)\n    try:\n        codecaliases = mod.getaliases()\n    except AttributeError:\n        pass\n    else:\n        for alias in codecaliases:\n            if alias not in _aliases:\n                _aliases[alias] = modname\n\n    # Return the registry entry\n    return entry\n\n# Register the search_function in the Python codec registry\ncodecs.register(search_function)\n", 
    "encodings.aliases": "\"\"\" Encoding Aliases Support\n\n    This module is used by the encodings package search function to\n    map encodings names to module names.\n\n    Note that the search function normalizes the encoding names before\n    doing the lookup, so the mapping will have to map normalized\n    encoding names to module names.\n\n    Contents:\n\n        The following aliases dictionary contains mappings of all IANA\n        character set names for which the Python core library provides\n        codecs. In addition to these, a few Python specific codec\n        aliases have also been added.\n\n\"\"\"\naliases = {\n\n    # Please keep this list sorted alphabetically by value !\n\n    # ascii codec\n    '646'                : 'ascii',\n    'ansi_x3.4_1968'     : 'ascii',\n    'ansi_x3_4_1968'     : 'ascii', # some email headers use this non-standard name\n    'ansi_x3.4_1986'     : 'ascii',\n    'cp367'              : 'ascii',\n    'csascii'            : 'ascii',\n    'ibm367'             : 'ascii',\n    'iso646_us'          : 'ascii',\n    'iso_646.irv_1991'   : 'ascii',\n    'iso_ir_6'           : 'ascii',\n    'us'                 : 'ascii',\n    'us_ascii'           : 'ascii',\n\n    # base64_codec codec\n    'base64'             : 'base64_codec',\n    'base_64'            : 'base64_codec',\n\n    # big5 codec\n    'big5_tw'            : 'big5',\n    'csbig5'             : 'big5',\n\n    # big5hkscs codec\n    'big5_hkscs'         : 'big5hkscs',\n    'hkscs'              : 'big5hkscs',\n\n    # bz2_codec codec\n    'bz2'                : 'bz2_codec',\n\n    # cp037 codec\n    '037'                : 'cp037',\n    'csibm037'           : 'cp037',\n    'ebcdic_cp_ca'       : 'cp037',\n    'ebcdic_cp_nl'       : 'cp037',\n    'ebcdic_cp_us'       : 'cp037',\n    'ebcdic_cp_wt'       : 'cp037',\n    'ibm037'             : 'cp037',\n    'ibm039'             : 'cp037',\n\n    # cp1026 codec\n    '1026'               : 'cp1026',\n    'csibm1026'          : 'cp1026',\n    'ibm1026'            : 'cp1026',\n\n    # cp1140 codec\n    '1140'               : 'cp1140',\n    'ibm1140'            : 'cp1140',\n\n    # cp1250 codec\n    '1250'               : 'cp1250',\n    'windows_1250'       : 'cp1250',\n\n    # cp1251 codec\n    '1251'               : 'cp1251',\n    'windows_1251'       : 'cp1251',\n\n    # cp1252 codec\n    '1252'               : 'cp1252',\n    'windows_1252'       : 'cp1252',\n\n    # cp1253 codec\n    '1253'               : 'cp1253',\n    'windows_1253'       : 'cp1253',\n\n    # cp1254 codec\n    '1254'               : 'cp1254',\n    'windows_1254'       : 'cp1254',\n\n    # cp1255 codec\n    '1255'               : 'cp1255',\n    'windows_1255'       : 'cp1255',\n\n    # cp1256 codec\n    '1256'               : 'cp1256',\n    'windows_1256'       : 'cp1256',\n\n    # cp1257 codec\n    '1257'               : 'cp1257',\n    'windows_1257'       : 'cp1257',\n\n    # cp1258 codec\n    '1258'               : 'cp1258',\n    'windows_1258'       : 'cp1258',\n\n    # cp424 codec\n    '424'                : 'cp424',\n    'csibm424'           : 'cp424',\n    'ebcdic_cp_he'       : 'cp424',\n    'ibm424'             : 'cp424',\n\n    # cp437 codec\n    '437'                : 'cp437',\n    'cspc8codepage437'   : 'cp437',\n    'ibm437'             : 'cp437',\n\n    # cp500 codec\n    '500'                : 'cp500',\n    'csibm500'           : 'cp500',\n    'ebcdic_cp_be'       : 'cp500',\n    'ebcdic_cp_ch'       : 'cp500',\n    'ibm500'             : 'cp500',\n\n    # cp775 codec\n    '775'                : 'cp775',\n    'cspc775baltic'      : 'cp775',\n    'ibm775'             : 'cp775',\n\n    # cp850 codec\n    '850'                : 'cp850',\n    'cspc850multilingual' : 'cp850',\n    'ibm850'             : 'cp850',\n\n    # cp852 codec\n    '852'                : 'cp852',\n    'cspcp852'           : 'cp852',\n    'ibm852'             : 'cp852',\n\n    # cp855 codec\n    '855'                : 'cp855',\n    'csibm855'           : 'cp855',\n    'ibm855'             : 'cp855',\n\n    # cp857 codec\n    '857'                : 'cp857',\n    'csibm857'           : 'cp857',\n    'ibm857'             : 'cp857',\n\n    # cp858 codec\n    '858'                : 'cp858',\n    'csibm858'           : 'cp858',\n    'ibm858'             : 'cp858',\n\n    # cp860 codec\n    '860'                : 'cp860',\n    'csibm860'           : 'cp860',\n    'ibm860'             : 'cp860',\n\n    # cp861 codec\n    '861'                : 'cp861',\n    'cp_is'              : 'cp861',\n    'csibm861'           : 'cp861',\n    'ibm861'             : 'cp861',\n\n    # cp862 codec\n    '862'                : 'cp862',\n    'cspc862latinhebrew' : 'cp862',\n    'ibm862'             : 'cp862',\n\n    # cp863 codec\n    '863'                : 'cp863',\n    'csibm863'           : 'cp863',\n    'ibm863'             : 'cp863',\n\n    # cp864 codec\n    '864'                : 'cp864',\n    'csibm864'           : 'cp864',\n    'ibm864'             : 'cp864',\n\n    # cp865 codec\n    '865'                : 'cp865',\n    'csibm865'           : 'cp865',\n    'ibm865'             : 'cp865',\n\n    # cp866 codec\n    '866'                : 'cp866',\n    'csibm866'           : 'cp866',\n    'ibm866'             : 'cp866',\n\n    # cp869 codec\n    '869'                : 'cp869',\n    'cp_gr'              : 'cp869',\n    'csibm869'           : 'cp869',\n    'ibm869'             : 'cp869',\n\n    # cp932 codec\n    '932'                : 'cp932',\n    'ms932'              : 'cp932',\n    'mskanji'            : 'cp932',\n    'ms_kanji'           : 'cp932',\n\n    # cp949 codec\n    '949'                : 'cp949',\n    'ms949'              : 'cp949',\n    'uhc'                : 'cp949',\n\n    # cp950 codec\n    '950'                : 'cp950',\n    'ms950'              : 'cp950',\n\n    # euc_jis_2004 codec\n    'jisx0213'           : 'euc_jis_2004',\n    'eucjis2004'         : 'euc_jis_2004',\n    'euc_jis2004'        : 'euc_jis_2004',\n\n    # euc_jisx0213 codec\n    'eucjisx0213'        : 'euc_jisx0213',\n\n    # euc_jp codec\n    'eucjp'              : 'euc_jp',\n    'ujis'               : 'euc_jp',\n    'u_jis'              : 'euc_jp',\n\n    # euc_kr codec\n    'euckr'              : 'euc_kr',\n    'korean'             : 'euc_kr',\n    'ksc5601'            : 'euc_kr',\n    'ks_c_5601'          : 'euc_kr',\n    'ks_c_5601_1987'     : 'euc_kr',\n    'ksx1001'            : 'euc_kr',\n    'ks_x_1001'          : 'euc_kr',\n\n    # gb18030 codec\n    'gb18030_2000'       : 'gb18030',\n\n    # gb2312 codec\n    'chinese'            : 'gb2312',\n    'csiso58gb231280'    : 'gb2312',\n    'euc_cn'             : 'gb2312',\n    'euccn'              : 'gb2312',\n    'eucgb2312_cn'       : 'gb2312',\n    'gb2312_1980'        : 'gb2312',\n    'gb2312_80'          : 'gb2312',\n    'iso_ir_58'          : 'gb2312',\n\n    # gbk codec\n    '936'                : 'gbk',\n    'cp936'              : 'gbk',\n    'ms936'              : 'gbk',\n\n    # hex_codec codec\n    'hex'                : 'hex_codec',\n\n    # hp_roman8 codec\n    'roman8'             : 'hp_roman8',\n    'r8'                 : 'hp_roman8',\n    'csHPRoman8'         : 'hp_roman8',\n\n    # hz codec\n    'hzgb'               : 'hz',\n    'hz_gb'              : 'hz',\n    'hz_gb_2312'         : 'hz',\n\n    # iso2022_jp codec\n    'csiso2022jp'        : 'iso2022_jp',\n    'iso2022jp'          : 'iso2022_jp',\n    'iso_2022_jp'        : 'iso2022_jp',\n\n    # iso2022_jp_1 codec\n    'iso2022jp_1'        : 'iso2022_jp_1',\n    'iso_2022_jp_1'      : 'iso2022_jp_1',\n\n    # iso2022_jp_2 codec\n    'iso2022jp_2'        : 'iso2022_jp_2',\n    'iso_2022_jp_2'      : 'iso2022_jp_2',\n\n    # iso2022_jp_2004 codec\n    'iso_2022_jp_2004'   : 'iso2022_jp_2004',\n    'iso2022jp_2004'     : 'iso2022_jp_2004',\n\n    # iso2022_jp_3 codec\n    'iso2022jp_3'        : 'iso2022_jp_3',\n    'iso_2022_jp_3'      : 'iso2022_jp_3',\n\n    # iso2022_jp_ext codec\n    'iso2022jp_ext'      : 'iso2022_jp_ext',\n    'iso_2022_jp_ext'    : 'iso2022_jp_ext',\n\n    # iso2022_kr codec\n    'csiso2022kr'        : 'iso2022_kr',\n    'iso2022kr'          : 'iso2022_kr',\n    'iso_2022_kr'        : 'iso2022_kr',\n\n    # iso8859_10 codec\n    'csisolatin6'        : 'iso8859_10',\n    'iso_8859_10'        : 'iso8859_10',\n    'iso_8859_10_1992'   : 'iso8859_10',\n    'iso_ir_157'         : 'iso8859_10',\n    'l6'                 : 'iso8859_10',\n    'latin6'             : 'iso8859_10',\n\n    # iso8859_11 codec\n    'thai'               : 'iso8859_11',\n    'iso_8859_11'        : 'iso8859_11',\n    'iso_8859_11_2001'   : 'iso8859_11',\n\n    # iso8859_13 codec\n    'iso_8859_13'        : 'iso8859_13',\n    'l7'                 : 'iso8859_13',\n    'latin7'             : 'iso8859_13',\n\n    # iso8859_14 codec\n    'iso_8859_14'        : 'iso8859_14',\n    'iso_8859_14_1998'   : 'iso8859_14',\n    'iso_celtic'         : 'iso8859_14',\n    'iso_ir_199'         : 'iso8859_14',\n    'l8'                 : 'iso8859_14',\n    'latin8'             : 'iso8859_14',\n\n    # iso8859_15 codec\n    'iso_8859_15'        : 'iso8859_15',\n    'l9'                 : 'iso8859_15',\n    'latin9'             : 'iso8859_15',\n\n    # iso8859_16 codec\n    'iso_8859_16'        : 'iso8859_16',\n    'iso_8859_16_2001'   : 'iso8859_16',\n    'iso_ir_226'         : 'iso8859_16',\n    'l10'                : 'iso8859_16',\n    'latin10'            : 'iso8859_16',\n\n    # iso8859_2 codec\n    'csisolatin2'        : 'iso8859_2',\n    'iso_8859_2'         : 'iso8859_2',\n    'iso_8859_2_1987'    : 'iso8859_2',\n    'iso_ir_101'         : 'iso8859_2',\n    'l2'                 : 'iso8859_2',\n    'latin2'             : 'iso8859_2',\n\n    # iso8859_3 codec\n    'csisolatin3'        : 'iso8859_3',\n    'iso_8859_3'         : 'iso8859_3',\n    'iso_8859_3_1988'    : 'iso8859_3',\n    'iso_ir_109'         : 'iso8859_3',\n    'l3'                 : 'iso8859_3',\n    'latin3'             : 'iso8859_3',\n\n    # iso8859_4 codec\n    'csisolatin4'        : 'iso8859_4',\n    'iso_8859_4'         : 'iso8859_4',\n    'iso_8859_4_1988'    : 'iso8859_4',\n    'iso_ir_110'         : 'iso8859_4',\n    'l4'                 : 'iso8859_4',\n    'latin4'             : 'iso8859_4',\n\n    # iso8859_5 codec\n    'csisolatincyrillic' : 'iso8859_5',\n    'cyrillic'           : 'iso8859_5',\n    'iso_8859_5'         : 'iso8859_5',\n    'iso_8859_5_1988'    : 'iso8859_5',\n    'iso_ir_144'         : 'iso8859_5',\n\n    # iso8859_6 codec\n    'arabic'             : 'iso8859_6',\n    'asmo_708'           : 'iso8859_6',\n    'csisolatinarabic'   : 'iso8859_6',\n    'ecma_114'           : 'iso8859_6',\n    'iso_8859_6'         : 'iso8859_6',\n    'iso_8859_6_1987'    : 'iso8859_6',\n    'iso_ir_127'         : 'iso8859_6',\n\n    # iso8859_7 codec\n    'csisolatingreek'    : 'iso8859_7',\n    'ecma_118'           : 'iso8859_7',\n    'elot_928'           : 'iso8859_7',\n    'greek'              : 'iso8859_7',\n    'greek8'             : 'iso8859_7',\n    'iso_8859_7'         : 'iso8859_7',\n    'iso_8859_7_1987'    : 'iso8859_7',\n    'iso_ir_126'         : 'iso8859_7',\n\n    # iso8859_8 codec\n    'csisolatinhebrew'   : 'iso8859_8',\n    'hebrew'             : 'iso8859_8',\n    'iso_8859_8'         : 'iso8859_8',\n    'iso_8859_8_1988'    : 'iso8859_8',\n    'iso_ir_138'         : 'iso8859_8',\n\n    # iso8859_9 codec\n    'csisolatin5'        : 'iso8859_9',\n    'iso_8859_9'         : 'iso8859_9',\n    'iso_8859_9_1989'    : 'iso8859_9',\n    'iso_ir_148'         : 'iso8859_9',\n    'l5'                 : 'iso8859_9',\n    'latin5'             : 'iso8859_9',\n\n    # johab codec\n    'cp1361'             : 'johab',\n    'ms1361'             : 'johab',\n\n    # koi8_r codec\n    'cskoi8r'            : 'koi8_r',\n\n    # latin_1 codec\n    #\n    # Note that the latin_1 codec is implemented internally in C and a\n    # lot faster than the charmap codec iso8859_1 which uses the same\n    # encoding. This is why we discourage the use of the iso8859_1\n    # codec and alias it to latin_1 instead.\n    #\n    '8859'               : 'latin_1',\n    'cp819'              : 'latin_1',\n    'csisolatin1'        : 'latin_1',\n    'ibm819'             : 'latin_1',\n    'iso8859'            : 'latin_1',\n    'iso8859_1'          : 'latin_1',\n    'iso_8859_1'         : 'latin_1',\n    'iso_8859_1_1987'    : 'latin_1',\n    'iso_ir_100'         : 'latin_1',\n    'l1'                 : 'latin_1',\n    'latin'              : 'latin_1',\n    'latin1'             : 'latin_1',\n\n    # mac_cyrillic codec\n    'maccyrillic'        : 'mac_cyrillic',\n\n    # mac_greek codec\n    'macgreek'           : 'mac_greek',\n\n    # mac_iceland codec\n    'maciceland'         : 'mac_iceland',\n\n    # mac_latin2 codec\n    'maccentraleurope'   : 'mac_latin2',\n    'maclatin2'          : 'mac_latin2',\n\n    # mac_roman codec\n    'macroman'           : 'mac_roman',\n\n    # mac_turkish codec\n    'macturkish'         : 'mac_turkish',\n\n    # mbcs codec\n    'dbcs'               : 'mbcs',\n\n    # ptcp154 codec\n    'csptcp154'          : 'ptcp154',\n    'pt154'              : 'ptcp154',\n    'cp154'              : 'ptcp154',\n    'cyrillic_asian'     : 'ptcp154',\n\n    # quopri_codec codec\n    'quopri'             : 'quopri_codec',\n    'quoted_printable'   : 'quopri_codec',\n    'quotedprintable'    : 'quopri_codec',\n\n    # rot_13 codec\n    'rot13'              : 'rot_13',\n\n    # shift_jis codec\n    'csshiftjis'         : 'shift_jis',\n    'shiftjis'           : 'shift_jis',\n    'sjis'               : 'shift_jis',\n    's_jis'              : 'shift_jis',\n\n    # shift_jis_2004 codec\n    'shiftjis2004'       : 'shift_jis_2004',\n    'sjis_2004'          : 'shift_jis_2004',\n    's_jis_2004'         : 'shift_jis_2004',\n\n    # shift_jisx0213 codec\n    'shiftjisx0213'      : 'shift_jisx0213',\n    'sjisx0213'          : 'shift_jisx0213',\n    's_jisx0213'         : 'shift_jisx0213',\n\n    # tactis codec\n    'tis260'             : 'tactis',\n\n    # tis_620 codec\n    'tis620'             : 'tis_620',\n    'tis_620_0'          : 'tis_620',\n    'tis_620_2529_0'     : 'tis_620',\n    'tis_620_2529_1'     : 'tis_620',\n    'iso_ir_166'         : 'tis_620',\n\n    # utf_16 codec\n    'u16'                : 'utf_16',\n    'utf16'              : 'utf_16',\n\n    # utf_16_be codec\n    'unicodebigunmarked' : 'utf_16_be',\n    'utf_16be'           : 'utf_16_be',\n\n    # utf_16_le codec\n    'unicodelittleunmarked' : 'utf_16_le',\n    'utf_16le'           : 'utf_16_le',\n\n    # utf_32 codec\n    'u32'                : 'utf_32',\n    'utf32'              : 'utf_32',\n\n    # utf_32_be codec\n    'utf_32be'           : 'utf_32_be',\n\n    # utf_32_le codec\n    'utf_32le'           : 'utf_32_le',\n\n    # utf_7 codec\n    'u7'                 : 'utf_7',\n    'utf7'               : 'utf_7',\n    'unicode_1_1_utf_7'  : 'utf_7',\n\n    # utf_8 codec\n    'u8'                 : 'utf_8',\n    'utf'                : 'utf_8',\n    'utf8'               : 'utf_8',\n    'utf8_ucs2'          : 'utf_8',\n    'utf8_ucs4'          : 'utf_8',\n\n    # uu_codec codec\n    'uu'                 : 'uu_codec',\n\n    # zlib_codec codec\n    'zip'                : 'zlib_codec',\n    'zlib'               : 'zlib_codec',\n\n}\n", 
    "encodings.ascii": "\"\"\" Python 'ascii' Codec\n\n\nWritten by Marc-Andre Lemburg (mal@lemburg.com).\n\n(c) Copyright CNRI, All Rights Reserved. NO WARRANTY.\n\n\"\"\"\nimport codecs\n\n### Codec APIs\n\nclass Codec(codecs.Codec):\n\n    # Note: Binding these as C functions will result in the class not\n    # converting them to methods. This is intended.\n    encode = codecs.ascii_encode\n    decode = codecs.ascii_decode\n\nclass IncrementalEncoder(codecs.IncrementalEncoder):\n    def encode(self, input, final=False):\n        return codecs.ascii_encode(input, self.errors)[0]\n\nclass IncrementalDecoder(codecs.IncrementalDecoder):\n    def decode(self, input, final=False):\n        return codecs.ascii_decode(input, self.errors)[0]\n\nclass StreamWriter(Codec,codecs.StreamWriter):\n    pass\n\nclass StreamReader(Codec,codecs.StreamReader):\n    pass\n\nclass StreamConverter(StreamWriter,StreamReader):\n\n    encode = codecs.ascii_decode\n    decode = codecs.ascii_encode\n\n### encodings module API\n\ndef getregentry():\n    return codecs.CodecInfo(\n        name='ascii',\n        encode=Codec.encode,\n        decode=Codec.decode,\n        incrementalencoder=IncrementalEncoder,\n        incrementaldecoder=IncrementalDecoder,\n        streamwriter=StreamWriter,\n        streamreader=StreamReader,\n    )\n", 
    "encodings.base64_codec": "\"\"\" Python 'base64_codec' Codec - base64 content transfer encoding\n\n    Unlike most of the other codecs which target Unicode, this codec\n    will return Python string objects for both encode and decode.\n\n    Written by Marc-Andre Lemburg (mal@lemburg.com).\n\n\"\"\"\nimport codecs, base64\n\n### Codec APIs\n\ndef base64_encode(input,errors='strict'):\n\n    \"\"\" Encodes the object input and returns a tuple (output\n        object, length consumed).\n\n        errors defines the error handling to apply. It defaults to\n        'strict' handling which is the only currently supported\n        error handling for this codec.\n\n    \"\"\"\n    assert errors == 'strict'\n    output = base64.encodestring(input)\n    return (output, len(input))\n\ndef base64_decode(input,errors='strict'):\n\n    \"\"\" Decodes the object input and returns a tuple (output\n        object, length consumed).\n\n        input must be an object which provides the bf_getreadbuf\n        buffer slot. Python strings, buffer objects and memory\n        mapped files are examples of objects providing this slot.\n\n        errors defines the error handling to apply. It defaults to\n        'strict' handling which is the only currently supported\n        error handling for this codec.\n\n    \"\"\"\n    assert errors == 'strict'\n    output = base64.decodestring(input)\n    return (output, len(input))\n\nclass Codec(codecs.Codec):\n\n    def encode(self, input,errors='strict'):\n        return base64_encode(input,errors)\n    def decode(self, input,errors='strict'):\n        return base64_decode(input,errors)\n\nclass IncrementalEncoder(codecs.IncrementalEncoder):\n    def encode(self, input, final=False):\n        assert self.errors == 'strict'\n        return base64.encodestring(input)\n\nclass IncrementalDecoder(codecs.IncrementalDecoder):\n    def decode(self, input, final=False):\n        assert self.errors == 'strict'\n        return base64.decodestring(input)\n\nclass StreamWriter(Codec,codecs.StreamWriter):\n    pass\n\nclass StreamReader(Codec,codecs.StreamReader):\n    pass\n\n### encodings module API\n\ndef getregentry():\n    return codecs.CodecInfo(\n        name='base64',\n        encode=base64_encode,\n        decode=base64_decode,\n        incrementalencoder=IncrementalEncoder,\n        incrementaldecoder=IncrementalDecoder,\n        streamwriter=StreamWriter,\n        streamreader=StreamReader,\n    )\n", 
    "encodings.hex_codec": "\"\"\" Python 'hex_codec' Codec - 2-digit hex content transfer encoding\n\n    Unlike most of the other codecs which target Unicode, this codec\n    will return Python string objects for both encode and decode.\n\n    Written by Marc-Andre Lemburg (mal@lemburg.com).\n\n\"\"\"\nimport codecs, binascii\n\n### Codec APIs\n\ndef hex_encode(input,errors='strict'):\n\n    \"\"\" Encodes the object input and returns a tuple (output\n        object, length consumed).\n\n        errors defines the error handling to apply. It defaults to\n        'strict' handling which is the only currently supported\n        error handling for this codec.\n\n    \"\"\"\n    assert errors == 'strict'\n    output = binascii.b2a_hex(input)\n    return (output, len(input))\n\ndef hex_decode(input,errors='strict'):\n\n    \"\"\" Decodes the object input and returns a tuple (output\n        object, length consumed).\n\n        input must be an object which provides the bf_getreadbuf\n        buffer slot. Python strings, buffer objects and memory\n        mapped files are examples of objects providing this slot.\n\n        errors defines the error handling to apply. It defaults to\n        'strict' handling which is the only currently supported\n        error handling for this codec.\n\n    \"\"\"\n    assert errors == 'strict'\n    output = binascii.a2b_hex(input)\n    return (output, len(input))\n\nclass Codec(codecs.Codec):\n\n    def encode(self, input,errors='strict'):\n        return hex_encode(input,errors)\n    def decode(self, input,errors='strict'):\n        return hex_decode(input,errors)\n\nclass IncrementalEncoder(codecs.IncrementalEncoder):\n    def encode(self, input, final=False):\n        assert self.errors == 'strict'\n        return binascii.b2a_hex(input)\n\nclass IncrementalDecoder(codecs.IncrementalDecoder):\n    def decode(self, input, final=False):\n        assert self.errors == 'strict'\n        return binascii.a2b_hex(input)\n\nclass StreamWriter(Codec,codecs.StreamWriter):\n    pass\n\nclass StreamReader(Codec,codecs.StreamReader):\n    pass\n\n### encodings module API\n\ndef getregentry():\n    return codecs.CodecInfo(\n        name='hex',\n        encode=hex_encode,\n        decode=hex_decode,\n        incrementalencoder=IncrementalEncoder,\n        incrementaldecoder=IncrementalDecoder,\n        streamwriter=StreamWriter,\n        streamreader=StreamReader,\n    )\n", 
    "encodings.latin_1": "\"\"\" Python 'latin-1' Codec\n\n\nWritten by Marc-Andre Lemburg (mal@lemburg.com).\n\n(c) Copyright CNRI, All Rights Reserved. NO WARRANTY.\n\n\"\"\"\nimport codecs\n\n### Codec APIs\n\nclass Codec(codecs.Codec):\n\n    # Note: Binding these as C functions will result in the class not\n    # converting them to methods. This is intended.\n    encode = codecs.latin_1_encode\n    decode = codecs.latin_1_decode\n\nclass IncrementalEncoder(codecs.IncrementalEncoder):\n    def encode(self, input, final=False):\n        return codecs.latin_1_encode(input,self.errors)[0]\n\nclass IncrementalDecoder(codecs.IncrementalDecoder):\n    def decode(self, input, final=False):\n        return codecs.latin_1_decode(input,self.errors)[0]\n\nclass StreamWriter(Codec,codecs.StreamWriter):\n    pass\n\nclass StreamReader(Codec,codecs.StreamReader):\n    pass\n\nclass StreamConverter(StreamWriter,StreamReader):\n\n    encode = codecs.latin_1_decode\n    decode = codecs.latin_1_encode\n\n### encodings module API\n\ndef getregentry():\n    return codecs.CodecInfo(\n        name='iso8859-1',\n        encode=Codec.encode,\n        decode=Codec.decode,\n        incrementalencoder=IncrementalEncoder,\n        incrementaldecoder=IncrementalDecoder,\n        streamreader=StreamReader,\n        streamwriter=StreamWriter,\n    )\n", 
    "encodings.raw_unicode_escape": "\"\"\" Python 'raw-unicode-escape' Codec\n\n\nWritten by Marc-Andre Lemburg (mal@lemburg.com).\n\n(c) Copyright CNRI, All Rights Reserved. NO WARRANTY.\n\n\"\"\"\nimport codecs\n\n### Codec APIs\n\nclass Codec(codecs.Codec):\n\n    # Note: Binding these as C functions will result in the class not\n    # converting them to methods. This is intended.\n    encode = codecs.raw_unicode_escape_encode\n    decode = codecs.raw_unicode_escape_decode\n\nclass IncrementalEncoder(codecs.IncrementalEncoder):\n    def encode(self, input, final=False):\n        return codecs.raw_unicode_escape_encode(input, self.errors)[0]\n\nclass IncrementalDecoder(codecs.IncrementalDecoder):\n    def decode(self, input, final=False):\n        return codecs.raw_unicode_escape_decode(input, self.errors)[0]\n\nclass StreamWriter(Codec,codecs.StreamWriter):\n    pass\n\nclass StreamReader(Codec,codecs.StreamReader):\n    pass\n\n### encodings module API\n\ndef getregentry():\n    return codecs.CodecInfo(\n        name='raw-unicode-escape',\n        encode=Codec.encode,\n        decode=Codec.decode,\n        incrementalencoder=IncrementalEncoder,\n        incrementaldecoder=IncrementalDecoder,\n        streamwriter=StreamWriter,\n        streamreader=StreamReader,\n    )\n", 
    "encodings.string_escape": "# -*- coding: utf-8 -*-\n\"\"\" Python 'escape' Codec\n\n\nWritten by Martin v. L\u00f6wis (martin@v.loewis.de).\n\n\"\"\"\nimport codecs\n\nclass Codec(codecs.Codec):\n\n    encode = codecs.escape_encode\n    decode = codecs.escape_decode\n\nclass IncrementalEncoder(codecs.IncrementalEncoder):\n    def encode(self, input, final=False):\n        return codecs.escape_encode(input, self.errors)[0]\n\nclass IncrementalDecoder(codecs.IncrementalDecoder):\n    def decode(self, input, final=False):\n        return codecs.escape_decode(input, self.errors)[0]\n\nclass StreamWriter(Codec,codecs.StreamWriter):\n    pass\n\nclass StreamReader(Codec,codecs.StreamReader):\n    pass\n\ndef getregentry():\n    return codecs.CodecInfo(\n        name='string-escape',\n        encode=Codec.encode,\n        decode=Codec.decode,\n        incrementalencoder=IncrementalEncoder,\n        incrementaldecoder=IncrementalDecoder,\n        streamwriter=StreamWriter,\n        streamreader=StreamReader,\n    )\n", 
    "encodings.unicode_escape": "\"\"\" Python 'unicode-escape' Codec\n\n\nWritten by Marc-Andre Lemburg (mal@lemburg.com).\n\n(c) Copyright CNRI, All Rights Reserved. NO WARRANTY.\n\n\"\"\"\nimport codecs\n\n### Codec APIs\n\nclass Codec(codecs.Codec):\n\n    # Note: Binding these as C functions will result in the class not\n    # converting them to methods. This is intended.\n    encode = codecs.unicode_escape_encode\n    decode = codecs.unicode_escape_decode\n\nclass IncrementalEncoder(codecs.IncrementalEncoder):\n    def encode(self, input, final=False):\n        return codecs.unicode_escape_encode(input, self.errors)[0]\n\nclass IncrementalDecoder(codecs.IncrementalDecoder):\n    def decode(self, input, final=False):\n        return codecs.unicode_escape_decode(input, self.errors)[0]\n\nclass StreamWriter(Codec,codecs.StreamWriter):\n    pass\n\nclass StreamReader(Codec,codecs.StreamReader):\n    pass\n\n### encodings module API\n\ndef getregentry():\n    return codecs.CodecInfo(\n        name='unicode-escape',\n        encode=Codec.encode,\n        decode=Codec.decode,\n        incrementalencoder=IncrementalEncoder,\n        incrementaldecoder=IncrementalDecoder,\n        streamwriter=StreamWriter,\n        streamreader=StreamReader,\n    )\n", 
    "encodings.unicode_internal": "\"\"\" Python 'unicode-internal' Codec\n\n\nWritten by Marc-Andre Lemburg (mal@lemburg.com).\n\n(c) Copyright CNRI, All Rights Reserved. NO WARRANTY.\n\n\"\"\"\nimport codecs\n\n### Codec APIs\n\nclass Codec(codecs.Codec):\n\n    # Note: Binding these as C functions will result in the class not\n    # converting them to methods. This is intended.\n    encode = codecs.unicode_internal_encode\n    decode = codecs.unicode_internal_decode\n\nclass IncrementalEncoder(codecs.IncrementalEncoder):\n    def encode(self, input, final=False):\n        return codecs.unicode_internal_encode(input, self.errors)[0]\n\nclass IncrementalDecoder(codecs.IncrementalDecoder):\n    def decode(self, input, final=False):\n        return codecs.unicode_internal_decode(input, self.errors)[0]\n\nclass StreamWriter(Codec,codecs.StreamWriter):\n    pass\n\nclass StreamReader(Codec,codecs.StreamReader):\n    pass\n\n### encodings module API\n\ndef getregentry():\n    return codecs.CodecInfo(\n        name='unicode-internal',\n        encode=Codec.encode,\n        decode=Codec.decode,\n        incrementalencoder=IncrementalEncoder,\n        incrementaldecoder=IncrementalDecoder,\n        streamwriter=StreamWriter,\n        streamreader=StreamReader,\n    )\n", 
    "encodings.utf_16": "\"\"\" Python 'utf-16' Codec\n\n\nWritten by Marc-Andre Lemburg (mal@lemburg.com).\n\n(c) Copyright CNRI, All Rights Reserved. NO WARRANTY.\n\n\"\"\"\nimport codecs, sys\n\n### Codec APIs\n\nencode = codecs.utf_16_encode\n\ndef decode(input, errors='strict'):\n    return codecs.utf_16_decode(input, errors, True)\n\nclass IncrementalEncoder(codecs.IncrementalEncoder):\n    def __init__(self, errors='strict'):\n        codecs.IncrementalEncoder.__init__(self, errors)\n        self.encoder = None\n\n    def encode(self, input, final=False):\n        if self.encoder is None:\n            result = codecs.utf_16_encode(input, self.errors)[0]\n            if sys.byteorder == 'little':\n                self.encoder = codecs.utf_16_le_encode\n            else:\n                self.encoder = codecs.utf_16_be_encode\n            return result\n        return self.encoder(input, self.errors)[0]\n\n    def reset(self):\n        codecs.IncrementalEncoder.reset(self)\n        self.encoder = None\n\n    def getstate(self):\n        # state info we return to the caller:\n        # 0: stream is in natural order for this platform\n        # 2: endianness hasn't been determined yet\n        # (we're never writing in unnatural order)\n        return (2 if self.encoder is None else 0)\n\n    def setstate(self, state):\n        if state:\n            self.encoder = None\n        else:\n            if sys.byteorder == 'little':\n                self.encoder = codecs.utf_16_le_encode\n            else:\n                self.encoder = codecs.utf_16_be_encode\n\nclass IncrementalDecoder(codecs.BufferedIncrementalDecoder):\n    def __init__(self, errors='strict'):\n        codecs.BufferedIncrementalDecoder.__init__(self, errors)\n        self.decoder = None\n\n    def _buffer_decode(self, input, errors, final):\n        if self.decoder is None:\n            (output, consumed, byteorder) = \\\n                codecs.utf_16_ex_decode(input, errors, 0, final)\n            if byteorder == -1:\n                self.decoder = codecs.utf_16_le_decode\n            elif byteorder == 1:\n                self.decoder = codecs.utf_16_be_decode\n            elif consumed >= 2:\n                raise UnicodeError(\"UTF-16 stream does not start with BOM\")\n            return (output, consumed)\n        return self.decoder(input, self.errors, final)\n\n    def reset(self):\n        codecs.BufferedIncrementalDecoder.reset(self)\n        self.decoder = None\n\nclass StreamWriter(codecs.StreamWriter):\n    def __init__(self, stream, errors='strict'):\n        codecs.StreamWriter.__init__(self, stream, errors)\n        self.encoder = None\n\n    def reset(self):\n        codecs.StreamWriter.reset(self)\n        self.encoder = None\n\n    def encode(self, input, errors='strict'):\n        if self.encoder is None:\n            result = codecs.utf_16_encode(input, errors)\n            if sys.byteorder == 'little':\n                self.encoder = codecs.utf_16_le_encode\n            else:\n                self.encoder = codecs.utf_16_be_encode\n            return result\n        else:\n            return self.encoder(input, errors)\n\nclass StreamReader(codecs.StreamReader):\n\n    def reset(self):\n        codecs.StreamReader.reset(self)\n        try:\n            del self.decode\n        except AttributeError:\n            pass\n\n    def decode(self, input, errors='strict'):\n        (object, consumed, byteorder) = \\\n            codecs.utf_16_ex_decode(input, errors, 0, False)\n        if byteorder == -1:\n            self.decode = codecs.utf_16_le_decode\n        elif byteorder == 1:\n            self.decode = codecs.utf_16_be_decode\n        elif consumed>=2:\n            raise UnicodeError,\"UTF-16 stream does not start with BOM\"\n        return (object, consumed)\n\n### encodings module API\n\ndef getregentry():\n    return codecs.CodecInfo(\n        name='utf-16',\n        encode=encode,\n        decode=decode,\n        incrementalencoder=IncrementalEncoder,\n        incrementaldecoder=IncrementalDecoder,\n        streamreader=StreamReader,\n        streamwriter=StreamWriter,\n    )\n", 
    "encodings.utf_8": "\"\"\" Python 'utf-8' Codec\n\n\nWritten by Marc-Andre Lemburg (mal@lemburg.com).\n\n(c) Copyright CNRI, All Rights Reserved. NO WARRANTY.\n\n\"\"\"\nimport codecs\n\n### Codec APIs\n\nencode = codecs.utf_8_encode\n\ndef decode(input, errors='strict'):\n    return codecs.utf_8_decode(input, errors, True)\n\nclass IncrementalEncoder(codecs.IncrementalEncoder):\n    def encode(self, input, final=False):\n        return codecs.utf_8_encode(input, self.errors)[0]\n\nclass IncrementalDecoder(codecs.BufferedIncrementalDecoder):\n    _buffer_decode = codecs.utf_8_decode\n\nclass StreamWriter(codecs.StreamWriter):\n    encode = codecs.utf_8_encode\n\nclass StreamReader(codecs.StreamReader):\n    decode = codecs.utf_8_decode\n\n### encodings module API\n\ndef getregentry():\n    return codecs.CodecInfo(\n        name='utf-8',\n        encode=encode,\n        decode=decode,\n        incrementalencoder=IncrementalEncoder,\n        incrementaldecoder=IncrementalDecoder,\n        streamreader=StreamReader,\n        streamwriter=StreamWriter,\n    )\n", 
    "fnmatch": "\"\"\"Filename matching with shell patterns.\n\nfnmatch(FILENAME, PATTERN) matches according to the local convention.\nfnmatchcase(FILENAME, PATTERN) always takes case in account.\n\nThe functions operate by translating the pattern into a regular\nexpression.  They cache the compiled regular expressions for speed.\n\nThe function translate(PATTERN) returns a regular expression\ncorresponding to PATTERN.  (It does not compile it.)\n\"\"\"\n\nimport re\n\n__all__ = [\"filter\", \"fnmatch\", \"fnmatchcase\", \"translate\"]\n\n_cache = {}\n_MAXCACHE = 100\n\ndef _purge():\n    \"\"\"Clear the pattern cache\"\"\"\n    _cache.clear()\n\ndef fnmatch(name, pat):\n    \"\"\"Test whether FILENAME matches PATTERN.\n\n    Patterns are Unix shell style:\n\n    *       matches everything\n    ?       matches any single character\n    [seq]   matches any character in seq\n    [!seq]  matches any char not in seq\n\n    An initial period in FILENAME is not special.\n    Both FILENAME and PATTERN are first case-normalized\n    if the operating system requires it.\n    If you don't want this, use fnmatchcase(FILENAME, PATTERN).\n    \"\"\"\n\n    import os\n    name = os.path.normcase(name)\n    pat = os.path.normcase(pat)\n    return fnmatchcase(name, pat)\n\ndef filter(names, pat):\n    \"\"\"Return the subset of the list NAMES that match PAT\"\"\"\n    import os,posixpath\n    result=[]\n    pat=os.path.normcase(pat)\n    if not pat in _cache:\n        res = translate(pat)\n        if len(_cache) >= _MAXCACHE:\n            _cache.clear()\n        _cache[pat] = re.compile(res)\n    match=_cache[pat].match\n    if os.path is posixpath:\n        # normcase on posix is NOP. Optimize it away from the loop.\n        for name in names:\n            if match(name):\n                result.append(name)\n    else:\n        for name in names:\n            if match(os.path.normcase(name)):\n                result.append(name)\n    return result\n\ndef fnmatchcase(name, pat):\n    \"\"\"Test whether FILENAME matches PATTERN, including case.\n\n    This is a version of fnmatch() which doesn't case-normalize\n    its arguments.\n    \"\"\"\n\n    if not pat in _cache:\n        res = translate(pat)\n        if len(_cache) >= _MAXCACHE:\n            _cache.clear()\n        _cache[pat] = re.compile(res)\n    return _cache[pat].match(name) is not None\n\ndef translate(pat):\n    \"\"\"Translate a shell PATTERN to a regular expression.\n\n    There is no way to quote meta-characters.\n    \"\"\"\n\n    i, n = 0, len(pat)\n    res = ''\n    while i < n:\n        c = pat[i]\n        i = i+1\n        if c == '*':\n            res = res + '.*'\n        elif c == '?':\n            res = res + '.'\n        elif c == '[':\n            j = i\n            if j < n and pat[j] == '!':\n                j = j+1\n            if j < n and pat[j] == ']':\n                j = j+1\n            while j < n and pat[j] != ']':\n                j = j+1\n            if j >= n:\n                res = res + '\\\\['\n            else:\n                stuff = pat[i:j].replace('\\\\','\\\\\\\\')\n                i = j+1\n                if stuff[0] == '!':\n                    stuff = '^' + stuff[1:]\n                elif stuff[0] == '^':\n                    stuff = '\\\\' + stuff\n                res = '%s[%s]' % (res, stuff)\n        else:\n            res = res + re.escape(c)\n    return res + '\\Z(?ms)'\n", 
    "functools": "\"\"\"functools.py - Tools for working with functions and callable objects\n\"\"\"\n# Python module wrapper for _functools C module\n# to allow utilities written in Python to be added\n# to the functools module.\n# Written by Nick Coghlan <ncoghlan at gmail.com>\n#   Copyright (C) 2006 Python Software Foundation.\n# See C source code for _functools credits/copyright\n\nfrom _functools import partial, reduce\n\n# update_wrapper() and wraps() are tools to help write\n# wrapper functions that can handle naive introspection\n\nWRAPPER_ASSIGNMENTS = ('__module__', '__name__', '__doc__')\nWRAPPER_UPDATES = ('__dict__',)\ndef update_wrapper(wrapper,\n                   wrapped,\n                   assigned = WRAPPER_ASSIGNMENTS,\n                   updated = WRAPPER_UPDATES):\n    \"\"\"Update a wrapper function to look like the wrapped function\n\n       wrapper is the function to be updated\n       wrapped is the original function\n       assigned is a tuple naming the attributes assigned directly\n       from the wrapped function to the wrapper function (defaults to\n       functools.WRAPPER_ASSIGNMENTS)\n       updated is a tuple naming the attributes of the wrapper that\n       are updated with the corresponding attribute from the wrapped\n       function (defaults to functools.WRAPPER_UPDATES)\n    \"\"\"\n    for attr in assigned:\n        setattr(wrapper, attr, getattr(wrapped, attr))\n    for attr in updated:\n        getattr(wrapper, attr).update(getattr(wrapped, attr, {}))\n    # Return the wrapper so this can be used as a decorator via partial()\n    return wrapper\n\ndef wraps(wrapped,\n          assigned = WRAPPER_ASSIGNMENTS,\n          updated = WRAPPER_UPDATES):\n    \"\"\"Decorator factory to apply update_wrapper() to a wrapper function\n\n       Returns a decorator that invokes update_wrapper() with the decorated\n       function as the wrapper argument and the arguments to wraps() as the\n       remaining arguments. Default arguments are as for update_wrapper().\n       This is a convenience function to simplify applying partial() to\n       update_wrapper().\n    \"\"\"\n    return partial(update_wrapper, wrapped=wrapped,\n                   assigned=assigned, updated=updated)\n\ndef total_ordering(cls):\n    \"\"\"Class decorator that fills in missing ordering methods\"\"\"\n    convert = {\n        '__lt__': [('__gt__', lambda self, other: not (self < other or self == other)),\n                   ('__le__', lambda self, other: self < other or self == other),\n                   ('__ge__', lambda self, other: not self < other)],\n        '__le__': [('__ge__', lambda self, other: not self <= other or self == other),\n                   ('__lt__', lambda self, other: self <= other and not self == other),\n                   ('__gt__', lambda self, other: not self <= other)],\n        '__gt__': [('__lt__', lambda self, other: not (self > other or self == other)),\n                   ('__ge__', lambda self, other: self > other or self == other),\n                   ('__le__', lambda self, other: not self > other)],\n        '__ge__': [('__le__', lambda self, other: (not self >= other) or self == other),\n                   ('__gt__', lambda self, other: self >= other and not self == other),\n                   ('__lt__', lambda self, other: not self >= other)]\n    }\n    roots = set(dir(cls)) & set(convert)\n    if not roots:\n        raise ValueError('must define at least one ordering operation: < > <= >=')\n    root = max(roots)       # prefer __lt__ to __le__ to __gt__ to __ge__\n    for opname, opfunc in convert[root]:\n        if opname not in roots:\n            opfunc.__name__ = opname\n            opfunc.__doc__ = getattr(int, opname).__doc__\n            setattr(cls, opname, opfunc)\n    return cls\n\ndef cmp_to_key(mycmp):\n    \"\"\"Convert a cmp= function into a key= function\"\"\"\n    class K(object):\n        __slots__ = ['obj']\n        def __init__(self, obj, *args):\n            self.obj = obj\n        def __lt__(self, other):\n            return mycmp(self.obj, other.obj) < 0\n        def __gt__(self, other):\n            return mycmp(self.obj, other.obj) > 0\n        def __eq__(self, other):\n            return mycmp(self.obj, other.obj) == 0\n        def __le__(self, other):\n            return mycmp(self.obj, other.obj) <= 0\n        def __ge__(self, other):\n            return mycmp(self.obj, other.obj) >= 0\n        def __ne__(self, other):\n            return mycmp(self.obj, other.obj) != 0\n        def __hash__(self):\n            raise TypeError('hash not implemented')\n    return K\n", 
    "genericpath": "\"\"\"\nPath operations common to more than one OS\nDo not use directly.  The OS specific modules import the appropriate\nfunctions from this module themselves.\n\"\"\"\nimport os\nimport stat\n\n__all__ = ['commonprefix', 'exists', 'getatime', 'getctime', 'getmtime',\n           'getsize', 'isdir', 'isfile']\n\n\n# Does a path exist?\n# This is false for dangling symbolic links on systems that support them.\ndef exists(path):\n    \"\"\"Test whether a path exists.  Returns False for broken symbolic links\"\"\"\n    try:\n        os.stat(path)\n    except os.error:\n        return False\n    return True\n\n\n# This follows symbolic links, so both islink() and isdir() can be true\n# for the same path on systems that support symlinks\ndef isfile(path):\n    \"\"\"Test whether a path is a regular file\"\"\"\n    try:\n        st = os.stat(path)\n    except os.error:\n        return False\n    return stat.S_ISREG(st.st_mode)\n\n\n# Is a path a directory?\n# This follows symbolic links, so both islink() and isdir()\n# can be true for the same path on systems that support symlinks\ndef isdir(s):\n    \"\"\"Return true if the pathname refers to an existing directory.\"\"\"\n    try:\n        st = os.stat(s)\n    except os.error:\n        return False\n    return stat.S_ISDIR(st.st_mode)\n\n\ndef getsize(filename):\n    \"\"\"Return the size of a file, reported by os.stat().\"\"\"\n    return os.stat(filename).st_size\n\n\ndef getmtime(filename):\n    \"\"\"Return the last modification time of a file, reported by os.stat().\"\"\"\n    return os.stat(filename).st_mtime\n\n\ndef getatime(filename):\n    \"\"\"Return the last access time of a file, reported by os.stat().\"\"\"\n    return os.stat(filename).st_atime\n\n\ndef getctime(filename):\n    \"\"\"Return the metadata change time of a file, reported by os.stat().\"\"\"\n    return os.stat(filename).st_ctime\n\n\n# Return the longest prefix of all list elements.\ndef commonprefix(m):\n    \"Given a list of pathnames, returns the longest common leading component\"\n    if not m: return ''\n    s1 = min(m)\n    s2 = max(m)\n    for i, c in enumerate(s1):\n        if c != s2[i]:\n            return s1[:i]\n    return s1\n\n# Split a path in root and extension.\n# The extension is everything starting at the last dot in the last\n# pathname component; the root is everything before that.\n# It is always true that root + ext == p.\n\n# Generic implementation of splitext, to be parametrized with\n# the separators\ndef _splitext(p, sep, altsep, extsep):\n    \"\"\"Split the extension from a pathname.\n\n    Extension is everything from the last dot to the end, ignoring\n    leading dots.  Returns \"(root, ext)\"; ext may be empty.\"\"\"\n\n    sepIndex = p.rfind(sep)\n    if altsep:\n        altsepIndex = p.rfind(altsep)\n        sepIndex = max(sepIndex, altsepIndex)\n\n    dotIndex = p.rfind(extsep)\n    if dotIndex > sepIndex:\n        # skip all leading dots\n        filenameIndex = sepIndex + 1\n        while filenameIndex < dotIndex:\n            if p[filenameIndex] != extsep:\n                return p[:dotIndex], p[dotIndex:]\n            filenameIndex += 1\n\n    return p, ''\n", 
    "getopt": "\"\"\"Parser for command line options.\n\nThis module helps scripts to parse the command line arguments in\nsys.argv.  It supports the same conventions as the Unix getopt()\nfunction (including the special meanings of arguments of the form `-'\nand `--').  Long options similar to those supported by GNU software\nmay be used as well via an optional third argument.  This module\nprovides two functions and an exception:\n\ngetopt() -- Parse command line options\ngnu_getopt() -- Like getopt(), but allow option and non-option arguments\nto be intermixed.\nGetoptError -- exception (class) raised with 'opt' attribute, which is the\noption involved with the exception.\n\"\"\"\n\n# Long option support added by Lars Wirzenius <liw@iki.fi>.\n#\n# Gerrit Holl <gerrit@nl.linux.org> moved the string-based exceptions\n# to class-based exceptions.\n#\n# Peter Astrand <astrand@lysator.liu.se> added gnu_getopt().\n#\n# TODO for gnu_getopt():\n#\n# - GNU getopt_long_only mechanism\n# - allow the caller to specify ordering\n# - RETURN_IN_ORDER option\n# - GNU extension with '-' as first character of option string\n# - optional arguments, specified by double colons\n# - a option string with a W followed by semicolon should\n#   treat \"-W foo\" as \"--foo\"\n\n__all__ = [\"GetoptError\",\"error\",\"getopt\",\"gnu_getopt\"]\n\nimport os\n\nclass GetoptError(Exception):\n    opt = ''\n    msg = ''\n    def __init__(self, msg, opt=''):\n        self.msg = msg\n        self.opt = opt\n        Exception.__init__(self, msg, opt)\n\n    def __str__(self):\n        return self.msg\n\nerror = GetoptError # backward compatibility\n\ndef getopt(args, shortopts, longopts = []):\n    \"\"\"getopt(args, options[, long_options]) -> opts, args\n\n    Parses command line options and parameter list.  args is the\n    argument list to be parsed, without the leading reference to the\n    running program.  Typically, this means \"sys.argv[1:]\".  shortopts\n    is the string of option letters that the script wants to\n    recognize, with options that require an argument followed by a\n    colon (i.e., the same format that Unix getopt() uses).  If\n    specified, longopts is a list of strings with the names of the\n    long options which should be supported.  The leading '--'\n    characters should not be included in the option name.  Options\n    which require an argument should be followed by an equal sign\n    ('=').\n\n    The return value consists of two elements: the first is a list of\n    (option, value) pairs; the second is the list of program arguments\n    left after the option list was stripped (this is a trailing slice\n    of the first argument).  Each option-and-value pair returned has\n    the option as its first element, prefixed with a hyphen (e.g.,\n    '-x'), and the option argument as its second element, or an empty\n    string if the option has no argument.  The options occur in the\n    list in the same order in which they were found, thus allowing\n    multiple occurrences.  Long and short options may be mixed.\n\n    \"\"\"\n\n    opts = []\n    if type(longopts) == type(\"\"):\n        longopts = [longopts]\n    else:\n        longopts = list(longopts)\n    while args and args[0].startswith('-') and args[0] != '-':\n        if args[0] == '--':\n            args = args[1:]\n            break\n        if args[0].startswith('--'):\n            opts, args = do_longs(opts, args[0][2:], longopts, args[1:])\n        else:\n            opts, args = do_shorts(opts, args[0][1:], shortopts, args[1:])\n\n    return opts, args\n\ndef gnu_getopt(args, shortopts, longopts = []):\n    \"\"\"getopt(args, options[, long_options]) -> opts, args\n\n    This function works like getopt(), except that GNU style scanning\n    mode is used by default. This means that option and non-option\n    arguments may be intermixed. The getopt() function stops\n    processing options as soon as a non-option argument is\n    encountered.\n\n    If the first character of the option string is `+', or if the\n    environment variable POSIXLY_CORRECT is set, then option\n    processing stops as soon as a non-option argument is encountered.\n\n    \"\"\"\n\n    opts = []\n    prog_args = []\n    if isinstance(longopts, str):\n        longopts = [longopts]\n    else:\n        longopts = list(longopts)\n\n    # Allow options after non-option arguments?\n    if shortopts.startswith('+'):\n        shortopts = shortopts[1:]\n        all_options_first = True\n    elif os.environ.get(\"POSIXLY_CORRECT\"):\n        all_options_first = True\n    else:\n        all_options_first = False\n\n    while args:\n        if args[0] == '--':\n            prog_args += args[1:]\n            break\n\n        if args[0][:2] == '--':\n            opts, args = do_longs(opts, args[0][2:], longopts, args[1:])\n        elif args[0][:1] == '-' and args[0] != '-':\n            opts, args = do_shorts(opts, args[0][1:], shortopts, args[1:])\n        else:\n            if all_options_first:\n                prog_args += args\n                break\n            else:\n                prog_args.append(args[0])\n                args = args[1:]\n\n    return opts, prog_args\n\ndef do_longs(opts, opt, longopts, args):\n    try:\n        i = opt.index('=')\n    except ValueError:\n        optarg = None\n    else:\n        opt, optarg = opt[:i], opt[i+1:]\n\n    has_arg, opt = long_has_args(opt, longopts)\n    if has_arg:\n        if optarg is None:\n            if not args:\n                raise GetoptError('option --%s requires argument' % opt, opt)\n            optarg, args = args[0], args[1:]\n    elif optarg is not None:\n        raise GetoptError('option --%s must not have an argument' % opt, opt)\n    opts.append(('--' + opt, optarg or ''))\n    return opts, args\n\n# Return:\n#   has_arg?\n#   full option name\ndef long_has_args(opt, longopts):\n    possibilities = [o for o in longopts if o.startswith(opt)]\n    if not possibilities:\n        raise GetoptError('option --%s not recognized' % opt, opt)\n    # Is there an exact match?\n    if opt in possibilities:\n        return False, opt\n    elif opt + '=' in possibilities:\n        return True, opt\n    # No exact match, so better be unique.\n    if len(possibilities) > 1:\n        # XXX since possibilities contains all valid continuations, might be\n        # nice to work them into the error msg\n        raise GetoptError('option --%s not a unique prefix' % opt, opt)\n    assert len(possibilities) == 1\n    unique_match = possibilities[0]\n    has_arg = unique_match.endswith('=')\n    if has_arg:\n        unique_match = unique_match[:-1]\n    return has_arg, unique_match\n\ndef do_shorts(opts, optstring, shortopts, args):\n    while optstring != '':\n        opt, optstring = optstring[0], optstring[1:]\n        if short_has_arg(opt, shortopts):\n            if optstring == '':\n                if not args:\n                    raise GetoptError('option -%s requires argument' % opt,\n                                      opt)\n                optstring, args = args[0], args[1:]\n            optarg, optstring = optstring, ''\n        else:\n            optarg = ''\n        opts.append(('-' + opt, optarg))\n    return opts, args\n\ndef short_has_arg(opt, shortopts):\n    for i in range(len(shortopts)):\n        if opt == shortopts[i] != ':':\n            return shortopts.startswith(':', i+1)\n    raise GetoptError('option -%s not recognized' % opt, opt)\n\nif __name__ == '__main__':\n    import sys\n    print getopt(sys.argv[1:], \"a:b\", [\"alpha=\", \"beta\"])\n", 
    "gettext": "\"\"\"Internationalization and localization support.\n\nThis module provides internationalization (I18N) and localization (L10N)\nsupport for your Python programs by providing an interface to the GNU gettext\nmessage catalog library.\n\nI18N refers to the operation by which a program is made aware of multiple\nlanguages.  L10N refers to the adaptation of your program, once\ninternationalized, to the local language and cultural habits.\n\n\"\"\"\n\n# This module represents the integration of work, contributions, feedback, and\n# suggestions from the following people:\n#\n# Martin von Loewis, who wrote the initial implementation of the underlying\n# C-based libintlmodule (later renamed _gettext), along with a skeletal\n# gettext.py implementation.\n#\n# Peter Funk, who wrote fintl.py, a fairly complete wrapper around intlmodule,\n# which also included a pure-Python implementation to read .mo files if\n# intlmodule wasn't available.\n#\n# James Henstridge, who also wrote a gettext.py module, which has some\n# interesting, but currently unsupported experimental features: the notion of\n# a Catalog class and instances, and the ability to add to a catalog file via\n# a Python API.\n#\n# Barry Warsaw integrated these modules, wrote the .install() API and code,\n# and conformed all C and Python code to Python's coding standards.\n#\n# Francois Pinard and Marc-Andre Lemburg also contributed valuably to this\n# module.\n#\n# J. David Ibanez implemented plural forms. Bruno Haible fixed some bugs.\n#\n# TODO:\n# - Lazy loading of .mo files.  Currently the entire catalog is loaded into\n#   memory, but that's probably bad for large translated programs.  Instead,\n#   the lexical sort of original strings in GNU .mo files should be exploited\n#   to do binary searches and lazy initializations.  Or you might want to use\n#   the undocumented double-hash algorithm for .mo files with hash tables, but\n#   you'll need to study the GNU gettext code to do this.\n#\n# - Support Solaris .mo file formats.  Unfortunately, we've been unable to\n#   find this format documented anywhere.\n\n\nimport locale, copy, os, re, struct, sys\nfrom errno import ENOENT\n\n\n__all__ = ['NullTranslations', 'GNUTranslations', 'Catalog',\n           'find', 'translation', 'install', 'textdomain', 'bindtextdomain',\n           'dgettext', 'dngettext', 'gettext', 'ngettext',\n           ]\n\n_default_localedir = os.path.join(sys.prefix, 'share', 'locale')\n\n\ndef test(condition, true, false):\n    \"\"\"\n    Implements the C expression:\n\n      condition ? true : false\n\n    Required to correctly interpret plural forms.\n    \"\"\"\n    if condition:\n        return true\n    else:\n        return false\n\n\ndef c2py(plural):\n    \"\"\"Gets a C expression as used in PO files for plural forms and returns a\n    Python lambda function that implements an equivalent expression.\n    \"\"\"\n    # Security check, allow only the \"n\" identifier\n    try:\n        from cStringIO import StringIO\n    except ImportError:\n        from StringIO import StringIO\n    import token, tokenize\n    tokens = tokenize.generate_tokens(StringIO(plural).readline)\n    try:\n        danger = [x for x in tokens if x[0] == token.NAME and x[1] != 'n']\n    except tokenize.TokenError:\n        raise ValueError, \\\n              'plural forms expression error, maybe unbalanced parenthesis'\n    else:\n        if danger:\n            raise ValueError, 'plural forms expression could be dangerous'\n\n    # Replace some C operators by their Python equivalents\n    plural = plural.replace('&&', ' and ')\n    plural = plural.replace('||', ' or ')\n\n    expr = re.compile(r'\\!([^=])')\n    plural = expr.sub(' not \\\\1', plural)\n\n    # Regular expression and replacement function used to transform\n    # \"a?b:c\" to \"test(a,b,c)\".\n    expr = re.compile(r'(.*?)\\?(.*?):(.*)')\n    def repl(x):\n        return \"test(%s, %s, %s)\" % (x.group(1), x.group(2),\n                                     expr.sub(repl, x.group(3)))\n\n    # Code to transform the plural expression, taking care of parentheses\n    stack = ['']\n    for c in plural:\n        if c == '(':\n            stack.append('')\n        elif c == ')':\n            if len(stack) == 1:\n                # Actually, we never reach this code, because unbalanced\n                # parentheses get caught in the security check at the\n                # beginning.\n                raise ValueError, 'unbalanced parenthesis in plural form'\n            s = expr.sub(repl, stack.pop())\n            stack[-1] += '(%s)' % s\n        else:\n            stack[-1] += c\n    plural = expr.sub(repl, stack.pop())\n\n    return eval('lambda n: int(%s)' % plural)\n\n\n\ndef _expand_lang(locale):\n    from locale import normalize\n    locale = normalize(locale)\n    COMPONENT_CODESET   = 1 << 0\n    COMPONENT_TERRITORY = 1 << 1\n    COMPONENT_MODIFIER  = 1 << 2\n    # split up the locale into its base components\n    mask = 0\n    pos = locale.find('@')\n    if pos >= 0:\n        modifier = locale[pos:]\n        locale = locale[:pos]\n        mask |= COMPONENT_MODIFIER\n    else:\n        modifier = ''\n    pos = locale.find('.')\n    if pos >= 0:\n        codeset = locale[pos:]\n        locale = locale[:pos]\n        mask |= COMPONENT_CODESET\n    else:\n        codeset = ''\n    pos = locale.find('_')\n    if pos >= 0:\n        territory = locale[pos:]\n        locale = locale[:pos]\n        mask |= COMPONENT_TERRITORY\n    else:\n        territory = ''\n    language = locale\n    ret = []\n    for i in range(mask+1):\n        if not (i & ~mask):  # if all components for this combo exist ...\n            val = language\n            if i & COMPONENT_TERRITORY: val += territory\n            if i & COMPONENT_CODESET:   val += codeset\n            if i & COMPONENT_MODIFIER:  val += modifier\n            ret.append(val)\n    ret.reverse()\n    return ret\n\n\n\nclass NullTranslations:\n    def __init__(self, fp=None):\n        self._info = {}\n        self._charset = None\n        self._output_charset = None\n        self._fallback = None\n        if fp is not None:\n            self._parse(fp)\n\n    def _parse(self, fp):\n        pass\n\n    def add_fallback(self, fallback):\n        if self._fallback:\n            self._fallback.add_fallback(fallback)\n        else:\n            self._fallback = fallback\n\n    def gettext(self, message):\n        if self._fallback:\n            return self._fallback.gettext(message)\n        return message\n\n    def lgettext(self, message):\n        if self._fallback:\n            return self._fallback.lgettext(message)\n        return message\n\n    def ngettext(self, msgid1, msgid2, n):\n        if self._fallback:\n            return self._fallback.ngettext(msgid1, msgid2, n)\n        if n == 1:\n            return msgid1\n        else:\n            return msgid2\n\n    def lngettext(self, msgid1, msgid2, n):\n        if self._fallback:\n            return self._fallback.lngettext(msgid1, msgid2, n)\n        if n == 1:\n            return msgid1\n        else:\n            return msgid2\n\n    def ugettext(self, message):\n        if self._fallback:\n            return self._fallback.ugettext(message)\n        return unicode(message)\n\n    def ungettext(self, msgid1, msgid2, n):\n        if self._fallback:\n            return self._fallback.ungettext(msgid1, msgid2, n)\n        if n == 1:\n            return unicode(msgid1)\n        else:\n            return unicode(msgid2)\n\n    def info(self):\n        return self._info\n\n    def charset(self):\n        return self._charset\n\n    def output_charset(self):\n        return self._output_charset\n\n    def set_output_charset(self, charset):\n        self._output_charset = charset\n\n    def install(self, unicode=False, names=None):\n        import __builtin__\n        __builtin__.__dict__['_'] = unicode and self.ugettext or self.gettext\n        if hasattr(names, \"__contains__\"):\n            if \"gettext\" in names:\n                __builtin__.__dict__['gettext'] = __builtin__.__dict__['_']\n            if \"ngettext\" in names:\n                __builtin__.__dict__['ngettext'] = (unicode and self.ungettext\n                                                             or self.ngettext)\n            if \"lgettext\" in names:\n                __builtin__.__dict__['lgettext'] = self.lgettext\n            if \"lngettext\" in names:\n                __builtin__.__dict__['lngettext'] = self.lngettext\n\n\nclass GNUTranslations(NullTranslations):\n    # Magic number of .mo files\n    LE_MAGIC = 0x950412deL\n    BE_MAGIC = 0xde120495L\n\n    def _parse(self, fp):\n        \"\"\"Override this method to support alternative .mo formats.\"\"\"\n        unpack = struct.unpack\n        filename = getattr(fp, 'name', '')\n        # Parse the .mo file header, which consists of 5 little endian 32\n        # bit words.\n        self._catalog = catalog = {}\n        self.plural = lambda n: int(n != 1) # germanic plural by default\n        buf = fp.read()\n        buflen = len(buf)\n        # Are we big endian or little endian?\n        magic = unpack('<I', buf[:4])[0]\n        if magic == self.LE_MAGIC:\n            version, msgcount, masteridx, transidx = unpack('<4I', buf[4:20])\n            ii = '<II'\n        elif magic == self.BE_MAGIC:\n            version, msgcount, masteridx, transidx = unpack('>4I', buf[4:20])\n            ii = '>II'\n        else:\n            raise IOError(0, 'Bad magic number', filename)\n        # Now put all messages from the .mo file buffer into the catalog\n        # dictionary.\n        for i in xrange(0, msgcount):\n            mlen, moff = unpack(ii, buf[masteridx:masteridx+8])\n            mend = moff + mlen\n            tlen, toff = unpack(ii, buf[transidx:transidx+8])\n            tend = toff + tlen\n            if mend < buflen and tend < buflen:\n                msg = buf[moff:mend]\n                tmsg = buf[toff:tend]\n            else:\n                raise IOError(0, 'File is corrupt', filename)\n            # See if we're looking at GNU .mo conventions for metadata\n            if mlen == 0:\n                # Catalog description\n                lastk = k = None\n                for item in tmsg.splitlines():\n                    item = item.strip()\n                    if not item:\n                        continue\n                    if ':' in item:\n                        k, v = item.split(':', 1)\n                        k = k.strip().lower()\n                        v = v.strip()\n                        self._info[k] = v\n                        lastk = k\n                    elif lastk:\n                        self._info[lastk] += '\\n' + item\n                    if k == 'content-type':\n                        self._charset = v.split('charset=')[1]\n                    elif k == 'plural-forms':\n                        v = v.split(';')\n                        plural = v[1].split('plural=')[1]\n                        self.plural = c2py(plural)\n            # Note: we unconditionally convert both msgids and msgstrs to\n            # Unicode using the character encoding specified in the charset\n            # parameter of the Content-Type header.  The gettext documentation\n            # strongly encourages msgids to be us-ascii, but some applications\n            # require alternative encodings (e.g. Zope's ZCML and ZPT).  For\n            # traditional gettext applications, the msgid conversion will\n            # cause no problems since us-ascii should always be a subset of\n            # the charset encoding.  We may want to fall back to 8-bit msgids\n            # if the Unicode conversion fails.\n            if '\\x00' in msg:\n                # Plural forms\n                msgid1, msgid2 = msg.split('\\x00')\n                tmsg = tmsg.split('\\x00')\n                if self._charset:\n                    msgid1 = unicode(msgid1, self._charset)\n                    tmsg = [unicode(x, self._charset) for x in tmsg]\n                for i in range(len(tmsg)):\n                    catalog[(msgid1, i)] = tmsg[i]\n            else:\n                if self._charset:\n                    msg = unicode(msg, self._charset)\n                    tmsg = unicode(tmsg, self._charset)\n                catalog[msg] = tmsg\n            # advance to next entry in the seek tables\n            masteridx += 8\n            transidx += 8\n\n    def gettext(self, message):\n        missing = object()\n        tmsg = self._catalog.get(message, missing)\n        if tmsg is missing:\n            if self._fallback:\n                return self._fallback.gettext(message)\n            return message\n        # Encode the Unicode tmsg back to an 8-bit string, if possible\n        if self._output_charset:\n            return tmsg.encode(self._output_charset)\n        elif self._charset:\n            return tmsg.encode(self._charset)\n        return tmsg\n\n    def lgettext(self, message):\n        missing = object()\n        tmsg = self._catalog.get(message, missing)\n        if tmsg is missing:\n            if self._fallback:\n                return self._fallback.lgettext(message)\n            return message\n        if self._output_charset:\n            return tmsg.encode(self._output_charset)\n        return tmsg.encode(locale.getpreferredencoding())\n\n    def ngettext(self, msgid1, msgid2, n):\n        try:\n            tmsg = self._catalog[(msgid1, self.plural(n))]\n            if self._output_charset:\n                return tmsg.encode(self._output_charset)\n            elif self._charset:\n                return tmsg.encode(self._charset)\n            return tmsg\n        except KeyError:\n            if self._fallback:\n                return self._fallback.ngettext(msgid1, msgid2, n)\n            if n == 1:\n                return msgid1\n            else:\n                return msgid2\n\n    def lngettext(self, msgid1, msgid2, n):\n        try:\n            tmsg = self._catalog[(msgid1, self.plural(n))]\n            if self._output_charset:\n                return tmsg.encode(self._output_charset)\n            return tmsg.encode(locale.getpreferredencoding())\n        except KeyError:\n            if self._fallback:\n                return self._fallback.lngettext(msgid1, msgid2, n)\n            if n == 1:\n                return msgid1\n            else:\n                return msgid2\n\n    def ugettext(self, message):\n        missing = object()\n        tmsg = self._catalog.get(message, missing)\n        if tmsg is missing:\n            if self._fallback:\n                return self._fallback.ugettext(message)\n            return unicode(message)\n        return tmsg\n\n    def ungettext(self, msgid1, msgid2, n):\n        try:\n            tmsg = self._catalog[(msgid1, self.plural(n))]\n        except KeyError:\n            if self._fallback:\n                return self._fallback.ungettext(msgid1, msgid2, n)\n            if n == 1:\n                tmsg = unicode(msgid1)\n            else:\n                tmsg = unicode(msgid2)\n        return tmsg\n\n\n# Locate a .mo file using the gettext strategy\ndef find(domain, localedir=None, languages=None, all=0):\n    # Get some reasonable defaults for arguments that were not supplied\n    if localedir is None:\n        localedir = _default_localedir\n    if languages is None:\n        languages = []\n        for envar in ('LANGUAGE', 'LC_ALL', 'LC_MESSAGES', 'LANG'):\n            val = os.environ.get(envar)\n            if val:\n                languages = val.split(':')\n                break\n        if 'C' not in languages:\n            languages.append('C')\n    # now normalize and expand the languages\n    nelangs = []\n    for lang in languages:\n        for nelang in _expand_lang(lang):\n            if nelang not in nelangs:\n                nelangs.append(nelang)\n    # select a language\n    if all:\n        result = []\n    else:\n        result = None\n    for lang in nelangs:\n        if lang == 'C':\n            break\n        mofile = os.path.join(localedir, lang, 'LC_MESSAGES', '%s.mo' % domain)\n        if os.path.exists(mofile):\n            if all:\n                result.append(mofile)\n            else:\n                return mofile\n    return result\n\n\n\n# a mapping between absolute .mo file path and Translation object\n_translations = {}\n\ndef translation(domain, localedir=None, languages=None,\n                class_=None, fallback=False, codeset=None):\n    if class_ is None:\n        class_ = GNUTranslations\n    mofiles = find(domain, localedir, languages, all=1)\n    if not mofiles:\n        if fallback:\n            return NullTranslations()\n        raise IOError(ENOENT, 'No translation file found for domain', domain)\n    # Avoid opening, reading, and parsing the .mo file after it's been done\n    # once.\n    result = None\n    for mofile in mofiles:\n        key = (class_, os.path.abspath(mofile))\n        t = _translations.get(key)\n        if t is None:\n            with open(mofile, 'rb') as fp:\n                t = _translations.setdefault(key, class_(fp))\n        # Copy the translation object to allow setting fallbacks and\n        # output charset. All other instance data is shared with the\n        # cached object.\n        t = copy.copy(t)\n        if codeset:\n            t.set_output_charset(codeset)\n        if result is None:\n            result = t\n        else:\n            result.add_fallback(t)\n    return result\n\n\ndef install(domain, localedir=None, unicode=False, codeset=None, names=None):\n    t = translation(domain, localedir, fallback=True, codeset=codeset)\n    t.install(unicode, names)\n\n\n\n# a mapping b/w domains and locale directories\n_localedirs = {}\n# a mapping b/w domains and codesets\n_localecodesets = {}\n# current global domain, `messages' used for compatibility w/ GNU gettext\n_current_domain = 'messages'\n\n\ndef textdomain(domain=None):\n    global _current_domain\n    if domain is not None:\n        _current_domain = domain\n    return _current_domain\n\n\ndef bindtextdomain(domain, localedir=None):\n    global _localedirs\n    if localedir is not None:\n        _localedirs[domain] = localedir\n    return _localedirs.get(domain, _default_localedir)\n\n\ndef bind_textdomain_codeset(domain, codeset=None):\n    global _localecodesets\n    if codeset is not None:\n        _localecodesets[domain] = codeset\n    return _localecodesets.get(domain)\n\n\ndef dgettext(domain, message):\n    try:\n        t = translation(domain, _localedirs.get(domain, None),\n                        codeset=_localecodesets.get(domain))\n    except IOError:\n        return message\n    return t.gettext(message)\n\ndef ldgettext(domain, message):\n    try:\n        t = translation(domain, _localedirs.get(domain, None),\n                        codeset=_localecodesets.get(domain))\n    except IOError:\n        return message\n    return t.lgettext(message)\n\ndef dngettext(domain, msgid1, msgid2, n):\n    try:\n        t = translation(domain, _localedirs.get(domain, None),\n                        codeset=_localecodesets.get(domain))\n    except IOError:\n        if n == 1:\n            return msgid1\n        else:\n            return msgid2\n    return t.ngettext(msgid1, msgid2, n)\n\ndef ldngettext(domain, msgid1, msgid2, n):\n    try:\n        t = translation(domain, _localedirs.get(domain, None),\n                        codeset=_localecodesets.get(domain))\n    except IOError:\n        if n == 1:\n            return msgid1\n        else:\n            return msgid2\n    return t.lngettext(msgid1, msgid2, n)\n\ndef gettext(message):\n    return dgettext(_current_domain, message)\n\ndef lgettext(message):\n    return ldgettext(_current_domain, message)\n\ndef ngettext(msgid1, msgid2, n):\n    return dngettext(_current_domain, msgid1, msgid2, n)\n\ndef lngettext(msgid1, msgid2, n):\n    return ldngettext(_current_domain, msgid1, msgid2, n)\n\n# dcgettext() has been deemed unnecessary and is not implemented.\n\n# James Henstridge's Catalog constructor from GNOME gettext.  Documented usage\n# was:\n#\n#    import gettext\n#    cat = gettext.Catalog(PACKAGE, localedir=LOCALEDIR)\n#    _ = cat.gettext\n#    print _('Hello World')\n\n# The resulting catalog object currently don't support access through a\n# dictionary API, which was supported (but apparently unused) in GNOME\n# gettext.\n\nCatalog = translation\n", 
    "graphene.__init__": "from .pyutils.version import get_version\n\n\ntry:\n    # This variable is injected in the __builtins__ by the build\n    # process. It used to enable importing subpackages when\n    # the required packages are not installed\n    __SETUP__\nexcept NameError:\n    __SETUP__ = False\n\n\nVERSION = (1, 0, 0, 'final', 0)\n\n__version__ = get_version(VERSION)\n\nif not __SETUP__:\n\n    from .types import (\n        AbstractType,\n        ObjectType,\n        InputObjectType,\n        Interface,\n        Mutation,\n        Field,\n        InputField,\n        Schema,\n        Scalar,\n        String, ID, Int, Float, Boolean,\n        List, NonNull,\n        Enum,\n        Argument,\n        Dynamic,\n        Union,\n    )\n    from .relay import (\n        Node,\n        is_node,\n        GlobalID,\n        ClientIDMutation,\n        Connection,\n        ConnectionField,\n        PageInfo\n    )\n    from .utils.resolve_only_args import resolve_only_args\n\n    __all__ = [\n        'AbstractType',\n        'ObjectType',\n        'InputObjectType',\n        'Interface',\n        'Mutation',\n        'Field',\n        'InputField',\n        'Schema',\n        'Scalar',\n        'String',\n        'ID',\n        'Int',\n        'Float',\n        'Enum',\n        'Boolean',\n        'List',\n        'NonNull',\n        'Argument',\n        'Dynamic',\n        'Union',\n        'resolve_only_args',\n        'Node',\n        'is_node',\n        'GlobalID',\n        'ClientIDMutation',\n        'Connection',\n        'ConnectionField',\n        'PageInfo']\n", 
    "graphene.pyutils.__init__": "", 
    "graphene.pyutils.enum": "\"\"\"Python Enumerations\"\"\"\n\nimport sys as _sys\n\n__all__ = ['Enum', 'IntEnum', 'unique']\n\nversion = 1, 1, 6\n\npyver = float('%s.%s' % _sys.version_info[:2])\n\ntry:\n    any\nexcept NameError:\n    def any(iterable):\n        for element in iterable:\n            if element:\n                return True\n        return False\n\ntry:\n    from collections import OrderedDict\nexcept ImportError:\n    OrderedDict = None\n\ntry:\n    basestring\nexcept NameError:\n    # In Python 2 basestring is the ancestor of both str and unicode\n    # in Python 3 it's just str, but was missing in 3.1\n    basestring = str\n\ntry:\n    unicode\nexcept NameError:\n    # In Python 3 unicode no longer exists (it's just str)\n    unicode = str\n\n\nclass _RouteClassAttributeToGetattr(object):\n    \"\"\"Route attribute access on a class to __getattr__.\n\n    This is a descriptor, used to define attributes that act differently when\n    accessed through an instance and through a class.  Instance access remains\n    normal, but access to an attribute through a class will be routed to the\n    class's __getattr__ method; this is done by raising AttributeError.\n\n    \"\"\"\n\n    def __init__(self, fget=None):\n        self.fget = fget\n\n    def __get__(self, instance, ownerclass=None):\n        if instance is None:\n            raise AttributeError()\n        return self.fget(instance)\n\n    def __set__(self, instance, value):\n        raise AttributeError(\"can't set attribute\")\n\n    def __delete__(self, instance):\n        raise AttributeError(\"can't delete attribute\")\n\n\ndef _is_descriptor(obj):\n    \"\"\"Returns True if obj is a descriptor, False otherwise.\"\"\"\n    return (\n        hasattr(obj, '__get__') or\n        hasattr(obj, '__set__') or\n        hasattr(obj, '__delete__'))\n\n\ndef _is_dunder(name):\n    \"\"\"Returns True if a __dunder__ name, False otherwise.\"\"\"\n    return (name[:2] == name[-2:] == '__' and\n            name[2:3] != '_' and\n            name[-3:-2] != '_' and\n            len(name) > 4)\n\n\ndef _is_sunder(name):\n    \"\"\"Returns True if a _sunder_ name, False otherwise.\"\"\"\n    return (name[0] == name[-1] == '_' and\n            name[1:2] != '_' and\n            name[-2:-1] != '_' and\n            len(name) > 2)\n\n\ndef _make_class_unpicklable(cls):\n    \"\"\"Make the given class un-picklable.\"\"\"\n\n    def _break_on_call_reduce(self, protocol=None):\n        raise TypeError('%r cannot be pickled' % self)\n    cls.__reduce_ex__ = _break_on_call_reduce\n    cls.__module__ = '<unknown>'\n\n\nclass _EnumDict(OrderedDict):\n    \"\"\"Track enum member order and ensure member names are not reused.\n\n    EnumMeta will use the names found in self._member_names as the\n    enumeration member names.\n\n    \"\"\"\n\n    def __init__(self):\n        super(_EnumDict, self).__init__()\n        self._member_names = []\n\n    def __setitem__(self, key, value):\n        \"\"\"Changes anything not dundered or not a descriptor.\n\n        If a descriptor is added with the same name as an enum member, the name\n        is removed from _member_names (this may leave a hole in the numerical\n        sequence of values).\n\n        If an enum member name is used twice, an error is raised; duplicate\n        values are not checked for.\n\n        Single underscore (sunder) names are reserved.\n\n        Note:   in 3.x __order__ is simply discarded as a not necessary piece\n                leftover from 2.x\n\n        \"\"\"\n        if pyver >= 3.0 and key in ('_order_', '__order__'):\n            return\n        elif key == '__order__':\n            key = '_order_'\n        if _is_sunder(key):\n            if key != '_order_':\n                raise ValueError('_names_ are reserved for future Enum use')\n        elif _is_dunder(key):\n            pass\n        elif key in self._member_names:\n            # descriptor overwriting an enum?\n            raise TypeError('Attempted to reuse key: %r' % key)\n        elif not _is_descriptor(value):\n            if key in self:\n                # enum overwriting a descriptor?\n                raise TypeError('Key already defined as: %r' % self[key])\n            self._member_names.append(key)\n        super(_EnumDict, self).__setitem__(key, value)\n\n\n# Dummy value for Enum as EnumMeta explicity checks for it, but of course until\n# EnumMeta finishes running the first time the Enum class doesn't exist.  This\n# is also why there are checks in EnumMeta like `if Enum is not None`\nEnum = None\n\n\nclass EnumMeta(type):\n    \"\"\"Metaclass for Enum\"\"\"\n    @classmethod\n    def __prepare__(metacls, cls, bases):\n        return _EnumDict()\n\n    def __new__(metacls, cls, bases, classdict):\n        # an Enum class is final once enumeration items have been defined; it\n        # cannot be mixed with other types (int, float, etc.) if it has an\n        # inherited __new__ unless a new __new__ is defined (or the resulting\n        # class will fail).\n        if isinstance(classdict, dict):\n            original_dict = classdict\n            classdict = _EnumDict()\n            for k, v in original_dict.items():\n                classdict[k] = v\n\n        member_type, first_enum = metacls._get_mixins_(bases)\n        __new__, save_new, use_args = metacls._find_new_(classdict, member_type,\n                                                         first_enum)\n        # save enum items into separate mapping so they don't get baked into\n        # the new class\n        members = dict((k, classdict[k]) for k in classdict._member_names)\n        for name in classdict._member_names:\n            del classdict[name]\n\n        # py2 support for definition order\n        _order_ = classdict.get('_order_')\n        if _order_ is None:\n            if pyver < 3.0:\n                try:\n                    _order_ = [name for (name, value) in sorted(members.items(), key=lambda item: item[1])]\n                except TypeError:\n                    _order_ = [name for name in sorted(members.keys())]\n            else:\n                _order_ = classdict._member_names\n        else:\n            del classdict['_order_']\n            if pyver < 3.0:\n                _order_ = _order_.replace(',', ' ').split()\n                aliases = [name for name in members if name not in _order_]\n                _order_ += aliases\n\n        # check for illegal enum names (any others?)\n        invalid_names = set(members) & set(['mro'])\n        if invalid_names:\n            raise ValueError('Invalid enum member name(s): %s' % (\n                ', '.join(invalid_names), ))\n\n        # save attributes from super classes so we know if we can take\n        # the shortcut of storing members in the class dict\n        base_attributes = set([a for b in bases for a in b.__dict__])\n        # create our new Enum type\n        enum_class = super(EnumMeta, metacls).__new__(metacls, cls, bases, classdict)\n        enum_class._member_names_ = []               # names in random order\n        if OrderedDict is not None:\n            enum_class._member_map_ = OrderedDict()\n        else:\n            enum_class._member_map_ = {}             # name->value map\n        enum_class._member_type_ = member_type\n\n        # Reverse value->name map for hashable values.\n        enum_class._value2member_map_ = {}\n\n        # instantiate them, checking for duplicates as we go\n        # we instantiate first instead of checking for duplicates first in case\n        # a custom __new__ is doing something funky with the values -- such as\n        # auto-numbering ;)\n        if __new__ is None:\n            __new__ = enum_class.__new__\n        for member_name in _order_:\n            value = members[member_name]\n            if not isinstance(value, tuple):\n                args = (value, )\n            else:\n                args = value\n            if member_type is tuple:   # special case for tuple enums\n                args = (args, )     # wrap it one more time\n            if not use_args or not args:\n                enum_member = __new__(enum_class)\n                if not hasattr(enum_member, '_value_'):\n                    enum_member._value_ = value\n            else:\n                enum_member = __new__(enum_class, *args)\n                if not hasattr(enum_member, '_value_'):\n                    enum_member._value_ = member_type(*args)\n            value = enum_member._value_\n            enum_member._name_ = member_name\n            enum_member.__objclass__ = enum_class\n            enum_member.__init__(*args)\n            # If another member with the same value was already defined, the\n            # new member becomes an alias to the existing one.\n            for name, canonical_member in enum_class._member_map_.items():\n                if canonical_member.value == enum_member._value_:\n                    enum_member = canonical_member\n                    break\n            else:\n                # Aliases don't appear in member names (only in __members__).\n                enum_class._member_names_.append(member_name)\n            # performance boost for any member that would not shadow\n            # a DynamicClassAttribute (aka _RouteClassAttributeToGetattr)\n            if member_name not in base_attributes:\n                setattr(enum_class, member_name, enum_member)\n            # now add to _member_map_\n            enum_class._member_map_[member_name] = enum_member\n            try:\n                # This may fail if value is not hashable. We can't add the value\n                # to the map, and by-value lookups for this value will be\n                # linear.\n                enum_class._value2member_map_[value] = enum_member\n            except TypeError:\n                pass\n\n        # If a custom type is mixed into the Enum, and it does not know how\n        # to pickle itself, pickle.dumps will succeed but pickle.loads will\n        # fail.  Rather than have the error show up later and possibly far\n        # from the source, sabotage the pickle protocol for this class so\n        # that pickle.dumps also fails.\n        #\n        # However, if the new class implements its own __reduce_ex__, do not\n        # sabotage -- it's on them to make sure it works correctly.  We use\n        # __reduce_ex__ instead of any of the others as it is preferred by\n        # pickle over __reduce__, and it handles all pickle protocols.\n        unpicklable = False\n        if '__reduce_ex__' not in classdict:\n            if member_type is not object:\n                methods = ('__getnewargs_ex__', '__getnewargs__',\n                           '__reduce_ex__', '__reduce__')\n                if not any(m in member_type.__dict__ for m in methods):\n                    _make_class_unpicklable(enum_class)\n                    unpicklable = True\n\n        # double check that repr and friends are not the mixin's or various\n        # things break (such as pickle)\n        for name in ('__repr__', '__str__', '__format__', '__reduce_ex__'):\n            class_method = getattr(enum_class, name)\n            getattr(member_type, name, None)\n            enum_method = getattr(first_enum, name, None)\n            if name not in classdict and class_method is not enum_method:\n                if name == '__reduce_ex__' and unpicklable:\n                    continue\n                setattr(enum_class, name, enum_method)\n\n        # method resolution and int's are not playing nice\n        # Python's less than 2.6 use __cmp__\n\n        if pyver < 2.6:\n\n            if issubclass(enum_class, int):\n                setattr(enum_class, '__cmp__', getattr(int, '__cmp__'))\n\n        elif pyver < 3.0:\n\n            if issubclass(enum_class, int):\n                for method in (\n                        '__le__',\n                        '__lt__',\n                        '__gt__',\n                        '__ge__',\n                        '__eq__',\n                        '__ne__',\n                        '__hash__',\n                ):\n                    setattr(enum_class, method, getattr(int, method))\n\n        # replace any other __new__ with our own (as long as Enum is not None,\n        # anyway) -- again, this is to support pickle\n        if Enum is not None:\n            # if the user defined their own __new__, save it before it gets\n            # clobbered in case they subclass later\n            if save_new:\n                setattr(enum_class, '__member_new__', enum_class.__dict__['__new__'])\n            setattr(enum_class, '__new__', Enum.__dict__['__new__'])\n        return enum_class\n\n    def __bool__(cls):\n        \"\"\"\n        classes/types should always be True.\n        \"\"\"\n        return True\n\n    def __call__(cls, value, names=None, module=None, type=None, start=1):\n        \"\"\"Either returns an existing member, or creates a new enum class.\n\n        This method is used both when an enum class is given a value to match\n        to an enumeration member (i.e. Color(3)) and for the functional API\n        (i.e. Color = Enum('Color', names='red green blue')).\n\n        When used for the functional API: `module`, if set, will be stored in\n        the new class' __module__ attribute; `type`, if set, will be mixed in\n        as the first base class.\n\n        Note: if `module` is not set this routine will attempt to discover the\n        calling module by walking the frame stack; if this is unsuccessful\n        the resulting class will not be pickleable.\n\n        \"\"\"\n        if names is None:  # simple value lookup\n            return cls.__new__(cls, value)\n        # otherwise, functional API: we're creating a new Enum type\n        return cls._create_(value, names, module=module, type=type, start=start)\n\n    def __contains__(cls, member):\n        return isinstance(member, cls) and member.name in cls._member_map_\n\n    def __delattr__(cls, attr):\n        # nicer error message when someone tries to delete an attribute\n        # (see issue19025).\n        if attr in cls._member_map_:\n            raise AttributeError(\n                \"%s: cannot delete Enum member.\" % cls.__name__)\n        super(EnumMeta, cls).__delattr__(attr)\n\n    def __dir__(self):\n        return (['__class__', '__doc__', '__members__', '__module__'] +\n                self._member_names_)\n\n    @property\n    def __members__(cls):\n        \"\"\"Returns a mapping of member name->value.\n\n        This mapping lists all enum members, including aliases. Note that this\n        is a copy of the internal mapping.\n\n        \"\"\"\n        return cls._member_map_.copy()\n\n    def __getattr__(cls, name):\n        \"\"\"Return the enum member matching `name`\n\n        We use __getattr__ instead of descriptors or inserting into the enum\n        class' __dict__ in order to support `name` and `value` being both\n        properties for enum members (which live in the class' __dict__) and\n        enum members themselves.\n\n        \"\"\"\n        if _is_dunder(name):\n            raise AttributeError(name)\n        try:\n            return cls._member_map_[name]\n        except KeyError:\n            raise AttributeError(name)\n\n    def __getitem__(cls, name):\n        return cls._member_map_[name]\n\n    def __iter__(cls):\n        return (cls._member_map_[name] for name in cls._member_names_)\n\n    def __reversed__(cls):\n        return (cls._member_map_[name] for name in reversed(cls._member_names_))\n\n    def __len__(cls):\n        return len(cls._member_names_)\n\n    __nonzero__ = __bool__\n\n    def __repr__(cls):\n        return \"<enum %r>\" % cls.__name__\n\n    def __setattr__(cls, name, value):\n        \"\"\"Block attempts to reassign Enum members.\n\n        A simple assignment to the class namespace only changes one of the\n        several possible ways to get an Enum member from the Enum class,\n        resulting in an inconsistent Enumeration.\n\n        \"\"\"\n        member_map = cls.__dict__.get('_member_map_', {})\n        if name in member_map:\n            raise AttributeError('Cannot reassign members.')\n        super(EnumMeta, cls).__setattr__(name, value)\n\n    def _create_(cls, class_name, names=None, module=None, type=None, start=1):\n        \"\"\"Convenience method to create a new Enum class.\n\n        `names` can be:\n\n        * A string containing member names, separated either with spaces or\n          commas.  Values are auto-numbered from 1.\n        * An iterable of member names.  Values are auto-numbered from 1.\n        * An iterable of (member name, value) pairs.\n        * A mapping of member name -> value.\n\n        \"\"\"\n        if pyver < 3.0:\n            # if class_name is unicode, attempt a conversion to ASCII\n            if isinstance(class_name, unicode):\n                try:\n                    class_name = class_name.encode('ascii')\n                except UnicodeEncodeError:\n                    raise TypeError('%r is not representable in ASCII' % class_name)\n        metacls = cls.__class__\n        if type is None:\n            bases = (cls, )\n        else:\n            bases = (type, cls)\n        classdict = metacls.__prepare__(class_name, bases)\n        _order_ = []\n\n        # special processing needed for names?\n        if isinstance(names, basestring):\n            names = names.replace(',', ' ').split()\n        if isinstance(names, (tuple, list)) and isinstance(names[0], basestring):\n            names = [(e, i + start) for (i, e) in enumerate(names)]\n\n        # Here, names is either an iterable of (name, value) or a mapping.\n        item = None  # in case names is empty\n        for item in names:\n            if isinstance(item, basestring):\n                member_name, member_value = item, names[item]\n            else:\n                member_name, member_value = item\n            classdict[member_name] = member_value\n            _order_.append(member_name)\n        # only set _order_ in classdict if name/value was not from a mapping\n        if not isinstance(item, basestring):\n            classdict['_order_'] = ' '.join(_order_)\n        enum_class = metacls.__new__(metacls, class_name, bases, classdict)\n\n        # TODO: replace the frame hack if a blessed way to know the calling\n        # module is ever developed\n        if module is None:\n            try:\n                module = _sys._getframe(2).f_globals['__name__']\n            except (AttributeError, ValueError):\n                pass\n        if module is None:\n            _make_class_unpicklable(enum_class)\n        else:\n            enum_class.__module__ = module\n\n        return enum_class\n\n    @staticmethod\n    def _get_mixins_(bases):\n        \"\"\"Returns the type for creating enum members, and the first inherited\n        enum class.\n\n        bases: the tuple of bases that was given to __new__\n\n        \"\"\"\n        if not bases or Enum is None:\n            return object, Enum\n\n        # double check that we are not subclassing a class with existing\n        # enumeration members; while we're at it, see if any other data\n        # type has been mixed in so we can use the correct __new__\n        member_type = first_enum = None\n        for base in bases:\n            if (base is not Enum and\n                    issubclass(base, Enum) and\n                    base._member_names_):\n                raise TypeError(\"Cannot extend enumerations\")\n        # base is now the last base in bases\n        if not issubclass(base, Enum):\n            raise TypeError(\"new enumerations must be created as \"\n                            \"`ClassName([mixin_type,] enum_type)`\")\n\n        # get correct mix-in type (either mix-in type of Enum subclass, or\n        # first base if last base is Enum)\n        if not issubclass(bases[0], Enum):\n            member_type = bases[0]     # first data type\n            first_enum = bases[-1]  # enum type\n        else:\n            for base in bases[0].__mro__:\n                # most common: (IntEnum, int, Enum, object)\n                # possible:    (<Enum 'AutoIntEnum'>, <Enum 'IntEnum'>,\n                #               <class 'int'>, <Enum 'Enum'>,\n                #               <class 'object'>)\n                if issubclass(base, Enum):\n                    if first_enum is None:\n                        first_enum = base\n                else:\n                    if member_type is None:\n                        member_type = base\n\n        return member_type, first_enum\n\n    if pyver < 3.0:\n        @staticmethod\n        def _find_new_(classdict, member_type, first_enum):\n            \"\"\"Returns the __new__ to be used for creating the enum members.\n\n            classdict: the class dictionary given to __new__\n            member_type: the data type whose __new__ will be used by default\n            first_enum: enumeration to check for an overriding __new__\n\n            \"\"\"\n            # now find the correct __new__, checking to see of one was defined\n            # by the user; also check earlier enum classes in case a __new__ was\n            # saved as __member_new__\n            __new__ = classdict.get('__new__', None)\n            if __new__:\n                return None, True, True      # __new__, save_new, use_args\n\n            N__new__ = getattr(None, '__new__')\n            O__new__ = getattr(object, '__new__')\n            if Enum is None:\n                E__new__ = N__new__\n            else:\n                E__new__ = Enum.__dict__['__new__']\n            # check all possibles for __member_new__ before falling back to\n            # __new__\n            for method in ('__member_new__', '__new__'):\n                for possible in (member_type, first_enum):\n                    try:\n                        target = possible.__dict__[method]\n                    except (AttributeError, KeyError):\n                        target = getattr(possible, method, None)\n                    if target not in [\n                            None,\n                            N__new__,\n                            O__new__,\n                            E__new__,\n                    ]:\n                        if method == '__member_new__':\n                            classdict['__new__'] = target\n                            return None, False, True\n                        if isinstance(target, staticmethod):\n                            target = target.__get__(member_type)\n                        __new__ = target\n                        break\n                if __new__ is not None:\n                    break\n            else:\n                __new__ = object.__new__\n\n            # if a non-object.__new__ is used then whatever value/tuple was\n            # assigned to the enum member name will be passed to __new__ and to the\n            # new enum member's __init__\n            if __new__ is object.__new__:\n                use_args = False\n            else:\n                use_args = True\n\n            return __new__, False, use_args\n    else:\n        @staticmethod\n        def _find_new_(classdict, member_type, first_enum):\n            \"\"\"Returns the __new__ to be used for creating the enum members.\n\n            classdict: the class dictionary given to __new__\n            member_type: the data type whose __new__ will be used by default\n            first_enum: enumeration to check for an overriding __new__\n\n            \"\"\"\n            # now find the correct __new__, checking to see of one was defined\n            # by the user; also check earlier enum classes in case a __new__ was\n            # saved as __member_new__\n            __new__ = classdict.get('__new__', None)\n\n            # should __new__ be saved as __member_new__ later?\n            save_new = __new__ is not None\n\n            if __new__ is None:\n                # check all possibles for __member_new__ before falling back to\n                # __new__\n                for method in ('__member_new__', '__new__'):\n                    for possible in (member_type, first_enum):\n                        target = getattr(possible, method, None)\n                        if target not in (\n                                None,\n                                None.__new__,\n                                object.__new__,\n                                Enum.__new__,\n                        ):\n                            __new__ = target\n                            break\n                    if __new__ is not None:\n                        break\n                else:\n                    __new__ = object.__new__\n\n            # if a non-object.__new__ is used then whatever value/tuple was\n            # assigned to the enum member name will be passed to __new__ and to the\n            # new enum member's __init__\n            if __new__ is object.__new__:\n                use_args = False\n            else:\n                use_args = True\n\n            return __new__, save_new, use_args\n\n\n########################################################\n# In order to support Python 2 and 3 with a single\n# codebase we have to create the Enum methods separately\n# and then use the `type(name, bases, dict)` method to\n# create the class.\n########################################################\ntemp_enum_dict = {}\ntemp_enum_dict['__doc__'] = \"Generic enumeration.\\n\\n    Derive from this class to define new enumerations.\\n\\n\"\n\n\ndef __new__(cls, value):\n    # all enum instances are actually created during class construction\n    # without calling this method; this method is called by the metaclass'\n    # __call__ (i.e. Color(3) ), and by pickle\n    if isinstance(value, cls):\n        # For lookups like Color(Color.red)\n        value = value.value\n        # return value\n    # by-value search for a matching enum member\n    # see if it's in the reverse mapping (for hashable values)\n    try:\n        if value in cls._value2member_map_:\n            return cls._value2member_map_[value]\n    except TypeError:\n        # not there, now do long search -- O(n) behavior\n        for member in cls._member_map_.values():\n            if member.value == value:\n                return member\n    raise ValueError(\"%s is not a valid %s\" % (value, cls.__name__))\ntemp_enum_dict['__new__'] = __new__\ndel __new__\n\n\ndef __repr__(self):\n    return \"<%s.%s: %r>\" % (\n        self.__class__.__name__, self._name_, self._value_)\ntemp_enum_dict['__repr__'] = __repr__\ndel __repr__\n\n\ndef __str__(self):\n    return \"%s.%s\" % (self.__class__.__name__, self._name_)\ntemp_enum_dict['__str__'] = __str__\ndel __str__\n\nif pyver >= 3.0:\n    def __dir__(self):\n        added_behavior = [\n            m\n            for cls in self.__class__.mro()\n            for m in cls.__dict__\n            if m[0] != '_' and m not in self._member_map_\n        ]\n        return (['__class__', '__doc__', '__module__', ] + added_behavior)\n    temp_enum_dict['__dir__'] = __dir__\n    del __dir__\n\n\ndef __format__(self, format_spec):\n    # mixed-in Enums should use the mixed-in type's __format__, otherwise\n    # we can get strange results with the Enum name showing up instead of\n    # the value\n\n    # pure Enum branch\n    if self._member_type_ is object:\n        cls = str\n        val = str(self)\n    # mix-in branch\n    else:\n        cls = self._member_type_\n        val = self.value\n    return cls.__format__(val, format_spec)\ntemp_enum_dict['__format__'] = __format__\ndel __format__\n\n\n####################################\n# Python's less than 2.6 use __cmp__\n\nif pyver < 2.6:\n\n    def __cmp__(self, other):\n        if isinstance(other, self.__class__):\n            if self is other:\n                return 0\n            return -1\n        return NotImplemented\n        raise TypeError(\"unorderable types: %s() and %s()\" % (self.__class__.__name__, other.__class__.__name__))\n    temp_enum_dict['__cmp__'] = __cmp__\n    del __cmp__\n\nelse:\n\n    def __le__(self, other):\n        raise TypeError(\"unorderable types: %s() <= %s()\" % (self.__class__.__name__, other.__class__.__name__))\n    temp_enum_dict['__le__'] = __le__\n    del __le__\n\n    def __lt__(self, other):\n        raise TypeError(\"unorderable types: %s() < %s()\" % (self.__class__.__name__, other.__class__.__name__))\n    temp_enum_dict['__lt__'] = __lt__\n    del __lt__\n\n    def __ge__(self, other):\n        raise TypeError(\"unorderable types: %s() >= %s()\" % (self.__class__.__name__, other.__class__.__name__))\n    temp_enum_dict['__ge__'] = __ge__\n    del __ge__\n\n    def __gt__(self, other):\n        raise TypeError(\"unorderable types: %s() > %s()\" % (self.__class__.__name__, other.__class__.__name__))\n    temp_enum_dict['__gt__'] = __gt__\n    del __gt__\n\n\ndef __eq__(self, other):\n    if isinstance(other, self.__class__):\n        return self is other\n    return NotImplemented\ntemp_enum_dict['__eq__'] = __eq__\ndel __eq__\n\n\ndef __ne__(self, other):\n    if isinstance(other, self.__class__):\n        return self is not other\n    return NotImplemented\ntemp_enum_dict['__ne__'] = __ne__\ndel __ne__\n\n\ndef __hash__(self):\n    return hash(self._name_)\ntemp_enum_dict['__hash__'] = __hash__\ndel __hash__\n\n\ndef __reduce_ex__(self, proto):\n    return self.__class__, (self._value_, )\ntemp_enum_dict['__reduce_ex__'] = __reduce_ex__\ndel __reduce_ex__\n\n# _RouteClassAttributeToGetattr is used to provide access to the `name`\n# and `value` properties of enum members while keeping some measure of\n# protection from modification, while still allowing for an enumeration\n# to have members named `name` and `value`.  This works because enumeration\n# members are not set directly on the enum class -- __getattr__ is\n# used to look them up.\n\n\n@_RouteClassAttributeToGetattr\ndef name(self):\n    return self._name_\ntemp_enum_dict['name'] = name\ndel name\n\n\n@_RouteClassAttributeToGetattr\ndef value(self):\n    return self._value_\ntemp_enum_dict['value'] = value\ndel value\n\n\n@classmethod\ndef _convert(cls, name, module, filter, source=None):\n    \"\"\"\n    Create a new Enum subclass that replaces a collection of global constants\n    \"\"\"\n    # convert all constants from source (or module) that pass filter() to\n    # a new Enum called name, and export the enum and its members back to\n    # module;\n    # also, replace the __reduce_ex__ method so unpickling works in\n    # previous Python versions\n    module_globals = vars(_sys.modules[module])\n    if source:\n        source = vars(source)\n    else:\n        source = module_globals\n    members = dict((name, value) for name, value in source.items() if filter(name))\n    cls = cls(name, members, module=module)\n    cls.__reduce_ex__ = _reduce_ex_by_name\n    module_globals.update(cls.__members__)\n    module_globals[name] = cls\n    return cls\ntemp_enum_dict['_convert'] = _convert\ndel _convert\n\nEnum = EnumMeta('Enum', (object, ), temp_enum_dict)\ndel temp_enum_dict\n\n# Enum has now been created\n###########################\n\n\nclass IntEnum(int, Enum):\n    \"\"\"Enum where members are also (and must be) ints\"\"\"\n\n\ndef _reduce_ex_by_name(self, proto):\n    return self.name\n\n\ndef unique(enumeration):\n    \"\"\"Class decorator that ensures only unique members exist in an enumeration.\"\"\"\n    duplicates = []\n    for name, member in enumeration.__members__.items():\n        if name != member.name:\n            duplicates.append((name, member.name))\n    if duplicates:\n        duplicate_names = ', '.join(\n            [\"%s -> %s\" % (alias, name) for (alias, name) in duplicates]\n        )\n        raise ValueError('duplicate names found in %r: %s' %\n                         (enumeration, duplicate_names)\n                         )\n    return enumeration\n", 
    "graphene.pyutils.version": "from __future__ import unicode_literals\n\nimport datetime\nimport os\nimport subprocess\n\n\ndef get_version(version=None):\n    \"Returns a PEP 440-compliant version number from VERSION.\"\n    version = get_complete_version(version)\n\n    # Now build the two parts of the version number:\n    # main = X.Y[.Z]\n    # sub = .devN - for pre-alpha releases\n    #     | {a|b|rc}N - for alpha, beta, and rc releases\n\n    main = get_main_version(version)\n\n    sub = ''\n    if version[3] == 'alpha' and version[4] == 0:\n        git_changeset = get_git_changeset()\n        if git_changeset:\n            sub = '.dev%s' % git_changeset\n        else:\n            sub = '.dev'\n    elif version[3] != 'final':\n        mapping = {'alpha': 'a', 'beta': 'b', 'rc': 'rc'}\n        sub = mapping[version[3]] + str(version[4])\n\n    return str(main + sub)\n\n\ndef get_main_version(version=None):\n    \"Returns main version (X.Y[.Z]) from VERSION.\"\n    version = get_complete_version(version)\n    parts = 2 if version[2] == 0 else 3\n    return '.'.join(str(x) for x in version[:parts])\n\n\ndef get_complete_version(version=None):\n    \"\"\"Returns a tuple of the graphene version. If version argument is non-empty,\n    then checks for correctness of the tuple provided.\n    \"\"\"\n    if version is None:\n        from graphene import VERSION as version\n    else:\n        assert len(version) == 5\n        assert version[3] in ('alpha', 'beta', 'rc', 'final')\n\n    return version\n\n\ndef get_docs_version(version=None):\n    version = get_complete_version(version)\n    if version[3] != 'final':\n        return 'dev'\n    else:\n        return '%d.%d' % version[:2]\n\n\ndef get_git_changeset():\n    \"\"\"Returns a numeric identifier of the latest git changeset.\n    The result is the UTC timestamp of the changeset in YYYYMMDDHHMMSS format.\n    This value isn't guaranteed to be unique, but collisions are very unlikely,\n    so it's sufficient for generating the development version numbers.\n    \"\"\"\n    repo_dir = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))\n    try:\n        git_log = subprocess.Popen(\n            'git log --pretty=format:%ct --quiet -1 HEAD',\n            stdout=subprocess.PIPE, stderr=subprocess.PIPE,\n            shell=True, cwd=repo_dir, universal_newlines=True,\n        )\n        timestamp = git_log.communicate()[0]\n        timestamp = datetime.datetime.utcfromtimestamp(int(timestamp))\n    except:\n        return None\n    return timestamp.strftime('%Y%m%d%H%M%S')\n", 
    "graphene.relay.__init__": "from .node import Node, is_node, GlobalID\nfrom .mutation import ClientIDMutation\nfrom .connection import Connection, ConnectionField, PageInfo\n\n__all__ = [\n    'Node',\n    'is_node',\n    'GlobalID',\n    'ClientIDMutation',\n    'Connection',\n    'ConnectionField',\n    'PageInfo',\n]\n", 
    "graphene.relay.connection": "import re\nfrom collections import Iterable, OrderedDict\nfrom functools import partial\n\nimport six\n\nfrom graphql_relay import connection_from_list\n\nfrom ..types import (AbstractType, Boolean, Enum, Int, Interface, List, NonNull, Scalar, String,\n                     Union)\nfrom ..types.field import Field\nfrom ..types.objecttype import ObjectType, ObjectTypeMeta\nfrom ..types.options import Options\nfrom ..utils.is_base_type import is_base_type\nfrom ..utils.props import props\nfrom .node import is_node\n\n\nclass PageInfo(ObjectType):\n    has_next_page = Boolean(\n        required=True,\n        name='hasNextPage',\n        description='When paginating forwards, are there more items?',\n    )\n\n    has_previous_page = Boolean(\n        required=True,\n        name='hasPreviousPage',\n        description='When paginating backwards, are there more items?',\n    )\n\n    start_cursor = String(\n        name='startCursor',\n        description='When paginating backwards, the cursor to continue.',\n    )\n\n    end_cursor = String(\n        name='endCursor',\n        description='When paginating forwards, the cursor to continue.',\n    )\n\n\nclass ConnectionMeta(ObjectTypeMeta):\n\n    def __new__(cls, name, bases, attrs):\n        # Also ensure initialization is only performed for subclasses of Model\n        # (excluding Model class itself).\n        if not is_base_type(bases, ConnectionMeta):\n            return type.__new__(cls, name, bases, attrs)\n\n        options = Options(\n            attrs.pop('Meta', None),\n            name=name,\n            description=None,\n            node=None,\n        )\n        options.interfaces = ()\n        options.local_fields = OrderedDict()\n\n        assert options.node, 'You have to provide a node in {}.Meta'.format(cls.__name__)\n        assert issubclass(options.node, (Scalar, Enum, ObjectType, Interface, Union, NonNull)), (\n            'Received incompatible node \"{}\" for Connection {}.'\n        ).format(options.node, name)\n\n        base_name = re.sub('Connection$', '', options.name) or options.node._meta.name\n        if not options.name:\n            options.name = '{}Connection'.format(base_name)\n\n        edge_class = attrs.pop('Edge', None)\n\n        class EdgeBase(AbstractType):\n            node = Field(options.node, description='The item at the end of the edge')\n            cursor = String(required=True, description='A cursor for use in pagination')\n\n        edge_name = '{}Edge'.format(base_name)\n        if edge_class and issubclass(edge_class, AbstractType):\n            edge = type(edge_name, (EdgeBase, edge_class, ObjectType, ), {})\n        else:\n            edge_attrs = props(edge_class) if edge_class else {}\n            edge = type(edge_name, (EdgeBase, ObjectType, ), edge_attrs)\n\n        class ConnectionBase(AbstractType):\n            page_info = Field(PageInfo, name='pageInfo', required=True)\n            edges = List(edge)\n\n        bases = (ConnectionBase, ) + bases\n        attrs = dict(attrs, _meta=options, Edge=edge)\n        return ObjectTypeMeta.__new__(cls, name, bases, attrs)\n\n\nclass Connection(six.with_metaclass(ConnectionMeta, ObjectType)):\n    pass\n\n\nclass IterableConnectionField(Field):\n\n    def __init__(self, type, *args, **kwargs):\n        super(IterableConnectionField, self).__init__(\n            type,\n            *args,\n            before=String(),\n            after=String(),\n            first=Int(),\n            last=Int(),\n            **kwargs\n        )\n\n    @property\n    def type(self):\n        type = super(IterableConnectionField, self).type\n        if is_node(type):\n            connection_type = type.Connection\n        else:\n            connection_type = type\n        assert issubclass(connection_type, Connection), (\n            '{} type have to be a subclass of Connection. Received \"{}\".'\n        ).format(str(self), connection_type)\n        return connection_type\n\n    @staticmethod\n    def connection_resolver(resolver, connection, root, args, context, info):\n        iterable = resolver(root, args, context, info)\n        assert isinstance(iterable, Iterable), (\n            'Resolved value from the connection field have to be iterable. '\n            'Received \"{}\"'\n        ).format(iterable)\n        connection = connection_from_list(\n            iterable,\n            args,\n            connection_type=connection,\n            edge_type=connection.Edge,\n            pageinfo_type=PageInfo\n        )\n        connection.iterable = iterable\n        return connection\n\n    def get_resolver(self, parent_resolver):\n        resolver = super(IterableConnectionField, self).get_resolver(parent_resolver)\n        return partial(self.connection_resolver, resolver, self.type)\n\nConnectionField = IterableConnectionField\n", 
    "graphene.relay.mutation": "import re\nfrom functools import partial\n\nimport six\n\nfrom promise import Promise\n\nfrom ..types import AbstractType, Argument, Field, InputObjectType, String\nfrom ..types.objecttype import ObjectType, ObjectTypeMeta\nfrom ..utils.is_base_type import is_base_type\nfrom ..utils.props import props\n\n\nclass ClientIDMutationMeta(ObjectTypeMeta):\n\n    def __new__(cls, name, bases, attrs):\n        # Also ensure initialization is only performed for subclasses of\n        # Mutation\n        if not is_base_type(bases, ClientIDMutationMeta):\n            return type.__new__(cls, name, bases, attrs)\n\n        input_class = attrs.pop('Input', None)\n        base_name = re.sub('Payload$', '', name)\n        if 'client_mutation_id' not in attrs:\n            attrs['client_mutation_id'] = String(name='clientMutationId')\n        cls = ObjectTypeMeta.__new__(cls, '{}Payload'.format(base_name), bases, attrs)\n        mutate_and_get_payload = getattr(cls, 'mutate_and_get_payload', None)\n        if cls.mutate and cls.mutate.__func__ == ClientIDMutation.mutate.__func__:\n            assert mutate_and_get_payload, (\n                \"{}.mutate_and_get_payload method is required\"\n                \" in a ClientIDMutation.\"\n            ).format(name)\n        input_attrs = {}\n        bases = ()\n        if not input_class:\n            input_attrs = {}\n        elif not issubclass(input_class, AbstractType):\n            input_attrs = props(input_class)\n        else:\n            bases += (input_class, )\n        input_attrs['client_mutation_id'] = String(name='clientMutationId')\n        cls.Input = type('{}Input'.format(base_name), bases + (InputObjectType,), input_attrs)\n        cls.Field = partial(Field, cls, resolver=cls.mutate, input=Argument(cls.Input, required=True))\n        return cls\n\n\nclass ClientIDMutation(six.with_metaclass(ClientIDMutationMeta, ObjectType)):\n\n    @classmethod\n    def mutate(cls, root, args, context, info):\n        input = args.get('input')\n\n        def on_resolve(payload):\n            try:\n                payload.client_mutation_id = input.get('clientMutationId')\n            except:\n                raise Exception((\n                    'Cannot set client_mutation_id in the payload object {}'\n                ).format(repr(payload)))\n            return payload\n\n        return Promise.resolve(\n            cls.mutate_and_get_payload(input, context, info)\n        ).then(on_resolve)\n", 
    "graphene.relay.node": "from functools import partial\n\nimport six\n\nfrom graphql_relay import from_global_id, to_global_id\n\nfrom ..types import ID, Field, Interface, ObjectType\nfrom ..types.interface import InterfaceMeta\n\n\ndef is_node(objecttype):\n    '''\n    Check if the given objecttype has Node as an interface\n    '''\n    assert issubclass(objecttype, ObjectType), (\n        'Only ObjectTypes can have a Node interface.'\n    )\n    for i in objecttype._meta.interfaces:\n        if issubclass(i, Node):\n            return True\n    return False\n\n\ndef get_default_connection(cls):\n    from .connection import Connection\n    assert issubclass(cls, ObjectType), (\n        'Can only get connection type on implemented Nodes.'\n    )\n\n    class Meta:\n        node = cls\n\n    return type('{}Connection'.format(cls.__name__), (Connection,), {'Meta': Meta})\n\n\nclass GlobalID(Field):\n\n    def __init__(self, node, *args, **kwargs):\n        super(GlobalID, self).__init__(ID, *args, **kwargs)\n        self.node = node\n\n    @staticmethod\n    def id_resolver(parent_resolver, node, root, args, context, info):\n        id = parent_resolver(root, args, context, info)\n        return node.to_global_id(info.parent_type.name, id)  # root._meta.name\n\n    def get_resolver(self, parent_resolver):\n        return partial(self.id_resolver, parent_resolver, self.node)\n\n\nclass NodeMeta(InterfaceMeta):\n\n    def __new__(cls, name, bases, attrs):\n        cls = InterfaceMeta.__new__(cls, name, bases, attrs)\n        cls._meta.fields['id'] = GlobalID(cls, required=True, description='The ID of the object.')\n        return cls\n\n\nclass NodeField(Field):\n\n    def __init__(self, node, type=False, deprecation_reason=None,\n                 name=None, **kwargs):\n        assert issubclass(node, Node), 'NodeField can only operate in Nodes'\n        type = type or node\n        super(NodeField, self).__init__(\n            type,\n            description='The ID of the object',\n            id=ID(required=True),\n            resolver=node.node_resolver\n        )\n\n\nclass Node(six.with_metaclass(NodeMeta, Interface)):\n    '''An object with an ID'''\n\n    @classmethod\n    def Field(cls, *args, **kwargs):  # noqa: N802\n        return NodeField(cls, *args, **kwargs)\n\n    @classmethod\n    def node_resolver(cls, root, args, context, info):\n        return cls.get_node_from_global_id(args.get('id'), context, info)\n\n    @classmethod\n    def get_node_from_global_id(cls, global_id, context, info):\n        try:\n            _type, _id = cls.from_global_id(global_id)\n            graphene_type = info.schema.get_type(_type).graphene_type\n            # We make sure the ObjectType implements the \"Node\" interface\n            assert cls in graphene_type._meta.interfaces\n        except:\n            return None\n        get_node = getattr(graphene_type, 'get_node', None)\n        if get_node:\n            return get_node(_id, context, info)\n\n    @classmethod\n    def from_global_id(cls, global_id):\n        return from_global_id(global_id)\n\n    @classmethod\n    def to_global_id(cls, type, id):\n        return to_global_id(type, id)\n\n    @classmethod\n    def implements(cls, objecttype):\n        get_connection = getattr(objecttype, 'get_connection', None)\n        if not get_connection:\n            get_connection = partial(get_default_connection, objecttype)\n\n        objecttype.Connection = get_connection()\n", 
    "graphene.types.__init__": "# flake8: noqa\n\nfrom .objecttype import ObjectType\nfrom .abstracttype import AbstractType\nfrom .interface import Interface\nfrom .mutation import Mutation\nfrom .scalars import Scalar, String, ID, Int, Float, Boolean\nfrom .schema import Schema\nfrom .structures import List, NonNull\nfrom .enum import Enum\nfrom .field import Field\nfrom .inputfield import InputField\nfrom .argument import Argument\nfrom .inputobjecttype import InputObjectType\nfrom .dynamic import Dynamic\nfrom .union import Union\n\n\n__all__ = [\n    'AbstractType',\n    'ObjectType',\n    'InputObjectType',\n    'Interface',\n    'Mutation',\n    'Enum',\n    'Field',\n    'InputField',\n    'Schema',\n    'Scalar',\n    'String',\n    'ID',\n    'Int',\n    'Float',\n    'Boolean',\n    'List',\n    'NonNull',\n    'Argument',\n    'Dynamic',\n    'Union',\n]\n", 
    "graphene.types.abstracttype": "import six\n\nfrom ..utils.is_base_type import is_base_type\nfrom .options import Options\nfrom .utils import get_base_fields, merge, yank_fields_from_attrs\n\n\nclass AbstractTypeMeta(type):\n    '''\n    AbstractType Definition\n\n    When we want to share fields across multiple types, like a Interface,\n    a ObjectType and a Input ObjectType we can use AbstractTypes for defining\n    our fields that the other types will inherit from.\n    '''\n\n    def __new__(cls, name, bases, attrs):\n        # Also ensure initialization is only performed for subclasses of\n        # AbstractType\n        if not is_base_type(bases, AbstractTypeMeta):\n            return type.__new__(cls, name, bases, attrs)\n\n        for base in bases:\n            if not issubclass(base, AbstractType) and issubclass(type(base), AbstractTypeMeta):\n                # raise Exception('You can only extend AbstractTypes after the base definition.')\n                return type.__new__(cls, name, bases, attrs)\n\n        base_fields = get_base_fields(bases, _as=None)\n\n        fields = yank_fields_from_attrs(attrs, _as=None)\n\n        options = Options(\n            fields=merge(base_fields, fields)\n        )\n        cls = type.__new__(cls, name, bases, dict(attrs, _meta=options))\n\n        return cls\n\n\nclass AbstractType(six.with_metaclass(AbstractTypeMeta)):\n    pass\n", 
    "graphene.types.argument": "from collections import OrderedDict\nfrom itertools import chain\n\nfrom ..utils.orderedtype import OrderedType\nfrom .structures import NonNull\n\n\nclass Argument(OrderedType):\n\n    def __init__(self, type, default_value=None, description=None, name=None, required=False, _creation_counter=None):\n        super(Argument, self).__init__(_creation_counter=_creation_counter)\n\n        if required:\n            type = NonNull(type)\n\n        self.name = name\n        self.type = type\n        self.default_value = default_value\n        self.description = description\n\n\ndef to_arguments(args, extra_args):\n    from .unmountedtype import UnmountedType\n    extra_args = sorted(extra_args.items(), key=lambda f: f[1])\n    iter_arguments = chain(args.items(), extra_args)\n    arguments = OrderedDict()\n    for default_name, arg in iter_arguments:\n        if isinstance(arg, UnmountedType):\n            arg = arg.Argument()\n\n        if not isinstance(arg, Argument):\n            raise ValueError('Unknown argument \"{}\".'.format(default_name))\n\n        arg_name = default_name or arg.name\n        assert arg_name not in arguments, 'More than one Argument have same name \"{}\".'.format(arg_name)\n        arguments[arg_name] = arg\n\n    return arguments\n", 
    "graphene.types.definitions": "from graphql import (GraphQLEnumType, GraphQLInputObjectType,\n                     GraphQLInterfaceType, GraphQLObjectType,\n                     GraphQLScalarType, GraphQLUnionType)\n\n\nclass GrapheneGraphQLType(object):\n    '''\n    A class for extending the base GraphQLType with the related\n    graphene_type\n    '''\n\n    def __init__(self, *args, **kwargs):\n        self.graphene_type = kwargs.pop('graphene_type')\n        super(GrapheneGraphQLType, self).__init__(*args, **kwargs)\n\n\nclass GrapheneInterfaceType(GrapheneGraphQLType, GraphQLInterfaceType):\n    pass\n\n\nclass GrapheneUnionType(GrapheneGraphQLType, GraphQLUnionType):\n    pass\n\n\nclass GrapheneObjectType(GrapheneGraphQLType, GraphQLObjectType):\n    pass\n\n\nclass GrapheneScalarType(GrapheneGraphQLType, GraphQLScalarType):\n    pass\n\n\nclass GrapheneEnumType(GrapheneGraphQLType, GraphQLEnumType):\n    pass\n\n\nclass GrapheneInputObjectType(GrapheneGraphQLType, GraphQLInputObjectType):\n    pass\n", 
    "graphene.types.dynamic": "import inspect\n\nfrom ..utils.orderedtype import OrderedType\n\n\nclass Dynamic(OrderedType):\n    '''\n    A Dynamic Type let us get the type in runtime when we generate\n    the schema. So we can have lazy fields.\n    '''\n\n    def __init__(self, type, _creation_counter=None):\n        super(Dynamic, self).__init__(_creation_counter=_creation_counter)\n        assert inspect.isfunction(type)\n        self.type = type\n\n    def get_type(self):\n        return self.type()\n", 
    "graphene.types.enum": "from collections import OrderedDict\n\nimport six\n\nfrom ..utils.is_base_type import is_base_type\nfrom .options import Options\nfrom .unmountedtype import UnmountedType\n\ntry:\n    from enum import Enum as PyEnum\nexcept ImportError:\n    from ..pyutils.enum import Enum as PyEnum\n\n\nclass EnumTypeMeta(type):\n\n    def __new__(cls, name, bases, attrs):\n        # Also ensure initialization is only performed for subclasses of Model\n        # (excluding Model class itself).\n        if not is_base_type(bases, EnumTypeMeta):\n            return type.__new__(cls, name, bases, attrs)\n\n        options = Options(\n            attrs.pop('Meta', None),\n            name=name,\n            description=attrs.get('__doc__'),\n            enum=None,\n        )\n        if not options.enum:\n            options.enum = PyEnum(cls.__name__, attrs)\n\n        new_attrs = OrderedDict(attrs, _meta=options, **options.enum.__members__)\n        return type.__new__(cls, name, bases, new_attrs)\n\n    def __prepare__(name, bases, **kwargs):  # noqa: N805\n        return OrderedDict()\n\n    def __call__(cls, *args, **kwargs):  # noqa: N805\n        if cls is Enum:\n            description = kwargs.pop('description', None)\n            return cls.from_enum(PyEnum(*args, **kwargs), description=description)\n        return super(EnumTypeMeta, cls).__call__(*args, **kwargs)\n\n    def from_enum(cls, enum, description=None):  # noqa: N805\n        meta_class = type('Meta', (object,), {'enum': enum, 'description': description})\n        return type(meta_class.enum.__name__, (Enum,), {'Meta': meta_class})\n\n    def __str__(cls):  # noqa: N805\n        return cls._meta.name\n\n\nclass Enum(six.with_metaclass(EnumTypeMeta, UnmountedType)):\n    '''\n    Enum Type Definition\n\n    Some leaf values of requests and input values are Enums. GraphQL serializes\n    Enum values as strings, however internally Enums can be represented by any\n    kind of type, often integers.\n    '''\n\n    def get_type(self):\n        return type(self)\n", 
    "graphene.types.field": "import inspect\nfrom collections import Mapping, OrderedDict\nfrom functools import partial\n\nfrom ..utils.orderedtype import OrderedType\nfrom .argument import Argument, to_arguments\nfrom .structures import NonNull\nfrom .unmountedtype import UnmountedType\n\n\nbase_type = type\n\n\ndef source_resolver(source, root, args, context, info):\n    resolved = getattr(root, source, None)\n    if inspect.isfunction(resolved):\n        return resolved()\n    return resolved\n\n\nclass Field(OrderedType):\n\n    def __init__(self, type, args=None, resolver=None, source=None,\n                 deprecation_reason=None, name=None, description=None,\n                 required=False, _creation_counter=None, default_value=None,\n                 **extra_args):\n        super(Field, self).__init__(_creation_counter=_creation_counter)\n        assert not args or isinstance(args, Mapping), (\n            'Arguments in a field have to be a mapping, received \"{}\".'\n        ).format(args)\n        assert not (source and resolver), (\n            'A Field cannot have a source and a resolver in at the same time.'\n        )\n        assert not callable(default_value), (\n            'The default value can not be a function but received \"{}\".'\n        ).format(base_type(default_value))\n\n        if required:\n            type = NonNull(type)\n\n        # Check if name is actually an argument of the field\n        if isinstance(name, (Argument, UnmountedType)):\n            extra_args['name'] = name\n            name = None\n\n        # Check if source is actually an argument of the field\n        if isinstance(source, (Argument, UnmountedType)):\n            extra_args['source'] = source\n            source = None\n\n        self.name = name\n        self._type = type\n        self.args = to_arguments(args or OrderedDict(), extra_args)\n        if source:\n            resolver = partial(source_resolver, source)\n        self.resolver = resolver\n        self.deprecation_reason = deprecation_reason\n        self.description = description\n        self.default_value = default_value\n\n    @property\n    def type(self):\n        if inspect.isfunction(self._type):\n            return self._type()\n        return self._type\n\n    def get_resolver(self, parent_resolver):\n        return self.resolver or parent_resolver\n", 
    "graphene.types.inputfield": "from ..utils.orderedtype import OrderedType\nfrom .structures import NonNull\n\n\nclass InputField(OrderedType):\n\n    def __init__(self, type, name=None, default_value=None,\n                 deprecation_reason=None, description=None,\n                 required=False, _creation_counter=None, **extra_args):\n        super(InputField, self).__init__(_creation_counter=_creation_counter)\n        self.name = name\n        if required:\n            type = NonNull(type)\n        self.type = type\n        self.deprecation_reason = deprecation_reason\n        self.default_value = default_value\n        self.description = description\n", 
    "graphene.types.inputobjecttype": "import six\n\nfrom ..utils.is_base_type import is_base_type\nfrom .abstracttype import AbstractTypeMeta\nfrom .inputfield import InputField\nfrom .options import Options\nfrom .unmountedtype import UnmountedType\nfrom .utils import get_base_fields, merge, yank_fields_from_attrs\n\n\nclass InputObjectTypeMeta(AbstractTypeMeta):\n\n    def __new__(cls, name, bases, attrs):\n        # Also ensure initialization is only performed for subclasses of\n        # InputObjectType\n        if not is_base_type(bases, InputObjectTypeMeta):\n            return type.__new__(cls, name, bases, attrs)\n\n        options = Options(\n            attrs.pop('Meta', None),\n            name=name,\n            description=attrs.get('__doc__'),\n            local_fields=None,\n        )\n\n        options.base_fields = get_base_fields(bases, _as=InputField)\n\n        if not options.local_fields:\n            options.local_fields = yank_fields_from_attrs(attrs, _as=InputField)\n\n        options.fields = merge(\n            options.base_fields,\n            options.local_fields\n        )\n        return type.__new__(cls, name, bases, dict(attrs, _meta=options))\n\n    def __str__(cls):  # noqa: N802\n        return cls._meta.name\n\n\nclass InputObjectType(six.with_metaclass(InputObjectTypeMeta, UnmountedType)):\n    '''\n    Input Object Type Definition\n\n    An input object defines a structured collection of fields which may be\n    supplied to a field argument.\n\n    Using `NonNull` will ensure that a value must be provided by the query\n    '''\n\n    @classmethod\n    def get_type(cls):\n        return cls\n", 
    "graphene.types.interface": "import six\n\nfrom ..utils.is_base_type import is_base_type\nfrom .abstracttype import AbstractTypeMeta\nfrom .field import Field\nfrom .options import Options\nfrom .utils import get_base_fields, merge, yank_fields_from_attrs\n\n\nclass InterfaceMeta(AbstractTypeMeta):\n\n    def __new__(cls, name, bases, attrs):\n        # Also ensure initialization is only performed for subclasses of\n        # Interface\n        if not is_base_type(bases, InterfaceMeta):\n            return type.__new__(cls, name, bases, attrs)\n\n        options = Options(\n            attrs.pop('Meta', None),\n            name=name,\n            description=attrs.get('__doc__'),\n            local_fields=None,\n        )\n\n        options.base_fields = get_base_fields(bases, _as=Field)\n\n        if not options.local_fields:\n            options.local_fields = yank_fields_from_attrs(attrs, _as=Field)\n\n        options.fields = merge(\n            options.base_fields,\n            options.local_fields\n        )\n\n        return type.__new__(cls, name, bases, dict(attrs, _meta=options))\n\n    def __str__(cls):  # noqa: N802\n        return cls._meta.name\n\n\nclass Interface(six.with_metaclass(InterfaceMeta)):\n    '''\n    Interface Type Definition\n\n    When a field can return one of a heterogeneous set of types, a Interface type\n    is used to describe what types are possible, what fields are in common across\n    all types, as well as a function to determine which type is actually used\n    when the field is resolved.\n    '''\n\n    resolve_type = None\n\n    def __init__(self, *args, **kwargs):\n        raise Exception(\"An Interface cannot be intitialized\")\n\n    @classmethod\n    def implements(cls, objecttype):\n        pass\n", 
    "graphene.types.mutation": "from functools import partial\n\nimport six\n\nfrom ..utils.is_base_type import is_base_type\nfrom ..utils.props import props\nfrom .field import Field\nfrom .objecttype import ObjectType, ObjectTypeMeta\n\n\nclass MutationMeta(ObjectTypeMeta):\n\n    def __new__(cls, name, bases, attrs):\n        # Also ensure initialization is only performed for subclasses of\n        # Mutation\n        if not is_base_type(bases, MutationMeta):\n            return type.__new__(cls, name, bases, attrs)\n\n        input_class = attrs.pop('Input', None)\n\n        cls = ObjectTypeMeta.__new__(cls, name, bases, attrs)\n        field_args = props(input_class) if input_class else {}\n        resolver = getattr(cls, 'mutate', None)\n        assert resolver, 'All mutations must define a mutate method in it'\n        cls.Field = partial(Field, cls, args=field_args, resolver=resolver)\n        return cls\n\n\nclass Mutation(six.with_metaclass(MutationMeta, ObjectType)):\n    pass\n", 
    "graphene.types.objecttype": "from collections import OrderedDict\n\nimport six\n\nfrom ..utils.is_base_type import is_base_type\nfrom .abstracttype import AbstractTypeMeta\nfrom .field import Field\nfrom .interface import Interface\nfrom .options import Options\nfrom .utils import get_base_fields, merge, yank_fields_from_attrs\n\n\nclass ObjectTypeMeta(AbstractTypeMeta):\n\n    def __new__(cls, name, bases, attrs):\n        # Also ensure initialization is only performed for subclasses of\n        # ObjectType\n        if not is_base_type(bases, ObjectTypeMeta):\n            return type.__new__(cls, name, bases, attrs)\n\n        _meta = attrs.pop('_meta', None)\n        options = _meta or Options(\n            attrs.pop('Meta', None),\n            name=name,\n            description=attrs.get('__doc__'),\n            interfaces=(),\n            local_fields=OrderedDict(),\n        )\n        options.base_fields = get_base_fields(bases, _as=Field)\n\n        if not options.local_fields:\n            options.local_fields = yank_fields_from_attrs(attrs=attrs, _as=Field)\n\n        options.interface_fields = OrderedDict()\n        for interface in options.interfaces:\n            assert issubclass(interface, Interface), (\n                'All interfaces of {} must be a subclass of Interface. Received \"{}\".'\n            ).format(name, interface)\n            options.interface_fields.update(interface._meta.fields)\n\n        options.fields = merge(\n            options.interface_fields,\n            options.base_fields,\n            options.local_fields\n        )\n\n        cls = type.__new__(cls, name, bases, dict(attrs, _meta=options))\n\n        for interface in options.interfaces:\n            interface.implements(cls)\n\n        return cls\n\n    def __str__(cls):  # noqa: N802\n        return cls._meta.name\n\n\nclass ObjectType(six.with_metaclass(ObjectTypeMeta)):\n    '''\n    Object Type Definition\n\n    Almost all of the GraphQL types you define will be object types. Object types\n    have a name, but most importantly describe their fields.\n    '''\n\n    @classmethod\n    def is_type_of(cls, root, context, info):\n        if isinstance(root, cls):\n            return True\n\n    def __init__(self, *args, **kwargs):\n        # ObjectType acting as container\n        args_len = len(args)\n        fields = self._meta.fields.items()\n        if args_len > len(fields):\n            # Daft, but matches old exception sans the err msg.\n            raise IndexError(\"Number of args exceeds number of fields\")\n        fields_iter = iter(fields)\n\n        if not kwargs:\n            for val, (name, field) in zip(args, fields_iter):\n                setattr(self, name, val)\n        else:\n            for val, (name, field) in zip(args, fields_iter):\n                setattr(self, name, val)\n                kwargs.pop(name, None)\n\n        for name, field in fields_iter:\n            try:\n                val = kwargs.pop(name)\n                setattr(self, name, val)\n            except KeyError:\n                pass\n\n        if kwargs:\n            for prop in list(kwargs):\n                try:\n                    if isinstance(getattr(self.__class__, prop), property) or prop.startswith('_'):\n                        setattr(self, prop, kwargs.pop(prop))\n                except AttributeError:\n                    pass\n            if kwargs:\n                raise TypeError(\n                    \"'{}' is an invalid keyword argument for {}\".format(\n                        list(kwargs)[0],\n                        self.__class__.__name__\n                    )\n                )\n", 
    "graphene.types.options": "import inspect\n\nfrom ..utils.props import props\n\n\nclass Options(object):\n    '''\n    This is the class wrapper around Meta.\n    It helps to validate and cointain the attributes inside\n    '''\n\n    def __init__(self, meta=None, **defaults):\n        if meta:\n            assert inspect.isclass(meta), (\n                'Meta have to be a class, received \"{}\".'.format(repr(meta))\n            )\n\n        meta_attrs = props(meta) if meta else {}\n        for attr_name, value in defaults.items():\n            if attr_name in meta_attrs:\n                value = meta_attrs.pop(attr_name)\n            elif hasattr(meta, attr_name):\n                value = getattr(meta, attr_name)\n            setattr(self, attr_name, value)\n\n        # If meta_attrs is not empty, it implicit means\n        # it received invalid attributes\n        if meta_attrs:\n            raise TypeError(\n                \"Invalid attributes: {}\".format(\n                    ','.join(meta_attrs.keys())\n                )\n            )\n\n    def __repr__(self):\n        return '<Meta \\n{} >'.format(props(self))\n", 
    "graphene.types.scalars": "import six\n\nfrom graphql.language.ast import (BooleanValue, FloatValue, IntValue,\n                                  StringValue)\n\nfrom ..utils.is_base_type import is_base_type\nfrom .options import Options\nfrom .unmountedtype import UnmountedType\n\n\nclass ScalarTypeMeta(type):\n\n    def __new__(cls, name, bases, attrs):\n        # Also ensure initialization is only performed for subclasses of Model\n        # (excluding Model class itself).\n        if not is_base_type(bases, ScalarTypeMeta):\n            return type.__new__(cls, name, bases, attrs)\n\n        options = Options(\n            attrs.pop('Meta', None),\n            name=name,\n            description=attrs.get('__doc__'),\n        )\n\n        return type.__new__(cls, name, bases, dict(attrs, _meta=options))\n\n    def __str__(cls):  # noqa: N802\n        return cls._meta.name\n\n\nclass Scalar(six.with_metaclass(ScalarTypeMeta, UnmountedType)):\n    '''\n    Scalar Type Definition\n\n    The leaf values of any request and input values to arguments are\n    Scalars (or Enums) and are defined with a name and a series of functions\n    used to parse input from ast or variables and to ensure validity.\n    '''\n\n    serialize = None\n    parse_value = None\n    parse_literal = None\n\n    @classmethod\n    def get_type(cls):\n        return cls\n\n# As per the GraphQL Spec, Integers are only treated as valid when a valid\n# 32-bit signed integer, providing the broadest support across platforms.\n#\n# n.b. JavaScript's integers are safe between -(2^53 - 1) and 2^53 - 1 because\n# they are internally represented as IEEE 754 doubles.\nMAX_INT = 2147483647\nMIN_INT = -2147483648\n\n\nclass Int(Scalar):\n    '''\n    The `Int` scalar type represents non-fractional signed whole numeric\n    values. Int can represent values between -(2^53 - 1) and 2^53 - 1 since\n    represented in JSON as double-precision floating point numbers specified\n    by [IEEE 754](http://en.wikipedia.org/wiki/IEEE_floating_point).\n    '''\n\n    @staticmethod\n    def coerce_int(value):\n        try:\n            num = int(value)\n        except ValueError:\n            try:\n                num = int(float(value))\n            except ValueError:\n                return None\n        if MIN_INT <= num <= MAX_INT:\n            return num\n\n    serialize = coerce_int\n    parse_value = coerce_int\n\n    @staticmethod\n    def parse_literal(ast):\n        if isinstance(ast, IntValue):\n            num = int(ast.value)\n            if MIN_INT <= num <= MAX_INT:\n                return num\n\n\nclass Float(Scalar):\n    '''\n    The `Float` scalar type represents signed double-precision fractional\n    values as specified by\n    [IEEE 754](http://en.wikipedia.org/wiki/IEEE_floating_point).\n    '''\n\n    @staticmethod\n    def coerce_float(value):\n        try:\n            return float(value)\n        except ValueError:\n            return None\n\n    serialize = coerce_float\n    parse_value = coerce_float\n\n    @staticmethod\n    def parse_literal(ast):\n        if isinstance(ast, (FloatValue, IntValue)):\n            return float(ast.value)\n\n\nclass String(Scalar):\n    '''\n    The `String` scalar type represents textual data, represented as UTF-8\n    character sequences. The String type is most often used by GraphQL to\n    represent free-form human-readable text.\n    '''\n\n    @staticmethod\n    def coerce_string(value):\n        if isinstance(value, bool):\n            return u'true' if value else u'false'\n        return six.text_type(value)\n\n    serialize = coerce_string\n    parse_value = coerce_string\n\n    @staticmethod\n    def parse_literal(ast):\n        if isinstance(ast, StringValue):\n            return ast.value\n\n\nclass Boolean(Scalar):\n    '''\n    The `Boolean` scalar type represents `true` or `false`.\n    '''\n\n    serialize = bool\n    parse_value = bool\n\n    @staticmethod\n    def parse_literal(ast):\n        if isinstance(ast, BooleanValue):\n            return ast.value\n\n\nclass ID(Scalar):\n    '''\n    The `ID` scalar type represents a unique identifier, often used to\n    refetch an object or as key for a cache. The ID type appears in a JSON\n    response as a String; however, it is not intended to be human-readable.\n    When expected as an input type, any string (such as `\"4\"`) or integer\n    (such as `4`) input value will be accepted as an ID.\n    '''\n\n    serialize = str\n    parse_value = str\n\n    @staticmethod\n    def parse_literal(ast):\n        if isinstance(ast, (StringValue, IntValue)):\n            return ast.value\n", 
    "graphene.types.schema": "\nfrom graphql import GraphQLSchema, graphql, is_type\nfrom graphql.type.directives import (GraphQLDirective, GraphQLIncludeDirective,\n                                     GraphQLSkipDirective)\nfrom graphql.type.introspection import IntrospectionSchema\nfrom graphql.utils.introspection_query import introspection_query\nfrom graphql.utils.schema_printer import print_schema\n\nfrom .typemap import TypeMap, is_graphene_type\n\n\nclass Schema(GraphQLSchema):\n    '''\n    Schema Definition\n\n    A Schema is created by supplying the root types of each type of operation,\n    query and mutation (optional).\n    '''\n\n    def __init__(self, query=None, mutation=None, subscription=None,\n                 directives=None, types=None, auto_camelcase=True):\n        self._query = query\n        self._mutation = mutation\n        self._subscription = subscription\n        self.types = types\n        self.auto_camelcase = auto_camelcase\n        if directives is None:\n            directives = [\n                GraphQLIncludeDirective,\n                GraphQLSkipDirective\n            ]\n\n        assert all(isinstance(d, GraphQLDirective) for d in directives), \\\n            'Schema directives must be List[GraphQLDirective] if provided but got: {}.'.format(\n                directives\n        )\n        self._directives = directives\n        self.build_typemap()\n\n    def get_query_type(self):\n        return self.get_graphql_type(self._query)\n\n    def get_mutation_type(self):\n        return self.get_graphql_type(self._mutation)\n\n    def get_subscription_type(self):\n        return self.get_graphql_type(self._subscription)\n\n    def get_graphql_type(self, _type):\n        if not _type:\n            return _type\n        if is_type(_type):\n            return _type\n        if is_graphene_type(_type):\n            graphql_type = self.get_type(_type._meta.name)\n            assert graphql_type, \"Type {} not found in this schema.\".format(_type._meta.name)\n            assert graphql_type.graphene_type == _type\n            return graphql_type\n        raise Exception(\"{} is not a valid GraphQL type.\".format(_type))\n\n    def execute(self, *args, **kwargs):\n        return graphql(self, *args, **kwargs)\n\n    def register(self, object_type):\n        self.types.append(object_type)\n\n    def introspect(self):\n        return self.execute(introspection_query).data\n\n    def __str__(self):\n        return print_schema(self)\n\n    def lazy(self, _type):\n        return lambda: self.get_type(_type)\n\n    def build_typemap(self):\n        initial_types = [\n            self._query,\n            self._mutation,\n            self._subscription,\n            IntrospectionSchema\n        ]\n        if self.types:\n            initial_types += self.types\n        self._type_map = TypeMap(initial_types, auto_camelcase=self.auto_camelcase)\n", 
    "graphene.types.structures": "from .unmountedtype import UnmountedType\n\n\nclass Structure(UnmountedType):\n    '''\n    A structure is a GraphQL type instance that\n    wraps a main type with certain structure.\n    '''\n\n    def __init__(self, of_type, *args, **kwargs):\n        super(Structure, self).__init__(*args, **kwargs)\n        self.of_type = of_type\n\n    def get_type(self):\n        return self\n\n\nclass List(Structure):\n    '''\n    List Modifier\n\n    A list is a kind of type marker, a wrapping type which points to another\n    type. Lists are often created within the context of defining the fields of\n    an object type.\n    '''\n\n    def __str__(self):\n        return '[{}]'.format(self.of_type)\n\n\nclass NonNull(Structure):\n    '''\n    Non-Null Modifier\n\n    A non-null is a kind of type marker, a wrapping type which points to another\n    type. Non-null types enforce that their values are never null and can ensure\n    an error is raised if this ever occurs during a request. It is useful for\n    fields which you can make a strong guarantee on non-nullability, for example\n    usually the id field of a database row will never be null.\n\n    Note: the enforcement of non-nullability occurs within the executor.\n    '''\n\n    def __init__(self, *args, **kwargs):\n        super(NonNull, self).__init__(*args, **kwargs)\n        assert not isinstance(self.of_type, NonNull), (\n            'Can only create NonNull of a Nullable GraphQLType but got: {}.'\n        ).format(type)\n\n    def __str__(self):\n        return '{}!'.format(self.of_type)\n", 
    "graphene.types.typemap": "import inspect\nfrom collections import OrderedDict\nfrom functools import partial\n\nfrom graphql import (GraphQLArgument, GraphQLBoolean, GraphQLField,\n                     GraphQLFloat, GraphQLID, GraphQLInputObjectField,\n                     GraphQLInt, GraphQLList, GraphQLNonNull, GraphQLString)\nfrom graphql.type import GraphQLEnumValue\nfrom graphql.type.typemap import GraphQLTypeMap\n\nfrom ..utils.str_converters import to_camel_case\nfrom .dynamic import Dynamic\nfrom .enum import Enum\nfrom .inputobjecttype import InputObjectType\nfrom .interface import Interface\nfrom .objecttype import ObjectType\nfrom .scalars import ID, Boolean, Float, Int, Scalar, String\nfrom .structures import List, NonNull\nfrom .union import Union\n\n\ndef is_graphene_type(_type):\n    if isinstance(_type, (List, NonNull)):\n        return True\n    if inspect.isclass(_type) and issubclass(_type, (ObjectType, InputObjectType, Scalar, Interface, Union, Enum)):\n        return True\n\n\ndef resolve_type(resolve_type_func, map, root, args, info):\n    _type = resolve_type_func(root, args, info)\n    # assert inspect.isclass(_type) and issubclass(_type, ObjectType), (\n    #     'Received incompatible type \"{}\".'.format(_type)\n    # )\n    if inspect.isclass(_type) and issubclass(_type, ObjectType):\n        graphql_type = map.get(_type._meta.name)\n        assert graphql_type and graphql_type.graphene_type == _type\n        return graphql_type\n    return _type\n\n\nclass TypeMap(GraphQLTypeMap):\n\n    def __init__(self, types, auto_camelcase=True):\n        self.auto_camelcase = auto_camelcase\n        super(TypeMap, self).__init__(types)\n\n    def reducer(self, map, type):\n        if not type:\n            return map\n        if inspect.isfunction(type):\n            type = type()\n        if is_graphene_type(type):\n            return self.graphene_reducer(map, type)\n        return GraphQLTypeMap.reducer(map, type)\n\n    def graphene_reducer(self, map, type):\n        if isinstance(type, (List, NonNull)):\n            return self.reducer(map, type.of_type)\n        if type._meta.name in map:\n            _type = map[type._meta.name]\n            if is_graphene_type(_type):\n                assert _type.graphene_type == type\n            return map\n        if issubclass(type, ObjectType):\n            return self.construct_objecttype(map, type)\n        if issubclass(type, InputObjectType):\n            return self.construct_inputobjecttype(map, type)\n        if issubclass(type, Interface):\n            return self.construct_interface(map, type)\n        if issubclass(type, Scalar):\n            return self.construct_scalar(map, type)\n        if issubclass(type, Enum):\n            return self.construct_enum(map, type)\n        if issubclass(type, Union):\n            return self.construct_union(map, type)\n        return map\n\n    def construct_scalar(self, map, type):\n        from .definitions import GrapheneScalarType\n        _scalars = {\n            String: GraphQLString,\n            Int: GraphQLInt,\n            Float: GraphQLFloat,\n            Boolean: GraphQLBoolean,\n            ID: GraphQLID\n        }\n        if type in _scalars:\n            map[type._meta.name] = _scalars[type]\n        else:\n            map[type._meta.name] = GrapheneScalarType(\n                graphene_type=type,\n                name=type._meta.name,\n                description=type._meta.description,\n\n                serialize=getattr(type, 'serialize', None),\n                parse_value=getattr(type, 'parse_value', None),\n                parse_literal=getattr(type, 'parse_literal', None),\n            )\n        return map\n\n    def construct_enum(self, map, type):\n        from .definitions import GrapheneEnumType\n        values = OrderedDict()\n        for name, value in type._meta.enum.__members__.items():\n            values[name] = GraphQLEnumValue(\n                name=name,\n                value=value.value,\n                description=getattr(value, 'description', None),\n                deprecation_reason=getattr(value, 'deprecation_reason', None)\n            )\n        map[type._meta.name] = GrapheneEnumType(\n            graphene_type=type,\n            values=values,\n            name=type._meta.name,\n            description=type._meta.description,\n        )\n        return map\n\n    def construct_objecttype(self, map, type):\n        from .definitions import GrapheneObjectType\n        if type._meta.name in map:\n            _type = map[type._meta.name]\n            if is_graphene_type(_type):\n                assert _type.graphene_type == type\n            return map\n        map[type._meta.name] = GrapheneObjectType(\n            graphene_type=type,\n            name=type._meta.name,\n            description=type._meta.description,\n            fields=None,\n            is_type_of=type.is_type_of,\n            interfaces=None\n        )\n        interfaces = []\n        for i in type._meta.interfaces:\n            map = self.reducer(map, i)\n            interfaces.append(map[i._meta.name])\n        map[type._meta.name]._provided_interfaces = interfaces\n        map[type._meta.name]._fields = self.construct_fields_for_type(map, type)\n        # self.reducer(map, map[type._meta.name])\n        return map\n\n    def construct_interface(self, map, type):\n        from .definitions import GrapheneInterfaceType\n        _resolve_type = None\n        if type.resolve_type:\n            _resolve_type = partial(resolve_type, type.resolve_type, map)\n        map[type._meta.name] = GrapheneInterfaceType(\n            graphene_type=type,\n            name=type._meta.name,\n            description=type._meta.description,\n            fields=None,\n            resolve_type=_resolve_type,\n        )\n        map[type._meta.name]._fields = self.construct_fields_for_type(map, type)\n        # self.reducer(map, map[type._meta.name])\n        return map\n\n    def construct_inputobjecttype(self, map, type):\n        from .definitions import GrapheneInputObjectType\n        map[type._meta.name] = GrapheneInputObjectType(\n            graphene_type=type,\n            name=type._meta.name,\n            description=type._meta.description,\n            fields=None,\n        )\n        map[type._meta.name]._fields = self.construct_fields_for_type(map, type, is_input_type=True)\n        return map\n\n    def construct_union(self, map, type):\n        from .definitions import GrapheneUnionType\n        _resolve_type = None\n        if type.resolve_type:\n            _resolve_type = partial(resolve_type, type.resolve_type, map)\n        types = []\n        for i in type._meta.types:\n            map = self.construct_objecttype(map, i)\n            types.append(map[i._meta.name])\n        map[type._meta.name] = GrapheneUnionType(\n            graphene_type=type,\n            name=type._meta.name,\n            types=types,\n            resolve_type=_resolve_type,\n        )\n        map[type._meta.name].types = types\n        return map\n\n    def get_name(self, name):\n        if self.auto_camelcase:\n            return to_camel_case(name)\n        return name\n\n    def default_resolver(self, attname, default_value, root, *_):\n        return getattr(root, attname, default_value)\n\n    def construct_fields_for_type(self, map, type, is_input_type=False):\n        fields = OrderedDict()\n        for name, field in type._meta.fields.items():\n            if isinstance(field, Dynamic):\n                field = field.get_type()\n                if not field:\n                    continue\n            map = self.reducer(map, field.type)\n            field_type = self.get_field_type(map, field.type)\n            if is_input_type:\n                _field = GraphQLInputObjectField(\n                    field_type,\n                    default_value=field.default_value,\n                    out_name=field.name or name,\n                    description=field.description\n                )\n            else:\n                args = OrderedDict()\n                for arg_name, arg in field.args.items():\n                    map = self.reducer(map, arg.type)\n                    arg_type = self.get_field_type(map, arg.type)\n                    processed_arg_name = arg.name or self.get_name(arg_name)\n                    args[processed_arg_name] = GraphQLArgument(\n                        arg_type,\n                        out_name=arg.name or arg_name,\n                        description=arg.description,\n                        default_value=arg.default_value\n                    )\n                _field = GraphQLField(\n                    field_type,\n                    args=args,\n                    resolver=field.get_resolver(self.get_resolver_for_type(type, name, field.default_value)),\n                    deprecation_reason=field.deprecation_reason,\n                    description=field.description\n                )\n            field_name = field.name or self.get_name(name)\n            fields[field_name] = _field\n        return fields\n\n    def get_resolver_for_type(self, type, name, default_value):\n        if not issubclass(type, ObjectType):\n            return\n        resolver = getattr(type, 'resolve_{}'.format(name), None)\n        if not resolver:\n            # If we don't find the resolver in the ObjectType class, then try to\n            # find it in each of the interfaces\n            interface_resolver = None\n            for interface in type._meta.interfaces:\n                if name not in interface._meta.fields:\n                    continue\n                interface_resolver = getattr(interface, 'resolve_{}'.format(name), None)\n                if interface_resolver:\n                    break\n            resolver = interface_resolver\n        # Only if is not decorated with classmethod\n        if resolver:\n            if not getattr(resolver, '__self__', True):\n                return resolver.__func__\n            return resolver\n\n        return partial(self.default_resolver, name, default_value)\n\n    def get_field_type(self, map, type):\n        if isinstance(type, List):\n            return GraphQLList(self.get_field_type(map, type.of_type))\n        if isinstance(type, NonNull):\n            return GraphQLNonNull(self.get_field_type(map, type.of_type))\n        if inspect.isfunction(type):\n            type = type()\n        return map.get(type._meta.name)\n", 
    "graphene.types.union": "import six\n\nfrom ..utils.is_base_type import is_base_type\nfrom .options import Options\n\n\nclass UnionMeta(type):\n\n    def __new__(cls, name, bases, attrs):\n        # Also ensure initialization is only performed for subclasses of\n        # Union\n        if not is_base_type(bases, UnionMeta):\n            return type.__new__(cls, name, bases, attrs)\n\n        options = Options(\n            attrs.pop('Meta', None),\n            name=name,\n            description=attrs.get('__doc__'),\n            types=(),\n        )\n\n        assert (\n            isinstance(options.types, (list, tuple)) and\n            len(options.types) > 0\n        ), 'Must provide types for Union {}.'.format(options.name)\n\n        return type.__new__(cls, name, bases, dict(attrs, _meta=options))\n\n    def __str__(cls):  # noqa: N805\n        return cls._meta.name\n\n\nclass Union(six.with_metaclass(UnionMeta)):\n    '''\n    Union Type Definition\n\n    When a field can return one of a heterogeneous set of types, a Union type\n    is used to describe what types are possible as well as providing a function\n    to determine which type is actually used when the field is resolved.\n    '''\n\n    resolve_type = None\n\n    def __init__(self, *args, **kwargs):\n        raise Exception(\"An Union cannot be intitialized\")\n", 
    "graphene.types.unmountedtype": "from ..utils.orderedtype import OrderedType\n\n\nclass UnmountedType(OrderedType):\n    '''\n    This class acts a proxy for a Graphene Type, so it can be mounted\n    dynamically as Field, InputField or Argument.\n\n    Instead of writing\n    >>> class MyObjectType(ObjectType):\n    >>>     my_field = Field(String(), description='Description here')\n\n    It let you write\n    >>> class MyObjectType(ObjectType):\n    >>>     my_field = String(description='Description here')\n    '''\n\n    def __init__(self, *args, **kwargs):\n        super(UnmountedType, self).__init__()\n        self.args = args\n        self.kwargs = kwargs\n\n    def get_type(self):\n        raise NotImplementedError(\"get_type not implemented in {}\".format(self))\n\n    def Field(self):  # noqa: N802\n        '''\n        Mount the UnmountedType as Field\n        '''\n        from .field import Field\n        return Field(\n            self.get_type(),\n            *self.args,\n            _creation_counter=self.creation_counter,\n            **self.kwargs\n        )\n\n    def InputField(self):  # noqa: N802\n        '''\n        Mount the UnmountedType as InputField\n        '''\n        from .inputfield import InputField\n        return InputField(\n            self.get_type(),\n            *self.args,\n            _creation_counter=self.creation_counter,\n            **self.kwargs\n        )\n\n    def Argument(self):  # noqa: N802\n        '''\n        Mount the UnmountedType as Argument\n        '''\n        from .argument import Argument\n        return Argument(\n            self.get_type(),\n            *self.args,\n            _creation_counter=self.creation_counter,\n            **self.kwargs\n        )\n\n    def __eq__(self, other):\n        return (\n            self is other or (\n                isinstance(other, UnmountedType) and\n                self.get_type() == other.get_type() and\n                self.args == other.args and\n                self.kwargs == other.kwargs\n            )\n        )\n", 
    "graphene.types.utils": "from collections import OrderedDict\n\nfrom .dynamic import Dynamic\nfrom .field import Field\nfrom .inputfield import InputField\nfrom .unmountedtype import UnmountedType\n\n\ndef merge(*dicts):\n    '''\n    Merge the dicts into one\n    '''\n    merged = OrderedDict()\n    for _dict in dicts:\n        merged.update(_dict)\n    return merged\n\n\ndef get_base_fields(bases, _as=None):\n    '''\n    Get all the fields in the given bases\n    '''\n    fields = OrderedDict()\n    from ..types import AbstractType, Interface\n    # We allow inheritance in AbstractTypes and Interfaces but not ObjectTypes\n    inherited_bases = (AbstractType, Interface)\n    for base in bases:\n        if base in inherited_bases or not issubclass(base, inherited_bases):\n            continue\n        for name, field in base._meta.fields.items():\n            if name in fields:\n                continue\n            fields[name] = get_field_as(field, _as=_as)\n\n    return fields\n\n\ndef mount_as(unmounted_field, _as):\n    '''\n    Mount the UnmountedType dinamically as Field or InputField\n    '''\n    if _as is None:\n        return unmounted_field\n\n    elif _as is Field:\n        return unmounted_field.Field()\n\n    elif _as is InputField:\n        return unmounted_field.InputField()\n\n    raise Exception(\n        'Unmounted field \"{}\" cannot be mounted in {}.'.format(\n            unmounted_field, _as\n        )\n    )\n\n\ndef get_field_as(value, _as=None):\n    '''\n    Get type mounted\n    '''\n    if isinstance(value, (Field, InputField, Dynamic)):\n        return value\n    elif isinstance(value, UnmountedType):\n        return mount_as(value, _as)\n\n\ndef yank_fields_from_attrs(attrs, _as=None, delete=True, sort=True):\n    '''\n    Extract all the fields in given attributes (dict)\n    and return them ordered\n    '''\n    fields_with_names = []\n    for attname, value in list(attrs.items()):\n        field = get_field_as(value, _as)\n        if not field:\n            continue\n        fields_with_names.append((attname, field))\n        if delete:\n            del attrs[attname]\n\n    if sort:\n        fields_with_names = sorted(fields_with_names, key=lambda f: f[1])\n    return OrderedDict(fields_with_names)\n", 
    "graphene.utils.__init__": "", 
    "graphene.utils.is_base_type": "\ndef is_base_type(bases, _type):\n    return any(b for b in bases if isinstance(b, _type))\n", 
    "graphene.utils.orderedtype": "from functools import total_ordering\n\n\n@total_ordering\nclass OrderedType(object):\n    creation_counter = 1\n\n    def __init__(self, _creation_counter=None):\n        self.creation_counter = _creation_counter or self.gen_counter()\n\n    @staticmethod\n    def gen_counter():\n        counter = OrderedType.creation_counter\n        OrderedType.creation_counter += 1\n        return counter\n\n    def reset_counter(self):\n        self.creation_counter = self.gen_counter()\n\n    def __eq__(self, other):\n        # Needed for @total_ordering\n        if isinstance(self, type(other)):\n            return self.creation_counter == other.creation_counter\n        return NotImplemented\n\n    def __lt__(self, other):\n        # This is needed because bisect does not take a comparison function.\n        if isinstance(other, OrderedType):\n            return self.creation_counter < other.creation_counter\n        return NotImplemented\n\n    def __gt__(self, other):\n        # This is needed because bisect does not take a comparison function.\n        if isinstance(other, OrderedType):\n            return self.creation_counter > other.creation_counter\n        return NotImplemented\n\n    def __hash__(self):\n        return hash((self.creation_counter))\n", 
    "graphene.utils.props": "class _OldClass:\n    pass\n\n\nclass _NewClass(object):\n    pass\n\n\n_all_vars = set(dir(_OldClass) + dir(_NewClass))\n\n\ndef props(x):\n    return {\n        key: value for key, value in vars(x).items() if key not in _all_vars\n    }\n", 
    "graphene.utils.resolve_only_args": "from functools import wraps\n\n\ndef resolve_only_args(func):\n    @wraps(func)\n    def inner(root, args, context, info):\n        return func(root, **args)\n    return inner\n", 
    "graphene.utils.str_converters": "import re\n\n\n# From this response in Stackoverflow\n# http://stackoverflow.com/a/19053800/1072990\ndef to_camel_case(snake_str):\n    components = snake_str.split('_')\n    # We capitalize the first letter of each component except the first one\n    # with the 'title' method and join them together.\n    return components[0] + \"\".join(x.title() if x else '_' for x in components[1:])\n\n\n# From this response in Stackoverflow\n# http://stackoverflow.com/a/1176023/1072990\ndef to_snake_case(name):\n    s1 = re.sub('(.)([A-Z][a-z]+)', r'\\1_\\2', name)\n    return re.sub('([a-z0-9])([A-Z])', r'\\1_\\2', s1).lower()\n\n\ndef to_const(string):\n    return re.sub('[\\W|^]+', '_', string).upper()\n", 
    "graphql.__init__": "'''\nGraphQL.js provides a reference implementation for the GraphQL specification\nbut is also a useful utility for operating on GraphQL files and building\nsophisticated tools.\n\nThis primary module exports a general purpose function for fulfilling all\nsteps of the GraphQL specification in a single operation, but also includes\nutilities for every part of the GraphQL specification:\n\n  - Parsing the GraphQL language.\n  - Building a GraphQL type schema.\n  - Validating a GraphQL request against a type schema.\n  - Executing a GraphQL request against a type schema.\n\nThis also includes utility functions for operating on GraphQL types and\nGraphQL documents to facilitate building tools.\n\nYou may also import from each sub-directory directly. For example, the\nfollowing two import statements are equivalent:\n\n    from graphql import parse\n    from graphql.language.base import parse\n'''\nfrom .pyutils.version import get_version\n\n\ntry:\n    # This variable is injected in the __builtins__ by the build\n    # process. It used to enable importing subpackages when\n    # the required packages are not installed\n    __GRAPHQL_SETUP__\nexcept NameError:\n    __GRAPHQL_SETUP__ = False\n\n\nVERSION = (1, 0, 0, 'final', 0)\n\n__version__ = get_version(VERSION)\n\n\nif not __GRAPHQL_SETUP__:\n    # The primary entry point into fulfilling a GraphQL request.\n    from .graphql import (\n        graphql\n    )\n\n    # Create and operate on GraphQL type definitions and schema.\n    from .type import (  # no import order\n        GraphQLSchema,\n\n        # Definitions\n        GraphQLScalarType,\n        GraphQLObjectType,\n        GraphQLInterfaceType,\n        GraphQLUnionType,\n        GraphQLEnumType,\n        GraphQLInputObjectType,\n        GraphQLList,\n        GraphQLNonNull,\n        GraphQLField,\n        GraphQLInputObjectField,\n        GraphQLArgument,\n\n        # \"Enum\" of Type Kinds\n        TypeKind,\n\n        # \"Enum\" of Directive locations\n        DirectiveLocation,\n\n        # Scalars\n        GraphQLInt,\n        GraphQLFloat,\n        GraphQLString,\n        GraphQLBoolean,\n        GraphQLID,\n\n        # Directive definition\n        GraphQLDirective,\n\n        # Built-in directives defined by the Spec\n        specified_directives,\n        GraphQLSkipDirective,\n        GraphQLIncludeDirective,\n        GraphQLDeprecatedDirective,\n\n        # Constant Deprecation Reason\n        DEFAULT_DEPRECATION_REASON,\n\n        # GraphQL Types for introspection.\n        __Schema,\n        __Directive,\n        __DirectiveLocation,\n        __Type,\n        __Field,\n        __InputValue,\n        __EnumValue,\n        __TypeKind,\n\n        # Meta-field definitions.\n        SchemaMetaFieldDef,\n        TypeMetaFieldDef,\n        TypeNameMetaFieldDef,\n\n        # Predicates\n        is_type,\n        is_input_type,\n        is_output_type,\n        is_leaf_type,\n        is_composite_type,\n        is_abstract_type,\n\n        # Un-modifiers\n        get_nullable_type,\n        get_named_type,\n    )\n\n    # Parse and operate on GraphQL language source files.\n    from .language.base import (  # no import order\n        Source,\n        get_location,\n\n        # Parse\n        parse,\n        parse_value,\n\n        # Print\n        print_ast,\n\n        # Visit\n        visit,\n        ParallelVisitor,\n        TypeInfoVisitor,\n        BREAK,\n    )\n\n    # Execute GraphQL queries.\n    from .execution import (  # no import order\n        execute,\n        MiddlewareManager,\n        middlewares\n    )\n\n    # Validate GraphQL queries.\n    from .validation import (  # no import order\n        validate,\n        specified_rules,\n    )\n\n    # Create and format GraphQL errors.\n    from .error import (\n        GraphQLError,\n        format_error,\n    )\n\n    # Utilities for operating on GraphQL type schema and parsed sources.\n    from .utils.base import (\n        # The GraphQL query recommended for a full schema introspection.\n        introspection_query,\n\n        # Gets the target Operation from a Document\n        get_operation_ast,\n\n        # Build a GraphQLSchema from an introspection result.\n        build_client_schema,\n\n        # Build a GraphQLSchema from a parsed GraphQL Schema language AST.\n        build_ast_schema,\n\n        # Extends an existing GraphQLSchema from a parsed GraphQL Schema\n        # language AST.\n        extend_schema,\n\n        # Print a GraphQLSchema to GraphQL Schema language.\n        print_schema,\n\n        # Create a GraphQLType from a GraphQL language AST.\n        type_from_ast,\n\n        # Create a JavaScript value from a GraphQL language AST.\n        value_from_ast,\n\n        # Create a GraphQL language AST from a JavaScript value.\n        ast_from_value,\n\n        # A helper to use within recursive-descent visitors which need to be aware of\n        # the GraphQL type system.\n        TypeInfo,\n\n        # Determine if JavaScript values adhere to a GraphQL type.\n        is_valid_value,\n\n        # Determine if AST values adhere to a GraphQL type.\n        is_valid_literal_value,\n\n        # Concatenates multiple AST together.\n        concat_ast,\n\n        # Comparators for types\n        is_equal_type,\n        is_type_sub_type_of,\n        do_types_overlap,\n\n        # Asserts a string is a valid GraphQL name.\n        assert_valid_name,\n    )\n\n    __all__ = (\n        'graphql',\n        'GraphQLBoolean',\n        'GraphQLEnumType',\n        'GraphQLFloat',\n        'GraphQLID',\n        'GraphQLInputObjectType',\n        'GraphQLInt',\n        'GraphQLInterfaceType',\n        'GraphQLList',\n        'GraphQLNonNull',\n        'GraphQLField',\n        'GraphQLInputObjectField',\n        'GraphQLArgument',\n        'GraphQLObjectType',\n        'GraphQLScalarType',\n        'GraphQLSchema',\n        'GraphQLString',\n        'GraphQLUnionType',\n        'GraphQLDirective',\n        'specified_directives',\n        'GraphQLSkipDirective',\n        'GraphQLIncludeDirective',\n        'GraphQLDeprecatedDirective',\n        'DEFAULT_DEPRECATION_REASON',\n        'TypeKind',\n        'DirectiveLocation',\n        '__Schema',\n        '__Directive',\n        '__DirectiveLocation',\n        '__Type',\n        '__Field',\n        '__InputValue',\n        '__EnumValue',\n        '__TypeKind',\n        'SchemaMetaFieldDef',\n        'TypeMetaFieldDef',\n        'TypeNameMetaFieldDef',\n        'get_named_type',\n        'get_nullable_type',\n        'is_abstract_type',\n        'is_composite_type',\n        'is_input_type',\n        'is_leaf_type',\n        'is_output_type',\n        'is_type',\n        'BREAK',\n        'ParallelVisitor',\n        'Source',\n        'TypeInfoVisitor',\n        'get_location',\n        'parse',\n        'parse_value',\n        'print_ast',\n        'visit',\n        'execute',\n        'MiddlewareManager',\n        'middlewares',\n        'specified_rules',\n        'validate',\n        'GraphQLError',\n        'format_error',\n        'TypeInfo',\n        'assert_valid_name',\n        'ast_from_value',\n        'build_ast_schema',\n        'build_client_schema',\n        'concat_ast',\n        'do_types_overlap',\n        'extend_schema',\n        'get_operation_ast',\n        'introspection_query',\n        'is_equal_type',\n        'is_type_sub_type_of',\n        'is_valid_literal_value',\n        'is_valid_value',\n        'print_schema',\n        'type_from_ast',\n        'value_from_ast',\n        'get_version',\n    )\n", 
    "graphql.error.__init__": "from .base import GraphQLError\nfrom .located_error import GraphQLLocatedError\nfrom .syntax_error import GraphQLSyntaxError\nfrom .format_error import format_error\n\n__all__ = ['GraphQLError', 'GraphQLLocatedError', 'GraphQLSyntaxError', 'format_error']\n", 
    "graphql.error.base": "from ..language.location import get_location\n\n\nclass GraphQLError(Exception):\n    __slots__ = 'message', 'nodes', 'stack', 'original_error', '_source', '_positions'\n\n    def __init__(self, message, nodes=None, stack=None, source=None, positions=None):\n        super(GraphQLError, self).__init__(message)\n        self.message = message\n        self.nodes = nodes\n        self.stack = stack\n        self._source = source\n        self._positions = positions\n\n    @property\n    def source(self):\n        if self._source:\n            return self._source\n        if self.nodes:\n            node = self.nodes[0]\n            return node and node.loc and node.loc.source\n\n    @property\n    def positions(self):\n        if self._positions:\n            return self._positions\n        if self.nodes is not None:\n            node_positions = [node.loc and node.loc.start for node in self.nodes]\n            if any(node_positions):\n                return node_positions\n\n    @property\n    def locations(self):\n        source = self.source\n        if self.positions and source:\n            return [get_location(source, pos) for pos in self.positions]\n", 
    "graphql.error.format_error": "def format_error(error):\n    formatted_error = {\n        'message': error.message,\n    }\n    if error.locations is not None:\n        formatted_error['locations'] = [\n            {'line': loc.line, 'column': loc.column}\n            for loc in error.locations\n        ]\n\n    return formatted_error\n", 
    "graphql.error.located_error": "import sys\n\nfrom .base import GraphQLError\n\n__all__ = ['GraphQLLocatedError']\n\n\nclass GraphQLLocatedError(GraphQLError):\n\n    def __init__(self, nodes, original_error=None):\n        if original_error:\n            message = str(original_error)\n        else:\n            message = 'An unknown error occurred.'\n\n        if hasattr(original_error, 'stack'):\n            stack = original_error.stack\n        else:\n            stack = sys.exc_info()[2]\n\n        super(GraphQLLocatedError, self).__init__(\n            message=message,\n            nodes=nodes,\n            stack=stack\n        )\n        self.original_error = original_error\n", 
    "graphql.error.syntax_error": "from ..language.location import get_location\nfrom .base import GraphQLError\n\n__all__ = ['GraphQLSyntaxError']\n\n\nclass GraphQLSyntaxError(GraphQLError):\n\n    def __init__(self, source, position, description):\n        location = get_location(source, position)\n        super(GraphQLSyntaxError, self).__init__(\n            message=u'Syntax Error {} ({}:{}) {}\\n\\n{}'.format(\n                source.name,\n                location.line,\n                location.column,\n                description,\n                highlight_source_at_location(source, location),\n            ),\n            source=source,\n            positions=[position],\n        )\n\n\ndef highlight_source_at_location(source, location):\n    line = location.line\n    lines = source.body.splitlines()\n    pad_len = len(str(line + 1))\n    result = u''\n    format = (u'{:>' + str(pad_len) + '}: {}\\n').format\n    if line >= 2:\n        result += format(line - 1, lines[line - 2])\n    result += format(line, lines[line - 1])\n    result += ' ' * (1 + pad_len + location.column) + '^\\n'\n    if line < len(lines):\n        result += format(line + 1, lines[line])\n    return result\n", 
    "graphql.execution.__init__": "# -*- coding: utf-8 -*-\n\"\"\"\nTerminology\n\n\"Definitions\" are the generic name for top-level statements in the document.\nExamples of this include:\n1) Operations (such as a query)\n2) Fragments\n\n\"Operations\" are a generic name for requests in the document.\nExamples of this include:\n1) query,\n2) mutation\n\n\"Selections\" are the statements that can appear legally and at\nsingle level of the query. These include:\n1) field references e.g \"a\"\n2) fragment \"spreads\" e.g. \"...c\"\n3) inline fragment \"spreads\" e.g. \"...on Type { a }\"\n\"\"\"\nfrom .executor import execute\nfrom .base import ExecutionResult\nfrom .middleware import middlewares, MiddlewareManager\n\n\n__all__ = ['execute', 'ExecutionResult', 'MiddlewareManager', 'middlewares']\n", 
    "graphql.execution.base": "# -*- coding: utf-8 -*-\nfrom ..error import GraphQLError\nfrom ..language import ast\nfrom ..pyutils.default_ordered_dict import DefaultOrderedDict\nfrom ..type.definition import GraphQLInterfaceType, GraphQLUnionType\nfrom ..type.directives import GraphQLIncludeDirective, GraphQLSkipDirective\nfrom ..type.introspection import (SchemaMetaFieldDef, TypeMetaFieldDef,\n                                  TypeNameMetaFieldDef)\nfrom ..utils.type_from_ast import type_from_ast\nfrom .values import get_argument_values, get_variable_values\n\nUndefined = object()\n\n\nclass ExecutionContext(object):\n    \"\"\"Data that must be available at all points during query execution.\n\n    Namely, schema of the type system that is currently executing,\n    and the fragments defined in the query document\"\"\"\n\n    __slots__ = 'schema', 'fragments', 'root_value', 'operation', 'variable_values', 'errors', 'context_value', \\\n                'argument_values_cache', 'executor', 'middleware', '_subfields_cache'\n\n    def __init__(self, schema, document_ast, root_value, context_value, variable_values, operation_name, executor, middleware):\n        \"\"\"Constructs a ExecutionContext object from the arguments passed\n        to execute, which we will pass throughout the other execution\n        methods.\"\"\"\n        errors = []\n        operation = None\n        fragments = {}\n\n        for definition in document_ast.definitions:\n            if isinstance(definition, ast.OperationDefinition):\n                if not operation_name and operation:\n                    raise GraphQLError('Must provide operation name if query contains multiple operations.')\n\n                if not operation_name or definition.name and definition.name.value == operation_name:\n                    operation = definition\n\n            elif isinstance(definition, ast.FragmentDefinition):\n                fragments[definition.name.value] = definition\n\n            else:\n                raise GraphQLError(\n                    u'GraphQL cannot execute a request containing a {}.'.format(definition.__class__.__name__),\n                    definition\n                )\n\n        if not operation:\n            if operation_name:\n                raise GraphQLError(u'Unknown operation named \"{}\".'.format(operation_name))\n\n            else:\n                raise GraphQLError('Must provide an operation.')\n\n        variable_values = get_variable_values(schema, operation.variable_definitions or [], variable_values)\n\n        self.schema = schema\n        self.fragments = fragments\n        self.root_value = root_value\n        self.operation = operation\n        self.variable_values = variable_values\n        self.errors = errors\n        self.context_value = context_value\n        self.argument_values_cache = {}\n        self.executor = executor\n        self.middleware = middleware\n        self._subfields_cache = {}\n\n    def get_field_resolver(self, field_resolver):\n        if not self.middleware:\n            return field_resolver\n        return self.middleware.get_field_resolver(field_resolver)\n\n    def get_argument_values(self, field_def, field_ast):\n        k = field_def, field_ast\n        result = self.argument_values_cache.get(k)\n\n        if not result:\n            result = self.argument_values_cache[k] = get_argument_values(field_def.args, field_ast.arguments,\n                                                                         self.variable_values)\n\n        return result\n\n    def get_sub_fields(self, return_type, field_asts):\n        k = return_type, tuple(field_asts)\n        if k not in self._subfields_cache:\n            subfield_asts = DefaultOrderedDict(list)\n            visited_fragment_names = set()\n            for field_ast in field_asts:\n                selection_set = field_ast.selection_set\n                if selection_set:\n                    subfield_asts = collect_fields(\n                        self, return_type, selection_set,\n                        subfield_asts, visited_fragment_names\n                    )\n            self._subfields_cache[k] = subfield_asts\n        return self._subfields_cache[k]\n\n\nclass ExecutionResult(object):\n    \"\"\"The result of execution. `data` is the result of executing the\n    query, `errors` is null if no errors occurred, and is a\n    non-empty array if an error occurred.\"\"\"\n\n    __slots__ = 'data', 'errors', 'invalid'\n\n    def __init__(self, data=None, errors=None, invalid=False):\n        self.data = data\n        self.errors = errors\n\n        if invalid:\n            assert data is None\n\n        self.invalid = invalid\n\n    def __eq__(self, other):\n        return (\n            self is other or (\n                isinstance(other, ExecutionResult) and\n                self.data == other.data and\n                self.errors == other.errors and\n                self.invalid == other.invalid\n            )\n        )\n\n\ndef get_operation_root_type(schema, operation):\n    op = operation.operation\n    if op == 'query':\n        return schema.get_query_type()\n\n    elif op == 'mutation':\n        mutation_type = schema.get_mutation_type()\n\n        if not mutation_type:\n            raise GraphQLError(\n                'Schema is not configured for mutations',\n                [operation]\n            )\n\n        return mutation_type\n\n    elif op == 'subscription':\n        subscription_type = schema.get_subscription_type()\n\n        if not subscription_type:\n            raise GraphQLError(\n                'Schema is not configured for subscriptions',\n                [operation]\n            )\n\n        return subscription_type\n\n    raise GraphQLError(\n        'Can only execute queries, mutations and subscriptions',\n        [operation]\n    )\n\n\ndef collect_fields(ctx, runtime_type, selection_set, fields, prev_fragment_names):\n    \"\"\"\n    Given a selectionSet, adds all of the fields in that selection to\n    the passed in map of fields, and returns it at the end.\n\n    collect_fields requires the \"runtime type\" of an object. For a field which\n    returns and Interface or Union type, the \"runtime type\" will be the actual\n    Object type returned by that field.\n    \"\"\"\n    for selection in selection_set.selections:\n        directives = selection.directives\n\n        if isinstance(selection, ast.Field):\n            if not should_include_node(ctx, directives):\n                continue\n\n            name = get_field_entry_key(selection)\n            fields[name].append(selection)\n\n        elif isinstance(selection, ast.InlineFragment):\n            if not should_include_node(\n                    ctx, directives) or not does_fragment_condition_match(\n                    ctx, selection, runtime_type):\n                continue\n\n            collect_fields(ctx, runtime_type, selection.selection_set, fields, prev_fragment_names)\n\n        elif isinstance(selection, ast.FragmentSpread):\n            frag_name = selection.name.value\n\n            if frag_name in prev_fragment_names or not should_include_node(ctx, directives):\n                continue\n\n            prev_fragment_names.add(frag_name)\n            fragment = ctx.fragments.get(frag_name)\n            frag_directives = fragment.directives\n            if not fragment or not \\\n                    should_include_node(ctx, frag_directives) or not \\\n                    does_fragment_condition_match(ctx, fragment, runtime_type):\n                continue\n\n            collect_fields(ctx, runtime_type, fragment.selection_set, fields, prev_fragment_names)\n\n    return fields\n\n\ndef should_include_node(ctx, directives):\n    \"\"\"Determines if a field should be included based on the @include and\n    @skip directives, where @skip has higher precidence than @include.\"\"\"\n    # TODO: Refactor based on latest code\n    if directives:\n        skip_ast = None\n\n        for directive in directives:\n            if directive.name.value == GraphQLSkipDirective.name:\n                skip_ast = directive\n                break\n\n        if skip_ast:\n            args = get_argument_values(\n                GraphQLSkipDirective.args,\n                skip_ast.arguments,\n                ctx.variable_values,\n            )\n            if args.get('if') is True:\n                return False\n\n        include_ast = None\n\n        for directive in directives:\n            if directive.name.value == GraphQLIncludeDirective.name:\n                include_ast = directive\n                break\n\n        if include_ast:\n            args = get_argument_values(\n                GraphQLIncludeDirective.args,\n                include_ast.arguments,\n                ctx.variable_values,\n            )\n\n            if args.get('if') is False:\n                return False\n\n    return True\n\n\ndef does_fragment_condition_match(ctx, fragment, type_):\n    type_condition_ast = fragment.type_condition\n    if not type_condition_ast:\n        return True\n\n    conditional_type = type_from_ast(ctx.schema, type_condition_ast)\n    if conditional_type.is_same_type(type_):\n        return True\n\n    if isinstance(conditional_type, (GraphQLInterfaceType, GraphQLUnionType)):\n        return ctx.schema.is_possible_type(conditional_type, type_)\n\n    return False\n\n\ndef get_field_entry_key(node):\n    \"\"\"Implements the logic to compute the key of a given field's entry\"\"\"\n    if node.alias:\n        return node.alias.value\n    return node.name.value\n\n\nclass ResolveInfo(object):\n    __slots__ = ('field_name', 'field_asts', 'return_type', 'parent_type',\n                 'schema', 'fragments', 'root_value', 'operation', 'variable_values')\n\n    def __init__(self, field_name, field_asts, return_type, parent_type,\n                 schema, fragments, root_value, operation, variable_values):\n        self.field_name = field_name\n        self.field_asts = field_asts\n        self.return_type = return_type\n        self.parent_type = parent_type\n        self.schema = schema\n        self.fragments = fragments\n        self.root_value = root_value\n        self.operation = operation\n        self.variable_values = variable_values\n\n\ndef default_resolve_fn(source, args, context, info):\n    \"\"\"If a resolve function is not given, then a default resolve behavior is used which takes the property of the source object\n    of the same name as the field and returns it as the result, or if it's a function, returns the result of calling that function.\"\"\"\n    name = info.field_name\n    property = getattr(source, name, None)\n    if callable(property):\n        return property()\n    return property\n\n\ndef get_field_def(schema, parent_type, field_name):\n    \"\"\"This method looks up the field on the given type defintion.\n    It has special casing for the two introspection fields, __schema\n    and __typename. __typename is special because it can always be\n    queried as a field, even in situations where no other fields\n    are allowed, like on a Union. __schema could get automatically\n    added to the query type, but that would require mutating type\n    definitions, which would cause issues.\"\"\"\n    if field_name == '__schema' and schema.get_query_type() == parent_type:\n        return SchemaMetaFieldDef\n    elif field_name == '__type' and schema.get_query_type() == parent_type:\n        return TypeMetaFieldDef\n    elif field_name == '__typename':\n        return TypeNameMetaFieldDef\n    return parent_type.fields.get(field_name)\n", 
    "graphql.execution.executor": "import collections\nimport functools\nimport logging\nimport sys\n\nfrom promise import Promise, promise_for_dict, promisify\n\nfrom ..error import GraphQLError, GraphQLLocatedError\nfrom ..pyutils.default_ordered_dict import DefaultOrderedDict\nfrom ..pyutils.ordereddict import OrderedDict\nfrom ..type import (GraphQLEnumType, GraphQLInterfaceType, GraphQLList,\n                    GraphQLNonNull, GraphQLObjectType, GraphQLScalarType,\n                    GraphQLSchema, GraphQLUnionType)\nfrom .base import (ExecutionContext, ExecutionResult, ResolveInfo, Undefined,\n                   collect_fields, default_resolve_fn, get_field_def,\n                   get_operation_root_type)\nfrom .executors.sync import SyncExecutor\nfrom .middleware import MiddlewareManager\n\nlogger = logging.getLogger(__name__)\n\n\ndef is_promise(obj):\n    return type(obj) == Promise\n\n\ndef execute(schema, document_ast, root_value=None, context_value=None,\n            variable_values=None, operation_name=None, executor=None,\n            return_promise=False, middleware=None):\n    assert schema, 'Must provide schema'\n    assert isinstance(schema, GraphQLSchema), (\n        'Schema must be an instance of GraphQLSchema. Also ensure that there are ' +\n        'not multiple versions of GraphQL installed in your node_modules directory.'\n    )\n    if middleware:\n        if not isinstance(middleware, MiddlewareManager):\n            middleware = MiddlewareManager(*middleware)\n\n        assert isinstance(middleware, MiddlewareManager), (\n            'middlewares have to be an instance'\n            ' of MiddlewareManager. Received \"{}\".'.format(middleware)\n        )\n\n    if executor is None:\n        executor = SyncExecutor()\n\n    context = ExecutionContext(\n        schema,\n        document_ast,\n        root_value,\n        context_value,\n        variable_values,\n        operation_name,\n        executor,\n        middleware\n    )\n\n    def executor(resolve, reject):\n        return resolve(execute_operation(context, context.operation, root_value))\n\n    def on_rejected(error):\n        context.errors.append(error)\n        return None\n\n    def on_resolve(data):\n        return ExecutionResult(data=data, errors=context.errors)\n\n    promise = Promise(executor).catch(on_rejected).then(on_resolve)\n    if return_promise:\n        return promise\n    context.executor.wait_until_finished()\n    return promise.get()\n\n\ndef execute_operation(exe_context, operation, root_value):\n    type = get_operation_root_type(exe_context.schema, operation)\n    fields = collect_fields(\n        exe_context,\n        type,\n        operation.selection_set,\n        DefaultOrderedDict(list),\n        set()\n    )\n\n    if operation.operation == 'mutation':\n        return execute_fields_serially(exe_context, type, root_value, fields)\n\n    return execute_fields(exe_context, type, root_value, fields)\n\n\ndef execute_fields_serially(exe_context, parent_type, source_value, fields):\n    def execute_field_callback(results, response_name):\n        field_asts = fields[response_name]\n        result = resolve_field(\n            exe_context,\n            parent_type,\n            source_value,\n            field_asts\n        )\n        if result is Undefined:\n            return results\n\n        if is_promise(result):\n            def collect_result(resolved_result):\n                results[response_name] = resolved_result\n                return results\n\n            return promisify(result).then(collect_result, None)\n\n        results[response_name] = result\n        return results\n\n    def execute_field(prev_promise, response_name):\n        return prev_promise.then(lambda results: execute_field_callback(results, response_name))\n\n    return functools.reduce(execute_field, fields.keys(), Promise.resolve(collections.OrderedDict()))\n\n\ndef execute_fields(exe_context, parent_type, source_value, fields):\n    contains_promise = False\n\n    final_results = OrderedDict()\n\n    for response_name, field_asts in fields.items():\n        result = resolve_field(exe_context, parent_type, source_value, field_asts)\n        if result is Undefined:\n            continue\n\n        final_results[response_name] = result\n        if is_promise(result):\n            contains_promise = True\n\n    if not contains_promise:\n        return final_results\n\n    return promise_for_dict(final_results)\n\n\ndef resolve_field(exe_context, parent_type, source, field_asts):\n    field_ast = field_asts[0]\n    field_name = field_ast.name.value\n\n    field_def = get_field_def(exe_context.schema, parent_type, field_name)\n    if not field_def:\n        return Undefined\n\n    return_type = field_def.type\n    resolve_fn = field_def.resolver or default_resolve_fn\n\n    # We wrap the resolve_fn from the middleware\n    resolve_fn_middleware = exe_context.get_field_resolver(resolve_fn)\n\n    # Build a dict of arguments from the field.arguments AST, using the variables scope to\n    # fulfill any variable references.\n    args = exe_context.get_argument_values(field_def, field_ast)\n\n    # The resolve function's optional third argument is a context value that\n    # is provided to every resolve function within an execution. It is commonly\n    # used to represent an authenticated user, or request-specific caches.\n    context = exe_context.context_value\n\n    # The resolve function's optional third argument is a collection of\n    # information about the current execution state.\n    info = ResolveInfo(\n        field_name,\n        field_asts,\n        return_type,\n        parent_type,\n        schema=exe_context.schema,\n        fragments=exe_context.fragments,\n        root_value=exe_context.root_value,\n        operation=exe_context.operation,\n        variable_values=exe_context.variable_values,\n    )\n\n    executor = exe_context.executor\n    result = resolve_or_error(resolve_fn_middleware, source, args, context, info, executor)\n\n    return complete_value_catching_error(\n        exe_context,\n        return_type,\n        field_asts,\n        info,\n        result\n    )\n\n\ndef resolve_or_error(resolve_fn, source, args, context, info, executor):\n    try:\n        return executor.execute(resolve_fn, source, args, context, info)\n    except Exception as e:\n        logger.exception(\"An error occurred while resolving field {}.{}\".format(\n            info.parent_type.name, info.field_name\n        ))\n        e.stack = sys.exc_info()[2]\n        return e\n\n\ndef complete_value_catching_error(exe_context, return_type, field_asts, info, result):\n    # If the field type is non-nullable, then it is resolved without any\n    # protection from errors.\n    if isinstance(return_type, GraphQLNonNull):\n        return complete_value(exe_context, return_type, field_asts, info, result)\n\n    # Otherwise, error protection is applied, logging the error and\n    # resolving a null value for this field if one is encountered.\n    try:\n        completed = complete_value(exe_context, return_type, field_asts, info, result)\n        if is_promise(completed):\n            def handle_error(error):\n                exe_context.errors.append(error)\n                return Promise.fulfilled(None)\n\n            return promisify(completed).then(None, handle_error)\n\n        return completed\n    except Exception as e:\n        exe_context.errors.append(e)\n        return None\n\n\ndef complete_value(exe_context, return_type, field_asts, info, result):\n    \"\"\"\n    Implements the instructions for completeValue as defined in the\n    \"Field entries\" section of the spec.\n\n    If the field type is Non-Null, then this recursively completes the value for the inner type. It throws a field\n    error if that completion returns null, as per the \"Nullability\" section of the spec.\n\n    If the field type is a List, then this recursively completes the value for the inner type on each item in the\n    list.\n\n    If the field type is a Scalar or Enum, ensures the completed value is a legal value of the type by calling the\n    `serialize` method of GraphQL type definition.\n\n    If the field is an abstract type, determine the runtime type of the value and then complete based on that type.\n\n    Otherwise, the field type expects a sub-selection set, and will complete the value by evaluating all\n    sub-selections.\n    \"\"\"\n    # If field type is NonNull, complete for inner type, and throw field error if result is null.\n\n    if is_promise(result):\n        return promisify(result).then(\n            lambda resolved: complete_value(\n                exe_context,\n                return_type,\n                field_asts,\n                info,\n                resolved\n            ),\n            lambda error: Promise.rejected(GraphQLLocatedError(field_asts, original_error=error))\n        )\n\n    # print return_type, type(result)\n    if isinstance(result, Exception):\n        raise GraphQLLocatedError(field_asts, original_error=result)\n\n    if isinstance(return_type, GraphQLNonNull):\n        return complete_nonnull_value(exe_context, return_type, field_asts, info, result)\n\n    # If result is null-like, return null.\n    if result is None:\n        return None\n\n    # If field type is List, complete each item in the list with the inner type\n    if isinstance(return_type, GraphQLList):\n        return complete_list_value(exe_context, return_type, field_asts, info, result)\n\n    # If field type is Scalar or Enum, serialize to a valid value, returning null if coercion is not possible.\n    if isinstance(return_type, (GraphQLScalarType, GraphQLEnumType)):\n        return complete_leaf_value(return_type, result)\n\n    if isinstance(return_type, (GraphQLInterfaceType, GraphQLUnionType)):\n        return complete_abstract_value(exe_context, return_type, field_asts, info, result)\n\n    if isinstance(return_type, GraphQLObjectType):\n        return complete_object_value(exe_context, return_type, field_asts, info, result)\n\n    assert False, u'Cannot complete value of unexpected type \"{}\".'.format(return_type)\n\n\ndef complete_list_value(exe_context, return_type, field_asts, info, result):\n    \"\"\"\n    Complete a list value by completing each item in the list with the inner type\n    \"\"\"\n    assert isinstance(result, collections.Iterable), \\\n        ('User Error: expected iterable, but did not find one ' +\n         'for field {}.{}.').format(info.parent_type, info.field_name)\n\n    item_type = return_type.of_type\n    completed_results = []\n    contains_promise = False\n    for item in result:\n        completed_item = complete_value_catching_error(exe_context, item_type, field_asts, info, item)\n        if not contains_promise and is_promise(completed_item):\n            contains_promise = True\n\n        completed_results.append(completed_item)\n\n    return Promise.all(completed_results) if contains_promise else completed_results\n\n\ndef complete_leaf_value(return_type, result):\n    \"\"\"\n    Complete a Scalar or Enum by serializing to a valid value, returning null if serialization is not possible.\n    \"\"\"\n    # serialize = getattr(return_type, 'serialize', None)\n    # assert serialize, 'Missing serialize method on type'\n\n    return return_type.serialize(result)\n\n\ndef complete_abstract_value(exe_context, return_type, field_asts, info, result):\n    \"\"\"\n    Complete an value of an abstract type by determining the runtime type of that value, then completing based\n    on that type.\n    \"\"\"\n    runtime_type = None\n\n    # Field type must be Object, Interface or Union and expect sub-selections.\n    if isinstance(return_type, (GraphQLInterfaceType, GraphQLUnionType)):\n        if return_type.resolve_type:\n            runtime_type = return_type.resolve_type(result, exe_context.context_value, info)\n        else:\n            runtime_type = get_default_resolve_type_fn(result, exe_context.context_value, info, return_type)\n\n    if not isinstance(runtime_type, GraphQLObjectType):\n        raise GraphQLError(\n            ('Abstract type {} must resolve to an Object type at runtime ' +\n             'for field {}.{} with value \"{}\", received \"{}\".').format(\n                 return_type,\n                 info.parent_type,\n                 info.field_name,\n                 result,\n                 runtime_type,\n                 ),\n            field_asts\n        )\n\n    if not exe_context.schema.is_possible_type(return_type, runtime_type):\n        raise GraphQLError(\n            u'Runtime Object type \"{}\" is not a possible type for \"{}\".'.format(runtime_type, return_type),\n            field_asts\n        )\n\n    return complete_object_value(exe_context, runtime_type, field_asts, info, result)\n\n\ndef get_default_resolve_type_fn(value, context, info, abstract_type):\n    possible_types = info.schema.get_possible_types(abstract_type)\n    for type in possible_types:\n        if callable(type.is_type_of) and type.is_type_of(value, context, info):\n            return type\n\n\ndef complete_object_value(exe_context, return_type, field_asts, info, result):\n    \"\"\"\n    Complete an Object value by evaluating all sub-selections.\n    \"\"\"\n    if return_type.is_type_of and not return_type.is_type_of(result, exe_context.context_value, info):\n        raise GraphQLError(\n            u'Expected value of type \"{}\" but got: {}.'.format(return_type, type(result).__name__),\n            field_asts\n        )\n\n    # Collect sub-fields to execute to complete this value.\n    subfield_asts = exe_context.get_sub_fields(return_type, field_asts)\n    return execute_fields(exe_context, return_type, result, subfield_asts)\n\n\ndef complete_nonnull_value(exe_context, return_type, field_asts, info, result):\n    \"\"\"\n    Complete a NonNull value by completing the inner type\n    \"\"\"\n    completed = complete_value(\n        exe_context, return_type.of_type, field_asts, info, result\n    )\n    if completed is None:\n        raise GraphQLError(\n            'Cannot return null for non-nullable field {}.{}.'.format(info.parent_type, info.field_name),\n            field_asts\n        )\n\n    return completed\n", 
    "graphql.execution.executors.__init__": "", 
    "graphql.execution.executors.sync": "class SyncExecutor(object):\n\n    def wait_until_finished(self):\n        pass\n\n    def execute(self, fn, *args, **kwargs):\n        return fn(*args, **kwargs)\n", 
    "graphql.execution.middleware": "import inspect\nfrom functools import partial\nfrom itertools import chain\n\nfrom promise import Promise\n\nMIDDLEWARE_RESOLVER_FUNCTION = 'resolve'\n\n\nclass MiddlewareManager(object):\n\n    def __init__(self, *middlewares):\n        self.middlewares = middlewares\n        self._middleware_resolvers = list(get_middleware_resolvers(middlewares))\n        self._cached_resolvers = {}\n\n    def get_field_resolver(self, field_resolver):\n        if field_resolver not in self._cached_resolvers:\n            self._cached_resolvers[field_resolver] = middleware_chain(field_resolver, self._middleware_resolvers)\n\n        return self._cached_resolvers[field_resolver]\n\n\nmiddlewares = MiddlewareManager\n\n\ndef get_middleware_resolvers(middlewares):\n    for middleware in middlewares:\n        # If the middleware is a function instead of a class\n        if inspect.isfunction(middleware):\n            yield middleware\n        if not hasattr(middleware, MIDDLEWARE_RESOLVER_FUNCTION):\n            continue\n        yield getattr(middleware, MIDDLEWARE_RESOLVER_FUNCTION)\n\n\ndef middleware_chain(func, middlewares):\n    if not middlewares:\n        return func\n    middlewares = chain((func, make_it_promise), middlewares)\n    last_func = None\n    for middleware in middlewares:\n        last_func = partial(middleware, last_func) if last_func else middleware\n\n    return last_func\n\n\ndef make_it_promise(next, *a, **b):\n    return Promise.resolve(next(*a, **b))\n", 
    "graphql.execution.values": "import collections\nimport json\n\nfrom six import string_types\n\nfrom ..error import GraphQLError\nfrom ..language.printer import print_ast\nfrom ..type import (GraphQLEnumType, GraphQLInputObjectType, GraphQLList,\n                    GraphQLNonNull, GraphQLScalarType, is_input_type)\nfrom ..utils.is_valid_value import is_valid_value\nfrom ..utils.type_from_ast import type_from_ast\nfrom ..utils.value_from_ast import value_from_ast\n\n__all__ = ['get_variable_values', 'get_argument_values']\n\n\ndef get_variable_values(schema, definition_asts, inputs):\n    \"\"\"Prepares an object map of variables of the correct type based on the provided variable definitions and arbitrary input.\n    If the input cannot be parsed to match the variable definitions, a GraphQLError will be thrown.\"\"\"\n    if inputs is None:\n        inputs = {}\n\n    values = {}\n    for def_ast in definition_asts:\n        var_name = def_ast.variable.name.value\n        value = get_variable_value(schema, def_ast, inputs.get(var_name))\n        values[var_name] = value\n\n    return values\n\n\ndef get_argument_values(arg_defs, arg_asts, variables=None):\n    \"\"\"Prepares an object map of argument values given a list of argument\n    definitions and list of argument AST nodes.\"\"\"\n    if not arg_defs:\n        return {}\n\n    if arg_asts:\n        arg_ast_map = {arg.name.value: arg for arg in arg_asts}\n    else:\n        arg_ast_map = {}\n\n    result = {}\n    for name, arg_def in arg_defs.items():\n        value_ast = arg_ast_map.get(name)\n        if value_ast:\n            value_ast = value_ast.value\n\n        value = value_from_ast(\n            value_ast,\n            arg_def.type,\n            variables\n        )\n\n        if value is None:\n            value = arg_def.default_value\n\n        if value is not None:\n            # We use out_name as the output name for the\n            # dict if exists\n            result[arg_def.out_name or name] = value\n\n    return result\n\n\ndef get_variable_value(schema, definition_ast, input):\n    \"\"\"Given a variable definition, and any value of input, return a value which adheres to the variable definition,\n    or throw an error.\"\"\"\n    type = type_from_ast(schema, definition_ast.type)\n    variable = definition_ast.variable\n\n    if not type or not is_input_type(type):\n        raise GraphQLError(\n            'Variable \"${}\" expected value of type \"{}\" which cannot be used as an input type.'.format(\n                variable.name.value,\n                print_ast(definition_ast.type),\n            ),\n            [definition_ast]\n        )\n\n    input_type = type\n    errors = is_valid_value(input, input_type)\n    if not errors:\n        if input is None:\n            default_value = definition_ast.default_value\n            if default_value:\n                return value_from_ast(default_value, input_type)\n\n        return coerce_value(input_type, input)\n\n    if input is None:\n        raise GraphQLError(\n            'Variable \"${}\" of required type \"{}\" was not provided.'.format(\n                variable.name.value,\n                print_ast(definition_ast.type)\n            ),\n            [definition_ast]\n        )\n\n    message = (u'\\n' + u'\\n'.join(errors)) if errors else u''\n    raise GraphQLError(\n        'Variable \"${}\" got invalid value {}.{}'.format(\n            variable.name.value,\n            json.dumps(input, sort_keys=True),\n            message\n        ),\n        [definition_ast]\n    )\n\n\ndef coerce_value(type, value):\n    \"\"\"Given a type and any value, return a runtime value coerced to match the type.\"\"\"\n    if isinstance(type, GraphQLNonNull):\n        # Note: we're not checking that the result of coerceValue is\n        # non-null.\n        # We only call this function after calling isValidValue.\n        return coerce_value(type.of_type, value)\n\n    if value is None:\n        return None\n\n    if isinstance(type, GraphQLList):\n        item_type = type.of_type\n        if not isinstance(value, string_types) and isinstance(value, collections.Iterable):\n            return [coerce_value(item_type, item) for item in value]\n        else:\n            return [coerce_value(item_type, value)]\n\n    if isinstance(type, GraphQLInputObjectType):\n        fields = type.fields\n        obj = {}\n        for field_name, field in fields.items():\n            field_value = coerce_value(field.type, value.get(field_name))\n            if field_value is None:\n                field_value = field.default_value\n\n            if field_value is not None:\n                # We use out_name as the output name for the\n                # dict if exists\n                obj[field.out_name or field_name] = field_value\n\n        return obj\n\n    assert isinstance(type, (GraphQLScalarType, GraphQLEnumType)), \\\n        'Must be input type'\n\n    return type.parse_value(value)\n", 
    "graphql.graphql": "from .execution import ExecutionResult, execute\nfrom .language.parser import parse\nfrom .language.source import Source\nfrom .validation import validate\n\n\n# This is the primary entry point function for fulfilling GraphQL operations\n# by parsing, validating, and executing a GraphQL document along side a\n# GraphQL schema.\n\n# More sophisticated GraphQL servers, such as those which persist queries,\n# may wish to separate the validation and execution phases to a static time\n# tooling step, and a server runtime step.\n\n# schema:\n#    The GraphQL type system to use when validating and executing a query.\n# requestString:\n#    A GraphQL language formatted string representing the requested operation.\n# rootValue:\n#    The value provided as the first argument to resolver functions on the top\n#    level type (e.g. the query object type).\n# variableValues:\n#    A mapping of variable name to runtime value to use for all variables\n#    defined in the requestString.\n# operationName:\n#    The name of the operation to use if requestString contains multiple\n#    possible operations. Can be omitted if requestString contains only\n#    one operation.\ndef graphql(schema, request_string='', root_value=None, context_value=None,\n            variable_values=None, operation_name=None, executor=None,\n            return_promise=False, middleware=None):\n    try:\n        source = Source(request_string, 'GraphQL request')\n        ast = parse(source)\n        validation_errors = validate(schema, ast)\n        if validation_errors:\n            return ExecutionResult(\n                errors=validation_errors,\n                invalid=True,\n            )\n        return execute(\n            schema,\n            ast,\n            root_value,\n            context_value,\n            operation_name=operation_name,\n            variable_values=variable_values or {},\n            executor=executor,\n            return_promise=return_promise,\n            middleware=middleware,\n        )\n    except Exception as e:\n        return ExecutionResult(\n            errors=[e],\n            invalid=True,\n        )\n", 
    "graphql.language.__init__": "", 
    "graphql.language.ast": "# This is autogenerated code. DO NOT change this manually.\n# Run scripts/generate_ast.py to generate this file.\n\n\nclass Node(object):\n    __slots__ = ()\n\n\nclass Definition(Node):\n    __slots__ = ()\n\n\nclass Document(Node):\n    __slots__ = ('loc', 'definitions',)\n    _fields = ('definitions',)\n\n    def __init__(self, definitions, loc=None):\n        self.loc = loc\n        self.definitions = definitions\n\n    def __eq__(self, other):\n        return (\n            self is other or (\n                isinstance(other, Document) and\n                # self.loc == other.loc and\n                self.definitions == other.definitions\n            )\n        )\n\n    def __repr__(self):\n        return ('Document('\n                'definitions={self.definitions!r}'\n                ')').format(self=self)\n\n    def __copy__(self):\n        return type(self)(\n            self.definitions,\n            self.loc\n        )\n\n    def __hash__(self):\n        return id(self)\n\n\nclass OperationDefinition(Definition):\n    __slots__ = ('loc', 'operation', 'name', 'variable_definitions', 'directives', 'selection_set',)\n    _fields = ('operation', 'name', 'variable_definitions', 'directives', 'selection_set',)\n\n    def __init__(self, operation, selection_set, name=None, variable_definitions=None, directives=None, loc=None):\n        self.loc = loc\n        self.operation = operation\n        self.name = name\n        self.variable_definitions = variable_definitions\n        self.directives = directives\n        self.selection_set = selection_set\n\n    def __eq__(self, other):\n        return (\n            self is other or (\n                isinstance(other, OperationDefinition) and\n                # self.loc == other.loc and\n                self.operation == other.operation and\n                self.name == other.name and\n                self.variable_definitions == other.variable_definitions and\n                self.directives == other.directives and\n                self.selection_set == other.selection_set\n            )\n        )\n\n    def __repr__(self):\n        return ('OperationDefinition('\n                'operation={self.operation!r}'\n                ', name={self.name!r}'\n                ', variable_definitions={self.variable_definitions!r}'\n                ', directives={self.directives!r}'\n                ', selection_set={self.selection_set!r}'\n                ')').format(self=self)\n\n    def __copy__(self):\n        return type(self)(\n            self.operation,\n            self.selection_set,\n            self.name,\n            self.variable_definitions,\n            self.directives,\n            self.loc\n        )\n\n    def __hash__(self):\n        return id(self)\n\n\nclass VariableDefinition(Node):\n    __slots__ = ('loc', 'variable', 'type', 'default_value',)\n    _fields = ('variable', 'type', 'default_value',)\n\n    def __init__(self, variable, type, default_value=None, loc=None):\n        self.loc = loc\n        self.variable = variable\n        self.type = type\n        self.default_value = default_value\n\n    def __eq__(self, other):\n        return (\n            self is other or (\n                isinstance(other, VariableDefinition) and\n                # self.loc == other.loc and\n                self.variable == other.variable and\n                self.type == other.type and\n                self.default_value == other.default_value\n            )\n        )\n\n    def __repr__(self):\n        return ('VariableDefinition('\n                'variable={self.variable!r}'\n                ', type={self.type!r}'\n                ', default_value={self.default_value!r}'\n                ')').format(self=self)\n\n    def __copy__(self):\n        return type(self)(\n            self.variable,\n            self.type,\n            self.default_value,\n            self.loc\n        )\n\n    def __hash__(self):\n        return id(self)\n\n\nclass SelectionSet(Node):\n    __slots__ = ('loc', 'selections',)\n    _fields = ('selections',)\n\n    def __init__(self, selections, loc=None):\n        self.loc = loc\n        self.selections = selections\n\n    def __eq__(self, other):\n        return (\n            self is other or (\n                isinstance(other, SelectionSet) and\n                # self.loc == other.loc and\n                self.selections == other.selections\n            )\n        )\n\n    def __repr__(self):\n        return ('SelectionSet('\n                'selections={self.selections!r}'\n                ')').format(self=self)\n\n    def __copy__(self):\n        return type(self)(\n            self.selections,\n            self.loc\n        )\n\n    def __hash__(self):\n        return id(self)\n\n\nclass Selection(Node):\n    __slots__ = ()\n\n\nclass Field(Selection):\n    __slots__ = ('loc', 'alias', 'name', 'arguments', 'directives', 'selection_set',)\n    _fields = ('alias', 'name', 'arguments', 'directives', 'selection_set',)\n\n    def __init__(self, name, alias=None, arguments=None, directives=None, selection_set=None, loc=None):\n        self.loc = loc\n        self.alias = alias\n        self.name = name\n        self.arguments = arguments\n        self.directives = directives\n        self.selection_set = selection_set\n\n    def __eq__(self, other):\n        return (\n            self is other or (\n                isinstance(other, Field) and\n                # self.loc == other.loc and\n                self.alias == other.alias and\n                self.name == other.name and\n                self.arguments == other.arguments and\n                self.directives == other.directives and\n                self.selection_set == other.selection_set\n            )\n        )\n\n    def __repr__(self):\n        return ('Field('\n                'alias={self.alias!r}'\n                ', name={self.name!r}'\n                ', arguments={self.arguments!r}'\n                ', directives={self.directives!r}'\n                ', selection_set={self.selection_set!r}'\n                ')').format(self=self)\n\n    def __copy__(self):\n        return type(self)(\n            self.name,\n            self.alias,\n            self.arguments,\n            self.directives,\n            self.selection_set,\n            self.loc\n        )\n\n    def __hash__(self):\n        return id(self)\n\n\nclass Argument(Node):\n    __slots__ = ('loc', 'name', 'value',)\n    _fields = ('name', 'value',)\n\n    def __init__(self, name, value, loc=None):\n        self.loc = loc\n        self.name = name\n        self.value = value\n\n    def __eq__(self, other):\n        return (\n            self is other or (\n                isinstance(other, Argument) and\n                # self.loc == other.loc and\n                self.name == other.name and\n                self.value == other.value\n            )\n        )\n\n    def __repr__(self):\n        return ('Argument('\n                'name={self.name!r}'\n                ', value={self.value!r}'\n                ')').format(self=self)\n\n    def __copy__(self):\n        return type(self)(\n            self.name,\n            self.value,\n            self.loc\n        )\n\n    def __hash__(self):\n        return id(self)\n\n\nclass FragmentSpread(Selection):\n    __slots__ = ('loc', 'name', 'directives',)\n    _fields = ('name', 'directives',)\n\n    def __init__(self, name, directives=None, loc=None):\n        self.loc = loc\n        self.name = name\n        self.directives = directives\n\n    def __eq__(self, other):\n        return (\n            self is other or (\n                isinstance(other, FragmentSpread) and\n                # self.loc == other.loc and\n                self.name == other.name and\n                self.directives == other.directives\n            )\n        )\n\n    def __repr__(self):\n        return ('FragmentSpread('\n                'name={self.name!r}'\n                ', directives={self.directives!r}'\n                ')').format(self=self)\n\n    def __copy__(self):\n        return type(self)(\n            self.name,\n            self.directives,\n            self.loc\n        )\n\n    def __hash__(self):\n        return id(self)\n\n\nclass InlineFragment(Selection):\n    __slots__ = ('loc', 'type_condition', 'directives', 'selection_set',)\n    _fields = ('type_condition', 'directives', 'selection_set',)\n\n    def __init__(self, type_condition, selection_set, directives=None, loc=None):\n        self.loc = loc\n        self.type_condition = type_condition\n        self.directives = directives\n        self.selection_set = selection_set\n\n    def __eq__(self, other):\n        return (\n            self is other or (\n                isinstance(other, InlineFragment) and\n                # self.loc == other.loc and\n                self.type_condition == other.type_condition and\n                self.directives == other.directives and\n                self.selection_set == other.selection_set\n            )\n        )\n\n    def __repr__(self):\n        return ('InlineFragment('\n                'type_condition={self.type_condition!r}'\n                ', directives={self.directives!r}'\n                ', selection_set={self.selection_set!r}'\n                ')').format(self=self)\n\n    def __copy__(self):\n        return type(self)(\n            self.type_condition,\n            self.selection_set,\n            self.directives,\n            self.loc\n        )\n\n    def __hash__(self):\n        return id(self)\n\n\nclass FragmentDefinition(Definition):\n    __slots__ = ('loc', 'name', 'type_condition', 'directives', 'selection_set',)\n    _fields = ('name', 'type_condition', 'directives', 'selection_set',)\n\n    def __init__(self, name, type_condition, selection_set, directives=None, loc=None):\n        self.loc = loc\n        self.name = name\n        self.type_condition = type_condition\n        self.directives = directives\n        self.selection_set = selection_set\n\n    def __eq__(self, other):\n        return (\n            self is other or (\n                isinstance(other, FragmentDefinition) and\n                # self.loc == other.loc and\n                self.name == other.name and\n                self.type_condition == other.type_condition and\n                self.directives == other.directives and\n                self.selection_set == other.selection_set\n            )\n        )\n\n    def __repr__(self):\n        return ('FragmentDefinition('\n                'name={self.name!r}'\n                ', type_condition={self.type_condition!r}'\n                ', directives={self.directives!r}'\n                ', selection_set={self.selection_set!r}'\n                ')').format(self=self)\n\n    def __copy__(self):\n        return type(self)(\n            self.name,\n            self.type_condition,\n            self.selection_set,\n            self.directives,\n            self.loc\n        )\n\n    def __hash__(self):\n        return id(self)\n\n\nclass Value(Node):\n    __slots__ = ()\n\n\nclass Variable(Value):\n    __slots__ = ('loc', 'name',)\n    _fields = ('name',)\n\n    def __init__(self, name, loc=None):\n        self.loc = loc\n        self.name = name\n\n    def __eq__(self, other):\n        return (\n            self is other or (\n                isinstance(other, Variable) and\n                # self.loc == other.loc and\n                self.name == other.name\n            )\n        )\n\n    def __repr__(self):\n        return ('Variable('\n                'name={self.name!r}'\n                ')').format(self=self)\n\n    def __copy__(self):\n        return type(self)(\n            self.name,\n            self.loc\n        )\n\n    def __hash__(self):\n        return id(self)\n\n\nclass IntValue(Value):\n    __slots__ = ('loc', 'value',)\n    _fields = ('value',)\n\n    def __init__(self, value, loc=None):\n        self.loc = loc\n        self.value = value\n\n    def __eq__(self, other):\n        return (\n            self is other or (\n                isinstance(other, IntValue) and\n                # self.loc == other.loc and\n                self.value == other.value\n            )\n        )\n\n    def __repr__(self):\n        return ('IntValue('\n                'value={self.value!r}'\n                ')').format(self=self)\n\n    def __copy__(self):\n        return type(self)(\n            self.value,\n            self.loc\n        )\n\n    def __hash__(self):\n        return id(self)\n\n\nclass FloatValue(Value):\n    __slots__ = ('loc', 'value',)\n    _fields = ('value',)\n\n    def __init__(self, value, loc=None):\n        self.loc = loc\n        self.value = value\n\n    def __eq__(self, other):\n        return (\n            self is other or (\n                isinstance(other, FloatValue) and\n                # self.loc == other.loc and\n                self.value == other.value\n            )\n        )\n\n    def __repr__(self):\n        return ('FloatValue('\n                'value={self.value!r}'\n                ')').format(self=self)\n\n    def __copy__(self):\n        return type(self)(\n            self.value,\n            self.loc\n        )\n\n    def __hash__(self):\n        return id(self)\n\n\nclass StringValue(Value):\n    __slots__ = ('loc', 'value',)\n    _fields = ('value',)\n\n    def __init__(self, value, loc=None):\n        self.loc = loc\n        self.value = value\n\n    def __eq__(self, other):\n        return (\n            self is other or (\n                isinstance(other, StringValue) and\n                # self.loc == other.loc and\n                self.value == other.value\n            )\n        )\n\n    def __repr__(self):\n        return ('StringValue('\n                'value={self.value!r}'\n                ')').format(self=self)\n\n    def __copy__(self):\n        return type(self)(\n            self.value,\n            self.loc\n        )\n\n    def __hash__(self):\n        return id(self)\n\n\nclass BooleanValue(Value):\n    __slots__ = ('loc', 'value',)\n    _fields = ('value',)\n\n    def __init__(self, value, loc=None):\n        self.loc = loc\n        self.value = value\n\n    def __eq__(self, other):\n        return (\n            self is other or (\n                isinstance(other, BooleanValue) and\n                # self.loc == other.loc and\n                self.value == other.value\n            )\n        )\n\n    def __repr__(self):\n        return ('BooleanValue('\n                'value={self.value!r}'\n                ')').format(self=self)\n\n    def __copy__(self):\n        return type(self)(\n            self.value,\n            self.loc\n        )\n\n    def __hash__(self):\n        return id(self)\n\n\nclass EnumValue(Value):\n    __slots__ = ('loc', 'value',)\n    _fields = ('value',)\n\n    def __init__(self, value, loc=None):\n        self.loc = loc\n        self.value = value\n\n    def __eq__(self, other):\n        return (\n            self is other or (\n                isinstance(other, EnumValue) and\n                # self.loc == other.loc and\n                self.value == other.value\n            )\n        )\n\n    def __repr__(self):\n        return ('EnumValue('\n                'value={self.value!r}'\n                ')').format(self=self)\n\n    def __copy__(self):\n        return type(self)(\n            self.value,\n            self.loc\n        )\n\n    def __hash__(self):\n        return id(self)\n\n\nclass ListValue(Value):\n    __slots__ = ('loc', 'values',)\n    _fields = ('values',)\n\n    def __init__(self, values, loc=None):\n        self.loc = loc\n        self.values = values\n\n    def __eq__(self, other):\n        return (\n            self is other or (\n                isinstance(other, ListValue) and\n                # self.loc == other.loc and\n                self.values == other.values\n            )\n        )\n\n    def __repr__(self):\n        return ('ListValue('\n                'values={self.values!r}'\n                ')').format(self=self)\n\n    def __copy__(self):\n        return type(self)(\n            self.values,\n            self.loc\n        )\n\n    def __hash__(self):\n        return id(self)\n\n\nclass ObjectValue(Value):\n    __slots__ = ('loc', 'fields',)\n    _fields = ('fields',)\n\n    def __init__(self, fields, loc=None):\n        self.loc = loc\n        self.fields = fields\n\n    def __eq__(self, other):\n        return (\n            self is other or (\n                isinstance(other, ObjectValue) and\n                # self.loc == other.loc and\n                self.fields == other.fields\n            )\n        )\n\n    def __repr__(self):\n        return ('ObjectValue('\n                'fields={self.fields!r}'\n                ')').format(self=self)\n\n    def __copy__(self):\n        return type(self)(\n            self.fields,\n            self.loc\n        )\n\n    def __hash__(self):\n        return id(self)\n\n\nclass ObjectField(Node):\n    __slots__ = ('loc', 'name', 'value',)\n    _fields = ('name', 'value',)\n\n    def __init__(self, name, value, loc=None):\n        self.loc = loc\n        self.name = name\n        self.value = value\n\n    def __eq__(self, other):\n        return (\n            self is other or (\n                isinstance(other, ObjectField) and\n                # self.loc == other.loc and\n                self.name == other.name and\n                self.value == other.value\n            )\n        )\n\n    def __repr__(self):\n        return ('ObjectField('\n                'name={self.name!r}'\n                ', value={self.value!r}'\n                ')').format(self=self)\n\n    def __copy__(self):\n        return type(self)(\n            self.name,\n            self.value,\n            self.loc\n        )\n\n    def __hash__(self):\n        return id(self)\n\n\nclass Directive(Node):\n    __slots__ = ('loc', 'name', 'arguments',)\n    _fields = ('name', 'arguments',)\n\n    def __init__(self, name, arguments=None, loc=None):\n        self.loc = loc\n        self.name = name\n        self.arguments = arguments\n\n    def __eq__(self, other):\n        return (\n            self is other or (\n                isinstance(other, Directive) and\n                # self.loc == other.loc and\n                self.name == other.name and\n                self.arguments == other.arguments\n            )\n        )\n\n    def __repr__(self):\n        return ('Directive('\n                'name={self.name!r}'\n                ', arguments={self.arguments!r}'\n                ')').format(self=self)\n\n    def __copy__(self):\n        return type(self)(\n            self.name,\n            self.arguments,\n            self.loc\n        )\n\n    def __hash__(self):\n        return id(self)\n\n\nclass Type(Node):\n    __slots__ = ()\n\n\nclass NamedType(Type):\n    __slots__ = ('loc', 'name',)\n    _fields = ('name',)\n\n    def __init__(self, name, loc=None):\n        self.loc = loc\n        self.name = name\n\n    def __eq__(self, other):\n        return (\n            self is other or (\n                isinstance(other, NamedType) and\n                # self.loc == other.loc and\n                self.name == other.name\n            )\n        )\n\n    def __repr__(self):\n        return ('NamedType('\n                'name={self.name!r}'\n                ')').format(self=self)\n\n    def __copy__(self):\n        return type(self)(\n            self.name,\n            self.loc\n        )\n\n    def __hash__(self):\n        return id(self)\n\n\nclass ListType(Type):\n    __slots__ = ('loc', 'type',)\n    _fields = ('type',)\n\n    def __init__(self, type, loc=None):\n        self.loc = loc\n        self.type = type\n\n    def __eq__(self, other):\n        return (\n            self is other or (\n                isinstance(other, ListType) and\n                # self.loc == other.loc and\n                self.type == other.type\n            )\n        )\n\n    def __repr__(self):\n        return ('ListType('\n                'type={self.type!r}'\n                ')').format(self=self)\n\n    def __copy__(self):\n        return type(self)(\n            self.type,\n            self.loc\n        )\n\n    def __hash__(self):\n        return id(self)\n\n\nclass NonNullType(Type):\n    __slots__ = ('loc', 'type',)\n    _fields = ('type',)\n\n    def __init__(self, type, loc=None):\n        self.loc = loc\n        self.type = type\n\n    def __eq__(self, other):\n        return (\n            self is other or (\n                isinstance(other, NonNullType) and\n                # self.loc == other.loc and\n                self.type == other.type\n            )\n        )\n\n    def __repr__(self):\n        return ('NonNullType('\n                'type={self.type!r}'\n                ')').format(self=self)\n\n    def __copy__(self):\n        return type(self)(\n            self.type,\n            self.loc\n        )\n\n    def __hash__(self):\n        return id(self)\n\n\nclass Name(Node):\n    __slots__ = ('loc', 'value',)\n    _fields = ('value',)\n\n    def __init__(self, value, loc=None):\n        self.loc = loc\n        self.value = value\n\n    def __eq__(self, other):\n        return (\n            self is other or (\n                isinstance(other, Name) and\n                # self.loc == other.loc and\n                self.value == other.value\n            )\n        )\n\n    def __repr__(self):\n        return ('Name('\n                'value={self.value!r}'\n                ')').format(self=self)\n\n    def __copy__(self):\n        return type(self)(\n            self.value,\n            self.loc\n        )\n\n    def __hash__(self):\n        return id(self)\n\n\n# Type System Definition\n\nclass TypeDefinition(Node):\n    pass\n\n\nclass TypeSystemDefinition(TypeDefinition):\n    pass\n\n\nclass SchemaDefinition(TypeSystemDefinition):\n    __slots__ = ('loc', 'directives', 'operation_types',)\n    _fields = ('operation_types',)\n\n    def __init__(self, operation_types, loc=None, directives=None):\n        self.operation_types = operation_types\n        self.loc = loc\n        self.directives = directives\n\n    def __eq__(self, other):\n        return (\n            self is other or (\n                isinstance(other, SchemaDefinition) and\n                self.operation_types == other.operation_types and\n                self.directives == other.directives\n            )\n        )\n\n    def __repr__(self):\n        return ('SchemaDefinition('\n                'operation_types={self.operation_types!r}'\n                ', directives={self.directives!r}'\n                ')').format(self=self)\n\n    def __copy__(self):\n        return type(self)(\n            self.operation_types,\n            self.loc,\n            self.directives,\n        )\n\n    def __hash__(self):\n        return id(self)\n\n\nclass OperationTypeDefinition(Node):\n    __slots__ = ('loc', 'operation', 'type',)\n    _fields = ('operation', 'type',)\n\n    def __init__(self, operation, type, loc=None):\n        self.operation = operation\n        self.type = type\n        self.loc = loc\n\n    def __eq__(self, other):\n        return (\n            self is other or (\n                isinstance(other, OperationTypeDefinition) and\n                self.operation == other.operation and\n                self.type == other.type\n            )\n        )\n\n    def __repr__(self):\n        return ('OperationTypeDefinition('\n                'operation={self.operation!r}'\n                ', type={self.type!r}'\n                ')').format(self=self)\n\n    def __copy__(self):\n        return type(self)(\n            self.operation,\n            self.type,\n            self.loc\n        )\n\n    def __hash__(self):\n        return id(self)\n\n\nclass ObjectTypeDefinition(TypeDefinition):\n    __slots__ = ('loc', 'name', 'interfaces', 'directives', 'fields',)\n    _fields = ('name', 'interfaces', 'fields',)\n\n    def __init__(self, name, fields, interfaces=None, loc=None, directives=None):\n        self.loc = loc\n        self.name = name\n        self.interfaces = interfaces\n        self.fields = fields\n        self.directives = directives\n\n    def __eq__(self, other):\n        return (\n            self is other or (\n                isinstance(other, ObjectTypeDefinition) and\n                # self.loc == other.loc and\n                self.name == other.name and\n                self.interfaces == other.interfaces and\n                self.fields == other.fields and\n                self.directives == other.directives\n            )\n        )\n\n    def __repr__(self):\n        return ('ObjectTypeDefinition('\n                'name={self.name!r}'\n                ', interfaces={self.interfaces!r}'\n                ', fields={self.fields!r}'\n                ', directives={self.directives!r}'\n                ')').format(self=self)\n\n    def __copy__(self):\n        return type(self)(\n            self.name,\n            self.fields,\n            self.interfaces,\n            self.loc,\n            self.directives,\n        )\n\n    def __hash__(self):\n        return id(self)\n\n\nclass FieldDefinition(Node):\n    __slots__ = ('loc', 'name', 'arguments', 'type', 'directives',)\n    _fields = ('name', 'arguments', 'type',)\n\n    def __init__(self, name, arguments, type, loc=None, directives=None):\n        self.loc = loc\n        self.name = name\n        self.arguments = arguments\n        self.type = type\n        self.directives = directives\n\n    def __eq__(self, other):\n        return (\n            self is other or (\n                isinstance(other, FieldDefinition) and\n                # self.loc == other.loc and\n                self.name == other.name and\n                self.arguments == other.arguments and\n                self.type == other.type and\n                self.directives == other.directives\n            )\n        )\n\n    def __repr__(self):\n        return ('FieldDefinition('\n                'name={self.name!r}'\n                ', arguments={self.arguments!r}'\n                ', type={self.type!r}'\n                ')').format(self=self)\n\n    def __copy__(self):\n        return type(self)(\n            self.name,\n            self.arguments,\n            self.type,\n            self.loc,\n            self.directives,\n        )\n\n    def __hash__(self):\n        return id(self)\n\n\nclass InputValueDefinition(Node):\n    __slots__ = ('loc', 'name', 'type', 'default_value', 'directives')\n    _fields = ('name', 'type', 'default_value',)\n\n    def __init__(self, name, type, default_value=None, loc=None,\n                 directives=None):\n        self.loc = loc\n        self.name = name\n        self.type = type\n        self.default_value = default_value\n        self.directives = directives\n\n    def __eq__(self, other):\n        return (\n            self is other or (\n                isinstance(other, InputValueDefinition) and\n                # self.loc == other.loc and\n                self.name == other.name and\n                self.type == other.type and\n                self.default_value == other.default_value and\n                self.directives == other.directives\n            )\n        )\n\n    def __repr__(self):\n        return ('InputValueDefinition('\n                'name={self.name!r}'\n                ', type={self.type!r}'\n                ', default_value={self.default_value!r}'\n                ', directives={self.directives!r}'\n                ')').format(self=self)\n\n    def __copy__(self):\n        return type(self)(\n            self.name,\n            self.type,\n            self.default_value,\n            self.loc,\n            self.directives,\n        )\n\n    def __hash__(self):\n        return id(self)\n\n\nclass InterfaceTypeDefinition(TypeDefinition):\n    __slots__ = ('loc', 'name', 'fields', 'directives',)\n    _fields = ('name', 'fields',)\n\n    def __init__(self, name, fields, loc=None, directives=None):\n        self.loc = loc\n        self.name = name\n        self.fields = fields\n        self.directives = directives\n\n    def __eq__(self, other):\n        return (\n            self is other or (\n                isinstance(other, InterfaceTypeDefinition) and\n                # self.loc == other.loc and\n                self.name == other.name and\n                self.fields == other.fields and\n                self.directives == other.directives\n            )\n        )\n\n    def __repr__(self):\n        return ('InterfaceTypeDefinition('\n                'name={self.name!r}'\n                ', fields={self.fields!r}'\n                ', directives={self.directives!r}'\n                ')').format(self=self)\n\n    def __copy__(self):\n        return type(self)(\n            self.name,\n            self.fields,\n            self.loc,\n            self.directives,\n        )\n\n    def __hash__(self):\n        return id(self)\n\n\nclass UnionTypeDefinition(TypeDefinition):\n    __slots__ = ('loc', 'name', 'types', 'directives',)\n    _fields = ('name', 'types',)\n\n    def __init__(self, name, types, loc=None, directives=None):\n        self.loc = loc\n        self.name = name\n        self.types = types\n        self.directives = directives\n\n    def __eq__(self, other):\n        return (\n            self is other or (\n                isinstance(other, UnionTypeDefinition) and\n                # self.loc == other.loc and\n                self.name == other.name and\n                self.types == other.types and\n                self.directives == other.directives\n            )\n        )\n\n    def __repr__(self):\n        return ('UnionTypeDefinition('\n                'name={self.name!r}'\n                ', types={self.types!r}'\n                ', directives={self.directives!r}'\n                ')').format(self=self)\n\n    def __copy__(self):\n        return type(self)(\n            self.name,\n            self.types,\n            self.loc,\n            self.directives,\n        )\n\n    def __hash__(self):\n        return id(self)\n\n\nclass ScalarTypeDefinition(TypeDefinition):\n    __slots__ = ('loc', 'name', 'directives',)\n    _fields = ('name',)\n\n    def __init__(self, name, loc=None, directives=None):\n        self.loc = loc\n        self.name = name\n        self.directives = directives\n\n    def __eq__(self, other):\n        return (\n            self is other or (\n                isinstance(other, ScalarTypeDefinition) and\n                # self.loc == other.loc and\n                self.name == other.name and\n                self.directives == other.directives\n            )\n        )\n\n    def __repr__(self):\n        return ('ScalarTypeDefinition('\n                'name={self.name!r}'\n                'directives={self.directives!r}'\n                ')').format(self=self)\n\n    def __copy__(self):\n        return type(self)(\n            self.name,\n            self.loc,\n            self.directives\n        )\n\n    def __hash__(self):\n        return id(self)\n\n\nclass EnumTypeDefinition(TypeDefinition):\n    __slots__ = ('loc', 'name', 'values', 'directives',)\n    _fields = ('name', 'values',)\n\n    def __init__(self, name, values, loc=None, directives=None):\n        self.loc = loc\n        self.name = name\n        self.values = values\n        self.directives = directives\n\n    def __eq__(self, other):\n        return (\n            self is other or (\n                isinstance(other, EnumTypeDefinition) and\n                # self.loc == other.loc and\n                self.name == other.name and\n                self.values == other.values and\n                self.directives == other.directives\n            )\n        )\n\n    def __repr__(self):\n        return ('EnumTypeDefinition('\n                'name={self.name!r}'\n                ', values={self.values!r}'\n                ', directives={self.directives!r}'\n                ')').format(self=self)\n\n    def __copy__(self):\n        return type(self)(\n            self.name,\n            self.values,\n            self.loc,\n            self.directives,\n        )\n\n    def __hash__(self):\n        return id(self)\n\n\nclass EnumValueDefinition(Node):\n    __slots__ = ('loc', 'name', 'directives',)\n    _fields = ('name',)\n\n    def __init__(self, name, loc=None, directives=None):\n        self.loc = loc\n        self.name = name\n        self.directives = directives\n\n    def __eq__(self, other):\n        return (\n            self is other or (\n                isinstance(other, EnumValueDefinition) and\n                # self.loc == other.loc and\n                self.name == other.name and\n                self.directives == other.directives\n            )\n        )\n\n    def __repr__(self):\n        return ('EnumValueDefinition('\n                'name={self.name!r}'\n                ', directives={self.directives!r}'\n                ')').format(self=self)\n\n    def __copy__(self):\n        return type(self)(\n            self.name,\n            self.loc,\n            self.directives,\n        )\n\n    def __hash__(self):\n        return id(self)\n\n\nclass InputObjectTypeDefinition(TypeDefinition):\n    __slots__ = ('loc', 'name', 'fields', 'directives',)\n    _fields = ('name', 'fields',)\n\n    def __init__(self, name, fields, loc=None, directives=None):\n        self.loc = loc\n        self.name = name\n        self.fields = fields\n        self.directives = directives\n\n    def __eq__(self, other):\n        return (\n            self is other or (\n                isinstance(other, InputObjectTypeDefinition) and\n                # self.loc == other.loc and\n                self.name == other.name and\n                self.fields == other.fields and\n                self.directives == other.directives\n            )\n        )\n\n    def __repr__(self):\n        return ('InputObjectTypeDefinition('\n                'name={self.name!r}'\n                ', fields={self.fields!r}'\n                ', directives={self.directives!r}'\n                ')').format(self=self)\n\n    def __copy__(self):\n        return type(self)(\n            self.name,\n            self.fields,\n            self.loc,\n            self.directives,\n        )\n\n    def __hash__(self):\n        return id(self)\n\n\nclass TypeExtensionDefinition(TypeSystemDefinition):\n    __slots__ = ('loc', 'definition',)\n    _fields = ('definition',)\n\n    def __init__(self, definition, loc=None):\n        self.loc = loc\n        self.definition = definition\n\n    def __eq__(self, other):\n        return (\n            self is other or (\n                isinstance(other, TypeExtensionDefinition) and\n                # self.loc == other.loc and\n                self.definition == other.definition\n            )\n        )\n\n    def __repr__(self):\n        return ('TypeExtensionDefinition('\n                'definition={self.definition!r}'\n                ')').format(self=self)\n\n    def __copy__(self):\n        return type(self)(\n            self.definition,\n            self.loc\n        )\n\n    def __hash__(self):\n        return id(self)\n\n\nclass DirectiveDefinition(TypeSystemDefinition):\n    __slots__ = ('loc', 'name', 'arguments', 'locations')\n    _fields = ('name', 'locations')\n\n    def __init__(self, name, locations, arguments=None, loc=None):\n        self.name = name\n        self.locations = locations\n        self.loc = loc\n        self.arguments = arguments\n\n    def __eq__(self, other):\n        return (\n            self is other or (\n                isinstance(other, DirectiveDefinition) and\n                self.name == other.name and\n                self.locations == other.locations and\n                # self.loc == other.loc and\n                self.arguments == other.arguments\n            )\n        )\n\n    def __repr__(self):\n        return ('DirectiveDefinition('\n                'name={self.name!r}, '\n                'locations={self.locations!r}'\n                ')').format(self=self)\n\n    def __copy__(self):\n        return type(self)(\n            self.name,\n            self.locations,\n            self.arguments,\n            self.loc,\n        )\n\n    def __hash__(self):\n        return id(self)\n", 
    "graphql.language.base": "from .lexer import Lexer\nfrom .location import get_location\nfrom .parser import parse, parse_value\nfrom .printer import print_ast\nfrom .source import Source\nfrom .visitor import BREAK, ParallelVisitor, TypeInfoVisitor, visit\n\n__all__ = [\n    'Lexer',\n    'get_location',\n    'parse',\n    'parse_value',\n    'print_ast',\n    'Source',\n    'BREAK',\n    'ParallelVisitor',\n    'TypeInfoVisitor',\n    'visit',\n]\n", 
    "graphql.language.lexer": "import json\n\nfrom six import unichr\n\nfrom ..error import GraphQLSyntaxError\n\n__all__ = ['Token', 'Lexer', 'TokenKind',\n           'get_token_desc', 'get_token_kind_desc']\n\n\nclass Token(object):\n    __slots__ = 'kind', 'start', 'end', 'value'\n\n    def __init__(self, kind, start, end, value=None):\n        self.kind = kind\n        self.start = start\n        self.end = end\n        self.value = value\n\n    def __repr__(self):\n        return u'<Token kind={} at {}..{} value={}>'.format(\n            get_token_kind_desc(self.kind),\n            self.start,\n            self.end,\n            repr(self.value)\n        )\n\n    def __eq__(self, other):\n        return (self.kind == other.kind and\n                self.start == other.start and\n                self.end == other.end and\n                self.value == other.value)\n\n\nclass Lexer(object):\n    __slots__ = 'source', 'prev_position'\n\n    def __init__(self, source):\n        self.source = source\n        self.prev_position = 0\n\n    def next_token(self, reset_position=None):\n        if reset_position is None:\n            reset_position = self.prev_position\n        token = read_token(self.source, reset_position)\n        self.prev_position = token.end\n        return token\n\n\nclass TokenKind(object):\n    EOF = 1\n    BANG = 2\n    DOLLAR = 3\n    PAREN_L = 4\n    PAREN_R = 5\n    SPREAD = 6\n    COLON = 7\n    EQUALS = 8\n    AT = 9\n    BRACKET_L = 10\n    BRACKET_R = 11\n    BRACE_L = 12\n    PIPE = 13\n    BRACE_R = 14\n    NAME = 15\n    VARIABLE = 16\n    INT = 17\n    FLOAT = 18\n    STRING = 19\n\n\ndef get_token_desc(token):\n    if token.value:\n        return u'{} \"{}\"'.format(\n            get_token_kind_desc(token.kind),\n            token.value\n        )\n    else:\n        return get_token_kind_desc(token.kind)\n\n\ndef get_token_kind_desc(kind):\n    return TOKEN_DESCRIPTION[kind]\n\n\nTOKEN_DESCRIPTION = {\n    TokenKind.EOF: 'EOF',\n    TokenKind.BANG: '!',\n    TokenKind.DOLLAR: '$',\n    TokenKind.PAREN_L: '(',\n    TokenKind.PAREN_R: ')',\n    TokenKind.SPREAD: '...',\n    TokenKind.COLON: ':',\n    TokenKind.EQUALS: '=',\n    TokenKind.AT: '@',\n    TokenKind.BRACKET_L: '[',\n    TokenKind.BRACKET_R: ']',\n    TokenKind.BRACE_L: '{',\n    TokenKind.PIPE: '|',\n    TokenKind.BRACE_R: '}',\n    TokenKind.NAME: 'Name',\n    TokenKind.VARIABLE: 'Variable',\n    TokenKind.INT: 'Int',\n    TokenKind.FLOAT: 'Float',\n    TokenKind.STRING: 'String',\n}\n\n\ndef char_code_at(s, pos):\n    if 0 <= pos < len(s):\n        return ord(s[pos])\n\n    return None\n\n\nPUNCT_CODE_TO_KIND = {\n    ord('!'): TokenKind.BANG,\n    ord('$'): TokenKind.DOLLAR,\n    ord('('): TokenKind.PAREN_L,\n    ord(')'): TokenKind.PAREN_R,\n    ord(':'): TokenKind.COLON,\n    ord('='): TokenKind.EQUALS,\n    ord('@'): TokenKind.AT,\n    ord('['): TokenKind.BRACKET_L,\n    ord(']'): TokenKind.BRACKET_R,\n    ord('{'): TokenKind.BRACE_L,\n    ord('|'): TokenKind.PIPE,\n    ord('}'): TokenKind.BRACE_R,\n}\n\n\ndef print_char_code(code):\n    if code is None:\n        return '<EOF>'\n\n    if code < 0x007F:\n        return json.dumps(unichr(code))\n\n    return '\"\\\\u%04X\"' % code\n\n\ndef read_token(source, from_position):\n    \"\"\"Gets the next token from the source starting at the given position.\n\n    This skips over whitespace and comments until it finds the next lexable\n    token, then lexes punctuators immediately or calls the appropriate\n    helper fucntion for more complicated tokens.\"\"\"\n    body = source.body\n    body_length = len(body)\n\n    position = position_after_whitespace(body, from_position)\n\n    if position >= body_length:\n        return Token(TokenKind.EOF, position, position)\n\n    code = char_code_at(body, position)\n\n    if code < 0x0020 and code not in (0x0009, 0x000A, 0x000D):\n        raise GraphQLSyntaxError(\n            source, position,\n            u'Invalid character {}.'.format(print_char_code(code))\n        )\n\n    kind = PUNCT_CODE_TO_KIND.get(code)\n    if kind is not None:\n        return Token(kind, position, position + 1)\n\n    if code == 46:  # .\n        if char_code_at(body, position + 1) == char_code_at(body, position + 2) == 46:\n            return Token(TokenKind.SPREAD, position, position + 3)\n\n    elif 65 <= code <= 90 or code == 95 or 97 <= code <= 122:\n        # A-Z, _, a-z\n        return read_name(source, position)\n\n    elif code == 45 or 48 <= code <= 57:  # -, 0-9\n        return read_number(source, position, code)\n\n    elif code == 34:  # \"\n        return read_string(source, position)\n\n    raise GraphQLSyntaxError(\n        source, position,\n        u'Unexpected character {}.'.format(print_char_code(code)))\n\nignored_whitespace_characters = frozenset([\n    # BOM\n    0xFEFF,\n    # White Space\n    0x0009,  # tab\n    0x0020,  # space\n    # Line Terminator\n    0x000A,  # new line\n    0x000D,  # carriage return\n    # Comma\n    0x002C\n])\n\n\ndef position_after_whitespace(body, start_position):\n    \"\"\"Reads from body starting at start_position until it finds a\n    non-whitespace or commented character, then returns the position of\n    that character for lexing.\"\"\"\n    body_length = len(body)\n    position = start_position\n    while position < body_length:\n        code = char_code_at(body, position)\n        if code in ignored_whitespace_characters:\n            position += 1\n\n        elif code == 35:  # #, skip comments\n            position += 1\n            while position < body_length:\n                code = char_code_at(body, position)\n                if not (code is not None and (code > 0x001F or code == 0x0009) and code not in (0x000A, 0x000D)):\n                    break\n\n                position += 1\n        else:\n            break\n    return position\n\n\ndef read_number(source, start, first_code):\n    \"\"\"Reads a number token from the source file, either a float\n    or an int depending on whether a decimal point appears.\n\n    Int:   -?(0|[1-9][0-9]*)\n    Float: -?(0|[1-9][0-9]*)(\\.[0-9]+)?((E|e)(+|-)?[0-9]+)?\"\"\"\n    code = first_code\n    body = source.body\n    position = start\n    is_float = False\n\n    if code == 45:  # -\n        position += 1\n        code = char_code_at(body, position)\n\n    if code == 48:  # 0\n        position += 1\n        code = char_code_at(body, position)\n\n        if code is not None and 48 <= code <= 57:\n            raise GraphQLSyntaxError(\n                source,\n                position,\n                u'Invalid number, unexpected digit after 0: {}.'.format(print_char_code(code))\n            )\n    else:\n        position = read_digits(source, position, code)\n        code = char_code_at(body, position)\n\n    if code == 46:  # .\n        is_float = True\n\n        position += 1\n        code = char_code_at(body, position)\n        position = read_digits(source, position, code)\n        code = char_code_at(body, position)\n\n    if code in (69, 101):  # E e\n        is_float = True\n        position += 1\n        code = char_code_at(body, position)\n        if code in (43, 45):  # + -\n            position += 1\n            code = char_code_at(body, position)\n\n        position = read_digits(source, position, code)\n\n    return Token(\n        TokenKind.FLOAT if is_float else TokenKind.INT,\n        start,\n        position,\n        body[start:position]\n    )\n\n\ndef read_digits(source, start, first_code):\n    body = source.body\n    position = start\n    code = first_code\n\n    if code is not None and 48 <= code <= 57:  # 0 - 9\n        while True:\n            position += 1\n            code = char_code_at(body, position)\n\n            if not (code is not None and 48 <= code <= 57):\n                break\n\n        return position\n\n    raise GraphQLSyntaxError(\n        source,\n        position,\n        u'Invalid number, expected digit but got: {}.'.format(print_char_code(code))\n    )\n\n\nESCAPED_CHAR_CODES = {\n    34: '\"',\n    47: '/',\n    92: '\\\\',\n    98: '\\b',\n    102: '\\f',\n    110: '\\n',\n    114: '\\r',\n    116: '\\t',\n}\n\n\ndef read_string(source, start):\n    \"\"\"Reads a string token from the source file.\n\n    \"([^\"\\\\\\u000A\\u000D\\u2028\\u2029]|(\\\\(u[0-9a-fA-F]{4}|[\"\\\\/bfnrt])))*\"\n    \"\"\"\n    body = source.body\n    body_length = len(body)\n\n    position = start + 1\n    chunk_start = position\n    code = 0\n    value = []\n    append = value.append\n\n    while position < body_length:\n        code = char_code_at(body, position)\n        if not (\n            code is not None and\n            code not in (\n                # LineTerminator\n                0x000A, 0x000D,\n                # Quote\n                34\n            )\n        ):\n            break\n\n        if code < 0x0020 and code != 0x0009:\n            raise GraphQLSyntaxError(\n                source,\n                position,\n                u'Invalid character within String: {}.'.format(print_char_code(code))\n            )\n\n        position += 1\n        if code == 92:  # \\\n            append(body[chunk_start:position - 1])\n\n            code = char_code_at(body, position)\n            escaped = ESCAPED_CHAR_CODES.get(code)\n            if escaped is not None:\n                append(escaped)\n\n            elif code == 117:  # u\n                char_code = uni_char_code(\n                    char_code_at(body, position + 1) or 0,\n                    char_code_at(body, position + 2) or 0,\n                    char_code_at(body, position + 3) or 0,\n                    char_code_at(body, position + 4) or 0,\n                )\n\n                if char_code < 0:\n                    raise GraphQLSyntaxError(\n                        source, position,\n                        u'Invalid character escape sequence: \\\\u{}.'.format(body[position + 1: position + 5])\n                    )\n\n                append(unichr(char_code))\n                position += 4\n            else:\n                raise GraphQLSyntaxError(\n                    source, position,\n                    u'Invalid character escape sequence: \\\\{}.'.format(unichr(code))\n                )\n\n            position += 1\n            chunk_start = position\n\n    if code != 34:  # Quote (\")\n        raise GraphQLSyntaxError(source, position, 'Unterminated string')\n\n    append(body[chunk_start:position])\n    return Token(TokenKind.STRING, start, position + 1, u''.join(value))\n\n\ndef uni_char_code(a, b, c, d):\n    \"\"\"Converts four hexidecimal chars to the integer that the\n    string represents. For example, uniCharCode('0','0','0','f')\n    will return 15, and uniCharCode('0','0','f','f') returns 255.\n\n    Returns a negative number on error, if a char was invalid.\n\n    This is implemented by noting that char2hex() returns -1 on error,\n    which means the result of ORing the char2hex() will also be negative.\n    \"\"\"\n    return (char2hex(a) << 12 | char2hex(b) << 8 |\n            char2hex(c) << 4 | char2hex(d))\n\n\ndef char2hex(a):\n    \"\"\"Converts a hex character to its integer value.\n    '0' becomes 0, '9' becomes 9\n    'A' becomes 10, 'F' becomes 15\n    'a' becomes 10, 'f' becomes 15\n\n    Returns -1 on error.\"\"\"\n    if 48 <= a <= 57:  # 0-9\n        return a - 48\n    elif 65 <= a <= 70:  # A-F\n        return a - 55\n    elif 97 <= a <= 102:  # a-f\n        return a - 87\n    return -1\n\n\ndef read_name(source, position):\n    \"\"\"Reads an alphanumeric + underscore name from the source.\n\n    [_A-Za-z][_0-9A-Za-z]*\"\"\"\n    body = source.body\n    body_length = len(body)\n    end = position + 1\n\n    while end != body_length:\n        code = char_code_at(body, end)\n        if not (code is not None and (\n            code == 95 or  # _\n            48 <= code <= 57 or  # 0-9\n            65 <= code <= 90 or  # A-Z\n            97 <= code <= 122  # a-z\n        )):\n            break\n\n        end += 1\n\n    return Token(TokenKind.NAME, position, end, body[position:end])\n", 
    "graphql.language.location": "__all__ = ['get_location', 'SourceLocation']\n\n\nclass SourceLocation(object):\n    __slots__ = 'line', 'column'\n\n    def __init__(self, line, column):\n        self.line = line\n        self.column = column\n\n    def __repr__(self):\n        return '<SourceLocation line={} column={}>'.format(self.line, self.column)\n\n    def __eq__(self, other):\n        return (\n            isinstance(other, SourceLocation) and\n            self.line == other.line and\n            self.column == other.column\n        )\n\n\ndef get_location(source, position):\n    lines = source.body[:position].splitlines()\n    if lines:\n        line = len(lines)\n        column = len(lines[-1]) + 1\n    else:\n        line = 1\n        column = 1\n    return SourceLocation(line, column)\n", 
    "graphql.language.parser": "from six import string_types\n\nfrom . import ast\nfrom ..error import GraphQLSyntaxError\nfrom .lexer import Lexer, TokenKind, get_token_desc, get_token_kind_desc\nfrom .source import Source\n\n__all__ = ['parse']\n\n\ndef parse(source, **kwargs):\n    \"\"\"Given a GraphQL source, parses it into a Document.\"\"\"\n    options = {'no_location': False, 'no_source': False}\n    options.update(kwargs)\n    source_obj = source\n\n    if isinstance(source, string_types):\n        source_obj = Source(source)\n\n    parser = Parser(source_obj, options)\n    return parse_document(parser)\n\n\ndef parse_value(source, **kwargs):\n    options = {'no_location': False, 'no_source': False}\n    options.update(kwargs)\n    source_obj = source\n\n    if isinstance(source, string_types):\n        source_obj = Source(source)\n\n    parser = Parser(source_obj, options)\n    return parse_value_literal(parser, False)\n\n\nclass Parser(object):\n    __slots__ = 'lexer', 'source', 'options', 'prev_end', 'token'\n\n    def __init__(self, source, options):\n        self.lexer = Lexer(source)\n        self.source = source\n        self.options = options\n        self.prev_end = 0\n        self.token = self.lexer.next_token()\n\n\nclass Loc(object):\n    __slots__ = 'start', 'end', 'source'\n\n    def __init__(self, start, end, source=None):\n        self.start = start\n        self.end = end\n        self.source = source\n\n    def __repr__(self):\n        source = ' source={}'.format(self.source) if self.source else ''\n        return '<Loc start={} end={}{}>'.format(self.start, self.end, source)\n\n    def __eq__(self, other):\n        return (\n            isinstance(other, Loc) and\n            self.start == other.start and\n            self.end == other.end and\n            self.source == other.source\n        )\n\n\ndef loc(parser, start):\n    \"\"\"Returns a location object, used to identify the place in\n    the source that created a given parsed object.\"\"\"\n    if parser.options['no_location']:\n        return None\n\n    if parser.options['no_source']:\n        return Loc(start, parser.prev_end)\n\n    return Loc(start, parser.prev_end, parser.source)\n\n\ndef advance(parser):\n    \"\"\"Moves the internal parser object to the next lexed token.\"\"\"\n    prev_end = parser.token.end\n    parser.prev_end = prev_end\n    parser.token = parser.lexer.next_token(prev_end)\n\n\ndef peek(parser, kind):\n    \"\"\"Determines if the next token is of a given kind\"\"\"\n    return parser.token.kind == kind\n\n\ndef skip(parser, kind):\n    \"\"\"If the next token is of the given kind, return true after advancing\n    the parser. Otherwise, do not change the parser state\n    and throw an error.\"\"\"\n    match = parser.token.kind == kind\n    if match:\n        advance(parser)\n\n    return match\n\n\ndef expect(parser, kind):\n    \"\"\"If the next token is of the given kind, return that token after\n    advancing the parser. Otherwise, do not change the parser state and\n    return False.\"\"\"\n    token = parser.token\n    if token.kind == kind:\n        advance(parser)\n        return token\n\n    raise GraphQLSyntaxError(\n        parser.source,\n        token.start,\n        u'Expected {}, found {}'.format(\n            get_token_kind_desc(kind),\n            get_token_desc(token)\n        )\n    )\n\n\ndef expect_keyword(parser, value):\n    \"\"\"If the next token is a keyword with the given value, return that\n    token after advancing the parser. Otherwise, do not change the parser\n    state and return False.\"\"\"\n    token = parser.token\n    if token.kind == TokenKind.NAME and token.value == value:\n        advance(parser)\n        return token\n\n    raise GraphQLSyntaxError(\n        parser.source,\n        token.start,\n        u'Expected \"{}\", found {}'.format(value, get_token_desc(token))\n    )\n\n\ndef unexpected(parser, at_token=None):\n    \"\"\"Helper function for creating an error when an unexpected lexed token\n    is encountered.\"\"\"\n    token = at_token or parser.token\n    return GraphQLSyntaxError(\n        parser.source,\n        token.start,\n        u'Unexpected {}'.format(get_token_desc(token))\n    )\n\n\ndef any(parser, open_kind, parse_fn, close_kind):\n    \"\"\"Returns a possibly empty list of parse nodes, determined by\n    the parse_fn. This list begins with a lex token of openKind\n    and ends with a lex token of closeKind. Advances the parser\n    to the next lex token after the closing token.\"\"\"\n    expect(parser, open_kind)\n    nodes = []\n    while not skip(parser, close_kind):\n        nodes.append(parse_fn(parser))\n\n    return nodes\n\n\ndef many(parser, open_kind, parse_fn, close_kind):\n    \"\"\"Returns a non-empty list of parse nodes, determined by\n    the parse_fn. This list begins with a lex token of openKind\n    and ends with a lex token of closeKind. Advances the parser\n    to the next lex token after the closing token.\"\"\"\n    expect(parser, open_kind)\n    nodes = [parse_fn(parser)]\n    while not skip(parser, close_kind):\n        nodes.append(parse_fn(parser))\n\n    return nodes\n\n\ndef parse_name(parser):\n    \"\"\"Converts a name lex token into a name parse node.\"\"\"\n    token = expect(parser, TokenKind.NAME)\n    return ast.Name(\n        value=token.value,\n        loc=loc(parser, token.start)\n    )\n\n\n# Implements the parsing rules in the Document section.\n\ndef parse_document(parser):\n    start = parser.token.start\n    definitions = []\n    while True:\n        definitions.append(parse_definition(parser))\n\n        if skip(parser, TokenKind.EOF):\n            break\n\n    return ast.Document(\n        definitions=definitions,\n        loc=loc(parser, start)\n    )\n\n\ndef parse_definition(parser):\n    if peek(parser, TokenKind.BRACE_L):\n        return parse_operation_definition(parser)\n\n    if peek(parser, TokenKind.NAME):\n        name = parser.token.value\n\n        if name in ('query', 'mutation', 'subscription'):\n            return parse_operation_definition(parser)\n        elif name == 'fragment':\n            return parse_fragment_definition(parser)\n        elif name in ('schema', 'scalar', 'type', 'interface', 'union', 'enum', 'input', 'extend', 'directive'):\n            return parse_type_system_definition(parser)\n\n    raise unexpected(parser)\n\n\n# Implements the parsing rules in the Operations section.\ndef parse_operation_definition(parser):\n    start = parser.token.start\n    if peek(parser, TokenKind.BRACE_L):\n        return ast.OperationDefinition(\n            operation='query',\n            name=None,\n            variable_definitions=None,\n            directives=[],\n            selection_set=parse_selection_set(parser),\n            loc=loc(parser, start)\n        )\n\n    operation = parse_operation_type(parser)\n\n    name = None\n    if peek(parser, TokenKind.NAME):\n        name = parse_name(parser)\n\n    return ast.OperationDefinition(\n        operation=operation,\n        name=name,\n        variable_definitions=parse_variable_definitions(parser),\n        directives=parse_directives(parser),\n        selection_set=parse_selection_set(parser),\n        loc=loc(parser, start)\n    )\n\n\ndef parse_operation_type(parser):\n    operation_token = expect(parser, TokenKind.NAME)\n    operation = operation_token.value\n    if operation == 'query':\n        return 'query'\n    elif operation == 'mutation':\n        return 'mutation'\n    elif operation == 'subscription':\n        return 'subscription'\n\n    raise unexpected(parser, operation_token)\n\n\ndef parse_variable_definitions(parser):\n    if peek(parser, TokenKind.PAREN_L):\n        return many(\n            parser,\n            TokenKind.PAREN_L,\n            parse_variable_definition,\n            TokenKind.PAREN_R\n        )\n\n    return []\n\n\ndef parse_variable_definition(parser):\n    start = parser.token.start\n\n    return ast.VariableDefinition(\n        variable=parse_variable(parser),\n        type=expect(parser, TokenKind.COLON) and parse_type(parser),\n        default_value=parse_value_literal(parser, True) if skip(parser, TokenKind.EQUALS) else None,\n        loc=loc(parser, start)\n    )\n\n\ndef parse_variable(parser):\n    start = parser.token.start\n    expect(parser, TokenKind.DOLLAR)\n\n    return ast.Variable(\n        name=parse_name(parser),\n        loc=loc(parser, start)\n    )\n\n\ndef parse_selection_set(parser):\n    start = parser.token.start\n    return ast.SelectionSet(\n        selections=many(parser, TokenKind.BRACE_L, parse_selection, TokenKind.BRACE_R),\n        loc=loc(parser, start)\n    )\n\n\ndef parse_selection(parser):\n    if peek(parser, TokenKind.SPREAD):\n        return parse_fragment(parser)\n    else:\n        return parse_field(parser)\n\n\ndef parse_field(parser):\n    # Corresponds to both Field and Alias in the spec\n    start = parser.token.start\n\n    name_or_alias = parse_name(parser)\n    if skip(parser, TokenKind.COLON):\n        alias = name_or_alias\n        name = parse_name(parser)\n    else:\n        alias = None\n        name = name_or_alias\n\n    return ast.Field(\n        alias=alias,\n        name=name,\n        arguments=parse_arguments(parser),\n        directives=parse_directives(parser),\n        selection_set=parse_selection_set(parser) if peek(parser, TokenKind.BRACE_L) else None,\n        loc=loc(parser, start)\n    )\n\n\ndef parse_arguments(parser):\n    if peek(parser, TokenKind.PAREN_L):\n        return many(\n            parser, TokenKind.PAREN_L,\n            parse_argument, TokenKind.PAREN_R)\n\n    return []\n\n\ndef parse_argument(parser):\n    start = parser.token.start\n\n    return ast.Argument(\n        name=parse_name(parser),\n        value=expect(parser, TokenKind.COLON) and parse_value_literal(parser, False),\n        loc=loc(parser, start)\n    )\n\n\n# Implements the parsing rules in the Fragments section.\n\ndef parse_fragment(parser):\n    # Corresponds to both FragmentSpread and InlineFragment in the spec\n    start = parser.token.start\n    expect(parser, TokenKind.SPREAD)\n\n    if peek(parser, TokenKind.NAME) and parser.token.value != 'on':\n        return ast.FragmentSpread(\n            name=parse_fragment_name(parser),\n            directives=parse_directives(parser),\n            loc=loc(parser, start)\n        )\n\n    type_condition = None\n    if parser.token.value == 'on':\n        advance(parser)\n        type_condition = parse_named_type(parser)\n\n    return ast.InlineFragment(\n        type_condition=type_condition,\n        directives=parse_directives(parser),\n        selection_set=parse_selection_set(parser),\n        loc=loc(parser, start)\n    )\n\n\ndef parse_fragment_definition(parser):\n    start = parser.token.start\n    expect_keyword(parser, 'fragment')\n\n    return ast.FragmentDefinition(\n        name=parse_fragment_name(parser),\n        type_condition=expect_keyword(parser, 'on') and parse_named_type(parser),\n        directives=parse_directives(parser),\n        selection_set=parse_selection_set(parser),\n        loc=loc(parser, start)\n    )\n\n\ndef parse_fragment_name(parser):\n    if parser.token.value == 'on':\n        raise unexpected(parser)\n\n    return parse_name(parser)\n\n\ndef parse_value_literal(parser, is_const):\n    token = parser.token\n    if token.kind == TokenKind.BRACKET_L:\n        return parse_list(parser, is_const)\n\n    elif token.kind == TokenKind.BRACE_L:\n        return parse_object(parser, is_const)\n\n    elif token.kind == TokenKind.INT:\n        advance(parser)\n        return ast.IntValue(value=token.value, loc=loc(parser, token.start))\n\n    elif token.kind == TokenKind.FLOAT:\n        advance(parser)\n        return ast.FloatValue(value=token.value, loc=loc(parser, token.start))\n\n    elif token.kind == TokenKind.STRING:\n        advance(parser)\n        return ast.StringValue(value=token.value, loc=loc(parser, token.start))\n\n    elif token.kind == TokenKind.NAME:\n        if token.value in ('true', 'false'):\n            advance(parser)\n            return ast.BooleanValue(value=token.value == 'true', loc=loc(parser, token.start))\n\n        if token.value != 'null':\n            advance(parser)\n            return ast.EnumValue(value=token.value, loc=loc(parser, token.start))\n\n    elif token.kind == TokenKind.DOLLAR:\n        if not is_const:\n            return parse_variable(parser)\n\n    raise unexpected(parser)\n\n\n# Implements the parsing rules in the Values section.\ndef parse_variable_value(parser):\n    return parse_value_literal(parser, False)\n\n\ndef parse_const_value(parser):\n    return parse_value_literal(parser, True)\n\n\ndef parse_list(parser, is_const):\n    start = parser.token.start\n    item = parse_const_value if is_const else parse_variable_value\n\n    return ast.ListValue(\n        values=any(\n            parser, TokenKind.BRACKET_L,\n            item, TokenKind.BRACKET_R),\n        loc=loc(parser, start)\n    )\n\n\ndef parse_object(parser, is_const):\n    start = parser.token.start\n    expect(parser, TokenKind.BRACE_L)\n    fields = []\n\n    while not skip(parser, TokenKind.BRACE_R):\n        fields.append(parse_object_field(parser, is_const))\n\n    return ast.ObjectValue(fields=fields, loc=loc(parser, start))\n\n\ndef parse_object_field(parser, is_const):\n    start = parser.token.start\n    return ast.ObjectField(\n        name=parse_name(parser),\n        value=expect(parser, TokenKind.COLON) and parse_value_literal(parser, is_const),\n        loc=loc(parser, start)\n    )\n\n\n# Implements the parsing rules in the Directives section.\n\ndef parse_directives(parser):\n    directives = []\n    while peek(parser, TokenKind.AT):\n        directives.append(parse_directive(parser))\n    return directives\n\n\ndef parse_directive(parser):\n    start = parser.token.start\n    expect(parser, TokenKind.AT)\n\n    return ast.Directive(\n        name=parse_name(parser),\n        arguments=parse_arguments(parser),\n        loc=loc(parser, start),\n    )\n\n\n# Implements the parsing rules in the Types section.\ndef parse_type(parser):\n    \"\"\"Handles the 'Type': TypeName, ListType, and NonNullType\n    parsing rules.\"\"\"\n    start = parser.token.start\n    if skip(parser, TokenKind.BRACKET_L):\n        ast_type = parse_type(parser)\n        expect(parser, TokenKind.BRACKET_R)\n        ast_type = ast.ListType(type=ast_type, loc=loc(parser, start))\n\n    else:\n        ast_type = parse_named_type(parser)\n\n    if skip(parser, TokenKind.BANG):\n        return ast.NonNullType(type=ast_type, loc=loc(parser, start))\n\n    return ast_type\n\n\ndef parse_named_type(parser):\n    start = parser.token.start\n    return ast.NamedType(\n        name=parse_name(parser),\n        loc=loc(parser, start),\n    )\n\n\ndef parse_type_system_definition(parser):\n    '''\n      TypeSystemDefinition :\n        - SchemaDefinition\n        - TypeDefinition\n        - TypeExtensionDefinition\n        - DirectiveDefinition\n\n      TypeDefinition :\n      - ScalarTypeDefinition\n      - ObjectTypeDefinition\n      - InterfaceTypeDefinition\n      - UnionTypeDefinition\n      - EnumTypeDefinition\n      - InputObjectTypeDefinition\n    '''\n    if not peek(parser, TokenKind.NAME):\n        raise unexpected(parser)\n\n    name = parser.token.value\n\n    if name == 'schema':\n        return parse_schema_definition(parser)\n\n    elif name == 'scalar':\n        return parse_scalar_type_definition(parser)\n\n    elif name == 'type':\n        return parse_object_type_definition(parser)\n\n    elif name == 'interface':\n        return parse_interface_type_definition(parser)\n\n    elif name == 'union':\n        return parse_union_type_definition(parser)\n\n    elif name == 'enum':\n        return parse_enum_type_definition(parser)\n\n    elif name == 'input':\n        return parse_input_object_type_definition(parser)\n\n    elif name == 'extend':\n        return parse_type_extension_definition(parser)\n\n    elif name == 'directive':\n        return parse_directive_definition(parser)\n\n    raise unexpected(parser)\n\n\ndef parse_schema_definition(parser):\n    start = parser.token.start\n    expect_keyword(parser, 'schema')\n    directives = parse_directives(parser)\n    operation_types = many(\n        parser,\n        TokenKind.BRACE_L,\n        parse_operation_type_definition,\n        TokenKind.BRACE_R\n    )\n\n    return ast.SchemaDefinition(\n        directives=directives,\n        operation_types=operation_types,\n        loc=loc(parser, start)\n    )\n\n\ndef parse_operation_type_definition(parser):\n    start = parser.token.start\n    operation = parse_operation_type(parser)\n    expect(parser, TokenKind.COLON)\n\n    return ast.OperationTypeDefinition(\n        operation=operation,\n        type=parse_named_type(parser),\n        loc=loc(parser, start)\n    )\n\n\ndef parse_scalar_type_definition(parser):\n    start = parser.token.start\n    expect_keyword(parser, 'scalar')\n\n    return ast.ScalarTypeDefinition(\n        name=parse_name(parser),\n        directives=parse_directives(parser),\n        loc=loc(parser, start),\n    )\n\n\ndef parse_object_type_definition(parser):\n    start = parser.token.start\n    expect_keyword(parser, 'type')\n    return ast.ObjectTypeDefinition(\n        name=parse_name(parser),\n        interfaces=parse_implements_interfaces(parser),\n        directives=parse_directives(parser),\n        fields=any(\n            parser,\n            TokenKind.BRACE_L,\n            parse_field_definition,\n            TokenKind.BRACE_R\n        ),\n        loc=loc(parser, start),\n    )\n\n\ndef parse_implements_interfaces(parser):\n    types = []\n    if parser.token.value == 'implements':\n        advance(parser)\n\n        while True:\n            types.append(parse_named_type(parser))\n\n            if not peek(parser, TokenKind.NAME):\n                break\n\n    return types\n\n\ndef parse_field_definition(parser):\n    start = parser.token.start\n\n    return ast.FieldDefinition(\n        name=parse_name(parser),\n        arguments=parse_argument_defs(parser),\n        type=expect(parser, TokenKind.COLON) and parse_type(parser),\n        directives=parse_directives(parser),\n        loc=loc(parser, start),\n    )\n\n\ndef parse_argument_defs(parser):\n    if not peek(parser, TokenKind.PAREN_L):\n        return []\n\n    return many(parser, TokenKind.PAREN_L, parse_input_value_def, TokenKind.PAREN_R)\n\n\ndef parse_input_value_def(parser):\n    start = parser.token.start\n\n    return ast.InputValueDefinition(\n        name=parse_name(parser),\n        type=expect(parser, TokenKind.COLON) and parse_type(parser),\n        default_value=parse_const_value(parser) if skip(parser, TokenKind.EQUALS) else None,\n        directives=parse_directives(parser),\n        loc=loc(parser, start),\n    )\n\n\ndef parse_interface_type_definition(parser):\n    start = parser.token.start\n    expect_keyword(parser, 'interface')\n\n    return ast.InterfaceTypeDefinition(\n        name=parse_name(parser),\n        directives=parse_directives(parser),\n        fields=any(parser, TokenKind.BRACE_L, parse_field_definition, TokenKind.BRACE_R),\n        loc=loc(parser, start),\n    )\n\n\ndef parse_union_type_definition(parser):\n    start = parser.token.start\n    expect_keyword(parser, 'union')\n\n    return ast.UnionTypeDefinition(\n        name=parse_name(parser),\n        directives=parse_directives(parser),\n        types=expect(parser, TokenKind.EQUALS) and parse_union_members(parser),\n        loc=loc(parser, start),\n    )\n\n\ndef parse_union_members(parser):\n    members = []\n\n    while True:\n        members.append(parse_named_type(parser))\n\n        if not skip(parser, TokenKind.PIPE):\n            break\n\n    return members\n\n\ndef parse_enum_type_definition(parser):\n    start = parser.token.start\n    expect_keyword(parser, 'enum')\n\n    return ast.EnumTypeDefinition(\n        name=parse_name(parser),\n        directives=parse_directives(parser),\n        values=many(parser, TokenKind.BRACE_L, parse_enum_value_definition, TokenKind.BRACE_R),\n        loc=loc(parser, start),\n    )\n\n\ndef parse_enum_value_definition(parser):\n    start = parser.token.start\n\n    return ast.EnumValueDefinition(\n        name=parse_name(parser),\n        directives=parse_directives(parser),\n        loc=loc(parser, start),\n    )\n\n\ndef parse_input_object_type_definition(parser):\n    start = parser.token.start\n    expect_keyword(parser, 'input')\n\n    return ast.InputObjectTypeDefinition(\n        name=parse_name(parser),\n        directives=parse_directives(parser),\n        fields=any(parser, TokenKind.BRACE_L, parse_input_value_def, TokenKind.BRACE_R),\n        loc=loc(parser, start),\n    )\n\n\ndef parse_type_extension_definition(parser):\n    start = parser.token.start\n    expect_keyword(parser, 'extend')\n\n    return ast.TypeExtensionDefinition(\n        definition=parse_object_type_definition(parser),\n        loc=loc(parser, start)\n    )\n\n\ndef parse_directive_definition(parser):\n    start = parser.token.start\n    expect_keyword(parser, 'directive')\n    expect(parser, TokenKind.AT)\n\n    name = parse_name(parser)\n    args = parse_argument_defs(parser)\n    expect_keyword(parser, 'on')\n\n    locations = parse_directive_locations(parser)\n    return ast.DirectiveDefinition(\n        name=name,\n        locations=locations,\n        arguments=args,\n        loc=loc(parser, start)\n    )\n\n\ndef parse_directive_locations(parser):\n    locations = []\n\n    while True:\n        locations.append(parse_name(parser))\n\n        if not skip(parser, TokenKind.PIPE):\n            break\n\n    return locations\n", 
    "graphql.language.printer": "import json\n\nfrom .visitor import Visitor, visit\n\n__all__ = ['print_ast']\n\n\ndef print_ast(ast):\n    return visit(ast, PrintingVisitor())\n\n\nclass PrintingVisitor(Visitor):\n    __slots__ = ()\n\n    def leave_Name(self, node, *args):\n        return node.value\n\n    def leave_Variable(self, node, *args):\n        return '$' + node.name\n\n    def leave_Document(self, node, *args):\n        return join(node.definitions, '\\n\\n') + '\\n'\n\n    def leave_OperationDefinition(self, node, *args):\n        name = node.name\n        selection_set = node.selection_set\n        op = node.operation\n        var_defs = wrap('(', join(node.variable_definitions, ', '), ')')\n        directives = join(node.directives, ' ')\n\n        if not name and not directives and not var_defs and op == 'query':\n            return selection_set\n\n        return join([op, join([name, var_defs]), directives, selection_set], ' ')\n\n    def leave_VariableDefinition(self, node, *args):\n        return node.variable + ': ' + node.type + wrap(' = ', node.default_value)\n\n    def leave_SelectionSet(self, node, *args):\n        return block(node.selections)\n\n    def leave_Field(self, node, *args):\n        return join([\n            wrap('', node.alias, ': ') + node.name + wrap('(', join(node.arguments, ', '), ')'),\n            join(node.directives, ' '),\n            node.selection_set\n        ], ' ')\n\n    def leave_Argument(self, node, *args):\n        return node.name + ': ' + node.value\n\n    # Fragments\n\n    def leave_FragmentSpread(self, node, *args):\n        return '...' + node.name + wrap(' ', join(node.directives, ' '))\n\n    def leave_InlineFragment(self, node, *args):\n        return join([\n            '...',\n            wrap('on ', node.type_condition),\n            join(node.directives, ''),\n            node.selection_set\n        ], ' ')\n\n    def leave_FragmentDefinition(self, node, *args):\n        return ('fragment {} on {} '.format(node.name, node.type_condition) +\n                wrap('', join(node.directives, ' '), ' ') +\n                node.selection_set)\n\n    # Value\n\n    def leave_IntValue(self, node, *args):\n        return node.value\n\n    def leave_FloatValue(self, node, *args):\n        return node.value\n\n    def leave_StringValue(self, node, *args):\n        return json.dumps(node.value)\n\n    def leave_BooleanValue(self, node, *args):\n        return json.dumps(node.value)\n\n    def leave_EnumValue(self, node, *args):\n        return node.value\n\n    def leave_ListValue(self, node, *args):\n        return '[' + join(node.values, ', ') + ']'\n\n    def leave_ObjectValue(self, node, *args):\n        return '{' + join(node.fields, ', ') + '}'\n\n    def leave_ObjectField(self, node, *args):\n        return node.name + ': ' + node.value\n\n    # Directive\n\n    def leave_Directive(self, node, *args):\n        return '@' + node.name + wrap('(', join(node.arguments, ', '), ')')\n\n    # Type\n\n    def leave_NamedType(self, node, *args):\n        return node.name\n\n    def leave_ListType(self, node, *args):\n        return '[' + node.type + ']'\n\n    def leave_NonNullType(self, node, *args):\n        return node.type + '!'\n\n    # Type Definitions:\n\n    def leave_SchemaDefinition(self, node, *args):\n        return join([\n            'schema',\n            join(node.directives, ' '),\n            block(node.operation_types),\n            ], ' ')\n\n    def leave_OperationTypeDefinition(self, node, *args):\n        return '{}: {}'.format(node.operation, node.type)\n\n    def leave_ScalarTypeDefinition(self, node, *args):\n        return 'scalar ' + node.name + wrap(' ', join(node.directives, ' '))\n\n    def leave_ObjectTypeDefinition(self, node, *args):\n        return join([\n            'type',\n            node.name,\n            wrap('implements ', join(node.interfaces, ', ')),\n            join(node.directives, ' '),\n            block(node.fields)\n        ], ' ')\n\n    def leave_FieldDefinition(self, node, *args):\n        return (\n            node.name +\n            wrap('(', join(node.arguments, ', '), ')') +\n            ': ' +\n            node.type +\n            wrap(' ', join(node.directives, ' '))\n        )\n\n    def leave_InputValueDefinition(self, node, *args):\n        return node.name + ': ' + node.type + wrap(' = ', node.default_value) + wrap(' ', join(node.directives, ' '))\n\n    def leave_InterfaceTypeDefinition(self, node, *args):\n        return 'interface ' + node.name + wrap(' ', join(node.directives, ' ')) + ' ' + block(node.fields)\n\n    def leave_UnionTypeDefinition(self, node, *args):\n        return 'union ' + node.name + wrap(' ', join(node.directives, ' ')) + ' = ' + join(node.types, ' | ')\n\n    def leave_EnumTypeDefinition(self, node, *args):\n        return 'enum ' + node.name + wrap(' ', join(node.directives, ' ')) + ' ' + block(node.values)\n\n    def leave_EnumValueDefinition(self, node, *args):\n        return node.name + wrap(' ', join(node.directives, ' '))\n\n    def leave_InputObjectTypeDefinition(self, node, *args):\n        return 'input ' + node.name + wrap(' ', join(node.directives, ' ')) + ' ' + block(node.fields)\n\n    def leave_TypeExtensionDefinition(self, node, *args):\n        return 'extend ' + node.definition\n\n    def leave_DirectiveDefinition(self, node, *args):\n        return 'directive @{}{} on {}'.format(node.name, wrap(\n            '(', join(node.arguments, ', '), ')'), ' | '.join(node.locations))\n\n\ndef join(maybe_list, separator=''):\n    if maybe_list:\n        return separator.join(filter(None, maybe_list))\n    return ''\n\n\ndef block(_list):\n    '''Given a list, print each item on its own line, wrapped in an indented \"{ }\" block.'''\n    if _list:\n        return indent('{\\n' + join(_list, '\\n')) + '\\n}'\n    return '{}'\n\n\ndef wrap(start, maybe_str, end=''):\n    if maybe_str:\n        return start + maybe_str + end\n    return ''\n\n\ndef indent(maybe_str):\n    if maybe_str:\n        return maybe_str.replace('\\n', '\\n  ')\n    return maybe_str\n", 
    "graphql.language.source": "__all__ = ['Source']\n\n\nclass Source(object):\n    __slots__ = 'body', 'name'\n\n    def __init__(self, body, name='GraphQL'):\n        self.body = body\n        self.name = name\n\n    def __eq__(self, other):\n        return (\n            self is other or (\n                isinstance(other, Source) and\n                self.body == other.body and\n                self.name == other.name\n            )\n        )\n", 
    "graphql.language.visitor": "from copy import copy\n\nimport six\n\nfrom . import ast\nfrom .visitor_meta import QUERY_DOCUMENT_KEYS, VisitorMeta\n\n\nclass Falsey(object):\n\n    def __nonzero__(self):\n        return False\n\n    def __bool__(self):\n        return False\n\n\nBREAK = object()\nREMOVE = Falsey()\n\n\nclass Stack(object):\n    __slots__ = 'in_array', 'index', 'keys', 'edits', 'prev'\n\n    def __init__(self, in_array, index, keys, edits, prev):\n        self.in_array = in_array\n        self.index = index\n        self.keys = keys\n        self.edits = edits\n        self.prev = prev\n\n\ndef visit(root, visitor, key_map=None):\n    visitor_keys = key_map or QUERY_DOCUMENT_KEYS\n\n    stack = None\n    in_array = isinstance(root, list)\n    keys = [root]\n    index = -1\n    edits = []\n    parent = None\n    path = []\n    ancestors = []\n    new_root = root\n    leave = visitor.leave\n    enter = visitor.enter\n    path_pop = path.pop\n    ancestors_pop = ancestors.pop\n    path_append = path.append\n    ancestors_append = ancestors.append\n\n    while True:\n        index += 1\n        is_leaving = index == len(keys)\n        is_edited = is_leaving and edits\n        if is_leaving:\n            key = path_pop() if ancestors else None\n            node = parent\n            parent = ancestors_pop() if ancestors else None\n\n            if is_edited:\n                if in_array:\n                    node = list(node)\n\n                else:\n                    node = copy(node)\n                edit_offset = 0\n                for edit_key, edit_value in edits:\n                    if in_array:\n                        edit_key -= edit_offset\n\n                    if in_array and edit_value is REMOVE:\n                        node.pop(edit_key)\n                        edit_offset += 1\n\n                    else:\n                        if isinstance(node, list):\n                            node[edit_key] = edit_value\n\n                        else:\n                            setattr(node, edit_key, edit_value)\n\n            index = stack.index\n            keys = stack.keys\n            edits = stack.edits\n            in_array = stack.in_array\n            stack = stack.prev\n\n        else:\n            if parent:\n                key = index if in_array else keys[index]\n                if isinstance(parent, list):\n                    node = parent[key]\n\n                else:\n                    node = getattr(parent, key, None)\n\n            else:\n                key = None\n                node = new_root\n\n            if node is REMOVE or node is None:\n                continue\n\n            if parent:\n                path_append(key)\n\n        result = None\n\n        if not isinstance(node, list):\n            assert isinstance(node, ast.Node), 'Invalid AST Node: ' + repr(node)\n\n            if is_leaving:\n                result = leave(node, key, parent, path, ancestors)\n\n            else:\n                result = enter(node, key, parent, path, ancestors)\n\n            if result is BREAK:\n                break\n\n            if result is False:\n                if not is_leaving:\n                    path_pop()\n                    continue\n\n            elif result is not None:\n                edits.append((key, result))\n                if not is_leaving:\n                    if isinstance(result, ast.Node):\n                        node = result\n\n                    else:\n                        path_pop()\n                        continue\n\n        if result is None and is_edited:\n            edits.append((key, node))\n\n        if not is_leaving:\n            stack = Stack(in_array, index, keys, edits, stack)\n            in_array = isinstance(node, list)\n            keys = node if in_array else visitor_keys.get(type(node), None) or []\n            index = -1\n            edits = []\n\n            if parent:\n                ancestors_append(parent)\n\n            parent = node\n\n        if not stack:\n            break\n\n    if edits:\n        new_root = edits[-1][1]\n\n    return new_root\n\n\n@six.add_metaclass(VisitorMeta)\nclass Visitor(object):\n    __slots__ = ()\n\n    def enter(self, node, key, parent, path, ancestors):\n        method = self._get_enter_handler(type(node))\n        if method:\n            return method(self, node, key, parent, path, ancestors)\n\n    def leave(self, node, key, parent, path, ancestors):\n        method = self._get_leave_handler(type(node))\n        if method:\n            return method(self, node, key, parent, path, ancestors)\n\n\nclass ParallelVisitor(Visitor):\n    __slots__ = 'skipping', 'visitors'\n\n    def __init__(self, visitors):\n        self.visitors = visitors\n        self.skipping = [None] * len(visitors)\n\n    def enter(self, node, key, parent, path, ancestors):\n        for i, visitor in enumerate(self.visitors):\n            if not self.skipping[i]:\n                result = visitor.enter(node, key, parent, path, ancestors)\n                if result is False:\n                    self.skipping[i] = node\n                elif result is BREAK:\n                    self.skipping[i] = BREAK\n                elif result is not None:\n                    return result\n\n    def leave(self, node, key, parent, path, ancestors):\n        for i, visitor in enumerate(self.visitors):\n            if not self.skipping[i]:\n                result = visitor.leave(node, key, parent, path, ancestors)\n                if result is BREAK:\n                    self.skipping[i] = BREAK\n                elif result is not None and result is not False:\n                    return result\n            elif self.skipping[i] == node:\n                self.skipping[i] = REMOVE\n\n\nclass TypeInfoVisitor(Visitor):\n    __slots__ = 'visitor', 'type_info'\n\n    def __init__(self, type_info, visitor):\n        self.type_info = type_info\n        self.visitor = visitor\n\n    def enter(self, node, key, parent, path, ancestors):\n        self.type_info.enter(node)\n        result = self.visitor.enter(node, key, parent, path, ancestors)\n        if result is not None:\n            self.type_info.leave(node)\n            if isinstance(result, ast.Node):\n                self.type_info.enter(result)\n        return result\n\n    def leave(self, node, key, parent, path, ancestors):\n        result = self.visitor.leave(node, key, parent, path, ancestors)\n        self.type_info.leave(node)\n        return result\n", 
    "graphql.language.visitor_meta": "from . import ast\n\nQUERY_DOCUMENT_KEYS = {\n    ast.Name: (),\n\n    ast.Document: ('definitions',),\n    ast.OperationDefinition: ('name', 'variable_definitions', 'directives', 'selection_set'),\n    ast.VariableDefinition: ('variable', 'type', 'default_value'),\n    ast.Variable: ('name',),\n    ast.SelectionSet: ('selections',),\n    ast.Field: ('alias', 'name', 'arguments', 'directives', 'selection_set'),\n    ast.Argument: ('name', 'value'),\n\n    ast.FragmentSpread: ('name', 'directives'),\n    ast.InlineFragment: ('type_condition', 'directives', 'selection_set'),\n    ast.FragmentDefinition: ('name', 'type_condition', 'directives', 'selection_set'),\n\n    ast.IntValue: (),\n    ast.FloatValue: (),\n    ast.StringValue: (),\n    ast.BooleanValue: (),\n    ast.EnumValue: (),\n    ast.ListValue: ('values',),\n    ast.ObjectValue: ('fields',),\n    ast.ObjectField: ('name', 'value'),\n\n    ast.Directive: ('name', 'arguments'),\n\n    ast.NamedType: ('name',),\n    ast.ListType: ('type',),\n    ast.NonNullType: ('type',),\n\n    ast.SchemaDefinition: ('directives', 'operation_types',),\n    ast.OperationTypeDefinition: ('type',),\n\n    ast.ScalarTypeDefinition: ('name', 'directives',),\n    ast.ObjectTypeDefinition: ('name', 'interfaces', 'directives', 'fields'),\n    ast.FieldDefinition: ('name', 'arguments', 'directives', 'type'),\n    ast.InputValueDefinition: ('name', 'type', 'directives', 'default_value'),\n    ast.InterfaceTypeDefinition: ('name', 'directives', 'fields'),\n    ast.UnionTypeDefinition: ('name', 'directives', 'types'),\n    ast.EnumTypeDefinition: ('name', 'directives', 'values'),\n    ast.EnumValueDefinition: ('name', 'directives',),\n    ast.InputObjectTypeDefinition: ('name', 'directives', 'fields'),\n\n    ast.TypeExtensionDefinition: ('definition',),\n\n    ast.DirectiveDefinition: ('name', 'arguments', 'locations'),\n}\n\nAST_KIND_TO_TYPE = {c.__name__: c for c in QUERY_DOCUMENT_KEYS.keys()}\n\n\nclass VisitorMeta(type):\n\n    def __new__(cls, name, bases, attrs):\n        enter_handlers = {}\n        leave_handlers = {}\n\n        for base in bases:\n            if hasattr(base, '_enter_handlers'):\n                enter_handlers.update(base._enter_handlers)\n\n            if hasattr(base, '_leave_handlers'):\n                leave_handlers.update(base._leave_handlers)\n\n        for attr, val in attrs.items():\n            if attr.startswith('enter_'):\n                ast_kind = attr[6:]\n                ast_type = AST_KIND_TO_TYPE.get(ast_kind)\n                enter_handlers[ast_type] = val\n\n            elif attr.startswith('leave_'):\n                ast_kind = attr[6:]\n                ast_type = AST_KIND_TO_TYPE.get(ast_kind)\n                leave_handlers[ast_type] = val\n\n        attrs['_enter_handlers'] = enter_handlers\n        attrs['_leave_handlers'] = leave_handlers\n        attrs['_get_enter_handler'] = enter_handlers.get\n        attrs['_get_leave_handler'] = leave_handlers.get\n        return super(VisitorMeta, cls).__new__(cls, name, bases, attrs)\n", 
    "graphql.pyutils.__init__": "", 
    "graphql.pyutils.cached_property": "class cached_property(object):\n    \"\"\" A property that is only computed once per instance and then replaces\n        itself with an ordinary attribute. Deleting the attribute resets the\n        property.\n\n        Source: https://github.com/bottlepy/bottle/commit/fa7733e075da0d790d809aa3d2f53071897e6f76\n        \"\"\"\n\n    def __init__(self, func):\n        self.__doc__ = getattr(func, '__doc__')\n        self.func = func\n\n    def __get__(self, obj, cls):\n        if obj is None:\n            return self\n        value = obj.__dict__[self.func.__name__] = self.func(obj)\n        return value\n", 
    "graphql.pyutils.default_ordered_dict": "import copy\nfrom collections import OrderedDict\n\n\nclass DefaultOrderedDict(OrderedDict):\n    __slots__ = 'default_factory',\n\n    # Source: http://stackoverflow.com/a/6190500/562769\n    def __init__(self, default_factory=None, *a, **kw):\n        if default_factory is not None and not callable(default_factory):\n            raise TypeError('first argument must be callable')\n\n        OrderedDict.__init__(self, *a, **kw)\n        self.default_factory = default_factory\n\n    def __missing__(self, key):\n        if self.default_factory is None:\n            raise KeyError(key)\n        self[key] = value = self.default_factory()\n        return value\n\n    def __reduce__(self):\n        if self.default_factory is None:\n            args = tuple()\n        else:\n            args = self.default_factory,\n        return type(self), args, None, None, iter(self.items())\n\n    def copy(self):\n        return self.__copy__()\n\n    def __copy__(self):\n        return type(self)(self.default_factory, self)\n\n    def __deepcopy__(self, memo):\n        return self.__class__(self.default_factory, copy.deepcopy(list(self.items())))\n\n    def __repr__(self):\n        return 'DefaultOrderedDict(%s, %s)' % (self.default_factory,\n                                               OrderedDict.__repr__(self)[19:-1])\n", 
    "graphql.pyutils.ordereddict": "try:\n    # Try to load the Cython performant OrderedDict (C)\n    # as is more performant than collections.OrderedDict (Python)\n    from cyordereddict import OrderedDict\nexcept ImportError:\n    from collections import OrderedDict\n\n__all__ = ['OrderedDict']\n", 
    "graphql.pyutils.pair_set": "class PairSet(object):\n    __slots__ = '_data',\n\n    def __init__(self):\n        self._data = {}\n\n    def __contains__(self, item):\n        return self.has(item[0], item[1], item[2])\n\n    def __str__(self):\n        return str(self._data)\n\n    def __repr__(self):\n        return str(self._data)\n\n    def has(self, a, b, are_mutually_exclusive):\n        first = self._data.get(a)\n        print(first)\n        result = first and first.get(b)\n        print(result)\n        if result is None:\n            return False\n\n        # are_mutually_exclusive being false is a superset of being true,\n        # hence if we want to know if this PairSet \"has\" these two with no\n        # exclusivity, we have to ensure it was added as such.\n        if not are_mutually_exclusive:\n            return not result\n\n        return True\n\n    def add(self, a, b, are_mutually_exclusive):\n        _pair_set_add(self._data, a, b, are_mutually_exclusive)\n        _pair_set_add(self._data, b, a, are_mutually_exclusive)\n        return self\n\n\ndef _pair_set_add(data, a, b, are_mutually_exclusive):\n    sub_dict = data.get(a)\n\n    if not sub_dict:\n        sub_dict = {}\n        data[a] = sub_dict\n\n    sub_dict[b] = are_mutually_exclusive\n", 
    "graphql.pyutils.version": "from __future__ import unicode_literals\n\nimport datetime\nimport os\nimport subprocess\n\n\ndef get_version(version=None):\n    \"Returns a PEP 440-compliant version number from VERSION.\"\n    version = get_complete_version(version)\n\n    # Now build the two parts of the version number:\n    # main = X.Y[.Z]\n    # sub = .devN - for pre-alpha releases\n    #     | {a|b|rc}N - for alpha, beta, and rc releases\n\n    main = get_main_version(version)\n\n    sub = ''\n    if version[3] == 'alpha' and version[4] == 0:\n        git_changeset = get_git_changeset()\n        if git_changeset:\n            sub = '.dev%s' % git_changeset\n        else:\n            sub = '.dev'\n    elif version[3] != 'final':\n        mapping = {'alpha': 'a', 'beta': 'b', 'rc': 'rc'}\n        sub = mapping[version[3]] + str(version[4])\n\n    return str(main + sub)\n\n\ndef get_main_version(version=None):\n    \"Returns main version (X.Y[.Z]) from VERSION.\"\n    version = get_complete_version(version)\n    parts = 2 if version[2] == 0 else 3\n    return '.'.join(str(x) for x in version[:parts])\n\n\ndef get_complete_version(version=None):\n    \"\"\"Returns a tuple of the graphql version. If version argument is non-empty,\n    then checks for correctness of the tuple provided.\n    \"\"\"\n    if version is None:\n        from graphql import VERSION as version\n    else:\n        assert len(version) == 5\n        assert version[3] in ('alpha', 'beta', 'rc', 'final')\n\n    return version\n\n\ndef get_docs_version(version=None):\n    version = get_complete_version(version)\n    if version[3] != 'final':\n        return 'dev'\n    else:\n        return '%d.%d' % version[:2]\n\n\ndef get_git_changeset():\n    \"\"\"Returns a numeric identifier of the latest git changeset.\n    The result is the UTC timestamp of the changeset in YYYYMMDDHHMMSS format.\n    This value isn't guaranteed to be unique, but collisions are very unlikely,\n    so it's sufficient for generating the development version numbers.\n    \"\"\"\n    repo_dir = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))\n    try:\n        git_log = subprocess.Popen(\n            'git log --pretty=format:%ct --quiet -1 HEAD',\n            stdout=subprocess.PIPE, stderr=subprocess.PIPE,\n            shell=True, cwd=repo_dir, universal_newlines=True,\n        )\n        timestamp = git_log.communicate()[0]\n        timestamp = datetime.datetime.utcfromtimestamp(int(timestamp))\n    except:\n        return None\n    return timestamp.strftime('%Y%m%d%H%M%S')\n", 
    "graphql.type.__init__": "# flake8: noqa\nfrom .definition import (  # no import order\n    GraphQLScalarType,\n    GraphQLObjectType,\n    GraphQLField,\n    GraphQLArgument,\n    GraphQLInterfaceType,\n    GraphQLUnionType,\n    GraphQLEnumType,\n    GraphQLEnumValue,\n    GraphQLInputObjectType,\n    GraphQLInputObjectField,\n    GraphQLList,\n    GraphQLNonNull,\n    get_named_type,\n    is_abstract_type,\n    is_composite_type,\n    is_input_type,\n    is_leaf_type,\n    is_type,\n    get_nullable_type,\n    is_output_type\n)\nfrom .directives import (\n    # \"Enum\" of Directive locations\n    DirectiveLocation,\n\n    # Directive definition\n    GraphQLDirective,\n\n    # Built-in directives defined by the Spec\n    specified_directives,\n    GraphQLSkipDirective,\n    GraphQLIncludeDirective,\n    GraphQLDeprecatedDirective,\n\n    # Constant Deprecation Reason\n    DEFAULT_DEPRECATION_REASON,\n)\nfrom .scalars import (  # no import order\n    GraphQLInt,\n    GraphQLFloat,\n    GraphQLString,\n    GraphQLBoolean,\n    GraphQLID,\n)\nfrom .schema import GraphQLSchema\n\nfrom .introspection import (\n    # \"Enum\" of Type Kinds\n    TypeKind,\n\n    # GraphQL Types for introspection.\n    __Schema,\n    __Directive,\n    __DirectiveLocation,\n    __Type,\n    __Field,\n    __InputValue,\n    __EnumValue,\n    __TypeKind,\n\n    # Meta-field definitions.\n    SchemaMetaFieldDef,\n    TypeMetaFieldDef,\n    TypeNameMetaFieldDef\n)\n", 
    "graphql.type.definition": "import collections\nimport copy\n\nfrom ..language import ast\nfrom ..pyutils.cached_property import cached_property\nfrom ..pyutils.ordereddict import OrderedDict\nfrom ..utils.assert_valid_name import assert_valid_name\n\n\ndef is_type(type):\n    return isinstance(type, (\n        GraphQLScalarType,\n        GraphQLObjectType,\n        GraphQLInterfaceType,\n        GraphQLUnionType,\n        GraphQLEnumType,\n        GraphQLInputObjectType,\n        GraphQLList,\n        GraphQLNonNull\n    ))\n\n\ndef is_input_type(type):\n    named_type = get_named_type(type)\n    return isinstance(named_type, (\n        GraphQLScalarType,\n        GraphQLEnumType,\n        GraphQLInputObjectType,\n    ))\n\n\ndef is_output_type(type):\n    named_type = get_named_type(type)\n    return isinstance(named_type, (\n        GraphQLScalarType,\n        GraphQLObjectType,\n        GraphQLInterfaceType,\n        GraphQLUnionType,\n        GraphQLEnumType\n    ))\n\n\ndef is_leaf_type(type):\n    named_type = get_named_type(type)\n    return isinstance(named_type, (\n        GraphQLScalarType,\n        GraphQLEnumType,\n    ))\n\n\ndef is_composite_type(type):\n    named_type = get_named_type(type)\n    return isinstance(named_type, (\n        GraphQLObjectType,\n        GraphQLInterfaceType,\n        GraphQLUnionType,\n    ))\n\n\ndef is_abstract_type(type):\n    return isinstance(type, (\n        GraphQLInterfaceType,\n        GraphQLUnionType\n    ))\n\n\ndef get_nullable_type(type):\n    if isinstance(type, GraphQLNonNull):\n        return type.of_type\n    return type\n\n\ndef get_named_type(type):\n    unmodified_type = type\n    while isinstance(unmodified_type, (GraphQLList, GraphQLNonNull)):\n        unmodified_type = unmodified_type.of_type\n\n    return unmodified_type\n\n\nclass GraphQLType(object):\n    __slots__ = 'name',\n\n    def __str__(self):\n        return self.name\n\n    def is_same_type(self, other):\n        return self.__class__ is other.__class__ and self.name == other.name\n\n\ndef none_func(x):\n    None\n\n\nclass GraphQLScalarType(GraphQLType):\n    \"\"\"Scalar Type Definition\n\n    The leaf values of any request and input values to arguments are\n    Scalars (or Enums) and are defined with a name and a series of coercion\n    functions used to ensure validity.\n\n    Example:\n\n        def coerce_odd(value):\n            if value % 2 == 1:\n                return value\n            return None\n\n        OddType = GraphQLScalarType(name='Odd', serialize=coerce_odd)\n    \"\"\"\n\n    __slots__ = 'name', 'description', 'serialize', 'parse_value', 'parse_literal'\n\n    def __init__(self, name, description=None, serialize=None, parse_value=None, parse_literal=None):\n        assert name, 'Type must be named.'\n        assert_valid_name(name)\n        self.name = name\n        self.description = description\n\n        assert callable(serialize), (\n            '{} must provide \"serialize\" function. If this custom Scalar is '\n            'also used as an input type, ensure \"parse_value\" and \"parse_literal\" '\n            'functions are also provided.'\n        ).format(self)\n\n        if parse_value is not None or parse_literal is not None:\n            assert callable(parse_value) and callable(parse_literal), (\n                '{} must provide both \"parse_value\" and \"parse_literal\" functions.'.format(self)\n            )\n\n        self.serialize = serialize\n        self.parse_value = parse_value or none_func\n        self.parse_literal = parse_literal or none_func\n\n    def __str__(self):\n        return self.name\n\n\nclass GraphQLObjectType(GraphQLType):\n    \"\"\"Object Type Definition\n\n    Almost all of the GraphQL types you define will be object types.\n    Object types have a name, but most importantly describe their fields.\n\n    Example:\n\n        AddressType = GraphQLObjectType('Address', {\n            'street': GraphQLField(GraphQLString),\n            'number': GraphQLField(GraphQLInt),\n            'formatted': GraphQLField(GraphQLString,\n                resolver=lambda obj, args, context, info: obj.number + ' ' + obj.street),\n        })\n\n    When two types need to refer to each other, or a type needs to refer to\n    itself in a field, you can use a static method to supply the fields\n    lazily.\n\n    Example:\n\n        PersonType = GraphQLObjectType('Person', lambda: {\n            'name': GraphQLField(GraphQLString),\n            'bestFriend': GraphQLField(PersonType)\n        })\n    \"\"\"\n    def __init__(self, name, fields, interfaces=None, is_type_of=None, description=None):\n        assert name, 'Type must be named.'\n        assert_valid_name(name)\n        self.name = name\n        self.description = description\n\n        if is_type_of is not None:\n            assert callable(is_type_of), '{} must provide \"is_type_of\" as a function.'.format(self)\n\n        self.is_type_of = is_type_of\n        self._fields = fields\n        self._provided_interfaces = interfaces\n        self._interfaces = None\n\n    @cached_property\n    def fields(self):\n        return define_field_map(self, self._fields)\n\n    @cached_property\n    def interfaces(self):\n        return define_interfaces(self, self._provided_interfaces)\n\n\ndef define_field_map(type, field_map):\n    if callable(field_map):\n        field_map = field_map()\n\n    assert isinstance(field_map, collections.Mapping) and len(field_map) > 0, (\n        '{} fields must be a mapping (dict / OrderedDict) with field names as keys or a '\n        'function which returns such a mapping.'\n    ).format(type)\n\n    for field_name, field in field_map.items():\n        assert_valid_name(field_name)\n        field_args = getattr(field, 'args', None)\n\n        if field_args:\n            assert isinstance(field_args, collections.Mapping), (\n                '{}.{} args must be a mapping (dict / OrderedDict) with argument names as keys.'.format(type,\n                                                                                                        field_name)\n            )\n\n            for arg_name, arg in field_args.items():\n                assert_valid_name(arg_name)\n\n    return OrderedDict(field_map)\n\n\ndef define_interfaces(type, interfaces):\n    if callable(interfaces):\n        interfaces = interfaces()\n\n    if interfaces is None:\n        interfaces = []\n\n    assert isinstance(interfaces, (list, tuple)), (\n        '{} interfaces must be a list/tuple or a function which returns a list/tuple.'.format(type)\n    )\n\n    for interface in interfaces:\n        assert isinstance(interface, GraphQLInterfaceType), (\n            '{} may only implement Interface types, it cannot implement: {}.'.format(type, interface)\n        )\n\n        if not callable(interface.resolve_type):\n            assert callable(type.is_type_of), (\n                'Interface Type {} does not provide a \"resolve_type\" function '\n                'and implementing Type {} does not provide a \"is_type_of\" '\n                'function. There is no way to resolve this implementing type '\n                'during execution.'\n            ).format(interface, type)\n\n    return interfaces\n\n\nclass GraphQLField(object):\n    __slots__ = 'type', 'args', 'resolver', 'deprecation_reason', 'description'\n\n    def __init__(self, type, args=None, resolver=None, deprecation_reason=None, description=None):\n        self.type = type\n        self.args = args or OrderedDict()\n        self.resolver = resolver\n        self.deprecation_reason = deprecation_reason\n        self.description = description\n\n    def __eq__(self, other):\n        return (\n            self is other or (\n                isinstance(other, GraphQLField) and\n                self.type == other.type and\n                self.args == other.args and\n                self.resolver == other.resolver and\n                self.deprecation_reason == other.deprecation_reason and\n                self.description == other.description\n            )\n        )\n\n    def __hash__(self):\n        return id(self)\n\n\nclass GraphQLArgument(object):\n    __slots__ = 'type', 'default_value', 'description', 'out_name'\n\n    def __init__(self, type, default_value=None, description=None, out_name=None):\n        self.type = type\n        self.default_value = default_value\n        self.description = description\n        self.out_name = out_name\n\n    def __eq__(self, other):\n        return (\n            self is other or (\n                isinstance(other, GraphQLArgument) and\n                self.type == other.type and\n                self.default_value == other.default_value and\n                self.description == other.description and\n                self.out_name == other.out_name\n            )\n        )\n\n    def __hash__(self):\n        return id(self)\n\n\nclass GraphQLInterfaceType(GraphQLType):\n    \"\"\"Interface Type Definition\n\n    When a field can return one of a heterogeneous set of types, a Interface type is used to describe what types are possible,\n    what fields are in common across all types, as well as a function to determine which type is actually used when the field is resolved.\n\n    Example:\n\n        EntityType = GraphQLInterfaceType(\n            name='Entity',\n            fields={\n                'name': GraphQLField(GraphQLString),\n            })\n    \"\"\"\n\n    def __init__(self, name, fields=None, resolve_type=None, description=None):\n        assert name, 'Type must be named.'\n        assert_valid_name(name)\n        self.name = name\n        self.description = description\n\n        if resolve_type is not None:\n            assert callable(resolve_type), '{} must provide \"resolve_type\" as a function.'.format(self)\n\n        self.resolve_type = resolve_type\n        self._fields = fields\n\n    @cached_property\n    def fields(self):\n        return define_field_map(self, self._fields)\n\n\nclass GraphQLUnionType(GraphQLType):\n    \"\"\"Union Type Definition\n\n    When a field can return one of a heterogeneous set of types, a Union type is used to describe what types are possible\n    as well as providing a function to determine which type is actually used when the field is resolved.\n\n    Example:\n\n        class PetType(GraphQLUnionType):\n            name = 'Pet'\n            types = [DogType, CatType]\n\n            def resolve_type(self, value):\n                if isinstance(value, Dog):\n                    return DogType()\n                if isinstance(value, Cat):\n                    return CatType()\n    \"\"\"\n\n    def __init__(self, name, types=None, resolve_type=None, description=None):\n        assert name, 'Type must be named.'\n        assert_valid_name(name)\n        self.name = name\n        self.description = description\n\n        if resolve_type is not None:\n            assert callable(resolve_type), '{} must provide \"resolve_type\" as a function.'.format(self)\n\n        self.resolve_type = resolve_type\n        self._types = types\n\n    @cached_property\n    def types(self):\n        return define_types(self, self._types)\n\n\ndef define_types(union_type, types):\n    if callable(types):\n        types = types()\n\n    assert isinstance(types, (list, tuple)) and len(\n        types) > 0, 'Must provide types for Union {}.'.format(union_type.name)\n    has_resolve_type_fn = callable(union_type.resolve_type)\n\n    for type in types:\n        assert isinstance(type, GraphQLObjectType), (\n            '{} may only contain Object types, it cannot contain: {}.'.format(union_type, type)\n        )\n\n        if not has_resolve_type_fn:\n            assert callable(type.is_type_of), (\n                'Union Type {} does not provide a \"resolve_type\" function '\n                'and possible Type {} does not provide a \"is_type_of\" '\n                'function. There is no way to resolve this possible type '\n                'during execution.'\n            ).format(union_type, type)\n\n    return types\n\n\nclass GraphQLEnumType(GraphQLType):\n    \"\"\"Enum Type Definition\n\n    Some leaf values of requests and input values are Enums. GraphQL serializes Enum values as strings,\n    however internally Enums can be represented by any kind of type, often integers.\n\n    Example:\n\n        RGBType = GraphQLEnumType(\n            name='RGB',\n            values=OrderedDict([\n                ('RED', GraphQLEnumValue(0)),\n                ('GREEN', GraphQLEnumValue(1)),\n                ('BLUE', GraphQLEnumValue(2))\n            ])\n        )\n\n    Note: If a value is not provided in a definition, the name of the enum value will be used as it's internal value.\n    \"\"\"\n\n    def __init__(self, name, values, description=None):\n        assert name, 'Type must provide name.'\n        assert_valid_name(name)\n        self.name = name\n        self.description = description\n\n        self.values = define_enum_values(self, values)\n\n    def serialize(self, value):\n        if isinstance(value, collections.Hashable):\n            enum_value = self._value_lookup.get(value)\n\n            if enum_value:\n                return enum_value.name\n\n        return None\n\n    def parse_value(self, value):\n        if isinstance(value, collections.Hashable):\n            enum_value = self._name_lookup.get(value)\n\n            if enum_value:\n                return enum_value.value\n\n        return None\n\n    def parse_literal(self, value_ast):\n        if isinstance(value_ast, ast.EnumValue):\n            enum_value = self._name_lookup.get(value_ast.value)\n\n            if enum_value:\n                return enum_value.value\n\n    @cached_property\n    def _value_lookup(self):\n        return {value.value: value for value in self.values}\n\n    @cached_property\n    def _name_lookup(self):\n        return {value.name: value for value in self.values}\n\n\ndef define_enum_values(type, value_map):\n    assert isinstance(value_map, collections.Mapping) and len(value_map) > 0, (\n        '{} values must be a mapping (dict / OrderedDict) with value names as keys.'.format(type)\n    )\n\n    values = []\n    if not isinstance(value_map, (collections.OrderedDict, OrderedDict)):\n        value_map = OrderedDict(sorted(list(value_map.items())))\n\n    for value_name, value in value_map.items():\n        assert_valid_name(value_name)\n        assert isinstance(value, GraphQLEnumValue), (\n            '{}.{} must be an instance of GraphQLEnumValue, but got: {}'.format(type, value_name, value)\n        )\n        value = copy.copy(value)\n        value.name = value_name\n        if value.value is None:\n            value.value = value_name\n\n        values.append(value)\n\n    return values\n\n\nclass GraphQLEnumValue(object):\n    __slots__ = 'name', 'value', 'deprecation_reason', 'description'\n\n    def __init__(self, value=None, deprecation_reason=None, description=None, name=None):\n        self.name = name\n        self.value = value\n        self.deprecation_reason = deprecation_reason\n        self.description = description\n\n    def __eq__(self, other):\n        return (\n            self is other or (\n                isinstance(other, GraphQLEnumValue) and\n                self.name == other.name and\n                self.value == other.value and\n                self.deprecation_reason == other.deprecation_reason and\n                self.description == other.description\n            )\n        )\n\n\nclass GraphQLInputObjectType(GraphQLType):\n    \"\"\"Input Object Type Definition\n\n    An input object defines a structured collection of fields which may be\n    supplied to a field argument.\n\n    Using `NonNull` will ensure that a value must be provided by the query\n\n    Example:\n\n        NonNullFloat = GraphQLNonNull(GraphQLFloat())\n\n        class GeoPoint(GraphQLInputObjectType):\n            name = 'GeoPoint'\n            fields = {\n                'lat': GraphQLInputObjectField(NonNullFloat),\n                'lon': GraphQLInputObjectField(NonNullFloat),\n                'alt': GraphQLInputObjectField(GraphQLFloat(),\n                    default_value=0)\n            }\n    \"\"\"\n    def __init__(self, name, fields, description=None):\n        assert name, 'Type must be named.'\n        self.name = name\n        self.description = description\n\n        self._fields = fields\n\n    @cached_property\n    def fields(self):\n        return self._define_field_map()\n\n    def _define_field_map(self):\n        fields = self._fields\n        if callable(fields):\n            fields = fields()\n\n        assert isinstance(fields, collections.Mapping) and len(fields) > 0, (\n            '{} fields must be a mapping (dict / OrderedDict) with field names as keys or a '\n            'function which returns such a mapping.'\n        ).format(self)\n        if not isinstance(fields, (collections.OrderedDict, OrderedDict)):\n            fields = OrderedDict(sorted(list(fields.items())))\n\n        for field_name, field in fields.items():\n            assert_valid_name(field_name)\n\n        return fields\n\n\nclass GraphQLInputObjectField(object):\n    __slots__ = 'type', 'default_value', 'description', 'out_name'\n\n    def __init__(self, type, default_value=None, description=None, out_name=None):\n        self.type = type\n        self.default_value = default_value\n        self.description = description\n        self.out_name = out_name\n\n    def __eq__(self, other):\n        return (\n            self is other or (\n                isinstance(other, GraphQLInputObjectField) and\n                self.type == other.type and\n                self.description == other.description and\n                self.out_name == other.out_name\n            )\n        )\n\n\nclass GraphQLList(GraphQLType):\n    \"\"\"List Modifier\n\n    A list is a kind of type marker, a wrapping type which points to another\n    type. Lists are often created within the context of defining the fields\n    of an object type.\n\n    Example:\n\n        class PersonType(GraphQLObjectType):\n            name = 'Person'\n\n            def get_fields(self):\n                return {\n                    'parents': GraphQLField(GraphQLList(PersonType())),\n                    'children': GraphQLField(GraphQLList(PersonType())),\n                }\n    \"\"\"\n    __slots__ = 'of_type',\n\n    def __init__(self, type):\n        assert is_type(type), 'Can only create List of a GraphQLType but got: {}.'.format(type)\n        self.of_type = type\n\n    def __str__(self):\n        return '[' + str(self.of_type) + ']'\n\n    def is_same_type(self, other):\n        return isinstance(other, GraphQLList) and self.of_type.is_same_type(other.of_type)\n\n\nclass GraphQLNonNull(GraphQLType):\n    \"\"\"Non-Null Modifier\n\n    A non-null is a kind of type marker, a wrapping type which points to another type. Non-null types enforce that their values are never null\n    and can ensure an error is raised if this ever occurs during a request. It is useful for fields which you can make a strong guarantee on\n    non-nullability, for example usually the id field of a database row will never be null.\n\n    Example:\n\n        class RowType(GraphQLObjectType):\n            name = 'Row'\n            fields = {\n                'id': GraphQLField(type=GraphQLNonNull(GraphQLString()))\n            }\n\n    Note: the enforcement of non-nullability occurs within the executor.\n    \"\"\"\n    __slots__ = 'of_type',\n\n    def __init__(self, type):\n        assert is_type(type) and not isinstance(type, GraphQLNonNull), (\n            'Can only create NonNull of a Nullable GraphQLType but got: {}.'.format(type)\n        )\n        self.of_type = type\n\n    def __str__(self):\n        return str(self.of_type) + '!'\n\n    def is_same_type(self, other):\n        return isinstance(other, GraphQLNonNull) and self.of_type.is_same_type(other.of_type)\n", 
    "graphql.type.directives": "import collections\n\nfrom ..pyutils.ordereddict import OrderedDict\nfrom ..utils.assert_valid_name import assert_valid_name\nfrom .definition import GraphQLArgument, GraphQLNonNull, is_input_type\nfrom .scalars import GraphQLBoolean, GraphQLString\n\n\nclass DirectiveLocation(object):\n    # Operations\n    QUERY = 'QUERY'\n    MUTATION = 'MUTATION'\n    SUBSCRIPTION = 'SUBSCRIPTION'\n    FIELD = 'FIELD'\n    FRAGMENT_DEFINITION = 'FRAGMENT_DEFINITION'\n    FRAGMENT_SPREAD = 'FRAGMENT_SPREAD'\n    INLINE_FRAGMENT = 'INLINE_FRAGMENT'\n\n    # Schema Definitions\n    SCHEMA = 'SCHEMA'\n    SCALAR = 'SCALAR'\n    OBJECT = 'OBJECT'\n    FIELD_DEFINITION = 'FIELD_DEFINITION'\n    ARGUMENT_DEFINITION = 'ARGUMENT_DEFINITION'\n    INTERFACE = 'INTERFACE'\n    UNION = 'UNION'\n    ENUM = 'ENUM'\n    ENUM_VALUE = 'ENUM_VALUE'\n    INPUT_OBJECT = 'INPUT_OBJECT'\n    INPUT_FIELD_DEFINITION = 'INPUT_FIELD_DEFINITION'\n\n    OPERATION_LOCATIONS = [\n        QUERY,\n        MUTATION,\n        SUBSCRIPTION\n    ]\n\n    FRAGMENT_LOCATIONS = [\n        FRAGMENT_DEFINITION,\n        FRAGMENT_SPREAD,\n        INLINE_FRAGMENT\n    ]\n\n    FIELD_LOCATIONS = [\n        FIELD\n    ]\n\n\nclass GraphQLDirective(object):\n    __slots__ = 'name', 'args', 'description', 'locations'\n\n    def __init__(self, name, description=None, args=None, locations=None):\n        assert name, 'Directive must be named.'\n        assert_valid_name(name)\n        assert isinstance(locations, collections.Iterable), 'Must provide locations for directive.'\n\n        self.name = name\n        self.description = description\n        self.locations = locations\n\n        if args:\n            assert isinstance(args, collections.Mapping), '{} args must be a dict with argument names as keys.'.format(name)\n            for arg_name, _arg in args.items():\n                assert_valid_name(arg_name)\n                assert is_input_type(_arg.type), '{}({}) argument type must be Input Type but got {}.'.format(\n                    name,\n                    arg_name,\n                    _arg.type)\n        self.args = args or OrderedDict()\n\n\"\"\"Used to conditionally include fields or fragments.\"\"\"\nGraphQLIncludeDirective = GraphQLDirective(\n    name='include',\n    description='Directs the executor to include this field or fragment only when the `if` argument is true.',\n    args={\n        'if': GraphQLArgument(\n            type=GraphQLNonNull(GraphQLBoolean),\n            description='Included when true.',\n        ),\n    },\n    locations=[\n        DirectiveLocation.FIELD,\n        DirectiveLocation.FRAGMENT_SPREAD,\n        DirectiveLocation.INLINE_FRAGMENT,\n    ]\n)\n\n\"\"\"Used to conditionally skip (exclude) fields or fragments.\"\"\"\nGraphQLSkipDirective = GraphQLDirective(\n    name='skip',\n    description='Directs the executor to skip this field or fragment when the `if` argument is true.',\n    args={\n        'if': GraphQLArgument(\n            type=GraphQLNonNull(GraphQLBoolean),\n            description='Skipped when true.',\n        ),\n    },\n    locations=[\n        DirectiveLocation.FIELD,\n        DirectiveLocation.FRAGMENT_SPREAD,\n        DirectiveLocation.INLINE_FRAGMENT,\n    ]\n)\n\n\"\"\"Constant string used for default reason for a deprecation.\"\"\"\nDEFAULT_DEPRECATION_REASON = 'No longer supported'\n\n\"\"\"Used to declare element of a GraphQL schema as deprecated.\"\"\"\nGraphQLDeprecatedDirective = GraphQLDirective(\n    name='deprecated',\n    description='Marks an element of a GraphQL schema as no longer supported.',\n    args={\n        'reason': GraphQLArgument(\n            type=GraphQLString,\n            description=('Explains why this element was deprecated, usually also including a suggestion for how to'\n                         'access supported similar data. Formatted in [Markdown]'\n                         '(https://daringfireball.net/projects/markdown/).'),\n            default_value=DEFAULT_DEPRECATION_REASON\n        ),\n    },\n    locations=[\n        DirectiveLocation.FIELD_DEFINITION,\n        DirectiveLocation.ENUM_VALUE,\n    ]\n)\n\nspecified_directives = [\n    GraphQLIncludeDirective,\n    GraphQLSkipDirective,\n    GraphQLDeprecatedDirective\n]\n", 
    "graphql.type.introspection": "from collections import OrderedDict, namedtuple\n\nfrom ..language.printer import print_ast\nfrom ..utils.ast_from_value import ast_from_value\nfrom .definition import (GraphQLArgument, GraphQLEnumType, GraphQLEnumValue,\n                         GraphQLField, GraphQLInputObjectType,\n                         GraphQLInterfaceType, GraphQLList, GraphQLNonNull,\n                         GraphQLObjectType, GraphQLScalarType,\n                         GraphQLUnionType)\nfrom .directives import DirectiveLocation\nfrom .scalars import GraphQLBoolean, GraphQLString\n\nInputField = namedtuple('InputField', ['name', 'description', 'type', 'default_value'])\nField = namedtuple('Field', ['name', 'type', 'description', 'args', 'deprecation_reason'])\n\n\ndef input_fields_to_list(input_fields):\n    fields = []\n    for field_name, field in input_fields.items():\n        fields.append(InputField(\n            name=field_name,\n            description=field.description,\n            type=field.type,\n            default_value=field.default_value))\n    return fields\n\n\n__Schema = GraphQLObjectType(\n    '__Schema',\n    description='A GraphQL Schema defines the capabilities of a GraphQL server. It '\n                'exposes all available types and directives on the server, as well as '\n                'the entry points for query, mutation and subscription operations.',\n    fields=lambda: OrderedDict([\n        ('types', GraphQLField(\n            description='A list of all types supported by this server.',\n            type=GraphQLNonNull(GraphQLList(GraphQLNonNull(__Type))),\n            resolver=lambda schema, *_: schema.get_type_map().values(),\n        )),\n        ('queryType', GraphQLField(\n            description='The type that query operations will be rooted at.',\n            type=GraphQLNonNull(__Type),\n            resolver=lambda schema, *_: schema.get_query_type(),\n        )),\n        ('mutationType', GraphQLField(\n            description='If this server supports mutation, the type that '\n                        'mutation operations will be rooted at.',\n            type=__Type,\n            resolver=lambda schema, *_: schema.get_mutation_type(),\n        )),\n        ('subscriptionType', GraphQLField(\n            description='If this server support subscription, the type '\n                        'that subscription operations will be rooted at.',\n            type=__Type,\n            resolver=lambda schema, *_: schema.get_subscription_type(),\n        )),\n        ('directives', GraphQLField(\n            description='A list of all directives supported by this server.',\n            type=GraphQLNonNull(GraphQLList(GraphQLNonNull(__Directive))),\n            resolver=lambda schema, *_: schema.get_directives(),\n        )),\n    ]))\n\n_on_operation_locations = set(DirectiveLocation.OPERATION_LOCATIONS)\n_on_fragment_locations = set(DirectiveLocation.FRAGMENT_LOCATIONS)\n_on_field_locations = set(DirectiveLocation.FIELD_LOCATIONS)\n\n__Directive = GraphQLObjectType(\n    '__Directive',\n    description='A Directive provides a way to describe alternate runtime execution and '\n                'type validation behavior in a GraphQL document.'\n                '\\n\\nIn some cases, you need to provide options to alter GraphQL\\'s '\n                'execution behavior in ways field arguments will not suffice, such as '\n                'conditionally including or skipping a field. Directives provide this by '\n                'describing additional information to the executor.',\n    fields=lambda: OrderedDict([\n        ('name', GraphQLField(GraphQLNonNull(GraphQLString))),\n        ('description', GraphQLField(GraphQLString)),\n        ('locations', GraphQLField(\n            type=GraphQLNonNull(GraphQLList(GraphQLNonNull(__DirectiveLocation))),\n        )),\n        ('args', GraphQLField(\n            type=GraphQLNonNull(GraphQLList(GraphQLNonNull(__InputValue))),\n            resolver=lambda directive, *args: input_fields_to_list(directive.args),\n        )),\n        ('onOperation', GraphQLField(\n            type=GraphQLNonNull(GraphQLBoolean),\n            deprecation_reason='Use `locations`.',\n            resolver=lambda directive, *args: set(directive.locations) & _on_operation_locations,\n        )),\n        ('onFragment', GraphQLField(\n            type=GraphQLNonNull(GraphQLBoolean),\n            deprecation_reason='Use `locations`.',\n            resolver=lambda directive, *args: set(directive.locations) & _on_fragment_locations,\n        )),\n        ('onField', GraphQLField(\n            type=GraphQLNonNull(GraphQLBoolean),\n            deprecation_reason='Use `locations`.',\n            resolver=lambda directive, *args: set(directive.locations) & _on_field_locations,\n        ))\n    ]))\n\n__DirectiveLocation = GraphQLEnumType(\n    '__DirectiveLocation',\n    description=(\n        'A Directive can be adjacent to many parts of the GraphQL language, a ' +\n        '__DirectiveLocation describes one such possible adjacencies.'\n    ),\n    values=OrderedDict([\n        ('QUERY', GraphQLEnumValue(\n            DirectiveLocation.QUERY,\n            description='Location adjacent to a query operation.'\n        )),\n        ('MUTATION', GraphQLEnumValue(\n            DirectiveLocation.MUTATION,\n            description='Location adjacent to a mutation operation.'\n        )),\n        ('SUBSCRIPTION', GraphQLEnumValue(\n            DirectiveLocation.SUBSCRIPTION,\n            description='Location adjacent to a subscription operation.'\n        )),\n        ('FIELD', GraphQLEnumValue(\n            DirectiveLocation.FIELD,\n            description='Location adjacent to a field.'\n        )),\n        ('FRAGMENT_DEFINITION', GraphQLEnumValue(\n            DirectiveLocation.FRAGMENT_DEFINITION,\n            description='Location adjacent to a fragment definition.'\n        )),\n        ('FRAGMENT_SPREAD', GraphQLEnumValue(\n            DirectiveLocation.FRAGMENT_SPREAD,\n            description='Location adjacent to a fragment spread.'\n        )),\n        ('INLINE_FRAGMENT', GraphQLEnumValue(\n            DirectiveLocation.INLINE_FRAGMENT,\n            description='Location adjacent to an inline fragment.'\n        )),\n        ('SCHEMA', GraphQLEnumValue(\n            DirectiveLocation.SCHEMA,\n            description='Location adjacent to a schema definition.'\n        )),\n        ('SCALAR', GraphQLEnumValue(\n            DirectiveLocation.SCALAR,\n            description='Location adjacent to a scalar definition.'\n        )),\n        ('OBJECT', GraphQLEnumValue(\n            DirectiveLocation.OBJECT,\n            description='Location adjacent to an object definition.'\n        )),\n        ('FIELD_DEFINITION', GraphQLEnumValue(\n            DirectiveLocation.FIELD_DEFINITION,\n            description='Location adjacent to a field definition.'\n        )),\n        ('ARGUMENT_DEFINITION', GraphQLEnumValue(\n            DirectiveLocation.ARGUMENT_DEFINITION,\n            description='Location adjacent to an argument definition.'\n        )),\n        ('INTERFACE', GraphQLEnumValue(\n            DirectiveLocation.INTERFACE,\n            description='Location adjacent to an interface definition.'\n        )),\n        ('UNION', GraphQLEnumValue(\n            DirectiveLocation.UNION,\n            description='Location adjacent to a union definition.'\n        )),\n        ('ENUM', GraphQLEnumValue(\n            DirectiveLocation.ENUM,\n            description='Location adjacent to an enum definition.'\n        )),\n        ('ENUM_VALUE', GraphQLEnumValue(\n            DirectiveLocation.ENUM_VALUE,\n            description='Location adjacent to an enum value definition.'\n        )),\n        ('INPUT_OBJECT', GraphQLEnumValue(\n            DirectiveLocation.INPUT_OBJECT,\n            description='Location adjacent to an input object definition.'\n        )),\n        ('INPUT_FIELD_DEFINITION', GraphQLEnumValue(\n            DirectiveLocation.INPUT_FIELD_DEFINITION,\n            description='Location adjacent to an input object field definition.'\n        )),\n    ]))\n\n\nclass TypeKind(object):\n    SCALAR = 'SCALAR'\n    OBJECT = 'OBJECT'\n    INTERFACE = 'INTERFACE'\n    UNION = 'UNION'\n    ENUM = 'ENUM'\n    INPUT_OBJECT = 'INPUT_OBJECT'\n    LIST = 'LIST'\n    NON_NULL = 'NON_NULL'\n\n\nclass TypeFieldResolvers(object):\n    _kinds = (\n        (GraphQLScalarType, TypeKind.SCALAR),\n        (GraphQLObjectType, TypeKind.OBJECT),\n        (GraphQLInterfaceType, TypeKind.INTERFACE),\n        (GraphQLUnionType, TypeKind.UNION),\n        (GraphQLEnumType, TypeKind.ENUM),\n        (GraphQLInputObjectType, TypeKind.INPUT_OBJECT),\n        (GraphQLList, TypeKind.LIST),\n        (GraphQLNonNull, TypeKind.NON_NULL),\n    )\n\n    @classmethod\n    def kind(cls, type, *_):\n        for klass, kind in cls._kinds:\n            if isinstance(type, klass):\n                return kind\n\n        raise Exception('Unknown kind of type: {}'.format(type))\n\n    @staticmethod\n    def fields(type, args, *_):\n        if isinstance(type, (GraphQLObjectType, GraphQLInterfaceType)):\n            fields = []\n            include_deprecated = args.get('includeDeprecated')\n            for field_name, field in type.fields.items():\n                if field.deprecation_reason and not include_deprecated:\n                    continue\n                fields.append(Field(\n                    name=field_name,\n                    description=field.description,\n                    type=field.type,\n                    args=field.args,\n                    deprecation_reason=field.deprecation_reason\n                ))\n            return fields\n        return None\n\n    @staticmethod\n    def interfaces(type, *_):\n        if isinstance(type, GraphQLObjectType):\n            return type.interfaces\n\n    @staticmethod\n    def possible_types(type, args, context, info):\n        if isinstance(type, (GraphQLInterfaceType, GraphQLUnionType)):\n            return info.schema.get_possible_types(type)\n\n    @staticmethod\n    def enum_values(type, args, *_):\n        if isinstance(type, GraphQLEnumType):\n            values = type.values\n            if not args.get('includeDeprecated'):\n                values = [v for v in values if not v.deprecation_reason]\n\n            return values\n\n    @staticmethod\n    def input_fields(type, *_):\n        if isinstance(type, GraphQLInputObjectType):\n            return input_fields_to_list(type.fields)\n\n\n__Type = GraphQLObjectType(\n    '__Type',\n    description='The fundamental unit of any GraphQL Schema is the type. There are '\n                'many kinds of types in GraphQL as represented by the `__TypeKind` enum.'\n                '\\n\\nDepending on the kind of a type, certain fields describe '\n                'information about that type. Scalar types provide no information '\n                'beyond a name and description, while Enum types provide their values. '\n                'Object and Interface types provide the fields they describe. Abstract '\n                'types, Union and Interface, provide the Object types possible '\n                'at runtime. List and NonNull types compose other types.',\n    fields=lambda: OrderedDict([\n        ('kind', GraphQLField(\n            type=GraphQLNonNull(__TypeKind),\n            resolver=TypeFieldResolvers.kind\n        )),\n        ('name', GraphQLField(GraphQLString)),\n        ('description', GraphQLField(GraphQLString)),\n        ('fields', GraphQLField(\n            type=GraphQLList(GraphQLNonNull(__Field)),\n            args={\n                'includeDeprecated': GraphQLArgument(\n                    GraphQLBoolean,\n                    default_value=False\n                )\n            },\n            resolver=TypeFieldResolvers.fields\n        )),\n        ('interfaces', GraphQLField(\n            type=GraphQLList(GraphQLNonNull(__Type)),\n            resolver=TypeFieldResolvers.interfaces\n        )),\n        ('possibleTypes', GraphQLField(\n            type=GraphQLList(GraphQLNonNull(__Type)),\n            resolver=TypeFieldResolvers.possible_types\n        )),\n        ('enumValues', GraphQLField(\n            type=GraphQLList(GraphQLNonNull(__EnumValue)),\n            args={\n                'includeDeprecated': GraphQLArgument(\n                    GraphQLBoolean,\n                    default_value=False\n                )\n            },\n            resolver=TypeFieldResolvers.enum_values\n        )),\n        ('inputFields', GraphQLField(\n            type=GraphQLList(GraphQLNonNull(__InputValue)),\n            resolver=TypeFieldResolvers.input_fields\n        )),\n        ('ofType', GraphQLField(\n            type=__Type,\n            resolver=lambda type, *_: getattr(type, 'of_type', None)\n        )),\n    ]))\n\n__Field = GraphQLObjectType(\n    '__Field',\n    description='Object and Interface types are described by a list of Fields, each of '\n                'which has a name, potentially a list of arguments, and a return type.',\n    fields=lambda: OrderedDict([\n        ('name', GraphQLField(GraphQLNonNull(GraphQLString))),\n        ('description', GraphQLField(GraphQLString)),\n        ('args', GraphQLField(\n            type=GraphQLNonNull(GraphQLList(GraphQLNonNull(__InputValue))),\n            resolver=lambda field, *_: input_fields_to_list(field.args)\n        )),\n        ('type', GraphQLField(GraphQLNonNull(__Type))),\n        ('isDeprecated', GraphQLField(\n            type=GraphQLNonNull(GraphQLBoolean),\n            resolver=lambda field, *_: bool(field.deprecation_reason)\n        )),\n        ('deprecationReason', GraphQLField(\n            type=GraphQLString,\n            resolver=lambda field, *_: field.deprecation_reason\n        ))\n    ])\n)\n\n__InputValue = GraphQLObjectType(\n    '__InputValue',\n    description='Arguments provided to Fields or Directives and the input fields of an '\n                'InputObject are represented as Input Values which describe their type '\n                'and optionally a default value.',\n    fields=lambda: OrderedDict([\n        ('name', GraphQLField(GraphQLNonNull(GraphQLString))),\n        ('description', GraphQLField(GraphQLString)),\n        ('type', GraphQLField(GraphQLNonNull(__Type))),\n        ('defaultValue', GraphQLField(\n            type=GraphQLString,\n            resolver=lambda input_val, *_:\n            None if input_val.default_value is None\n            else print_ast(ast_from_value(input_val.default_value, input_val))\n        ))\n    ]))\n\n__EnumValue = GraphQLObjectType(\n    '__EnumValue',\n    description='One possible value for a given Enum. Enum values are unique values, not '\n                'a placeholder for a string or numeric value. However an Enum value is '\n                'returned in a JSON response as a string.',\n    fields=lambda: OrderedDict([\n        ('name', GraphQLField(GraphQLNonNull(GraphQLString))),\n        ('description', GraphQLField(GraphQLString)),\n        ('isDeprecated', GraphQLField(\n            type=GraphQLNonNull(GraphQLBoolean),\n            resolver=lambda field, *_: bool(field.deprecation_reason)\n        )),\n        ('deprecationReason', GraphQLField(\n            type=GraphQLString,\n            resolver=lambda enum_value, *_: enum_value.deprecation_reason,\n        ))\n    ]))\n\n__TypeKind = GraphQLEnumType(\n    '__TypeKind',\n    description='An enum describing what kind of type a given `__Type` is',\n    values=OrderedDict([\n        ('SCALAR', GraphQLEnumValue(\n            TypeKind.SCALAR,\n            description='Indicates this type is a scalar.'\n        )),\n        ('OBJECT', GraphQLEnumValue(\n            TypeKind.OBJECT,\n            description='Indicates this type is an object. '\n                        '`fields` and `interfaces` are valid fields.'\n        )),\n        ('INTERFACE', GraphQLEnumValue(\n            TypeKind.INTERFACE,\n            description='Indicates this type is an interface. '\n                        '`fields` and `possibleTypes` are valid fields.'\n        )),\n        ('UNION', GraphQLEnumValue(\n            TypeKind.UNION,\n            description='Indicates this type is a union. '\n                        '`possibleTypes` is a valid field.'\n        )),\n        ('ENUM', GraphQLEnumValue(\n            TypeKind.ENUM,\n            description='Indicates this type is an enum. '\n                        '`enumValues` is a valid field.'\n        )),\n        ('INPUT_OBJECT', GraphQLEnumValue(\n            TypeKind.INPUT_OBJECT,\n            description='Indicates this type is an input object. '\n                        '`inputFields` is a valid field.'\n        )),\n        ('LIST', GraphQLEnumValue(\n            TypeKind.LIST,\n            description='Indicates this type is a list. '\n                        '`ofType` is a valid field.'\n        )),\n        ('NON_NULL', GraphQLEnumValue(\n            TypeKind.NON_NULL,\n            description='Indicates this type is a non-null. '\n                        '`ofType` is a valid field.'\n        )),\n    ]))\n\nIntrospectionSchema = __Schema\n\nSchemaMetaFieldDef = GraphQLField(\n    # name='__schema',\n    type=GraphQLNonNull(__Schema),\n    description='Access the current type schema of this server.',\n    resolver=lambda source, args, context, info: info.schema,\n    args={}\n)\n\nTypeMetaFieldDef = GraphQLField(\n    type=__Type,\n    # name='__type',\n    description='Request the type information of a single type.',\n    args={'name': GraphQLArgument(GraphQLNonNull(GraphQLString))},\n    resolver=lambda source, args, context, info: info.schema.get_type(args['name'])\n)\n\nTypeNameMetaFieldDef = GraphQLField(\n    type=GraphQLNonNull(GraphQLString),\n    # name='__typename',\n    description='The name of the current Object type at runtime.',\n    resolver=lambda source, args, context, info: info.parent_type.name,\n    args={}\n)\n", 
    "graphql.type.scalars": "from six import string_types, text_type\n\nfrom ..language.ast import BooleanValue, FloatValue, IntValue, StringValue\nfrom .definition import GraphQLScalarType\n\n# As per the GraphQL Spec, Integers are only treated as valid when a valid\n# 32-bit signed integer, providing the broadest support across platforms.\n#\n# n.b. JavaScript's integers are safe between -(2^53 - 1) and 2^53 - 1 because\n# they are internally represented as IEEE 754 doubles.\nMAX_INT = 2147483647\nMIN_INT = -2147483648\n\n\ndef coerce_int(value):\n    if isinstance(value, int):\n        num = value\n    else:\n        try:\n            num = int(value)\n        except ValueError:\n            num = int(float(value))\n    if MIN_INT <= num <= MAX_INT:\n        return num\n    raise Exception((\n        \"Int cannot represent non 32-bit signed integer value: {}\"\n    ).format(value))\n\n\ndef parse_int_literal(ast):\n    if isinstance(ast, IntValue):\n        num = int(ast.value)\n        if MIN_INT <= num <= MAX_INT:\n            return num\n\n\nGraphQLInt = GraphQLScalarType(\n    name='Int',\n    description='The `Int` scalar type represents non-fractional signed whole numeric '\n                'values. Int can represent values between -(2^53 - 1) and 2^53 - 1 since '\n                'represented in JSON as double-precision floating point numbers specified'\n                'by [IEEE 754](http://en.wikipedia.org/wiki/IEEE_floating_point).',\n    serialize=coerce_int,\n    parse_value=coerce_int,\n    parse_literal=parse_int_literal)\n\n\ndef coerce_float(value):\n    if isinstance(value, float):\n        return value\n    return float(value)\n\n\ndef parse_float_literal(ast):\n    if isinstance(ast, (FloatValue, IntValue)):\n        return float(ast.value)\n    return None\n\n\nGraphQLFloat = GraphQLScalarType(\n    name='Float',\n    description='The `Float` scalar type represents signed double-precision fractional '\n                'values as specified by '\n                '[IEEE 754](http://en.wikipedia.org/wiki/IEEE_floating_point). ',\n    serialize=coerce_float,\n    parse_value=coerce_float,\n    parse_literal=parse_float_literal)\n\n\ndef coerce_string(value):\n    if isinstance(value, string_types):\n        return value\n\n    if isinstance(value, bool):\n        return u'true' if value else u'false'\n\n    return text_type(value)\n\n\ndef coerce_str(value):\n    if isinstance(value, string_types):\n        return value\n\n    return text_type(value)\n\n\ndef parse_string_literal(ast):\n    if isinstance(ast, StringValue):\n        return ast.value\n\n    return None\n\n\nGraphQLString = GraphQLScalarType(\n    name='String',\n    description='The `String` scalar type represents textual data, represented as UTF-8 '\n                'character sequences. The String type is most often used by GraphQL to '\n                'represent free-form human-readable text.',\n    serialize=coerce_string,\n    parse_value=coerce_string,\n    parse_literal=parse_string_literal)\n\n\ndef parse_boolean_literal(ast):\n    if isinstance(ast, BooleanValue):\n        return ast.value\n    return None\n\n\nGraphQLBoolean = GraphQLScalarType(\n    name='Boolean',\n    description='The `Boolean` scalar type represents `true` or `false`.',\n    serialize=bool,\n    parse_value=bool,\n    parse_literal=parse_boolean_literal)\n\n\ndef parse_id_literal(ast):\n    if isinstance(ast, (StringValue, IntValue)):\n        return ast.value\n    return None\n\n\nGraphQLID = GraphQLScalarType(\n    name='ID',\n    description='The `ID` scalar type represents a unique identifier, often used to '\n                'refetch an object or as key for a cache. The ID type appears in a JSON '\n                'response as a String; however, it is not intended to be human-readable. '\n                'When expected as an input type, any string (such as `\"4\"`) or integer '\n                '(such as `4`) input value will be accepted as an ID.',\n    serialize=coerce_str,\n    parse_value=coerce_str,\n    parse_literal=parse_id_literal)\n", 
    "graphql.type.schema": "from collections import Iterable\n\nfrom .definition import GraphQLObjectType\nfrom .directives import GraphQLDirective, specified_directives\nfrom .introspection import IntrospectionSchema\nfrom .typemap import GraphQLTypeMap\n\n\nclass GraphQLSchema(object):\n    \"\"\"Schema Definition\n\n    A Schema is created by supplying the root types of each type of operation, query and mutation (optional).\n    A schema definition is then supplied to the validator and executor.\n\n    Example:\n\n        MyAppSchema = GraphQLSchema(\n            query=MyAppQueryRootType,\n            mutation=MyAppMutationRootType,\n        )\n\n    Note: If an array of `directives` are provided to GraphQLSchema, that will be\n    the exact list of directives represented and allowed. If `directives` is not\n    provided then a default set of the specified directives (e.g. @include and\n    @skip) will be used. If you wish to provide *additional* directives to these\n    specified directives, you must explicitly declare them. Example:\n\n      MyAppSchema = GraphQLSchema(\n          ...\n          directives=specified_directives.extend([MyCustomerDirective]),\n      )\n    \"\"\"\n    __slots__ = '_query', '_mutation', '_subscription', '_type_map', '_directives', '_implementations', '_possible_type_map'\n\n    def __init__(self, query, mutation=None, subscription=None, directives=None, types=None):\n        assert isinstance(query, GraphQLObjectType), 'Schema query must be Object Type but got: {}.'.format(query)\n        if mutation:\n            assert isinstance(mutation, GraphQLObjectType), \\\n                'Schema mutation must be Object Type but got: {}.'.format(mutation)\n\n        if subscription:\n            assert isinstance(subscription, GraphQLObjectType), \\\n                'Schema subscription must be Object Type but got: {}.'.format(subscription)\n\n        if types:\n            assert isinstance(types, Iterable), \\\n                'Schema types must be iterable if provided but got: {}.'.format(types)\n\n        self._query = query\n        self._mutation = mutation\n        self._subscription = subscription\n        if directives is None:\n            directives = specified_directives\n\n        assert all(isinstance(d, GraphQLDirective) for d in directives), \\\n            'Schema directives must be List[GraphQLDirective] if provided but got: {}.'.format(\n                directives\n        )\n        self._directives = directives\n\n        initial_types = [\n            query,\n            mutation,\n            subscription,\n            IntrospectionSchema\n        ]\n        if types:\n            initial_types += types\n        self._type_map = GraphQLTypeMap(initial_types)\n\n    def get_query_type(self):\n        return self._query\n\n    def get_mutation_type(self):\n        return self._mutation\n\n    def get_subscription_type(self):\n        return self._subscription\n\n    def get_type_map(self):\n        return self._type_map\n\n    def get_type(self, name):\n        return self._type_map.get(name)\n\n    def get_directives(self):\n        return self._directives\n\n    def get_directive(self, name):\n        for directive in self.get_directives():\n            if directive.name == name:\n                return directive\n\n        return None\n\n    def get_possible_types(self, abstract_type):\n        return self._type_map.get_possible_types(abstract_type)\n\n    def is_possible_type(self, abstract_type, possible_type):\n        return self._type_map.is_possible_type(abstract_type, possible_type)\n", 
    "graphql.type.typemap": "from collections import OrderedDict, Sequence, defaultdict\nfrom functools import reduce\n\nfrom ..utils.type_comparators import is_equal_type, is_type_sub_type_of\nfrom .definition import (GraphQLArgument, GraphQLField,\n                         GraphQLInputObjectField, GraphQLInputObjectType,\n                         GraphQLInterfaceType, GraphQLList, GraphQLNonNull,\n                         GraphQLObjectType, GraphQLUnionType, is_input_type,\n                         is_output_type)\n\n\nclass GraphQLTypeMap(OrderedDict):\n\n    def __init__(self, types):\n        super(GraphQLTypeMap, self).__init__()\n        self.update(reduce(self.reducer, types, OrderedDict()))\n        self._possible_type_map = defaultdict(set)\n\n        # Keep track of all implementations by interface name.\n        self._implementations = {}\n        for gql_type in self.values():\n            if isinstance(gql_type, GraphQLObjectType):\n                for interface in gql_type.interfaces:\n                    self._implementations.setdefault(interface.name, []).append(gql_type)\n\n        # Enforce correct interface implementations.\n        for type in self.values():\n            if isinstance(type, GraphQLObjectType):\n                for interface in type.interfaces:\n                    self.assert_object_implements_interface(self, type, interface)\n\n    def get_possible_types(self, abstract_type):\n        if isinstance(abstract_type, GraphQLUnionType):\n            return abstract_type.types\n        assert isinstance(abstract_type, GraphQLInterfaceType)\n        return self._implementations.get(abstract_type.name, None)\n\n    def is_possible_type(self, abstract_type, possible_type):\n        possible_types = self.get_possible_types(abstract_type)\n        assert isinstance(possible_types, Sequence), (\n            'Could not find possible implementing types for ${} in ' +\n            'schema. Check that schema.types is defined and is an array of' +\n            'all possible types in the schema.'\n            ).format(abstract_type)\n\n        if not self._possible_type_map[abstract_type.name]:\n            self._possible_type_map[abstract_type.name].update([p.name for p in possible_types])\n\n        return possible_type.name in self._possible_type_map[abstract_type.name]\n\n    @classmethod\n    def reducer(cls, map, type):\n        if not type:\n            return map\n\n        if isinstance(type, GraphQLList) or isinstance(type, GraphQLNonNull):\n            return cls.reducer(map, type.of_type)\n\n        if type.name in map:\n            assert map[type.name] == type, (\n                'Schema must contain unique named types but contains multiple types named \"{}\".'\n            ).format(type.name)\n\n            return map\n\n        map[type.name] = type\n\n        reduced_map = map\n\n        if isinstance(type, (GraphQLUnionType)):\n            for t in type.types:\n                reduced_map = cls.reducer(reduced_map, t)\n\n        if isinstance(type, GraphQLObjectType):\n            for t in type.interfaces:\n                reduced_map = cls.reducer(reduced_map, t)\n\n        if isinstance(type, (GraphQLObjectType, GraphQLInterfaceType, GraphQLInputObjectType)):\n            field_map = type.fields\n            type_is_input = isinstance(type, GraphQLInputObjectType)\n            for field_name, field in field_map.items():\n                if type_is_input:\n                    assert isinstance(field, GraphQLInputObjectField), (\n                        '{}.{} must be an instance of GraphQLInputObjectField.'.format(type, field_name)\n                    )\n                    assert is_input_type(field.type), (\n                        '{}.{} field type must be Input Type but got: {}.'.format(type, field_name, field.type)\n                    )\n                else:\n                    assert isinstance(field, (GraphQLField, GraphQLField)), (\n                        '{}.{} must be an instance of GraphQLField.'.format(type, field_name)\n                    )\n                    assert is_output_type(field.type), (\n                        '{}.{} field type must be Output Type but got: {}.'.format(type, field_name, field.type)\n                    )\n                    for arg_name, arg in field.args.items():\n                        assert isinstance(arg, (GraphQLArgument, GraphQLArgument)), (\n                            '{}.{}({}:) argument must be an instance of GraphQLArgument.'.format(type, field_name, arg_name)\n                        )\n                        assert is_input_type(arg.type), (\n                            '{}.{}({}:) argument type must be Input Type but got: {}.'.format(type, field_name, arg_name,\n                                                                                              arg.type)\n                        )\n                        reduced_map = cls.reducer(reduced_map, arg.type)\n\n                reduced_map = cls.reducer(reduced_map, getattr(field, 'type', None))\n\n        return reduced_map\n\n    @classmethod\n    def assert_object_implements_interface(cls, schema, object, interface):\n        object_field_map = object.fields\n        interface_field_map = interface.fields\n\n        for field_name, interface_field in interface_field_map.items():\n            object_field = object_field_map.get(field_name)\n\n            assert object_field, '\"{}\" expects field \"{}\" but \"{}\" does not provide it.'.format(\n                interface, field_name, object\n            )\n\n            assert is_type_sub_type_of(schema, object_field.type, interface_field.type), (\n                '{}.{} expects type \"{}\" but {}.{} provides type \"{}\".'\n            ).format(interface, field_name, interface_field.type, object, field_name, object_field.type)\n\n            for arg_name, interface_arg in interface_field.args.items():\n                object_arg = object_field.args.get(arg_name)\n\n                assert object_arg, (\n                    '{}.{} expects argument \"{}\" but {}.{} does not provide it.'\n                ).format(interface, field_name, arg_name, object, field_name)\n\n                assert is_equal_type(interface_arg.type, object_arg.type), (\n                    '{}.{}({}:) expects type \"{}\" but {}.{}({}:) provides type \"{}\".'\n                ).format(interface, field_name, arg_name, interface_arg.type, object, field_name, arg_name, object_arg.type)\n\n            for arg_name, object_arg in object_field.args.items():\n                interface_arg = interface_field.args.get(arg_name)\n                if not interface_arg:\n                    assert not isinstance(object_arg.type, GraphQLNonNull), (\n                        '{}.{}({}:) is of required type '\n                        '\"{}\" but is not also provided by the '\n                        'interface {}.{}.'\n                    ).format(object, field_name, arg_name, object_arg.type, interface, field_name)\n", 
    "graphql.utils.__init__": "", 
    "graphql.utils.assert_valid_name": "import re\n\nNAME_PATTERN = r'^[_a-zA-Z][_a-zA-Z0-9]*$'\nCOMPILED_NAME_PATTERN = re.compile(NAME_PATTERN)\n\n\ndef assert_valid_name(name):\n    '''Helper to assert that provided names are valid.'''\n    assert COMPILED_NAME_PATTERN.match(name), 'Names must match /{}/ but \"{}\" does not.'.format(NAME_PATTERN, name)\n", 
    "graphql.utils.ast_from_value": "import json\nimport re\nimport sys\n\nfrom six import string_types\n\nfrom ..language import ast\nfrom ..type.definition import (GraphQLEnumType, GraphQLInputObjectType,\n                               GraphQLList, GraphQLNonNull)\nfrom ..type.scalars import GraphQLFloat\n\n\ndef ast_from_value(value, type=None):\n    if isinstance(type, GraphQLNonNull):\n        return ast_from_value(value, type.of_type)\n\n    if value is None:\n        return None\n\n    if isinstance(value, list):\n        item_type = type.of_type if isinstance(type, GraphQLList) else None\n        return ast.ListValue([ast_from_value(item, item_type) for item in value])\n\n    elif isinstance(type, GraphQLList):\n        return ast_from_value(value, type.of_type)\n\n    if isinstance(value, bool):\n        return ast.BooleanValue(value)\n\n    if isinstance(value, (int, float)):\n        string_num = str(value)\n        int_value = int(value)\n        is_int_value = string_num.isdigit()\n\n        if is_int_value or (int_value == value and value < sys.maxsize):\n            if type == GraphQLFloat:\n                return ast.FloatValue(str(float(value)))\n\n            return ast.IntValue(str(int(value)))\n\n        return ast.FloatValue(string_num)\n\n    if isinstance(value, string_types):\n        if isinstance(type, GraphQLEnumType) and re.match(r'^[_a-zA-Z][_a-zA-Z0-9]*$', value):\n            return ast.EnumValue(value)\n\n        return ast.StringValue(json.dumps(value)[1:-1])\n\n    assert isinstance(value, dict)\n\n    fields = []\n    is_graph_ql_input_object_type = isinstance(type, GraphQLInputObjectType)\n\n    for field_name, field_value in value.items():\n        field_type = None\n        if is_graph_ql_input_object_type:\n            field_def = type.fields.get(field_name)\n            field_type = field_def and field_def.type\n\n        field_value = ast_from_value(field_value, field_type)\n        if field_value:\n            fields.append(ast.ObjectField(\n                ast.Name(field_name),\n                field_value\n            ))\n\n    return ast.ObjectValue(fields)\n", 
    "graphql.utils.base": "\"\"\"\n    Base GraphQL utilities\n    isort:skip_file\n\"\"\"\n\n# The GraphQL query recommended for a full schema introspection.\nfrom .introspection_query import introspection_query\n\n# Gets the target Operation from a Document\nfrom .get_operation_ast import get_operation_ast\n\n# Build a GraphQLSchema from an introspection result.\nfrom .build_client_schema import build_client_schema\n\n# Build a GraphQLSchema from a parsed GraphQL Schema language AST.\nfrom .build_ast_schema import build_ast_schema\n\n# Extends an existing GraphQLSchema from a parsed GraphQL Schema language AST.\nfrom .extend_schema import extend_schema\n\n# Print a GraphQLSchema to GraphQL Schema language.\nfrom .schema_printer import print_schema, print_introspection_schema\n\n# Create a GraphQLType from a GraphQL language AST.\nfrom .type_from_ast import type_from_ast\n\n# Create a JavaScript value from a GraphQL language AST.\nfrom .value_from_ast import value_from_ast\n\n# Create a GraphQL language AST from a JavaScript value.\nfrom .ast_from_value import ast_from_value\n\n# A helper to use within recursive-descent visitors which need to be aware of\n# the GraphQL type system.\nfrom .type_info import TypeInfo\n\n# Determine if JavaScript values adhere to a GraphQL type.\nfrom .is_valid_value import is_valid_value\n\n# Determine if AST values adhere to a GraphQL type.\nfrom .is_valid_literal_value import is_valid_literal_value\n\n# Concatenates multiple AST together.\nfrom .concat_ast import concat_ast\n\n# Comparators for types\nfrom .type_comparators import (\n    is_equal_type,\n    is_type_sub_type_of,\n    do_types_overlap\n)\n\n# Asserts that a string is a valid GraphQL name\nfrom .assert_valid_name import assert_valid_name\n\n__all__ = [\n    'introspection_query',\n    'get_operation_ast',\n    'build_client_schema',\n    'build_ast_schema',\n    'extend_schema',\n    'print_introspection_schema',\n    'print_schema',\n    'type_from_ast',\n    'value_from_ast',\n    'ast_from_value',\n    'TypeInfo',\n    'is_valid_value',\n    'is_valid_literal_value',\n    'concat_ast',\n    'do_types_overlap',\n    'is_equal_type',\n    'is_type_sub_type_of',\n    'assert_valid_name',\n]\n", 
    "graphql.utils.build_ast_schema": "from ..execution.values import get_argument_values\nfrom ..language import ast\nfrom ..pyutils.ordereddict import OrderedDict\nfrom ..type import (GraphQLArgument, GraphQLBoolean,\n                    GraphQLDeprecatedDirective, GraphQLDirective,\n                    GraphQLEnumType, GraphQLEnumValue, GraphQLField,\n                    GraphQLFloat, GraphQLID, GraphQLIncludeDirective,\n                    GraphQLInputObjectField, GraphQLInputObjectType,\n                    GraphQLInt, GraphQLInterfaceType, GraphQLList,\n                    GraphQLNonNull, GraphQLObjectType, GraphQLScalarType,\n                    GraphQLSchema, GraphQLSkipDirective, GraphQLString,\n                    GraphQLUnionType)\nfrom ..type.introspection import (__Directive, __DirectiveLocation,\n                                  __EnumValue, __Field, __InputValue, __Schema,\n                                  __Type, __TypeKind)\nfrom ..utils.value_from_ast import value_from_ast\n\n\ndef _build_wrapped_type(inner_type, input_type_ast):\n    if isinstance(input_type_ast, ast.ListType):\n        return GraphQLList(_build_wrapped_type(inner_type, input_type_ast.type))\n\n    if isinstance(input_type_ast, ast.NonNullType):\n        return GraphQLNonNull(_build_wrapped_type(inner_type, input_type_ast.type))\n\n    return inner_type\n\n\ndef _get_inner_type_name(type_ast):\n    if isinstance(type_ast, (ast.ListType, ast.NonNullType)):\n        return _get_inner_type_name(type_ast.type)\n\n    return type_ast.name.value\n\n\ndef _get_named_type_ast(type_ast):\n    named_type = type_ast\n    while isinstance(named_type, (ast.ListType, ast.NonNullType)):\n        named_type = named_type.type\n\n    return named_type\n\n\ndef _false(*_):\n    return False\n\n\ndef _none(*_):\n    return None\n\n\ndef build_ast_schema(document):\n    assert isinstance(document, ast.Document), 'must pass in Document ast.'\n\n    schema_def = None\n\n    type_asts = (\n        ast.ScalarTypeDefinition,\n        ast.ObjectTypeDefinition,\n        ast.InterfaceTypeDefinition,\n        ast.EnumTypeDefinition,\n        ast.UnionTypeDefinition,\n        ast.InputObjectTypeDefinition,\n    )\n\n    type_defs = []\n    directive_defs = []\n\n    for d in document.definitions:\n        if isinstance(d, ast.SchemaDefinition):\n            if schema_def:\n                raise Exception('Must provide only one schema definition.')\n            schema_def = d\n        if isinstance(d, type_asts):\n            type_defs.append(d)\n        elif isinstance(d, ast.DirectiveDefinition):\n            directive_defs.append(d)\n\n    if not schema_def:\n        raise Exception('Must provide a schema definition.')\n\n    query_type_name = None\n    mutation_type_name = None\n    subscription_type_name = None\n\n    for operation_type in schema_def.operation_types:\n        type_name = operation_type.type.name.value\n        if operation_type.operation == 'query':\n            if query_type_name:\n                raise Exception('Must provide only one query type in schema.')\n            query_type_name = type_name\n        elif operation_type.operation == 'mutation':\n            if mutation_type_name:\n                raise Exception('Must provide only one mutation type in schema.')\n            mutation_type_name = type_name\n        elif operation_type.operation == 'subscription':\n            if subscription_type_name:\n                raise Exception('Must provide only one subscription type in schema.')\n            subscription_type_name = type_name\n\n    if not query_type_name:\n        raise Exception('Must provide schema definition with query type.')\n\n    ast_map = {d.name.value: d for d in type_defs}\n\n    if query_type_name not in ast_map:\n        raise Exception('Specified query type \"{}\" not found in document.'.format(query_type_name))\n\n    if mutation_type_name and mutation_type_name not in ast_map:\n        raise Exception('Specified mutation type \"{}\" not found in document.'.format(mutation_type_name))\n\n    if subscription_type_name and subscription_type_name not in ast_map:\n        raise Exception('Specified subscription type \"{}\" not found in document.'.format(subscription_type_name))\n\n    inner_type_map = OrderedDict([\n        ('String', GraphQLString),\n        ('Int', GraphQLInt),\n        ('Float', GraphQLFloat),\n        ('Boolean', GraphQLBoolean),\n        ('ID', GraphQLID),\n        ('__Schema', __Schema),\n        ('__Directive', __Directive),\n        ('__DirectiveLocation', __DirectiveLocation),\n        ('__Type', __Type),\n        ('__Field', __Field),\n        ('__InputValue', __InputValue),\n        ('__EnumValue', __EnumValue),\n        ('__TypeKind', __TypeKind),\n    ])\n\n    def get_directive(directive_ast):\n        return GraphQLDirective(\n            name=directive_ast.name.value,\n            locations=[node.value for node in directive_ast.locations],\n            args=make_input_values(directive_ast.arguments, GraphQLArgument),\n        )\n\n    def get_object_type(type_ast):\n        type = type_def_named(type_ast.name.value)\n        assert isinstance(type, GraphQLObjectType), 'AST must provide object type'\n        return type\n\n    def produce_type_def(type_ast):\n        type_name = _get_named_type_ast(type_ast).name.value\n        type_def = type_def_named(type_name)\n        return _build_wrapped_type(type_def, type_ast)\n\n    def type_def_named(type_name):\n        if type_name in inner_type_map:\n            return inner_type_map[type_name]\n\n        if type_name not in ast_map:\n            raise Exception('Type \"{}\" not found in document'.format(type_name))\n\n        inner_type_def = make_schema_def(ast_map[type_name])\n        if not inner_type_def:\n            raise Exception('Nothing constructed for \"{}\".'.format(type_name))\n\n        inner_type_map[type_name] = inner_type_def\n        return inner_type_def\n\n    def make_schema_def(definition):\n        if not definition:\n            raise Exception('def must be defined.')\n\n        handler = _schema_def_handlers.get(type(definition))\n        if not handler:\n            raise Exception('Type kind \"{}\" not supported.'.format(type(definition).__name__))\n\n        return handler(definition)\n\n    def make_type_def(definition):\n        return GraphQLObjectType(\n            name=definition.name.value,\n            fields=lambda: make_field_def_map(definition),\n            interfaces=make_implemented_interfaces(definition)\n        )\n\n    def make_field_def_map(definition):\n        return OrderedDict(\n            (f.name.value, GraphQLField(\n                type=produce_type_def(f.type),\n                args=make_input_values(f.arguments, GraphQLArgument),\n                deprecation_reason=get_deprecation_reason(f.directives),\n            ))\n            for f in definition.fields\n        )\n\n    def make_implemented_interfaces(definition):\n        return [produce_type_def(i) for i in definition.interfaces]\n\n    def make_input_values(values, cls):\n        return OrderedDict(\n            (value.name.value, cls(\n                type=produce_type_def(value.type),\n                default_value=value_from_ast(value.default_value, produce_type_def(value.type))\n            ))\n            for value in values\n        )\n\n    def make_interface_def(definition):\n        return GraphQLInterfaceType(\n            name=definition.name.value,\n            resolve_type=_none,\n            fields=lambda: make_field_def_map(definition)\n        )\n\n    def make_enum_def(definition):\n        values = OrderedDict((v.name.value, GraphQLEnumValue(deprecation_reason=get_deprecation_reason(v.directives)))\n                             for v in definition.values)\n        return GraphQLEnumType(\n            name=definition.name.value,\n            values=values\n        )\n\n    def make_union_def(definition):\n        return GraphQLUnionType(\n            name=definition.name.value,\n            resolve_type=_none,\n            types=[produce_type_def(t) for t in definition.types]\n        )\n\n    def make_scalar_def(definition):\n        return GraphQLScalarType(\n            name=definition.name.value,\n            serialize=_none,\n            # Validation calls the parse functions to determine if a literal value is correct.\n            # Returning none, however would cause the scalar to fail validation. Returning false,\n            # will cause them to pass.\n            parse_literal=_false,\n            parse_value=_false\n        )\n\n    def make_input_object_def(definition):\n        return GraphQLInputObjectType(\n            name=definition.name.value,\n            fields=make_input_values(definition.fields, GraphQLInputObjectField)\n        )\n\n    _schema_def_handlers = {\n        ast.ObjectTypeDefinition: make_type_def,\n        ast.InterfaceTypeDefinition: make_interface_def,\n        ast.EnumTypeDefinition: make_enum_def,\n        ast.UnionTypeDefinition: make_union_def,\n        ast.ScalarTypeDefinition: make_scalar_def,\n        ast.InputObjectTypeDefinition: make_input_object_def\n    }\n    types = [type_def_named(definition.name.value) for definition in type_defs]\n    directives = [get_directive(d) for d in directive_defs]\n\n    # If specified directive were not explicitly declared, add them.\n    find_skip_directive = (directive.name for directive in directives if directive.name == 'skip')\n    find_include_directive = (directive.name for directive in directives if directive.name == 'include')\n    find_deprecated_directive = (directive.name for directive in directives if directive.name == 'deprecated')\n\n    if not next(find_skip_directive, None):\n        directives.append(GraphQLSkipDirective)\n\n    if not next(find_include_directive, None):\n        directives.append(GraphQLIncludeDirective)\n\n    if not next(find_deprecated_directive, None):\n        directives.append(GraphQLDeprecatedDirective)\n\n    schema_kwargs = {'query': get_object_type(ast_map[query_type_name])}\n\n    if mutation_type_name:\n        schema_kwargs['mutation'] = get_object_type(ast_map[mutation_type_name])\n\n    if subscription_type_name:\n        schema_kwargs['subscription'] = get_object_type(ast_map[subscription_type_name])\n\n    if directive_defs:\n        schema_kwargs['directives'] = directives\n\n    if types:\n        schema_kwargs['types'] = types\n\n    return GraphQLSchema(**schema_kwargs)\n\n\ndef get_deprecation_reason(directives):\n    deprecated_ast = next((directive for directive in directives\n                          if directive.name.value == GraphQLDeprecatedDirective.name),\n                          None)\n\n    if deprecated_ast:\n        args = get_argument_values(GraphQLDeprecatedDirective.args, deprecated_ast.arguments)\n        return args['reason']\n    else:\n        return None\n", 
    "graphql.utils.build_client_schema": "from ..language.parser import parse_value\nfrom ..pyutils.ordereddict import OrderedDict\nfrom ..type import (GraphQLArgument, GraphQLBoolean, GraphQLEnumType,\n                    GraphQLEnumValue, GraphQLField, GraphQLFloat, GraphQLID,\n                    GraphQLInputObjectField, GraphQLInputObjectType,\n                    GraphQLInt, GraphQLInterfaceType, GraphQLList,\n                    GraphQLNonNull, GraphQLObjectType, GraphQLScalarType,\n                    GraphQLSchema, GraphQLString, GraphQLUnionType,\n                    is_input_type, is_output_type)\nfrom ..type.directives import DirectiveLocation, GraphQLDirective\nfrom ..type.introspection import (TypeKind, __Directive, __DirectiveLocation,\n                                  __EnumValue, __Field, __InputValue, __Schema,\n                                  __Type, __TypeKind)\nfrom .value_from_ast import value_from_ast\n\n\ndef _false(*_):\n    return False\n\n\ndef _none(*_):\n    return None\n\n\ndef no_execution(*args):\n    raise Exception('Client Schema cannot be used for execution.')\n\n\ndef build_client_schema(introspection):\n    schema_introspection = introspection['__schema']\n\n    type_introspection_map = {t['name']: t for t in schema_introspection['types']}\n\n    type_def_cache = {\n        'String': GraphQLString,\n        'Int': GraphQLInt,\n        'Float': GraphQLFloat,\n        'Boolean': GraphQLBoolean,\n        'ID': GraphQLID,\n        '__Schema': __Schema,\n        '__Directive': __Directive,\n        '__DirectiveLocation': __DirectiveLocation,\n        '__Type': __Type,\n        '__Field': __Field,\n        '__InputValue': __InputValue,\n        '__EnumValue': __EnumValue,\n        '__TypeKind': __TypeKind,\n\n    }\n\n    def get_type(type_ref):\n        kind = type_ref.get('kind')\n\n        if kind == TypeKind.LIST:\n            item_ref = type_ref.get('ofType')\n\n            if not item_ref:\n                raise Exception('Decorated type deeper than introspection query.')\n\n            return GraphQLList(get_type(item_ref))\n\n        elif kind == TypeKind.NON_NULL:\n            nullable_ref = type_ref.get('ofType')\n            if not nullable_ref:\n                raise Exception('Decorated type deeper than introspection query.')\n\n            return GraphQLNonNull(get_type(nullable_ref))\n\n        return get_named_type(type_ref['name'])\n\n    def get_named_type(type_name):\n        if type_name in type_def_cache:\n            return type_def_cache[type_name]\n\n        type_introspection = type_introspection_map.get(type_name)\n        if not type_introspection:\n            raise Exception(\n                'Invalid or incomplete schema, unknown type: {}. Ensure that a full introspection query '\n                'is used in order to build a client schema.'.format(type_name)\n            )\n\n        type_def = type_def_cache[type_name] = build_type(type_introspection)\n        return type_def\n\n    def get_input_type(type_ref):\n        input_type = get_type(type_ref)\n        assert is_input_type(input_type), 'Introspection must provide input type for arguments.'\n        return input_type\n\n    def get_output_type(type_ref):\n        output_type = get_type(type_ref)\n        assert is_output_type(output_type), 'Introspection must provide output type for fields.'\n        return output_type\n\n    def get_object_type(type_ref):\n        object_type = get_type(type_ref)\n        assert isinstance(object_type, GraphQLObjectType), 'Introspection must provide object type for possibleTypes.'\n        return object_type\n\n    def get_interface_type(type_ref):\n        interface_type = get_type(type_ref)\n        assert isinstance(interface_type, GraphQLInterfaceType), \\\n            'Introspection must provide interface type for interfaces.'\n        return interface_type\n\n    def build_type(type):\n        type_kind = type.get('kind')\n        handler = type_builders.get(type_kind)\n        if not handler:\n            raise Exception(\n                'Invalid or incomplete schema, unknown kind: {}. Ensure that a full introspection query '\n                'is used in order to build a client schema.'.format(type_kind)\n            )\n\n        return handler(type)\n\n    def build_scalar_def(scalar_introspection):\n        return GraphQLScalarType(\n            name=scalar_introspection['name'],\n            description=scalar_introspection.get('description'),\n            serialize=_none,\n            parse_value=_false,\n            parse_literal=_false\n        )\n\n    def build_object_def(object_introspection):\n        return GraphQLObjectType(\n            name=object_introspection['name'],\n            description=object_introspection.get('description'),\n            interfaces=[get_interface_type(i) for i in object_introspection.get('interfaces', [])],\n            fields=lambda: build_field_def_map(object_introspection)\n        )\n\n    def build_interface_def(interface_introspection):\n        return GraphQLInterfaceType(\n            name=interface_introspection['name'],\n            description=interface_introspection.get('description'),\n            fields=lambda: build_field_def_map(interface_introspection),\n            resolve_type=no_execution\n        )\n\n    def build_union_def(union_introspection):\n        return GraphQLUnionType(\n            name=union_introspection['name'],\n            description=union_introspection.get('description'),\n            types=[get_object_type(t) for t in union_introspection.get('possibleTypes', [])],\n            resolve_type=no_execution\n        )\n\n    def build_enum_def(enum_introspection):\n        return GraphQLEnumType(\n            name=enum_introspection['name'],\n            description=enum_introspection.get('description'),\n            values=OrderedDict([(value_introspection['name'],\n                                 GraphQLEnumValue(description=value_introspection.get('description'),\n                                                  deprecation_reason=value_introspection.get('deprecationReason')))\n                                for value_introspection in enum_introspection.get('enumValues', [])\n                                ])\n        )\n\n    def build_input_object_def(input_object_introspection):\n        return GraphQLInputObjectType(\n            name=input_object_introspection['name'],\n            description=input_object_introspection.get('description'),\n            fields=lambda: build_input_value_def_map(\n                input_object_introspection.get('inputFields'), GraphQLInputObjectField\n            )\n        )\n\n    type_builders = {\n        TypeKind.SCALAR: build_scalar_def,\n        TypeKind.OBJECT: build_object_def,\n        TypeKind.INTERFACE: build_interface_def,\n        TypeKind.UNION: build_union_def,\n        TypeKind.ENUM: build_enum_def,\n        TypeKind.INPUT_OBJECT: build_input_object_def\n    }\n\n    def build_field_def_map(type_introspection):\n        return OrderedDict([\n            (f['name'], GraphQLField(\n                type=get_output_type(f['type']),\n                description=f.get('description'),\n                resolver=no_execution,\n                deprecation_reason=f.get('deprecationReason'),\n                args=build_input_value_def_map(f.get('args'), GraphQLArgument)))\n            for f in type_introspection.get('fields', [])\n        ])\n\n    def build_default_value(f):\n        default_value = f.get('defaultValue')\n        if default_value is None:\n            return None\n\n        return value_from_ast(parse_value(default_value), get_input_type(f['type']))\n\n    def build_input_value_def_map(input_value_introspection, argument_type):\n        return OrderedDict([\n            (f['name'], build_input_value(f, argument_type)) for f in input_value_introspection\n        ])\n\n    def build_input_value(input_value_introspection, argument_type):\n        input_value = argument_type(\n            description=input_value_introspection['description'],\n            type=get_input_type(input_value_introspection['type']),\n            default_value=build_default_value(input_value_introspection)\n        )\n        return input_value\n\n    def build_directive(directive_introspection):\n        # Support deprecated `on****` fields for building `locations`, as this\n        # is used by GraphiQL which may need to support outdated servers.\n        locations = list(directive_introspection.get('locations', []))\n        if not locations:\n            locations = []\n            if directive_introspection.get('onField', False):\n                locations += list(DirectiveLocation.FIELD_LOCATIONS)\n            if directive_introspection.get('onOperation', False):\n                locations += list(DirectiveLocation.OPERATION_LOCATIONS)\n            if directive_introspection.get('onFragment', False):\n                locations += list(DirectiveLocation.FRAGMENT_LOCATIONS)\n\n        return GraphQLDirective(\n            name=directive_introspection['name'],\n            description=directive_introspection.get('description'),\n            # TODO: {} ?\n            args=build_input_value_def_map(directive_introspection.get('args', {}), GraphQLArgument),\n            locations=locations\n        )\n\n    # Iterate through all types, getting the type definition for each, ensuring\n    # that any type not directly referenced by a field will get created.\n    types = [get_named_type(type_introspection_name) for type_introspection_name in type_introspection_map.keys()]\n\n    query_type = get_object_type(schema_introspection['queryType'])\n    mutation_type = get_object_type(\n        schema_introspection['mutationType']) if schema_introspection.get('mutationType') else None\n    subscription_type = get_object_type(schema_introspection['subscriptionType']) if \\\n        schema_introspection.get('subscriptionType') else None\n\n    directives = [build_directive(d) for d in schema_introspection['directives']] \\\n        if schema_introspection['directives'] else []\n\n    return GraphQLSchema(\n        query=query_type,\n        mutation=mutation_type,\n        subscription=subscription_type,\n        directives=directives,\n        types=types\n    )\n", 
    "graphql.utils.concat_ast": "import itertools\n\nfrom ..language.ast import Document\n\n\ndef concat_ast(asts):\n    return Document(definitions=list(itertools.chain.from_iterable(\n        document.definitions for document in asts\n    )))\n", 
    "graphql.utils.extend_schema": "from collections import defaultdict\n\nfrom ..error import GraphQLError\nfrom ..language import ast\nfrom ..pyutils.ordereddict import OrderedDict\nfrom ..type.definition import (GraphQLArgument, GraphQLEnumType,\n                               GraphQLEnumValue, GraphQLField,\n                               GraphQLInputObjectField, GraphQLInputObjectType,\n                               GraphQLInterfaceType, GraphQLList,\n                               GraphQLNonNull, GraphQLObjectType,\n                               GraphQLScalarType, GraphQLUnionType)\nfrom ..type.introspection import (__Directive, __DirectiveLocation,\n                                  __EnumValue, __Field, __InputValue, __Schema,\n                                  __Type, __TypeKind)\nfrom ..type.scalars import (GraphQLBoolean, GraphQLFloat, GraphQLID,\n                            GraphQLInt, GraphQLString)\nfrom ..type.schema import GraphQLSchema\nfrom .value_from_ast import value_from_ast\n\n\ndef extend_schema(schema, documentAST=None):\n    \"\"\"Produces a new schema given an existing schema and a document which may\n    contain GraphQL type extensions and definitions. The original schema will\n    remain unaltered.\n\n    Because a schema represents a graph of references, a schema cannot be\n    extended without effectively making an entire copy. We do not know until it's\n    too late if subgraphs remain unchanged.\n\n    This algorithm copies the provided schema, applying extensions while\n    producing the copy. The original schema remains unaltered.\"\"\"\n\n    assert isinstance(\n        schema, GraphQLSchema), 'Must provide valid GraphQLSchema'\n    assert documentAST and isinstance(\n        documentAST, ast.Document), 'Must provide valid Document AST'\n\n    # Collect the type definitions and extensions found in the document.\n    type_definition_map = {}\n    type_extensions_map = defaultdict(list)\n\n    for _def in documentAST.definitions:\n        if isinstance(_def, (\n            ast.ObjectTypeDefinition,\n            ast.InterfaceTypeDefinition,\n            ast.EnumTypeDefinition,\n            ast.UnionTypeDefinition,\n            ast.ScalarTypeDefinition,\n            ast.InputObjectTypeDefinition,\n        )):\n            # Sanity check that none of the defined types conflict with the\n            # schema's existing types.\n            type_name = _def.name.value\n            if schema.get_type(type_name):\n                raise GraphQLError(\n                    ('Type \"{}\" already exists in the schema. It cannot also ' +\n                     'be defined in this type definition.').format(type_name),\n                    [_def]\n                )\n\n            type_definition_map[type_name] = _def\n        elif isinstance(_def, ast.TypeExtensionDefinition):\n            # Sanity check that this type extension exists within the\n            # schema's existing types.\n            extended_type_name = _def.definition.name.value\n            existing_type = schema.get_type(extended_type_name)\n            if not existing_type:\n                raise GraphQLError(\n                    ('Cannot extend type \"{}\" because it does not ' +\n                     'exist in the existing schema.').format(extended_type_name),\n                    [_def.definition]\n                )\n            if not isinstance(existing_type, GraphQLObjectType):\n                raise GraphQLError(\n                    'Cannot extend non-object type \"{}\".'.format(\n                        extended_type_name),\n                    [_def.definition]\n                )\n\n            type_extensions_map[extended_type_name].append(_def)\n\n    # Below are functions used for producing this schema that have closed over\n    # this scope and have access to the schema, cache, and newly defined types.\n\n    def get_type_from_def(type_def):\n        type = _get_named_type(type_def.name)\n        assert type, 'Invalid schema'\n        return type\n\n    def get_type_from_AST(astNode):\n        type = _get_named_type(astNode.name.value)\n        if not type:\n            raise GraphQLError(\n                ('Unknown type: \"{}\". Ensure that this type exists ' +\n                 'either in the original schema, or is added in a type definition.').format(\n                    astNode.name.value),\n                [astNode]\n            )\n        return type\n\n    # Given a name, returns a type from either the existing schema or an\n    # added type.\n    def _get_named_type(typeName):\n        cached_type_def = type_def_cache.get(typeName)\n        if cached_type_def:\n            return cached_type_def\n\n        existing_type = schema.get_type(typeName)\n        if existing_type:\n            type_def = extend_type(existing_type)\n            type_def_cache[typeName] = type_def\n            return type_def\n\n        type_ast = type_definition_map.get(typeName)\n        if type_ast:\n            type_def = build_type(type_ast)\n            type_def_cache[typeName] = type_def\n            return type_def\n\n    # Given a type's introspection result, construct the correct\n    # GraphQLType instance.\n    def extend_type(type):\n        if isinstance(type, GraphQLObjectType):\n            return extend_object_type(type)\n        if isinstance(type, GraphQLInterfaceType):\n            return extend_interface_type(type)\n        if isinstance(type, GraphQLUnionType):\n            return extend_union_type(type)\n        return type\n\n    def extend_object_type(type):\n        return GraphQLObjectType(\n            name=type.name,\n            description=type.description,\n            interfaces=lambda: extend_implemented_interfaces(type),\n            fields=lambda: extend_field_map(type),\n        )\n\n    def extend_interface_type(type):\n        return GraphQLInterfaceType(\n            name=type.name,\n            description=type.description,\n            fields=lambda: extend_field_map(type),\n            resolve_type=cannot_execute_client_schema,\n        )\n\n    def extend_union_type(type):\n        return GraphQLUnionType(\n            name=type.name,\n            description=type.description,\n            types=list(map(get_type_from_def, type.types)),\n            resolve_type=cannot_execute_client_schema,\n        )\n\n    def extend_implemented_interfaces(type):\n        interfaces = list(map(get_type_from_def, type.interfaces))\n\n        # If there are any extensions to the interfaces, apply those here.\n        extensions = type_extensions_map[type.name]\n        for extension in extensions:\n            for namedType in extension.definition.interfaces:\n                interface_name = namedType.name.value\n                if any([_def.name == interface_name for _def in interfaces]):\n                    raise GraphQLError(\n                        ('Type \"{}\" already implements \"{}\". ' +\n                         'It cannot also be implemented in this type extension.').format(\n                            type.name, interface_name),\n                        [namedType]\n                    )\n                interfaces.append(get_type_from_AST(namedType))\n\n        return interfaces\n\n    def extend_field_map(type):\n        new_field_map = OrderedDict()\n        old_field_map = type.fields\n        for field_name, field in old_field_map.items():\n            new_field_map[field_name] = GraphQLField(\n                extend_field_type(field.type),\n                description=field.description,\n                deprecation_reason=field.deprecation_reason,\n                args=field.args,\n                resolver=cannot_execute_client_schema,\n            )\n\n        # If there are any extensions to the fields, apply those here.\n        extensions = type_extensions_map[type.name]\n        for extension in extensions:\n            for field in extension.definition.fields:\n                field_name = field.name.value\n                if field_name in old_field_map:\n                    raise GraphQLError(\n                        ('Field \"{}.{}\" already exists in the ' +\n                         'schema. It cannot also be defined in this type extension.').format(\n                            type.name, field_name),\n                        [field]\n                    )\n                new_field_map[field_name] = GraphQLField(\n                    build_field_type(field.type),\n                    args=build_input_values(field.arguments),\n                    resolver=cannot_execute_client_schema,\n                )\n\n        return new_field_map\n\n    def extend_field_type(type):\n        if isinstance(type, GraphQLList):\n            return GraphQLList(extend_field_type(type.of_type))\n        if isinstance(type, GraphQLNonNull):\n            return GraphQLNonNull(extend_field_type(type.of_type))\n        return get_type_from_def(type)\n\n    def build_type(type_ast):\n        _type_build = {\n            ast.ObjectTypeDefinition: build_object_type,\n            ast.InterfaceTypeDefinition: build_interface_type,\n            ast.UnionTypeDefinition: build_union_type,\n            ast.ScalarTypeDefinition: build_scalar_type,\n            ast.EnumTypeDefinition: build_enum_type,\n            ast.InputObjectTypeDefinition: build_input_object_type\n        }\n        func = _type_build.get(type(type_ast))\n        if func:\n            return func(type_ast)\n\n    def build_object_type(type_ast):\n        return GraphQLObjectType(\n            type_ast.name.value,\n            interfaces=lambda: build_implemented_interfaces(type_ast),\n            fields=lambda: build_field_map(type_ast),\n        )\n\n    def build_interface_type(type_ast):\n        return GraphQLInterfaceType(\n            type_ast.name.value,\n            fields=lambda: build_field_map(type_ast),\n            resolve_type=cannot_execute_client_schema,\n        )\n\n    def build_union_type(type_ast):\n        return GraphQLUnionType(\n            type_ast.name.value,\n            types=list(map(get_type_from_AST, type_ast.types)),\n            resolve_type=cannot_execute_client_schema,\n        )\n\n    def build_scalar_type(type_ast):\n        return GraphQLScalarType(\n            type_ast.name.value,\n            serialize=lambda *args, **kwargs: None,\n            # Note: validation calls the parse functions to determine if a\n            # literal value is correct. Returning null would cause use of custom\n            # scalars to always fail validation. Returning false causes them to\n            # always pass validation.\n            parse_value=lambda *args, **kwargs: False,\n            parse_literal=lambda *args, **kwargs: False,\n        )\n\n    def build_enum_type(type_ast):\n        return GraphQLEnumType(\n            type_ast.name.value,\n            values={v.name.value: GraphQLEnumValue() for v in type_ast.values},\n        )\n\n    def build_input_object_type(type_ast):\n        return GraphQLInputObjectType(\n            type_ast.name.value,\n            fields=lambda: build_input_values(\n                type_ast.fields, GraphQLInputObjectField),\n        )\n\n    def build_implemented_interfaces(type_ast):\n        return list(map(get_type_from_AST, type_ast.interfaces))\n\n    def build_field_map(type_ast):\n        return {\n            field.name.value: GraphQLField(\n                build_field_type(field.type),\n                args=build_input_values(field.arguments),\n                resolver=cannot_execute_client_schema,\n            ) for field in type_ast.fields\n        }\n\n    def build_input_values(values, input_type=GraphQLArgument):\n        input_values = OrderedDict()\n        for value in values:\n            type = build_field_type(value.type)\n            input_values[value.name.value] = input_type(\n                type,\n                default_value=value_from_ast(value.default_value, type)\n            )\n        return input_values\n\n    def build_field_type(type_ast):\n        if isinstance(type_ast, ast.ListType):\n            return GraphQLList(build_field_type(type_ast.type))\n        if isinstance(type_ast, ast.NonNullType):\n            return GraphQLNonNull(build_field_type(type_ast.type))\n        return get_type_from_AST(type_ast)\n\n    # If this document contains no new types, then return the same unmodified\n    # GraphQLSchema instance.\n    if not type_extensions_map and not type_definition_map:\n        return schema\n\n    # A cache to use to store the actual GraphQLType definition objects by name.\n    # Initialize to the GraphQL built in scalars and introspection types. All\n    # functions below are inline so that this type def cache is within the scope\n    # of the closure.\n\n    type_def_cache = {\n        'String': GraphQLString,\n        'Int': GraphQLInt,\n        'Float': GraphQLFloat,\n        'Boolean': GraphQLBoolean,\n        'ID': GraphQLID,\n        '__Schema': __Schema,\n        '__Directive': __Directive,\n        '__DirectiveLocation': __DirectiveLocation,\n        '__Type': __Type,\n        '__Field': __Field,\n        '__InputValue': __InputValue,\n        '__EnumValue': __EnumValue,\n        '__TypeKind': __TypeKind,\n    }\n\n    # Get the root Query, Mutation, and Subscription types.\n    query_type = get_type_from_def(schema.get_query_type())\n\n    existing_mutation_type = schema.get_mutation_type()\n    mutationType = existing_mutation_type and get_type_from_def(\n        existing_mutation_type) or None\n\n    existing_subscription_type = schema.get_subscription_type()\n    subscription_type = existing_subscription_type and get_type_from_def(\n        existing_subscription_type) or None\n\n    # Iterate through all types, getting the type definition for each, ensuring\n    # that any type not directly referenced by a field will get created.\n    types = [get_type_from_def(_def) for _def in schema.get_type_map().values()]\n\n    # Do the same with new types, appending to the list of defined types.\n    types += [get_type_from_AST(_def) for _def in type_definition_map.values()]\n\n    # Then produce and return a Schema with these types.\n    return GraphQLSchema(\n        query=query_type,\n        mutation=mutationType,\n        subscription=subscription_type,\n        # Copy directives.\n        directives=schema.get_directives(),\n        types=types\n    )\n\n\ndef cannot_execute_client_schema(*args, **kwargs):\n    raise Exception('Client Schema cannot be used for execution.')\n", 
    "graphql.utils.get_field_def": "from ..type.definition import (GraphQLInterfaceType, GraphQLObjectType,\n                               GraphQLUnionType)\nfrom ..type.introspection import (SchemaMetaFieldDef, TypeMetaFieldDef,\n                                  TypeNameMetaFieldDef)\n\n\ndef get_field_def(schema, parent_type, field_ast):\n    \"\"\"Not exactly the same as the executor's definition of get_field_def, in this\n    statically evaluated environment we do not always have an Object type,\n    and need to handle Interface and Union types.\"\"\"\n    name = field_ast.name.value\n    if name == '__schema' and schema.get_query_type() == parent_type:\n        return SchemaMetaFieldDef\n\n    elif name == '__type' and schema.get_query_type() == parent_type:\n        return TypeMetaFieldDef\n\n    elif name == '__typename' and \\\n            isinstance(parent_type, (\n                GraphQLObjectType,\n                GraphQLInterfaceType,\n                GraphQLUnionType,\n            )):\n        return TypeNameMetaFieldDef\n\n    elif isinstance(parent_type, (GraphQLObjectType, GraphQLInterfaceType)):\n        return parent_type.fields.get(name)\n", 
    "graphql.utils.get_operation_ast": "from ..language import ast\n\n\ndef get_operation_ast(document_ast, operation_name=None):\n    operation = None\n\n    for definition in document_ast.definitions:\n        if isinstance(definition, ast.OperationDefinition):\n            if not operation_name:\n                # If no operation name is provided, only return an Operation if it is the only one present in the\n                # document. This means that if we've encountered a second operation as we were iterating over the\n                # definitions in the document, there are more than one Operation defined, and we should return None.\n                if operation:\n                    return None\n\n                operation = definition\n\n            elif definition.name and definition.name.value == operation_name:\n                return definition\n\n    return operation\n", 
    "graphql.utils.introspection_query": "introspection_query = '''\n  query IntrospectionQuery {\n    __schema {\n      queryType { name }\n      mutationType { name }\n      subscriptionType { name }\n      types {\n        ...FullType\n      }\n      directives {\n        name\n        description\n        locations\n        args {\n          ...InputValue\n        }\n      }\n    }\n  }\n  fragment FullType on __Type {\n    kind\n    name\n    description\n    fields(includeDeprecated: true) {\n      name\n      description\n      args {\n        ...InputValue\n      }\n      type {\n        ...TypeRef\n      }\n      isDeprecated\n      deprecationReason\n    }\n    inputFields {\n      ...InputValue\n    }\n    interfaces {\n      ...TypeRef\n    }\n    enumValues(includeDeprecated: true) {\n      name\n      description\n      isDeprecated\n      deprecationReason\n    }\n    possibleTypes {\n      ...TypeRef\n    }\n  }\n  fragment InputValue on __InputValue {\n    name\n    description\n    type { ...TypeRef }\n    defaultValue\n  }\n  fragment TypeRef on __Type {\n    kind\n    name\n    ofType {\n      kind\n      name\n      ofType {\n        kind\n        name\n        ofType {\n          kind\n          name\n          ofType {\n            kind\n            name\n            ofType {\n              kind\n              name\n              ofType {\n                kind\n                name\n                ofType {\n                  kind\n                  name\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n  }\n'''\n", 
    "graphql.utils.is_valid_literal_value": "from ..language import ast\nfrom ..language.printer import print_ast\nfrom ..type.definition import (GraphQLEnumType, GraphQLInputObjectType,\n                               GraphQLList, GraphQLNonNull, GraphQLScalarType)\n\n_empty_list = []\n\n\ndef is_valid_literal_value(type, value_ast):\n    if isinstance(type, GraphQLNonNull):\n        of_type = type.of_type\n        if not value_ast:\n            return [u'Expected \"{}\", found null.'.format(type)]\n\n        return is_valid_literal_value(of_type, value_ast)\n\n    if not value_ast:\n        return _empty_list\n\n    if isinstance(value_ast, ast.Variable):\n        return _empty_list\n\n    if isinstance(type, GraphQLList):\n        item_type = type.of_type\n        if isinstance(value_ast, ast.ListValue):\n            errors = []\n\n            for i, item_ast in enumerate(value_ast.values):\n                item_errors = is_valid_literal_value(item_type, item_ast)\n                for error in item_errors:\n                    errors.append(u'In element #{}: {}'.format(i, error))\n\n            return errors\n\n        return is_valid_literal_value(item_type, value_ast)\n\n    if isinstance(type, GraphQLInputObjectType):\n        if not isinstance(value_ast, ast.ObjectValue):\n            return [u'Expected \"{}\", found not an object.'.format(type)]\n\n        fields = type.fields\n        field_asts = value_ast.fields\n\n        errors = []\n        for provided_field_ast in field_asts:\n            if provided_field_ast.name.value not in fields:\n                errors.append(u'In field \"{}\": Unknown field.'.format(provided_field_ast.name.value))\n\n        field_ast_map = {field_ast.name.value: field_ast for field_ast in field_asts}\n\n        def get_field_ast_value(field_name):\n            if field_name in field_ast_map:\n                return field_ast_map[field_name].value\n\n        for field_name, field in fields.items():\n            subfield_errors = is_valid_literal_value(field.type, get_field_ast_value(field_name))\n            errors.extend(u'In field \"{}\": {}'.format(field_name, e) for e in subfield_errors)\n\n        return errors\n\n    assert isinstance(type, (GraphQLScalarType, GraphQLEnumType)), 'Must be input type'\n\n    parse_result = type.parse_literal(value_ast)\n    if parse_result is None:\n        return [u'Expected type \"{}\", found {}.'.format(type.name, print_ast(value_ast))]\n\n    return _empty_list\n", 
    "graphql.utils.is_valid_value": "\"\"\"\n    Implementation of isValidJSValue from graphql.s\n\"\"\"\n\nimport collections\nimport json\n\nfrom six import string_types\n\nfrom ..type import (GraphQLEnumType, GraphQLInputObjectType, GraphQLList,\n                    GraphQLNonNull, GraphQLScalarType)\n\n_empty_list = []\n\n\ndef is_valid_value(value, type):\n    \"\"\"Given a type and any value, return True if that value is valid.\"\"\"\n    if isinstance(type, GraphQLNonNull):\n        of_type = type.of_type\n        if value is None:\n            return [u'Expected \"{}\", found null.'.format(type)]\n\n        return is_valid_value(value, of_type)\n\n    if value is None:\n        return _empty_list\n\n    if isinstance(type, GraphQLList):\n        item_type = type.of_type\n        if not isinstance(value, string_types) and isinstance(value, collections.Iterable):\n            errors = []\n            for i, item in enumerate(value):\n                item_errors = is_valid_value(item, item_type)\n                for error in item_errors:\n                    errors.append(u'In element #{}: {}'.format(i, error))\n\n            return errors\n\n        else:\n            return is_valid_value(value, item_type)\n\n    if isinstance(type, GraphQLInputObjectType):\n        if not isinstance(value, collections.Mapping):\n            return [u'Expected \"{}\", found not an object.'.format(type)]\n\n        fields = type.fields\n        errors = []\n\n        for provided_field in sorted(value.keys()):\n            if provided_field not in fields:\n                errors.append(u'In field \"{}\": Unknown field.'.format(provided_field))\n\n        for field_name, field in fields.items():\n            subfield_errors = is_valid_value(value.get(field_name), field.type)\n            errors.extend(u'In field \"{}\": {}'.format(field_name, e) for e in subfield_errors)\n\n        return errors\n\n    assert isinstance(type, (GraphQLScalarType, GraphQLEnumType)), \\\n        'Must be input type'\n\n    # Scalar/Enum input checks to ensure the type can parse the value to\n    # a non-null value.\n    parse_result = type.parse_value(value)\n    if parse_result is None:\n        return [u'Expected type \"{}\", found {}.'.format(type, json.dumps(value))]\n\n    return _empty_list\n", 
    "graphql.utils.quoted_or_list": "import functools\n\nMAX_LENGTH = 5\n\n\ndef quoted_or_list(items):\n    '''Given [ A, B, C ] return '\"A\", \"B\" or \"C\"'.'''\n    selected = items[:MAX_LENGTH]\n    quoted_items = ('\"{}\"'.format(t) for t in selected)\n\n    def quoted_or_text(text, quoted_and_index):\n        index = quoted_and_index[0]\n        quoted_item = quoted_and_index[1]\n        text += ((', ' if len(selected) > 2 and not index == len(selected) - 1 else ' ') +\n                 ('or ' if index == len(selected) - 1 else '') +\n                 quoted_item)\n        return text\n\n    enumerated_items = enumerate(quoted_items)\n    first_item = next(enumerated_items)[1]\n    return functools.reduce(quoted_or_text, enumerated_items, first_item)\n", 
    "graphql.utils.schema_printer": "from ..language.printer import print_ast\nfrom ..type.definition import (GraphQLEnumType, GraphQLInputObjectType,\n                               GraphQLInterfaceType, GraphQLObjectType,\n                               GraphQLScalarType, GraphQLUnionType)\nfrom ..type.directives import DEFAULT_DEPRECATION_REASON\nfrom .ast_from_value import ast_from_value\n\n\ndef print_schema(schema):\n    return _print_filtered_schema(schema, lambda n: not(is_spec_directive(n)), _is_defined_type)\n\n\ndef print_introspection_schema(schema):\n    return _print_filtered_schema(schema, is_spec_directive, _is_introspection_type)\n\n\ndef is_spec_directive(directive_name):\n    return directive_name in ('skip', 'include', 'deprecated')\n\n\ndef _is_defined_type(typename):\n    return not _is_introspection_type(typename) and not _is_builtin_scalar(typename)\n\n\ndef _is_introspection_type(typename):\n    return typename.startswith('__')\n\n\n_builtin_scalars = frozenset(['String', 'Boolean', 'Int', 'Float', 'ID'])\n\n\ndef _is_builtin_scalar(typename):\n    return typename in _builtin_scalars\n\n\ndef _print_filtered_schema(schema, directive_filter, type_filter):\n    return '\\n\\n'.join([\n        _print_schema_definition(schema)\n    ] + [\n        _print_directive(directive)\n        for directive in schema.get_directives()\n        if directive_filter(directive.name)\n    ] + [\n        _print_type(type)\n        for typename, type in sorted(schema.get_type_map().items())\n        if type_filter(typename)\n    ]) + '\\n'\n\n\ndef _print_schema_definition(schema):\n    operation_types = []\n\n    query_type = schema.get_query_type()\n    if query_type:\n        operation_types.append('  query: {}'.format(query_type))\n\n    mutation_type = schema.get_mutation_type()\n    if mutation_type:\n        operation_types.append('  mutation: {}'.format(mutation_type))\n\n    subscription_type = schema.get_subscription_type()\n    if subscription_type:\n        operation_types.append('  subscription: {}'.format(subscription_type))\n\n    return 'schema {{\\n{}\\n}}'.format('\\n'.join(operation_types))\n\n\ndef _print_type(type):\n    if isinstance(type, GraphQLScalarType):\n        return _print_scalar(type)\n\n    elif isinstance(type, GraphQLObjectType):\n        return _print_object(type)\n\n    elif isinstance(type, GraphQLInterfaceType):\n        return _print_interface(type)\n\n    elif isinstance(type, GraphQLUnionType):\n        return _print_union(type)\n\n    elif isinstance(type, GraphQLEnumType):\n        return _print_enum(type)\n\n    assert isinstance(type, GraphQLInputObjectType)\n    return _print_input_object(type)\n\n\ndef _print_scalar(type):\n    return 'scalar {}'.format(type.name)\n\n\ndef _print_object(type):\n    interfaces = type.interfaces\n    implemented_interfaces = \\\n        ' implements {}'.format(', '.join(i.name for i in interfaces)) if interfaces else ''\n\n    return (\n        'type {}{} {{\\n'\n        '{}\\n'\n        '}}'\n    ).format(type.name, implemented_interfaces, _print_fields(type))\n\n\ndef _print_interface(type):\n    return (\n        'interface {} {{\\n'\n        '{}\\n'\n        '}}'\n    ).format(type.name, _print_fields(type))\n\n\ndef _print_union(type):\n    return 'union {} = {}'.format(type.name, ' | '.join(str(t) for t in type.types))\n\n\ndef _print_enum(type):\n    return (\n        'enum {} {{\\n'\n        '{}\\n'\n        '}}'\n    ).format(type.name, '\\n'.join('  ' + v.name + _print_deprecated(v) for v in type.values))\n\n\ndef _print_input_object(type):\n    return (\n        'input {} {{\\n'\n        '{}\\n'\n        '}}'\n    ).format(type.name, '\\n'.join('  ' + _print_input_value(name, field) for name, field in type.fields.items()))\n\n\ndef _print_fields(type):\n    return '\\n'.join('  {}{}: {}{}'.format(f_name, _print_args(f), f.type, _print_deprecated(f))\n                     for f_name, f in type.fields.items())\n\n\ndef _print_deprecated(field_or_enum_value):\n    reason = field_or_enum_value.deprecation_reason\n\n    if reason is None:\n        return ''\n    elif reason in ('', DEFAULT_DEPRECATION_REASON):\n        return ' @deprecated'\n    else:\n        return ' @deprecated(reason: {})'.format(print_ast(ast_from_value(reason)))\n\n\ndef _print_args(field_or_directives):\n    if not field_or_directives.args:\n        return ''\n\n    return '({})'.format(', '.join(_print_input_value(arg_name, arg) for arg_name, arg in field_or_directives.args.items()))\n\n\ndef _print_input_value(name, arg):\n    if arg.default_value is not None:\n        default_value = ' = ' + print_ast(ast_from_value(arg.default_value, arg.type))\n    else:\n        default_value = ''\n\n    return '{}: {}{}'.format(name, arg.type, default_value)\n\n\ndef _print_directive(directive):\n    return 'directive @{}{} on {}'.format(directive.name, _print_args(directive), ' | '.join(directive.locations))\n\n\n__all__ = ['print_schema', 'print_introspection_schema']\n", 
    "graphql.utils.suggestion_list": "from collections import OrderedDict\n\n\ndef suggestion_list(inp, options):\n    '''\n     Given an invalid input string and a list of valid options, returns a filtered\n     list of valid options sorted based on their similarity with the input.\n    '''\n    options_by_distance = OrderedDict()\n    input_threshold = len(inp) / 2\n\n    for option in options:\n        distance = lexical_distance(inp, option)\n        threshold = max(input_threshold, len(option) / 2, 1)\n        if distance <= threshold:\n            options_by_distance[option] = distance\n\n    return sorted(list(options_by_distance.keys()), key=lambda k: options_by_distance[k])\n\n\ndef lexical_distance(a, b):\n    '''\n     Computes the lexical distance between strings A and B.\n     The \"distance\" between two strings is given by counting the minimum number\n     of edits needed to transform string A into string B. An edit can be an\n     insertion, deletion, or substitution of a single character, or a swap of two\n     adjacent characters.\n     This distance can be useful for detecting typos in input or sorting\n     @returns distance in number of edits\n    '''\n\n    d = [[i] for i in range(len(a) + 1)] or []\n    d_len = len(d) or 1\n    for i in range(d_len):\n        for j in range(1, len(b) + 1):\n            if i == 0:\n                d[i].append(j)\n            else:\n                d[i].append(0)\n\n    for i in range(1, len(a) + 1):\n        for j in range(1, len(b) + 1):\n            cost = 0 if a[i - 1] == b[j - 1] else 1\n\n            d[i][j] = min(\n                d[i - 1][j] + 1,\n                d[i][j - 1] + 1,\n                d[i - 1][j - 1] + cost\n            )\n\n            if (i > 1 and j < 1 and\n               a[i - 1] == b[j - 2] and\n               a[i - 2] == b[j - 1]):\n                d[i][j] = min(d[i][j], d[i - 2][j - 2] + cost)\n\n    return d[len(a)][len(b)]\n", 
    "graphql.utils.type_comparators": "from ..type.definition import (GraphQLInterfaceType, GraphQLList,\n                               GraphQLNonNull, GraphQLObjectType,\n                               GraphQLUnionType, is_abstract_type)\n\n\ndef is_equal_type(type_a, type_b):\n    if type_a is type_b:\n        return True\n\n    if isinstance(type_a, GraphQLNonNull) and isinstance(type_b, GraphQLNonNull):\n        return is_equal_type(type_a.of_type, type_b.of_type)\n\n    if isinstance(type_a, GraphQLList) and isinstance(type_b, GraphQLList):\n        return is_equal_type(type_a.of_type, type_b.of_type)\n\n    return False\n\n\ndef is_type_sub_type_of(schema, maybe_subtype, super_type):\n    if maybe_subtype is super_type:\n        return True\n\n    if isinstance(super_type, GraphQLNonNull):\n        if isinstance(maybe_subtype, GraphQLNonNull):\n            return is_type_sub_type_of(schema, maybe_subtype.of_type, super_type.of_type)\n        return False\n    elif isinstance(maybe_subtype, GraphQLNonNull):\n        return is_type_sub_type_of(schema, maybe_subtype.of_type, super_type)\n\n    if isinstance(super_type, GraphQLList):\n        if isinstance(maybe_subtype, GraphQLList):\n            return is_type_sub_type_of(schema, maybe_subtype.of_type, super_type.of_type)\n        return False\n    elif isinstance(maybe_subtype, GraphQLList):\n        return False\n\n    if is_abstract_type(super_type) and isinstance(\n            maybe_subtype, GraphQLObjectType) and schema.is_possible_type(\n            super_type, maybe_subtype):\n        return True\n\n    return False\n\n\ndef do_types_overlap(schema, t1, t2):\n    # print 'do_types_overlap', t1, t2\n    if t1 == t2:\n        # print '1'\n        return True\n\n    if isinstance(t1, (GraphQLInterfaceType, GraphQLUnionType)):\n        if isinstance(t2, (GraphQLInterfaceType, GraphQLUnionType)):\n            # If both types are abstract, then determine if there is any intersection\n            # between possible concrete types of each.\n            s = any([schema.is_possible_type(t2, type) for type in schema.get_possible_types(t1)])\n            # print '2',s\n            return s\n        # Determine if the latter type is a possible concrete type of the former.\n        r = schema.is_possible_type(t1, t2)\n        # print '3', r\n        return r\n\n    if isinstance(t2, (GraphQLInterfaceType, GraphQLUnionType)):\n        t = schema.is_possible_type(t2, t1)\n        # print '4', t\n        return t\n\n    # print '5'\n    return False\n", 
    "graphql.utils.type_from_ast": "from ..language import ast\nfrom ..type.definition import GraphQLList, GraphQLNonNull\n\n\ndef type_from_ast(schema, input_type_ast):\n    if isinstance(input_type_ast, ast.ListType):\n        inner_type = type_from_ast(schema, input_type_ast.type)\n        if inner_type:\n            return GraphQLList(inner_type)\n        else:\n            return None\n\n    if isinstance(input_type_ast, ast.NonNullType):\n        inner_type = type_from_ast(schema, input_type_ast.type)\n        if inner_type:\n            return GraphQLNonNull(inner_type)\n        else:\n            return None\n\n    assert isinstance(input_type_ast, ast.NamedType), 'Must be a type name.'\n    return schema.get_type(input_type_ast.name.value)\n", 
    "graphql.utils.type_info": "import six\n\nfrom ..language import visitor_meta\nfrom ..type.definition import (GraphQLInputObjectType, GraphQLList,\n                               get_named_type, get_nullable_type,\n                               is_composite_type)\nfrom .get_field_def import get_field_def\nfrom .type_from_ast import type_from_ast\n\n\ndef pop(lst):\n    if lst:\n        lst.pop()\n\n\n# noinspection PyPep8Naming\n@six.add_metaclass(visitor_meta.VisitorMeta)\nclass TypeInfo(object):\n    __slots__ = '_schema', '_type_stack', '_parent_type_stack', '_input_type_stack', '_field_def_stack', '_directive', \\\n                '_argument', '_get_field_def_fn'\n\n    def __init__(self, schema, get_field_def_fn=get_field_def):\n        self._schema = schema\n        self._type_stack = []\n        self._parent_type_stack = []\n        self._input_type_stack = []\n        self._field_def_stack = []\n        self._directive = None\n        self._argument = None\n        self._get_field_def_fn = get_field_def_fn\n\n    def get_type(self):\n        if self._type_stack:\n            return self._type_stack[-1]\n\n    def get_parent_type(self):\n        if self._parent_type_stack:\n            return self._parent_type_stack[-1]\n\n    def get_input_type(self):\n        if self._input_type_stack:\n            return self._input_type_stack[-1]\n\n    def get_field_def(self):\n        if self._field_def_stack:\n            return self._field_def_stack[-1]\n\n    def get_directive(self):\n        return self._directive\n\n    def get_argument(self):\n        return self._argument\n\n    def leave(self, node):\n        method = self._get_leave_handler(type(node))\n        if method:\n            return method(self)\n\n    def enter(self, node):\n        method = self._get_enter_handler(type(node))\n        if method:\n            return method(self, node)\n\n    def enter_SelectionSet(self, node):\n        named_type = get_named_type(self.get_type())\n        composite_type = None\n        if is_composite_type(named_type):\n            composite_type = named_type\n        self._parent_type_stack.append(composite_type)\n\n    def enter_Field(self, node):\n        parent_type = self.get_parent_type()\n        field_def = None\n        if parent_type:\n            field_def = self._get_field_def_fn(self._schema, parent_type, node)\n        self._field_def_stack.append(field_def)\n        self._type_stack.append(field_def and field_def.type)\n\n    def enter_Directive(self, node):\n        self._directive = self._schema.get_directive(node.name.value)\n\n    def enter_OperationDefinition(self, node):\n        definition_type = None\n        if node.operation == 'query':\n            definition_type = self._schema.get_query_type()\n        elif node.operation == 'mutation':\n            definition_type = self._schema.get_mutation_type()\n\n        self._type_stack.append(definition_type)\n\n    def enter_InlineFragment(self, node):\n        type_condition_ast = node.type_condition\n        type = type_from_ast(self._schema, type_condition_ast) if type_condition_ast else self.get_type()\n        self._type_stack.append(type)\n\n    enter_FragmentDefinition = enter_InlineFragment\n\n    def enter_VariableDefinition(self, node):\n        self._input_type_stack.append(type_from_ast(self._schema, node.type))\n\n    def enter_Argument(self, node):\n        arg_def = None\n        arg_type = None\n        field_or_directive = self.get_directive() or self.get_field_def()\n        if field_or_directive:\n            arg_def = field_or_directive.args.get(node.name.value)\n            if arg_def:\n                arg_type = arg_def.type\n        self._argument = arg_def\n        self._input_type_stack.append(arg_type)\n\n    def enter_ListValue(self, node):\n        list_type = get_nullable_type(self.get_input_type())\n        self._input_type_stack.append(\n            list_type.of_type if isinstance(list_type, GraphQLList) else None\n        )\n\n    def enter_ObjectField(self, node):\n        object_type = get_named_type(self.get_input_type())\n        field_type = None\n        if isinstance(object_type, GraphQLInputObjectType):\n            input_field = object_type.fields.get(node.name.value)\n            field_type = input_field.type if input_field else None\n        self._input_type_stack.append(field_type)\n\n    def leave_SelectionSet(self):\n        pop(self._parent_type_stack)\n\n    def leave_Field(self):\n        pop(self._field_def_stack)\n        pop(self._type_stack)\n\n    def leave_Directive(self):\n        self._directive = None\n\n    def leave_OperationDefinition(self):\n        pop(self._type_stack)\n\n    leave_InlineFragment = leave_OperationDefinition\n    leave_FragmentDefinition = leave_OperationDefinition\n\n    def leave_VariableDefinition(self):\n        pop(self._input_type_stack)\n\n    def leave_Argument(self):\n        self._argument = None\n        pop(self._input_type_stack)\n\n    def leave_ListType(self):\n        pop(self._input_type_stack)\n\n    leave_ObjectField = leave_ListType\n", 
    "graphql.utils.value_from_ast": "from ..language import ast\nfrom ..type import (GraphQLEnumType, GraphQLInputObjectType, GraphQLList,\n                    GraphQLNonNull, GraphQLScalarType)\n\n\ndef value_from_ast(value_ast, type, variables=None):\n    \"\"\"Given a type and a value AST node known to match this type, build a\n    runtime value.\"\"\"\n    if isinstance(type, GraphQLNonNull):\n        # Note: we're not checking that the result of coerceValueAST is non-null.\n        # We're assuming that this query has been validated and the value used here is of the correct type.\n        return value_from_ast(value_ast, type.of_type, variables)\n\n    if not value_ast:\n        return None\n\n    if isinstance(value_ast, ast.Variable):\n        variable_name = value_ast.name.value\n        if not variables or variable_name not in variables:\n            return None\n\n        # Note: we're not doing any checking that this variable is correct. We're assuming that this query\n        # has been validated and the variable usage here is of the correct type.\n        return variables[variable_name]\n\n    if isinstance(type, GraphQLList):\n        item_type = type.of_type\n        if isinstance(value_ast, ast.ListValue):\n            return [value_from_ast(item_ast, item_type, variables)\n                    for item_ast in value_ast.values]\n\n        else:\n            return [value_from_ast(value_ast, item_type, variables)]\n\n    if isinstance(type, GraphQLInputObjectType):\n        fields = type.fields\n        if not isinstance(value_ast, ast.ObjectValue):\n            return None\n\n        field_asts = {}\n\n        for field in value_ast.fields:\n            field_asts[field.name.value] = field\n\n        obj = {}\n        for field_name, field in fields.items():\n            field_ast = field_asts.get(field_name)\n            field_value_ast = None\n\n            if field_ast:\n                field_value_ast = field_ast.value\n\n            field_value = value_from_ast(\n                field_value_ast, field.type, variables\n            )\n            if field_value is None:\n                field_value = field.default_value\n\n            if field_value is not None:\n                # We use out_name as the output name for the\n                # dict if exists\n                obj[field.out_name or field_name] = field_value\n\n        return obj\n\n    assert isinstance(type, (GraphQLScalarType, GraphQLEnumType)), \\\n        'Must be input type'\n\n    return type.parse_literal(value_ast)\n", 
    "graphql.validation.__init__": "from .validation import validate\nfrom .rules import specified_rules\n\n__all__ = ['validate', 'specified_rules']\n", 
    "graphql.validation.rules.__init__": "from .arguments_of_correct_type import ArgumentsOfCorrectType\nfrom .default_values_of_correct_type import DefaultValuesOfCorrectType\nfrom .fields_on_correct_type import FieldsOnCorrectType\nfrom .fragments_on_composite_types import FragmentsOnCompositeTypes\nfrom .known_argument_names import KnownArgumentNames\nfrom .known_directives import KnownDirectives\nfrom .known_fragment_names import KnownFragmentNames\nfrom .known_type_names import KnownTypeNames\nfrom .lone_anonymous_operation import LoneAnonymousOperation\nfrom .no_fragment_cycles import NoFragmentCycles\nfrom .no_undefined_variables import NoUndefinedVariables\nfrom .no_unused_fragments import NoUnusedFragments\nfrom .no_unused_variables import NoUnusedVariables\nfrom .overlapping_fields_can_be_merged import OverlappingFieldsCanBeMerged\nfrom .possible_fragment_spreads import PossibleFragmentSpreads\nfrom .provided_non_null_arguments import ProvidedNonNullArguments\nfrom .scalar_leafs import ScalarLeafs\nfrom .unique_argument_names import UniqueArgumentNames\nfrom .unique_fragment_names import UniqueFragmentNames\nfrom .unique_input_field_names import UniqueInputFieldNames\nfrom .unique_operation_names import UniqueOperationNames\nfrom .unique_variable_names import UniqueVariableNames\nfrom .variables_are_input_types import VariablesAreInputTypes\nfrom .variables_in_allowed_position import VariablesInAllowedPosition\n\nspecified_rules = [\n    UniqueOperationNames,\n    LoneAnonymousOperation,\n    KnownTypeNames,\n    FragmentsOnCompositeTypes,\n    VariablesAreInputTypes,\n    ScalarLeafs,\n    FieldsOnCorrectType,\n    UniqueFragmentNames,\n    KnownFragmentNames,\n    NoUnusedFragments,\n    PossibleFragmentSpreads,\n    NoFragmentCycles,\n    NoUndefinedVariables,\n    NoUnusedVariables,\n    KnownDirectives,\n    KnownArgumentNames,\n    UniqueArgumentNames,\n    ArgumentsOfCorrectType,\n    ProvidedNonNullArguments,\n    DefaultValuesOfCorrectType,\n    VariablesInAllowedPosition,\n    OverlappingFieldsCanBeMerged,\n    UniqueInputFieldNames,\n    UniqueVariableNames\n]\n\n__all__ = [\n    'ArgumentsOfCorrectType',\n    'DefaultValuesOfCorrectType',\n    'FieldsOnCorrectType',\n    'FragmentsOnCompositeTypes',\n    'KnownArgumentNames',\n    'KnownDirectives',\n    'KnownFragmentNames',\n    'KnownTypeNames',\n    'LoneAnonymousOperation',\n    'NoFragmentCycles',\n    'UniqueVariableNames',\n    'NoUndefinedVariables',\n    'NoUnusedFragments',\n    'NoUnusedVariables',\n    'OverlappingFieldsCanBeMerged',\n    'PossibleFragmentSpreads',\n    'ProvidedNonNullArguments',\n    'ScalarLeafs',\n    'UniqueArgumentNames',\n    'UniqueFragmentNames',\n    'UniqueInputFieldNames',\n    'UniqueOperationNames',\n    'VariablesAreInputTypes',\n    'VariablesInAllowedPosition',\n    'specified_rules'\n]\n", 
    "graphql.validation.rules.arguments_of_correct_type": "from ...error import GraphQLError\nfrom ...language.printer import print_ast\nfrom ...utils.is_valid_literal_value import is_valid_literal_value\nfrom .base import ValidationRule\n\n\nclass ArgumentsOfCorrectType(ValidationRule):\n\n    def enter_Argument(self, node, key, parent, path, ancestors):\n        arg_def = self.context.get_argument()\n        if arg_def:\n            errors = is_valid_literal_value(arg_def.type, node.value)\n            if errors:\n                self.context.report_error(GraphQLError(\n                    self.bad_value_message(node.name.value, arg_def.type,\n                                           print_ast(node.value), errors),\n                    [node.value]\n                ))\n        return False\n\n    @staticmethod\n    def bad_value_message(arg_name, type, value, verbose_errors):\n        message = (u'\\n' + u'\\n'.join(verbose_errors)) if verbose_errors else ''\n        return 'Argument \"{}\" has invalid value {}.{}'.format(arg_name, value, message)\n", 
    "graphql.validation.rules.base": "from ...language.visitor import Visitor\n\n\nclass ValidationRule(Visitor):\n    __slots__ = 'context',\n\n    def __init__(self, context):\n        self.context = context\n", 
    "graphql.validation.rules.default_values_of_correct_type": "from ...error import GraphQLError\nfrom ...language.printer import print_ast\nfrom ...type.definition import GraphQLNonNull\nfrom ...utils.is_valid_literal_value import is_valid_literal_value\nfrom .base import ValidationRule\n\n\nclass DefaultValuesOfCorrectType(ValidationRule):\n\n    def enter_VariableDefinition(self, node, key, parent, path, ancestors):\n        name = node.variable.name.value\n        default_value = node.default_value\n        type = self.context.get_input_type()\n\n        if isinstance(type, GraphQLNonNull) and default_value:\n            self.context.report_error(GraphQLError(\n                self.default_for_non_null_arg_message(name, type, type.of_type),\n                [default_value]\n            ))\n\n        if type and default_value:\n            errors = is_valid_literal_value(type, default_value)\n            if errors:\n                self.context.report_error(GraphQLError(\n                    self.bad_value_for_default_arg_message(name, type, print_ast(default_value), errors),\n                    [default_value]\n                ))\n        return False\n\n    def enter_SelectionSet(self, node, key, parent, path, ancestors):\n        return False\n\n    def enter_FragmentDefinition(self, node, key, parent, path, ancestors):\n        return False\n\n    @staticmethod\n    def default_for_non_null_arg_message(var_name, type, guess_type):\n        return u'Variable \"${}\" of type \"{}\" is required and will not use the default value. ' \\\n               u'Perhaps you meant to use type \"{}\".'.format(var_name, type, guess_type)\n\n    @staticmethod\n    def bad_value_for_default_arg_message(var_name, type, value, verbose_errors):\n        message = (u'\\n' + u'\\n'.join(verbose_errors)) if verbose_errors else u''\n        return u'Variable \"${}\" of type \"{}\" has invalid default value: {}.{}'.format(var_name, type, value, message)\n", 
    "graphql.validation.rules.fields_on_correct_type": "from collections import Counter\n\nfrom ...error import GraphQLError\nfrom ...pyutils.ordereddict import OrderedDict\nfrom ...type.definition import (GraphQLInterfaceType, GraphQLObjectType,\n                                GraphQLUnionType)\nfrom ...utils.quoted_or_list import quoted_or_list\nfrom ...utils.suggestion_list import suggestion_list\nfrom .base import ValidationRule\n\ntry:\n    # Python 2\n    from itertools import izip\nexcept ImportError:\n    # Python 3\n    izip = zip\n\n\ndef _undefined_field_message(field_name, type, suggested_types,\n                             suggested_fields):\n    message = 'Cannot query field \"{}\" on type \"{}\".'.format(field_name, type)\n\n    if suggested_types:\n        suggestions = quoted_or_list(suggested_types)\n        message += \" Did you mean to use an inline fragment on {}?\".format(suggestions)\n    elif suggested_fields:\n        suggestions = quoted_or_list(suggested_fields)\n        message += \" Did you mean {}?\".format(suggestions)\n\n    return message\n\n\nclass OrderedCounter(Counter, OrderedDict):\n    pass\n\n\nclass FieldsOnCorrectType(ValidationRule):\n    '''Fields on correct type\n\n      A GraphQL document is only valid if all fields selected are defined by the\n      parent type, or are an allowed meta field such as __typenamme\n    '''\n\n    def enter_Field(self, node, key, parent, path, ancestors):\n        parent_type = self.context.get_parent_type()\n        if not parent_type:\n            return\n\n        field_def = self.context.get_field_def()\n        if not field_def:\n            #  This field doesn't exist, lets look for suggestions.\n            schema = self.context.get_schema()\n            field_name = node.name.value\n\n            # First determine if there are any suggested types to condition on.\n            suggested_type_names = get_suggested_type_names(schema, parent_type, field_name)\n            # if there are no suggested types perhaps it was a typo?\n            suggested_field_names = [] if suggested_type_names else get_suggested_field_names(schema, parent_type, field_name)\n\n            # report an error including helpful suggestions.\n            self.context.report_error(GraphQLError(\n                _undefined_field_message(field_name, parent_type.name, suggested_type_names, suggested_field_names),\n                [node]\n            ))\n\n\ndef get_suggested_type_names(schema, output_type, field_name):\n    '''Go through all of the implementations of type, as well as the interfaces\n      that they implement. If any of those types include the provided field,\n      suggest them, sorted by how often the type is referenced,  starting\n      with Interfaces.'''\n\n    if isinstance(output_type, (GraphQLInterfaceType, GraphQLUnionType)):\n        suggested_object_types = []\n        interface_usage_count = OrderedDict()\n        for possible_type in schema.get_possible_types(output_type):\n            if not possible_type.fields.get(field_name):\n                return\n\n            # This object type defines this field.\n            suggested_object_types.append(possible_type.name)\n\n            for possible_interface in possible_type.interfaces:\n                if not possible_interface.fields.get(field_name):\n                    continue\n\n                # This interface type defines this field.\n                interface_usage_count[possible_interface.name] = (\n                    interface_usage_count.get(possible_interface.name, 0) + 1)\n\n        # Suggest interface types based on how common they are.\n        suggested_interface_types = sorted(list(interface_usage_count.keys()), key=lambda k: interface_usage_count[k],\n                                           reverse=True)\n\n        # Suggest both interface and object types.\n        suggested_interface_types.extend(suggested_object_types)\n        return suggested_interface_types\n\n    # Otherwise, must be an Object type, which does not have possible fields.\n    return []\n\n\ndef get_suggested_field_names(schema, graphql_type, field_name):\n    '''For the field name provided, determine if there are any similar field names\n    that may be the result of a typo.'''\n\n    if isinstance(graphql_type, (GraphQLInterfaceType, GraphQLObjectType)):\n        possible_field_names = list(graphql_type.fields.keys())\n\n        return suggestion_list(field_name, possible_field_names)\n\n    # Otherwise, must be a Union type, which does not define fields.\n    return []\n", 
    "graphql.validation.rules.fragments_on_composite_types": "from ...error import GraphQLError\nfrom ...language.printer import print_ast\nfrom ...type.definition import is_composite_type\nfrom .base import ValidationRule\n\n\nclass FragmentsOnCompositeTypes(ValidationRule):\n\n    def enter_InlineFragment(self, node, key, parent, path, ancestors):\n        type = self.context.get_type()\n\n        if node.type_condition and type and not is_composite_type(type):\n            self.context.report_error(GraphQLError(\n                self.inline_fragment_on_non_composite_error_message(print_ast(node.type_condition)),\n                [node.type_condition]\n            ))\n\n    def enter_FragmentDefinition(self, node, key, parent, path, ancestors):\n        type = self.context.get_type()\n\n        if type and not is_composite_type(type):\n            self.context.report_error(GraphQLError(\n                self.fragment_on_non_composite_error_message(node.name.value, print_ast(node.type_condition)),\n                [node.type_condition]\n            ))\n\n    @staticmethod\n    def inline_fragment_on_non_composite_error_message(type):\n        return 'Fragment cannot condition on non composite type \"{}\".'.format(type)\n\n    @staticmethod\n    def fragment_on_non_composite_error_message(frag_name, type):\n        return 'Fragment \"{}\" cannot condition on non composite type \"{}\".'.format(frag_name, type)\n", 
    "graphql.validation.rules.known_argument_names": "from ...error import GraphQLError\nfrom ...language import ast\nfrom ...utils.quoted_or_list import quoted_or_list\nfrom ...utils.suggestion_list import suggestion_list\nfrom .base import ValidationRule\n\n\ndef _unknown_arg_message(arg_name, field_name, type, suggested_args):\n    message = 'Unknown argument \"{}\" on field \"{}\" of type \"{}\".'.format(arg_name, field_name, type)\n    if suggested_args:\n        message += ' Did you mean {}?'.format(quoted_or_list(suggested_args))\n\n    return message\n\n\ndef _unknown_directive_arg_message(arg_name, directive_name, suggested_args):\n    message = 'Unknown argument \"{}\" on directive \"@{}\".'.format(arg_name, directive_name)\n    if suggested_args:\n        message += ' Did you mean {}?'.format(quoted_or_list(suggested_args))\n\n    return message\n\n\nclass KnownArgumentNames(ValidationRule):\n\n    def enter_Argument(self, node, key, parent, path, ancestors):\n        argument_of = ancestors[-1]\n\n        if isinstance(argument_of, ast.Field):\n            field_def = self.context.get_field_def()\n            if not field_def:\n                return\n\n            field_arg_def = field_def.args.get(node.name.value)\n\n            print(field_def.args.items())\n\n            if not field_arg_def:\n                parent_type = self.context.get_parent_type()\n                assert parent_type\n                self.context.report_error(GraphQLError(\n                    _unknown_arg_message(\n                        node.name.value,\n                        argument_of.name.value,\n                        parent_type.name,\n                        suggestion_list(\n                            node.name.value,\n                            (arg_name for arg_name in field_def.args.keys())\n                        )\n                    ),\n                    [node]\n                ))\n\n        elif isinstance(argument_of, ast.Directive):\n            directive = self.context.get_directive()\n            if not directive:\n                return\n\n            directive_arg_def = directive.args.get(node.name.value)\n\n            if not directive_arg_def:\n                self.context.report_error(GraphQLError(\n                    _unknown_directive_arg_message(\n                        node.name.value,\n                        directive.name,\n                        suggestion_list(\n                            node.name.value,\n                            (arg_name for arg_name in directive.args.keys())\n                        )\n                    ),\n                    [node]\n                ))\n", 
    "graphql.validation.rules.known_directives": "from ...error import GraphQLError\nfrom ...language import ast\nfrom ...type.directives import DirectiveLocation\nfrom .base import ValidationRule\n\n\nclass KnownDirectives(ValidationRule):\n\n    def enter_Directive(self, node, key, parent, path, ancestors):\n        directive_def = next((\n            definition for definition in self.context.get_schema().get_directives()\n            if definition.name == node.name.value\n        ), None)\n\n        if not directive_def:\n            return self.context.report_error(GraphQLError(\n                self.unknown_directive_message(node.name.value),\n                [node]\n            ))\n\n        candidate_location = get_directive_location_for_ast_path(ancestors)\n        if not candidate_location:\n            self.context.report_error(GraphQLError(\n                self.misplaced_directive_message(node.name.value, node.type),\n                [node]\n            ))\n        elif candidate_location not in directive_def.locations:\n            self.context.report_error(GraphQLError(\n                self.misplaced_directive_message(node.name.value, candidate_location),\n                [node]\n            ))\n\n    @staticmethod\n    def unknown_directive_message(directive_name):\n        return 'Unknown directive \"{}\".'.format(directive_name)\n\n    @staticmethod\n    def misplaced_directive_message(directive_name, location):\n        return 'Directive \"{}\" may not be used on \"{}\".'.format(directive_name, location)\n\n\n_operation_definition_map = {\n    'query': DirectiveLocation.QUERY,\n    'mutation': DirectiveLocation.MUTATION,\n    'subscription': DirectiveLocation.SUBSCRIPTION,\n}\n\n\ndef get_directive_location_for_ast_path(ancestors):\n    applied_to = ancestors[-1]\n    if isinstance(applied_to, ast.OperationDefinition):\n        return _operation_definition_map.get(applied_to.operation)\n\n    elif isinstance(applied_to, ast.Field):\n        return DirectiveLocation.FIELD\n\n    elif isinstance(applied_to, ast.FragmentSpread):\n        return DirectiveLocation.FRAGMENT_SPREAD\n\n    elif isinstance(applied_to, ast.InlineFragment):\n        return DirectiveLocation.INLINE_FRAGMENT\n\n    elif isinstance(applied_to, ast.FragmentDefinition):\n        return DirectiveLocation.FRAGMENT_DEFINITION\n\n    elif isinstance(applied_to, ast.SchemaDefinition):\n        return DirectiveLocation.SCHEMA\n\n    elif isinstance(applied_to, ast.ScalarTypeDefinition):\n        return DirectiveLocation.SCALAR\n\n    elif isinstance(applied_to, ast.ObjectTypeDefinition):\n        return DirectiveLocation.OBJECT\n\n    elif isinstance(applied_to, ast.FieldDefinition):\n        return DirectiveLocation.FIELD_DEFINITION\n\n    elif isinstance(applied_to, ast.InterfaceTypeDefinition):\n        return DirectiveLocation.INTERFACE\n\n    elif isinstance(applied_to, ast.UnionTypeDefinition):\n        return DirectiveLocation.UNION\n\n    elif isinstance(applied_to, ast.EnumTypeDefinition):\n        return DirectiveLocation.ENUM\n\n    elif isinstance(applied_to, ast.EnumValueDefinition):\n        return DirectiveLocation.ENUM_VALUE\n\n    elif isinstance(applied_to, ast.InputObjectTypeDefinition):\n        return DirectiveLocation.INPUT_OBJECT\n\n    elif isinstance(applied_to, ast.InputValueDefinition):\n        parent_node = ancestors[-3]\n        return (DirectiveLocation.INPUT_FIELD_DEFINITION\n                if isinstance(parent_node, ast.InputObjectTypeDefinition)\n                else DirectiveLocation.ARGUMENT_DEFINITION)\n", 
    "graphql.validation.rules.known_fragment_names": "from ...error import GraphQLError\nfrom .base import ValidationRule\n\n\nclass KnownFragmentNames(ValidationRule):\n\n    def enter_FragmentSpread(self, node, key, parent, path, ancestors):\n        fragment_name = node.name.value\n        fragment = self.context.get_fragment(fragment_name)\n\n        if not fragment:\n            self.context.report_error(GraphQLError(\n                self.unknown_fragment_message(fragment_name),\n                [node.name]\n            ))\n\n    @staticmethod\n    def unknown_fragment_message(fragment_name):\n        return 'Unknown fragment \"{}\".'.format(fragment_name)\n", 
    "graphql.validation.rules.known_type_names": "from ...error import GraphQLError\nfrom ...utils.quoted_or_list import quoted_or_list\nfrom ...utils.suggestion_list import suggestion_list\nfrom .base import ValidationRule\n\n\ndef _unknown_type_message(type, suggested_types):\n    message = 'Unknown type \"{}\".'.format(type)\n    if suggested_types:\n        message += ' Perhaps you meant {}?'.format(quoted_or_list(suggested_types))\n\n    return message\n\n\nclass KnownTypeNames(ValidationRule):\n\n    def enter_ObjectTypeDefinition(self, node, *args):\n        return False\n\n    def enter_InterfaceTypeDefinition(self, node, *args):\n        return False\n\n    def enter_UnionTypeDefinition(self, node, *args):\n        return False\n\n    def enter_InputObjectTypeDefinition(self, node, *args):\n        return False\n\n    def enter_NamedType(self, node, *args):\n        schema = self.context.get_schema()\n        type_name = node.name.value\n        type = schema.get_type(type_name)\n\n        if not type:\n            self.context.report_error(\n                GraphQLError(\n                    _unknown_type_message(\n                        type_name,\n                        suggestion_list(type_name, list(schema.get_type_map().keys()))\n                    ),\n                    [node]\n                )\n            )\n", 
    "graphql.validation.rules.lone_anonymous_operation": "from ...error import GraphQLError\nfrom ...language import ast\nfrom .base import ValidationRule\n\n\nclass LoneAnonymousOperation(ValidationRule):\n    __slots__ = 'operation_count',\n\n    def __init__(self, context):\n        self.operation_count = 0\n        super(LoneAnonymousOperation, self).__init__(context)\n\n    def enter_Document(self, node, key, parent, path, ancestors):\n        self.operation_count = \\\n            sum(1 for definition in node.definitions if isinstance(definition, ast.OperationDefinition))\n\n    def enter_OperationDefinition(self, node, key, parent, path, ancestors):\n        if not node.name and self.operation_count > 1:\n            self.context.report_error(GraphQLError(self.anonymous_operation_not_alone_message(), [node]))\n\n    @staticmethod\n    def anonymous_operation_not_alone_message():\n        return 'This anonymous operation must be the only defined operation.'\n", 
    "graphql.validation.rules.no_fragment_cycles": "from ...error import GraphQLError\nfrom .base import ValidationRule\n\n\nclass NoFragmentCycles(ValidationRule):\n    __slots__ = 'errors', 'visited_frags', 'spread_path', 'spread_path_index_by_name'\n\n    def __init__(self, context):\n        super(NoFragmentCycles, self).__init__(context)\n        self.errors = []\n        self.visited_frags = set()\n        self.spread_path = []\n        self.spread_path_index_by_name = {}\n\n    def enter_OperationDefinition(self, node, key, parent, path, ancestors):\n        return False\n\n    def enter_FragmentDefinition(self, node, key, parent, path, ancestors):\n        if node.name.value not in self.visited_frags:\n            self.detect_cycle_recursive(node)\n        return False\n\n    def detect_cycle_recursive(self, fragment):\n        fragment_name = fragment.name.value\n        self.visited_frags.add(fragment_name)\n\n        spread_nodes = self.context.get_fragment_spreads(fragment.selection_set)\n        if not spread_nodes:\n            return\n\n        self.spread_path_index_by_name[fragment_name] = len(self.spread_path)\n\n        for spread_node in spread_nodes:\n            spread_name = spread_node.name.value\n            cycle_index = self.spread_path_index_by_name.get(spread_name)\n\n            if cycle_index is None:\n                self.spread_path.append(spread_node)\n                if spread_name not in self.visited_frags:\n                    spread_fragment = self.context.get_fragment(spread_name)\n                    if spread_fragment:\n                        self.detect_cycle_recursive(spread_fragment)\n                self.spread_path.pop()\n            else:\n                cycle_path = self.spread_path[cycle_index:]\n                self.context.report_error(GraphQLError(\n                    self.cycle_error_message(\n                        spread_name,\n                        [s.name.value for s in cycle_path]\n                    ),\n                    cycle_path + [spread_node]\n                ))\n\n        self.spread_path_index_by_name[fragment_name] = None\n\n    @staticmethod\n    def cycle_error_message(fragment_name, spread_names):\n        via = ' via {}'.format(', '.join(spread_names)) if spread_names else ''\n        return 'Cannot spread fragment \"{}\" within itself{}.'.format(fragment_name, via)\n", 
    "graphql.validation.rules.no_undefined_variables": "from ...error import GraphQLError\nfrom .base import ValidationRule\n\n\nclass NoUndefinedVariables(ValidationRule):\n    __slots__ = 'defined_variable_names',\n\n    def __init__(self, context):\n        self.defined_variable_names = set()\n        super(NoUndefinedVariables, self).__init__(context)\n\n    @staticmethod\n    def undefined_var_message(var_name, op_name=None):\n        if op_name:\n            return 'Variable \"${}\" is not defined by operation \"{}\".'.format(\n                var_name, op_name\n            )\n        return 'Variable \"${}\" is not defined.'.format(var_name)\n\n    def enter_OperationDefinition(self, operation, key, parent, path, ancestors):\n        self.defined_variable_names = set()\n\n    def leave_OperationDefinition(self, operation, key, parent, path, ancestors):\n        usages = self.context.get_recursive_variable_usages(operation)\n\n        for variable_usage in usages:\n            node = variable_usage.node\n            var_name = node.name.value\n            if var_name not in self.defined_variable_names:\n                self.context.report_error(GraphQLError(\n                    self.undefined_var_message(var_name, operation.name and operation.name.value),\n                    [node, operation]\n                ))\n\n    def enter_VariableDefinition(self, node, key, parent, path, ancestors):\n        self.defined_variable_names.add(node.variable.name.value)\n", 
    "graphql.validation.rules.no_unused_fragments": "from ...error import GraphQLError\nfrom .base import ValidationRule\n\n\nclass NoUnusedFragments(ValidationRule):\n    __slots__ = 'fragment_definitions', 'operation_definitions', 'fragment_adjacencies', 'spread_names'\n\n    def __init__(self, context):\n        super(NoUnusedFragments, self).__init__(context)\n        self.operation_definitions = []\n        self.fragment_definitions = []\n\n    def enter_OperationDefinition(self, node, key, parent, path, ancestors):\n        self.operation_definitions.append(node)\n        return False\n\n    def enter_FragmentDefinition(self, node, key, parent, path, ancestors):\n        self.fragment_definitions.append(node)\n        return False\n\n    def leave_Document(self, node, key, parent, path, ancestors):\n        fragment_names_used = set()\n\n        for operation in self.operation_definitions:\n            fragments = self.context.get_recursively_referenced_fragments(operation)\n            for fragment in fragments:\n                fragment_names_used.add(fragment.name.value)\n\n        for fragment_definition in self.fragment_definitions:\n            if fragment_definition.name.value not in fragment_names_used:\n                self.context.report_error(GraphQLError(\n                    self.unused_fragment_message(fragment_definition.name.value),\n                    [fragment_definition]\n                ))\n\n    @staticmethod\n    def unused_fragment_message(fragment_name):\n        return 'Fragment \"{}\" is never used.'.format(fragment_name)\n", 
    "graphql.validation.rules.no_unused_variables": "from ...error import GraphQLError\nfrom .base import ValidationRule\n\n\nclass NoUnusedVariables(ValidationRule):\n    __slots__ = 'variable_definitions'\n\n    def __init__(self, context):\n        self.variable_definitions = []\n        super(NoUnusedVariables, self).__init__(context)\n\n    def enter_OperationDefinition(self, node, key, parent, path, ancestors):\n        self.variable_definitions = []\n\n    def leave_OperationDefinition(self, operation, key, parent, path, ancestors):\n        variable_name_used = set()\n        usages = self.context.get_recursive_variable_usages(operation)\n        op_name = operation.name and operation.name.value or None\n\n        for variable_usage in usages:\n            variable_name_used.add(variable_usage.node.name.value)\n\n        for variable_definition in self.variable_definitions:\n            if variable_definition.variable.name.value not in variable_name_used:\n                self.context.report_error(GraphQLError(\n                    self.unused_variable_message(variable_definition.variable.name.value, op_name),\n                    [variable_definition]\n                ))\n\n    def enter_VariableDefinition(self, node, key, parent, path, ancestors):\n        self.variable_definitions.append(node)\n\n    @staticmethod\n    def unused_variable_message(variable_name, op_name):\n        if op_name:\n            return 'Variable \"${}\" is never used in operation \"{}\".'.format(variable_name, op_name)\n        return 'Variable \"${}\" is never used.'.format(variable_name)\n", 
    "graphql.validation.rules.overlapping_fields_can_be_merged": "import itertools\nfrom collections import OrderedDict\n\nfrom ...error import GraphQLError\nfrom ...language import ast\nfrom ...language.printer import print_ast\nfrom ...pyutils.pair_set import PairSet\nfrom ...type.definition import (GraphQLInterfaceType, GraphQLList,\n                                GraphQLNonNull, GraphQLObjectType,\n                                get_named_type, is_leaf_type)\nfrom ...utils.type_comparators import is_equal_type\nfrom ...utils.type_from_ast import type_from_ast\nfrom .base import ValidationRule\n\n\nclass OverlappingFieldsCanBeMerged(ValidationRule):\n    __slots__ = ('_compared_fragments', '_cached_fields_and_fragment_names', )\n\n    def __init__(self, context):\n        super(OverlappingFieldsCanBeMerged, self).__init__(context)\n        # A memoization for when two fragments are compared \"between\" each other for\n        # conflicts. Two fragments may be compared many times, so memoizing this can\n        # dramatically improve the performance of this validator.\n        self._compared_fragments = PairSet()\n\n        # A cache for the \"field map\" and list of fragment names found in any given\n        # selection set. Selection sets may be asked for this information multiple\n        # times, so this improves the performance of this validator.\n        self._cached_fields_and_fragment_names = {}\n\n    def leave_SelectionSet(self, node, key, parent, path, ancestors):\n        # Note: we validate on the reverse traversal so deeper conflicts will be\n        # caught first, for correct calculation of mutual exclusivity and for\n        # clearer error messages.\n        # field_map = _collect_field_asts_and_defs(\n        #     self.context,\n        #     self.context.get_parent_type(),\n        #     node\n        # )\n\n        # conflicts = _find_conflicts(self.context, False, field_map, self.compared_set)\n        conflicts = _find_conflicts_within_selection_set(self.context, self._cached_fields_and_fragment_names,\n                                                         self._compared_fragments, self.context.get_parent_type(),\n                                                         node)\n\n        for (reason_name, reason), fields1, fields2 in conflicts:\n            self.context.report_error(GraphQLError(\n                self.fields_conflict_message(reason_name, reason),\n                list(fields1) + list(fields2)\n            ))\n\n    @staticmethod\n    def same_type(type1, type2):\n        return is_equal_type(type1, type2)\n        # return type1.is_same_type(type2)\n\n    @classmethod\n    def fields_conflict_message(cls, reason_name, reason):\n        return (\n            'Fields \"{}\" conflict because {}. '\n            'Use different aliases on the fields to fetch both if this was '\n            'intentional.'\n        ).format(reason_name, cls.reason_message(reason))\n\n    @classmethod\n    def reason_message(cls, reason):\n        if isinstance(reason, list):\n            return ' and '.join('subfields \"{}\" conflict because {}'.format(reason_name, cls.reason_message(sub_reason))\n                                for reason_name, sub_reason in reason)\n\n        return reason\n\n\n# Algorithm:\n#\n#  Conflicts occur when two fields exist in a query which will produce the same\n#  response name, but represent differing values, thus creating a conflict.\n#  The algorithm below finds all conflicts via making a series of comparisons\n#  between fields. In order to compare as few fields as possible, this makes\n#  a series of comparisons \"within\" sets of fields and \"between\" sets of fields.\n#\n#  Given any selection set, a collection produces both a set of fields by\n#  also including all inline fragments, as well as a list of fragments\n#  referenced by fragment spreads.\n#\n#  A) Each selection set represented in the document first compares \"within\" its\n#  collected set of fields, finding any conflicts between every pair of\n#  overlapping fields.\n#  Note: This is the only time that a the fields \"within\" a set are compared\n#  to each other. After this only fields \"between\" sets are compared.\n#\n#  B) Also, if any fragment is referenced in a selection set, then a\n#  comparison is made \"between\" the original set of fields and the\n#  referenced fragment.\n#\n#  C) Also, if multiple fragments are referenced, then comparisons\n#  are made \"between\" each referenced fragment.\n#\n#  D) When comparing \"between\" a set of fields and a referenced fragment, first\n#  a comparison is made between each field in the original set of fields and\n#  each field in the the referenced set of fields.\n#\n#  E) Also, if any fragment is referenced in the referenced selection set,\n#  then a comparison is made \"between\" the original set of fields and the\n#  referenced fragment (recursively referring to step D).\n#\n#  F) When comparing \"between\" two fragments, first a comparison is made between\n#  each field in the first referenced set of fields and each field in the the\n#  second referenced set of fields.\n#\n#  G) Also, any fragments referenced by the first must be compared to the\n#  second, and any fragments referenced by the second must be compared to the\n#  first (recursively referring to step F).\n#\n#  H) When comparing two fields, if both have selection sets, then a comparison\n#  is made \"between\" both selection sets, first comparing the set of fields in\n#  the first selection set with the set of fields in the second.\n#\n#  I) Also, if any fragment is referenced in either selection set, then a\n#  comparison is made \"between\" the other set of fields and the\n#  referenced fragment.\n#\n#  J) Also, if two fragments are referenced in both selection sets, then a\n#  comparison is made \"between\" the two fragments.\n\ndef _find_conflicts_within_selection_set(context, cached_fields_and_fragment_names, compared_fragments, parent_type,\n                                         selection_set):\n    \"\"\"Find all conflicts found \"within\" a selection set, including those found via spreading in fragments.\n\n       Called when visiting each SelectionSet in the GraphQL Document.\n    \"\"\"\n    conflicts = []\n\n    field_map, fragment_names = _get_fields_and_fragments_names(context, cached_fields_and_fragment_names, parent_type,\n                                                                selection_set)\n\n    # (A) Find all conflicts \"within\" the fields of this selection set.\n    # Note: this is the *only place* `collect_conflicts_within` is called.\n    _collect_conflicts_within(\n        context,\n        conflicts,\n        cached_fields_and_fragment_names,\n        compared_fragments,\n        field_map\n    )\n\n    # (B) Then collect conflicts between these fields and those represented by\n    # each spread fragment name found.\n    for i, fragment_name in enumerate(fragment_names):\n        _collect_conflicts_between_fields_and_fragment(\n            context,\n            conflicts,\n            cached_fields_and_fragment_names,\n            compared_fragments,\n            False,\n            field_map,\n            fragment_name,\n        )\n\n        # (C) Then compare this fragment with all other fragments found in this\n        # selection set to collect conflicts within fragments spread together.\n        # This compares each item in the list of fragment names to every other item\n        # in that same list (except for itself).\n        for other_fragment_name in fragment_names[i+1:]:\n            _collect_conflicts_between_fragments(\n                context,\n                conflicts,\n                cached_fields_and_fragment_names,\n                compared_fragments,\n                False,\n                fragment_name,\n                other_fragment_name,\n            )\n\n    return conflicts\n\n\ndef _collect_conflicts_between_fields_and_fragment(context, conflicts, cached_fields_and_fragment_names,\n                                                   compared_fragments, are_mutually_exclusive, field_map,\n                                                   fragment_name):\n\n    fragment = context.get_fragment(fragment_name)\n\n    if not fragment:\n        return None\n\n    field_map2, fragment_names2 = _get_referenced_fields_and_fragment_names(context, cached_fields_and_fragment_names,\n                                                                            fragment)\n\n    # (D) First collect any conflicts between the provided collection of fields\n    # and the collection of fields represented by the given fragment.\n    _collect_conflicts_between(context, conflicts, cached_fields_and_fragment_names, compared_fragments,\n                               are_mutually_exclusive, field_map, field_map2)\n\n    # (E) Then collect any conflicts between the provided collection of fields\n    # and any fragment names found in the given fragment.\n    for fragment_name2 in fragment_names2:\n        _collect_conflicts_between_fields_and_fragment(context, conflicts, cached_fields_and_fragment_names,\n                                                       compared_fragments, are_mutually_exclusive, field_map,\n                                                       fragment_name2)\n\n\n# Collect all conflicts found between two fragments, including via spreading in\n# any nested fragments\ndef _collect_conflicts_between_fragments(context, conflicts, cached_fields_and_fragment_names, compared_fragments,\n                                         are_mutually_exclusive, fragment_name1, fragment_name2):\n\n    fragment1 = context.get_fragment(fragment_name1)\n    fragment2 = context.get_fragment(fragment_name2)\n\n    if not fragment1 or not fragment2:\n        return None\n\n    # No need to compare a fragment to itself.\n    if fragment1 == fragment2:\n        return None\n\n    # Memoize so two fragments are not compared for conflicts more than once.\n    if compared_fragments.has(fragment_name1, fragment_name2, are_mutually_exclusive):\n        return None\n\n    compared_fragments.add(fragment_name1, fragment_name2, are_mutually_exclusive)\n\n    field_map1, fragment_names1 = _get_referenced_fields_and_fragment_names(context, cached_fields_and_fragment_names,\n                                                                            fragment1)\n\n    field_map2, fragment_names2 = _get_referenced_fields_and_fragment_names(context, cached_fields_and_fragment_names,\n                                                                            fragment2)\n\n    # (F) First, collect all conflicts between these two collections of fields\n    # (not including any nested fragments)\n    _collect_conflicts_between(context, conflicts, cached_fields_and_fragment_names, compared_fragments,\n                               are_mutually_exclusive, field_map1, field_map2)\n\n    # (G) Then collect conflicts between the first fragment and any nested\n    # fragments spread in the second fragment.\n    for _fragment_name2 in fragment_names2:\n        _collect_conflicts_between_fragments(context, conflicts, cached_fields_and_fragment_names, compared_fragments,\n                                             are_mutually_exclusive, fragment_name1, _fragment_name2)\n\n    # (G) Then collect conflicts between the second fragment and any nested\n    # fragments spread in the first fragment.\n    for _fragment_name1 in fragment_names1:\n        _collect_conflicts_between_fragments(context, conflicts, cached_fields_and_fragment_names, compared_fragments,\n                                             are_mutually_exclusive, _fragment_name1, fragment_name2)\n\n\ndef _find_conflicts_between_sub_selection_sets(context, cached_fields_and_fragment_names, compared_fragments,\n                                               are_mutually_exclusive, parent_type1, selection_set1,\n                                               parent_type2, selection_set2):\n    \"\"\"Find all conflicts found between two selection sets.\n\n       Includes those found via spreading in fragments. Called when determining if conflicts exist\n       between the sub-fields of two overlapping fields.\n    \"\"\"\n\n    conflicts = []\n\n    field_map1, fragment_names1 = _get_fields_and_fragments_names(context, cached_fields_and_fragment_names,\n                                                                  parent_type1, selection_set1)\n\n    field_map2, fragment_names2 = _get_fields_and_fragments_names(context, cached_fields_and_fragment_names,\n                                                                  parent_type2, selection_set2)\n\n    # (H) First, collect all conflicts between these two collections of field.\n    _collect_conflicts_between(context, conflicts, cached_fields_and_fragment_names, compared_fragments,\n                               are_mutually_exclusive, field_map1, field_map2)\n\n    # (I) Then collect conflicts between the first collection of fields and\n    # those referenced by each fragment name associated with the second.\n    for fragment_name2 in fragment_names2:\n        _collect_conflicts_between_fields_and_fragment(context, conflicts, cached_fields_and_fragment_names,\n                                                       compared_fragments, are_mutually_exclusive, field_map1,\n                                                       fragment_name2)\n\n    # (I) Then collect conflicts between the second collection of fields and\n    #  those referenced by each fragment name associated with the first.\n    for fragment_name1 in fragment_names1:\n        _collect_conflicts_between_fields_and_fragment(context, conflicts, cached_fields_and_fragment_names,\n                                                       compared_fragments, are_mutually_exclusive, field_map2,\n                                                       fragment_name1)\n\n    # (J) Also collect conflicts between any fragment names by the first and\n    # fragment names by the second. This compares each item in the first set of\n    # names to each item in the second set of names.\n    for fragment_name1 in fragment_names1:\n        for fragment_name2 in fragment_names2:\n            _collect_conflicts_between_fragments(context, conflicts, cached_fields_and_fragment_names,\n                                                 compared_fragments, are_mutually_exclusive,\n                                                 fragment_name1, fragment_name2)\n\n    return conflicts\n\n\ndef _collect_conflicts_within(context, conflicts, cached_fields_and_fragment_names, compared_fragments, field_map):\n    \"\"\"Collect all Conflicts \"within\" one collection of fields.\"\"\"\n\n    # field map is a keyed collection, where each key represents a response\n    # name and the value at that key is a list of all fields which provide that\n    # response name. For every response name, if there are multiple fields, they\n    # must be compared to find a potential conflict.\n    for response_name, fields in list(field_map.items()):\n        # This compares every field in the list to every other field in this list\n        # (except to itself). If the list only has one item, nothing needs to\n        # be compared.\n        for i, field in enumerate(fields):\n            for other_field in fields[i+1:]:\n                # within one collection is never mutually exclusive\n                conflict = _find_conflict(context, cached_fields_and_fragment_names, compared_fragments, False,\n                                          response_name, field, other_field)\n                if conflict:\n                    conflicts.append(conflict)\n\n\ndef _collect_conflicts_between(context, conflicts, cached_fields_and_fragment_names, compared_fragments,\n                               parent_fields_are_mutually_exclusive, field_map1, field_map2):\n    \"\"\"Collect all Conflicts between two collections of fields.\n\n       This is similar to, but different from the `collect_conflicts_within` function above. This check assumes that\n       `collect_conflicts_within` has already been called on each provided collection of fields.\n       This is true because this validator traverses each individual selection set.\n    \"\"\"\n    # A field map is a keyed collection, where each key represents a response\n    # name and the value at that key is a list of all fields which provide that\n    # response name. For any response name which appears in both provided field\n    # maps, each field from the first field map must be compared to every field\n    # in the second field map to find potential conflicts.\n    for response_name, fields1 in list(field_map1.items()):\n        fields2 = field_map2.get(response_name)\n\n        if fields2:\n            for field1 in fields1:\n                for field2 in fields2:\n                    conflict = _find_conflict(context, cached_fields_and_fragment_names, compared_fragments,\n                                              parent_fields_are_mutually_exclusive, response_name, field1, field2)\n\n                    if conflict:\n                        conflicts.append(conflict)\n\n\ndef _find_conflict(context, cached_fields_and_fragment_names, compared_fragments, parent_fields_are_mutually_exclusive,\n                   response_name, field1, field2):\n    \"\"\"Determines if there is a conflict between two particular fields.\"\"\"\n    parent_type1, ast1, def1 = field1\n    parent_type2, ast2, def2 = field2\n\n    # If it is known that two fields could not possibly apply at the same\n    # time, due to the parent types, then it is safe to permit them to diverge\n    # in aliased field or arguments used as they will not present any ambiguity\n    # by differing.\n    # It is known that two parent types could never overlap if they are\n    # different Object types. Interface or Union types might overlap - if not\n    # in the current state of the schema, then perhaps in some future version,\n    # thus may not safely diverge.\n\n    are_mutually_exclusive = (\n        parent_fields_are_mutually_exclusive or (\n            parent_type1 != parent_type2 and\n            isinstance(parent_type1, GraphQLObjectType) and\n            isinstance(parent_type2, GraphQLObjectType)\n        )\n    )\n\n    # The return type for each field.\n    type1 = def1 and def1.type\n    type2 = def2 and def2.type\n\n    if not are_mutually_exclusive:\n        # Two aliases must refer to the same field.\n        name1 = ast1.name.value\n        name2 = ast2.name.value\n\n        if name1 != name2:\n            return (\n                (response_name, '{} and {} are different fields'.format(name1, name2)),\n                [ast1],\n                [ast2]\n            )\n\n        # Two field calls must have the same arguments.\n        if not _same_arguments(ast1.arguments, ast2.arguments):\n            return (\n                (response_name, 'they have differing arguments'),\n                [ast1],\n                [ast2]\n            )\n\n    if type1 and type2 and do_types_conflict(type1, type2):\n        return (\n            (response_name, 'they return conflicting types {} and {}'.format(type1, type2)),\n            [ast1],\n            [ast2]\n        )\n\n    #  Collect and compare sub-fields. Use the same \"visited fragment names\" list\n    # for both collections so fields in a fragment reference are never\n    # compared to themselves.\n    selection_set1 = ast1.selection_set\n    selection_set2 = ast2.selection_set\n\n    if selection_set1 and selection_set2:\n        conflicts = _find_conflicts_between_sub_selection_sets(context, cached_fields_and_fragment_names,\n                                                               compared_fragments, are_mutually_exclusive,\n                                                               get_named_type(type1), selection_set1,\n                                                               get_named_type(type2), selection_set2)\n\n        return _subfield_conflicts(conflicts, response_name, ast1, ast2)\n\n\ndef _get_fields_and_fragments_names(context, cached_fields_and_fragment_names, parent_type, selection_set):\n    cached = cached_fields_and_fragment_names.get(selection_set)\n\n    if not cached:\n        ast_and_defs = OrderedDict()\n        fragment_names = OrderedDict()\n        _collect_fields_and_fragment_names(context, parent_type, selection_set, ast_and_defs, fragment_names)\n        cached = [ast_and_defs, list(fragment_names.keys())]\n        cached_fields_and_fragment_names[selection_set] = cached\n\n    return cached\n\n\ndef _get_referenced_fields_and_fragment_names(context, cached_fields_and_fragment_names, fragment):\n    \"\"\"Given a reference to a fragment, return the represented collection of fields as well as a list of\n    nested fragment names referenced via fragment spreads.\"\"\"\n\n    # Short-circuit building a type from the AST if possible.\n    cached = cached_fields_and_fragment_names.get(fragment.selection_set)\n\n    if cached:\n        return cached\n\n    fragment_type = type_from_ast(context.get_schema(), fragment.type_condition)\n\n    return _get_fields_and_fragments_names(context, cached_fields_and_fragment_names,\n                                           fragment_type, fragment.selection_set)\n\n\ndef _collect_fields_and_fragment_names(context, parent_type, selection_set, ast_and_defs, fragment_names):\n\n    for selection in selection_set.selections:\n        if isinstance(selection, ast.Field):\n            field_name = selection.name.value\n            if isinstance(parent_type, (GraphQLObjectType, GraphQLInterfaceType)):\n                field_def = parent_type.fields.get(field_name)\n            else:\n                field_def = None\n\n            response_name = selection.alias.value if selection.alias else field_name\n\n            if not ast_and_defs.get(response_name):\n                ast_and_defs[response_name] = []\n\n            ast_and_defs[response_name].append([parent_type, selection, field_def])\n\n        elif isinstance(selection, ast.FragmentSpread):\n            fragment_names[selection.name.value] = True\n        elif isinstance(selection, ast.InlineFragment):\n            type_condition = selection.type_condition\n            if type_condition:\n                inline_fragment_type = type_from_ast(context.get_schema(), selection.type_condition)\n            else:\n                inline_fragment_type = parent_type\n\n            _collect_fields_and_fragment_names(context, inline_fragment_type, selection.selection_set, ast_and_defs,\n                                               fragment_names)\n\n\ndef _subfield_conflicts(conflicts, response_name, ast1, ast2):\n    \"\"\"Given a series of Conflicts which occurred between two sub-fields, generate a single Conflict.\"\"\"\n    if conflicts:\n        return (\n            (response_name, [conflict[0] for conflict in conflicts]),\n            tuple(itertools.chain([ast1], *[conflict[1] for conflict in conflicts])),\n            tuple(itertools.chain([ast2], *[conflict[2] for conflict in conflicts]))\n        )\n\n\ndef do_types_conflict(type1, type2):\n    if isinstance(type1, GraphQLList):\n        if isinstance(type2, GraphQLList):\n            return do_types_conflict(type1.of_type, type2.of_type)\n        return True\n\n    if isinstance(type2, GraphQLList):\n        if isinstance(type1, GraphQLList):\n            return do_types_conflict(type1.of_type, type2.of_type)\n        return True\n\n    if isinstance(type1, GraphQLNonNull):\n        if isinstance(type2, GraphQLNonNull):\n            return do_types_conflict(type1.of_type, type2.of_type)\n        return True\n\n    if isinstance(type2, GraphQLNonNull):\n        if isinstance(type1, GraphQLNonNull):\n            return do_types_conflict(type1.of_type, type2.of_type)\n        return True\n\n    if is_leaf_type(type1) or is_leaf_type(type2):\n        return type1 != type2\n\n    return False\n\n\ndef _same_value(value1, value2):\n    return (not value1 and not value2) or print_ast(value1) == print_ast(value2)\n\n\ndef _same_arguments(arguments1, arguments2):\n    # Check to see if they are empty arguments or nones. If they are, we can\n    # bail out early.\n    if not (arguments1 or arguments2):\n        return True\n\n    if len(arguments1) != len(arguments2):\n        return False\n\n    arguments2_values_to_arg = {a.name.value: a for a in arguments2}\n\n    for argument1 in arguments1:\n        argument2 = arguments2_values_to_arg.get(argument1.name.value)\n        if not argument2:\n            return False\n\n        if not _same_value(argument1.value, argument2.value):\n            return False\n\n    return True\n", 
    "graphql.validation.rules.possible_fragment_spreads": "from ...error import GraphQLError\nfrom ...utils.type_comparators import do_types_overlap\nfrom ...utils.type_from_ast import type_from_ast\nfrom .base import ValidationRule\n\n\nclass PossibleFragmentSpreads(ValidationRule):\n\n    def enter_InlineFragment(self, node, key, parent, path, ancestors):\n        frag_type = self.context.get_type()\n        parent_type = self.context.get_parent_type()\n        schema = self.context.get_schema()\n        if frag_type and parent_type and not do_types_overlap(schema, frag_type, parent_type):\n            self.context.report_error(GraphQLError(\n                self.type_incompatible_anon_spread_message(parent_type, frag_type),\n                [node]\n            ))\n\n    def enter_FragmentSpread(self, node, key, parent, path, ancestors):\n        frag_name = node.name.value\n        frag_type = self.get_fragment_type(self.context, frag_name)\n        parent_type = self.context.get_parent_type()\n        schema = self.context.get_schema()\n        if frag_type and parent_type and not do_types_overlap(schema, frag_type, parent_type):\n            self.context.report_error(GraphQLError(\n                self.type_incompatible_spread_message(frag_name, parent_type, frag_type),\n                [node]\n            ))\n\n    @staticmethod\n    def get_fragment_type(context, name):\n        frag = context.get_fragment(name)\n        return frag and type_from_ast(context.get_schema(), frag.type_condition)\n\n    @staticmethod\n    def type_incompatible_spread_message(frag_name, parent_type, frag_type):\n        return 'Fragment {} cannot be spread here as objects of type {} can never be of type {}'.format(frag_name,\n                                                                                                        parent_type,\n                                                                                                        frag_type)\n\n    @staticmethod\n    def type_incompatible_anon_spread_message(parent_type, frag_type):\n        return 'Fragment cannot be spread here as objects of type {} can never be of type {}'.format(parent_type,\n                                                                                                     frag_type)\n", 
    "graphql.validation.rules.provided_non_null_arguments": "from ...error import GraphQLError\nfrom ...type.definition import GraphQLNonNull\nfrom .base import ValidationRule\n\n\nclass ProvidedNonNullArguments(ValidationRule):\n\n    def leave_Field(self, node, key, parent, path, ancestors):\n        field_def = self.context.get_field_def()\n        if not field_def:\n            return False\n\n        arg_asts = node.arguments or []\n        arg_ast_map = {arg.name.value: arg for arg in arg_asts}\n\n        for arg_name, arg_def in field_def.args.items():\n            arg_ast = arg_ast_map.get(arg_name, None)\n            if not arg_ast and isinstance(arg_def.type, GraphQLNonNull):\n                self.context.report_error(GraphQLError(\n                    self.missing_field_arg_message(node.name.value, arg_name, arg_def.type),\n                    [node]\n                ))\n\n    def leave_Directive(self, node, key, parent, path, ancestors):\n        directive_def = self.context.get_directive()\n        if not directive_def:\n            return False\n\n        arg_asts = node.arguments or []\n        arg_ast_map = {arg.name.value: arg for arg in arg_asts}\n\n        for arg_name, arg_def in directive_def.args.items():\n            arg_ast = arg_ast_map.get(arg_name, None)\n            if not arg_ast and isinstance(arg_def.type, GraphQLNonNull):\n                self.context.report_error(GraphQLError(\n                    self.missing_directive_arg_message(node.name.value, arg_name, arg_def.type),\n                    [node]\n                ))\n\n    @staticmethod\n    def missing_field_arg_message(name, arg_name, type):\n        return 'Field \"{}\" argument \"{}\" of type \"{}\" is required but not provided.'.format(name, arg_name, type)\n\n    @staticmethod\n    def missing_directive_arg_message(name, arg_name, type):\n        return 'Directive \"{}\" argument \"{}\" of type \"{}\" is required but not provided.'.format(name, arg_name, type)\n", 
    "graphql.validation.rules.scalar_leafs": "from ...error import GraphQLError\nfrom ...type.definition import is_leaf_type\nfrom .base import ValidationRule\n\n\nclass ScalarLeafs(ValidationRule):\n\n    def enter_Field(self, node, key, parent, path, ancestors):\n        type = self.context.get_type()\n\n        if not type:\n            return\n\n        if is_leaf_type(type):\n            if node.selection_set:\n                self.context.report_error(GraphQLError(\n                    self.no_subselection_allowed_message(node.name.value, type),\n                    [node.selection_set]\n                ))\n\n        elif not node.selection_set:\n            self.context.report_error(GraphQLError(\n                self.required_subselection_message(node.name.value, type),\n                [node]\n            ))\n\n    @staticmethod\n    def no_subselection_allowed_message(field, type):\n        return 'Field \"{}\" of type \"{}\" must not have a sub selection.'.format(field, type)\n\n    @staticmethod\n    def required_subselection_message(field, type):\n        return 'Field \"{}\" of type \"{}\" must have a sub selection.'.format(field, type)\n", 
    "graphql.validation.rules.unique_argument_names": "from ...error import GraphQLError\nfrom .base import ValidationRule\n\n\nclass UniqueArgumentNames(ValidationRule):\n    __slots__ = 'known_arg_names',\n\n    def __init__(self, context):\n        super(UniqueArgumentNames, self).__init__(context)\n        self.known_arg_names = {}\n\n    def enter_Field(self, node, key, parent, path, ancestors):\n        self.known_arg_names = {}\n\n    def enter_Directive(self, node, key, parent, path, ancestors):\n        self.known_arg_names = {}\n\n    def enter_Argument(self, node, key, parent, path, ancestors):\n        arg_name = node.name.value\n\n        if arg_name in self.known_arg_names:\n            self.context.report_error(GraphQLError(\n                self.duplicate_arg_message(arg_name),\n                [self.known_arg_names[arg_name], node.name]\n            ))\n        else:\n            self.known_arg_names[arg_name] = node.name\n        return False\n\n    @staticmethod\n    def duplicate_arg_message(field):\n        return 'There can only be one argument named \"{}\".'.format(field)\n", 
    "graphql.validation.rules.unique_fragment_names": "from ...error import GraphQLError\nfrom .base import ValidationRule\n\n\nclass UniqueFragmentNames(ValidationRule):\n    __slots__ = 'known_fragment_names',\n\n    def __init__(self, context):\n        super(UniqueFragmentNames, self).__init__(context)\n        self.known_fragment_names = {}\n\n    def enter_OperationDefinition(self, node, key, parent, path, ancestors):\n        return False\n\n    def enter_FragmentDefinition(self, node, key, parent, path, ancestors):\n        fragment_name = node.name.value\n        if fragment_name in self.known_fragment_names:\n            self.context.report_error(GraphQLError(\n                self.duplicate_fragment_name_message(fragment_name),\n                [self.known_fragment_names[fragment_name], node.name]\n            ))\n        else:\n            self.known_fragment_names[fragment_name] = node.name\n        return False\n\n    @staticmethod\n    def duplicate_fragment_name_message(field):\n        return 'There can only be one fragment named \"{}\".'.format(field)\n", 
    "graphql.validation.rules.unique_input_field_names": "from ...error import GraphQLError\nfrom .base import ValidationRule\n\n\nclass UniqueInputFieldNames(ValidationRule):\n    __slots__ = 'known_names', 'known_names_stack'\n\n    def __init__(self, context):\n        super(UniqueInputFieldNames, self).__init__(context)\n        self.known_names = {}\n        self.known_names_stack = []\n\n    def enter_ObjectValue(self, node, key, parent, path, ancestors):\n        self.known_names_stack.append(self.known_names)\n        self.known_names = {}\n\n    def leave_ObjectValue(self, node, key, parent, path, ancestors):\n        self.known_names = self.known_names_stack.pop()\n\n    def enter_ObjectField(self, node, key, parent, path, ancestors):\n        field_name = node.name.value\n        if field_name in self.known_names:\n            self.context.report_error(GraphQLError(\n                self.duplicate_input_field_message(field_name),\n                [self.known_names[field_name], node.name]\n            ))\n        else:\n            self.known_names[field_name] = node.name\n        return False\n\n    @staticmethod\n    def duplicate_input_field_message(field_name):\n        return 'There can only be one input field named \"{}\".'.format(field_name)\n", 
    "graphql.validation.rules.unique_operation_names": "from ...error import GraphQLError\nfrom .base import ValidationRule\n\n\nclass UniqueOperationNames(ValidationRule):\n    __slots__ = 'known_operation_names',\n\n    def __init__(self, context):\n        super(UniqueOperationNames, self).__init__(context)\n        self.known_operation_names = {}\n\n    def enter_OperationDefinition(self, node, key, parent, path, ancestors):\n        operation_name = node.name\n        if not operation_name:\n            return\n\n        if operation_name.value in self.known_operation_names:\n            self.context.report_error(GraphQLError(\n                self.duplicate_operation_name_message(operation_name.value),\n                [self.known_operation_names[operation_name.value], operation_name]\n            ))\n        else:\n            self.known_operation_names[operation_name.value] = operation_name\n        return False\n\n    def enter_FragmentDefinition(self, node, key, parent, path, ancestors):\n        return False\n\n    @staticmethod\n    def duplicate_operation_name_message(operation_name):\n        return 'There can only be one operation named \"{}\".'.format(operation_name)\n", 
    "graphql.validation.rules.unique_variable_names": "from ...error import GraphQLError\nfrom .base import ValidationRule\n\n\nclass UniqueVariableNames(ValidationRule):\n    __slots__ = 'known_variable_names',\n\n    def __init__(self, context):\n        super(UniqueVariableNames, self).__init__(context)\n        self.known_variable_names = {}\n\n    def enter_OperationDefinition(self, node, key, parent, path, ancestors):\n        self.known_variable_names = {}\n\n    def enter_VariableDefinition(self, node, key, parent, path, ancestors):\n        variable_name = node.variable.name.value\n        if variable_name in self.known_variable_names:\n            self.context.report_error(GraphQLError(\n                self.duplicate_variable_message(variable_name),\n                [self.known_variable_names[variable_name], node.variable.name]\n            ))\n        else:\n            self.known_variable_names[variable_name] = node.variable.name\n\n    @staticmethod\n    def duplicate_variable_message(operation_name):\n        return 'There can be only one variable named \"{}\".'.format(operation_name)\n", 
    "graphql.validation.rules.variables_are_input_types": "from ...error import GraphQLError\nfrom ...language.printer import print_ast\nfrom ...type.definition import is_input_type\nfrom ...utils.type_from_ast import type_from_ast\nfrom .base import ValidationRule\n\n\nclass VariablesAreInputTypes(ValidationRule):\n\n    def enter_VariableDefinition(self, node, key, parent, path, ancestors):\n        type = type_from_ast(self.context.get_schema(), node.type)\n\n        if type and not is_input_type(type):\n            self.context.report_error(GraphQLError(\n                self.non_input_type_on_variable_message(node.variable.name.value, print_ast(node.type)),\n                [node.type]\n            ))\n\n    @staticmethod\n    def non_input_type_on_variable_message(variable_name, type_name):\n        return 'Variable \"${}\" cannot be non-input type \"{}\".'.format(variable_name, type_name)\n", 
    "graphql.validation.rules.variables_in_allowed_position": "from ...error import GraphQLError\nfrom ...type.definition import GraphQLNonNull\nfrom ...utils.type_comparators import is_type_sub_type_of\nfrom ...utils.type_from_ast import type_from_ast\nfrom .base import ValidationRule\n\n\nclass VariablesInAllowedPosition(ValidationRule):\n    __slots__ = 'var_def_map'\n\n    def __init__(self, context):\n        super(VariablesInAllowedPosition, self).__init__(context)\n        self.var_def_map = {}\n\n    def enter_OperationDefinition(self, node, key, parent, path, ancestors):\n        self.var_def_map = {}\n\n    def leave_OperationDefinition(self, operation, key, parent, path, ancestors):\n        usages = self.context.get_recursive_variable_usages(operation)\n\n        for usage in usages:\n            node = usage.node\n            type = usage.type\n            var_name = node.name.value\n            var_def = self.var_def_map.get(var_name)\n            if var_def and type:\n                # A var type is allowed if it is the same or more strict (e.g. is\n                # a subtype of) than the expected type. It can be more strict if\n                # the variable type is non-null when the expected type is nullable.\n                # If both are list types, the variable item type can be more strict\n                # than the expected item type (contravariant).\n                schema = self.context.get_schema()\n                var_type = type_from_ast(schema, var_def.type)\n                if var_type and not is_type_sub_type_of(schema, self.effective_type(var_type, var_def), type):\n                    self.context.report_error(GraphQLError(\n                        self.bad_var_pos_message(var_name, var_type, type),\n                        [var_def, node]\n                    ))\n\n    def enter_VariableDefinition(self, node, key, parent, path, ancestors):\n        self.var_def_map[node.variable.name.value] = node\n\n    @staticmethod\n    def effective_type(var_type, var_def):\n        if not var_def.default_value or isinstance(var_type, GraphQLNonNull):\n            return var_type\n\n        return GraphQLNonNull(var_type)\n\n    @staticmethod\n    def bad_var_pos_message(var_name, var_type, expected_type):\n        return 'Variable \"{}\" of type \"{}\" used in position expecting type \"{}\".'.format(var_name, var_type,\n                                                                                         expected_type)\n", 
    "graphql.validation.validation": "from ..language.ast import (FragmentDefinition, FragmentSpread,\n                            OperationDefinition)\nfrom ..language.visitor import ParallelVisitor, TypeInfoVisitor, Visitor, visit\nfrom ..type import GraphQLSchema\nfrom ..utils.type_info import TypeInfo\nfrom .rules import specified_rules\n\n\ndef validate(schema, ast, rules=specified_rules):\n    assert schema, 'Must provide schema'\n    assert ast, 'Must provide document'\n    assert isinstance(schema, GraphQLSchema)\n    type_info = TypeInfo(schema)\n    return visit_using_rules(schema, type_info, ast, rules)\n\n\ndef visit_using_rules(schema, type_info, ast, rules):\n    context = ValidationContext(schema, ast, type_info)\n    visitors = [rule(context) for rule in rules]\n    visit(ast, TypeInfoVisitor(type_info, ParallelVisitor(visitors)))\n    return context.get_errors()\n\n\nclass VariableUsage(object):\n    __slots__ = 'node', 'type'\n\n    def __init__(self, node, type):\n        self.node = node\n        self.type = type\n\n\nclass UsageVisitor(Visitor):\n    __slots__ = 'usages', 'type_info'\n\n    def __init__(self, usages, type_info):\n        self.usages = usages\n        self.type_info = type_info\n\n    def enter_VariableDefinition(self, node, key, parent, path, ancestors):\n        return False\n\n    def enter_Variable(self, node, key, parent, path, ancestors):\n        usage = VariableUsage(node, type=self.type_info.get_input_type())\n        self.usages.append(usage)\n\n\nclass ValidationContext(object):\n    __slots__ = ('_schema', '_ast', '_type_info', '_errors', '_fragments', '_fragment_spreads',\n                 '_recursively_referenced_fragments', '_variable_usages', '_recursive_variable_usages')\n\n    def __init__(self, schema, ast, type_info):\n        self._schema = schema\n        self._ast = ast\n        self._type_info = type_info\n        self._errors = []\n        self._fragments = None\n        self._fragment_spreads = {}\n        self._recursively_referenced_fragments = {}\n        self._variable_usages = {}\n        self._recursive_variable_usages = {}\n\n    def report_error(self, error):\n        self._errors.append(error)\n\n    def get_errors(self):\n        return self._errors\n\n    def get_schema(self):\n        return self._schema\n\n    def get_variable_usages(self, node):\n        usages = self._variable_usages.get(node)\n        if usages is None:\n            usages = []\n            sub_visitor = UsageVisitor(usages, self._type_info)\n            visit(node, TypeInfoVisitor(self._type_info, sub_visitor))\n            self._variable_usages[node] = usages\n\n        return usages\n\n    def get_recursive_variable_usages(self, operation):\n        assert isinstance(operation, OperationDefinition)\n        usages = self._recursive_variable_usages.get(operation)\n        if usages is None:\n            usages = self.get_variable_usages(operation)\n            fragments = self.get_recursively_referenced_fragments(operation)\n            for fragment in fragments:\n                usages.extend(self.get_variable_usages(fragment))\n            self._recursive_variable_usages[operation] = usages\n\n        return usages\n\n    def get_recursively_referenced_fragments(self, operation):\n        assert isinstance(operation, OperationDefinition)\n        fragments = self._recursively_referenced_fragments.get(operation)\n        if not fragments:\n            fragments = []\n            collected_names = set()\n            nodes_to_visit = [operation.selection_set]\n            while nodes_to_visit:\n                node = nodes_to_visit.pop()\n                spreads = self.get_fragment_spreads(node)\n                for spread in spreads:\n                    frag_name = spread.name.value\n                    if frag_name not in collected_names:\n                        collected_names.add(frag_name)\n                        fragment = self.get_fragment(frag_name)\n                        if fragment:\n                            fragments.append(fragment)\n                            nodes_to_visit.append(fragment.selection_set)\n            self._recursively_referenced_fragments[operation] = fragments\n        return fragments\n\n    def get_fragment_spreads(self, node):\n        spreads = self._fragment_spreads.get(node)\n        if not spreads:\n            spreads = []\n            sets_to_visit = [node]\n            while sets_to_visit:\n                _set = sets_to_visit.pop()\n                for selection in _set.selections:\n                    if isinstance(selection, FragmentSpread):\n                        spreads.append(selection)\n                    elif selection.selection_set:\n                        sets_to_visit.append(selection.selection_set)\n\n            self._fragment_spreads[node] = spreads\n        return spreads\n\n    def get_ast(self):\n        return self._ast\n\n    def get_fragment(self, name):\n        fragments = self._fragments\n        if fragments is None:\n            self._fragments = fragments = {}\n            for statement in self.get_ast().definitions:\n                if isinstance(statement, FragmentDefinition):\n                    fragments[statement.name.value] = statement\n        return fragments.get(name)\n\n    def get_type(self):\n        return self._type_info.get_type()\n\n    def get_parent_type(self):\n        return self._type_info.get_parent_type()\n\n    def get_input_type(self):\n        return self._type_info.get_input_type()\n\n    def get_field_def(self):\n        return self._type_info.get_field_def()\n\n    def get_directive(self):\n        return self._type_info.get_directive()\n\n    def get_argument(self):\n        return self._type_info.get_argument()\n", 
    "graphql_relay.__init__": "from .connection.connection import (\n    connection_args,\n    connection_definitions\n)\nfrom .connection.arrayconnection import (\n    connection_from_list,\n    connection_from_promised_list,\n    cursor_for_object_in_connection\n)\nfrom .node.node import (\n    node_definitions,\n    from_global_id,\n    to_global_id,\n    global_id_field,\n)\nfrom .mutation.mutation import (\n    mutation_with_client_mutation_id\n)\n\n__all__ = [\n    # Helpers for creating connection types in the schema\n    'connection_args', 'connection_definitions',\n    # Helpers for creating connections from arrays\n    'connection_from_list', 'connection_from_promised_list', 'cursor_for_object_in_connection',\n    # Helper for creating node definitions\n    'node_definitions',\n    # Utilities for creating global IDs in systems that don't have them\n    'from_global_id', 'to_global_id', 'global_id_field',\n    # Helper for creating mutations with client mutation IDs\n    'mutation_with_client_mutation_id'\n]\n", 
    "graphql_relay.connection.__init__": "", 
    "graphql_relay.connection.arrayconnection": "from promise import Promise\n\nfrom ..utils import base64, unbase64, is_str\nfrom .connectiontypes import Connection, PageInfo, Edge\n\n\ndef connection_from_list(data, args=None, **kwargs):\n    '''\n    A simple function that accepts an array and connection arguments, and returns\n    a connection object for use in GraphQL. It uses array offsets as pagination,\n    so pagination will only work if the array is static.\n    '''\n    _len = len(data)\n    return connection_from_list_slice(\n        data,\n        args,\n        slice_start=0,\n        list_length=_len,\n        list_slice_length=_len,\n        **kwargs\n    )\n\n\ndef connection_from_promised_list(data_promise, args=None, **kwargs):\n    '''\n    A version of `connectionFromArray` that takes a promised array, and returns a\n    promised connection.\n    '''\n    return data_promise.then(lambda data: connection_from_list(data, args, **kwargs))\n\n\ndef connection_from_list_slice(list_slice, args=None, connection_type=None,\n                               edge_type=None, pageinfo_type=None,\n                               slice_start=0, list_length=0, list_slice_length=None):\n    '''\n    Given a slice (subset) of an array, returns a connection object for use in\n    GraphQL.\n    This function is similar to `connectionFromArray`, but is intended for use\n    cases where you know the cardinality of the connection, consider it too large\n    to materialize the entire array, and instead wish pass in a slice of the\n    total result large enough to cover the range specified in `args`.\n    '''\n    connection_type = connection_type or Connection\n    edge_type = edge_type or Edge\n    pageinfo_type = pageinfo_type or PageInfo\n\n    args = args or {}\n\n    before = args.get('before')\n    after = args.get('after')\n    first = args.get('first')\n    last = args.get('last')\n    if list_slice_length is None:\n        list_slice_length = len(list_slice)\n    slice_end = slice_start + list_slice_length\n    before_offset = get_offset_with_default(before, list_length)\n    after_offset = get_offset_with_default(after, -1)\n\n    start_offset = max(\n        slice_start - 1,\n        after_offset,\n        -1\n    ) + 1\n    end_offset = min(\n        slice_end,\n        before_offset,\n        list_length\n    )\n    if isinstance(first, int):\n        end_offset = min(\n            end_offset,\n            start_offset + first\n        )\n    if isinstance(last, int):\n        start_offset = max(\n            start_offset,\n            end_offset - last\n        )\n\n    # If supplied slice is too large, trim it down before mapping over it.\n    _slice = list_slice[\n        max(start_offset - slice_start, 0):\n        list_slice_length - (slice_end - end_offset)\n    ]\n    edges = [\n        edge_type(\n            node=node,\n            cursor=offset_to_cursor(start_offset + i)\n        )\n        for i, node in enumerate(_slice)\n    ]\n\n\n    first_edge_cursor = edges[0].cursor if edges else None\n    last_edge_cursor = edges[-1].cursor if edges else None\n    lower_bound = after_offset + 1 if after else 0\n    upper_bound = before_offset if before else list_length\n\n    return connection_type(\n        edges=edges,\n        page_info=pageinfo_type(\n            start_cursor=first_edge_cursor,\n            end_cursor=last_edge_cursor,\n            has_previous_page=isinstance(last, int) and start_offset > lower_bound,\n            has_next_page=isinstance(first, int) and end_offset < upper_bound\n        )\n    )\n\n\nPREFIX = 'arrayconnection:'\n\n\ndef connection_from_promised_list_slice(data_promise, args=None, **kwargs):\n    return data_promise.then(lambda data: connection_from_list_slice(data, args, **kwargs))\n\n\ndef offset_to_cursor(offset):\n    '''\n    Creates the cursor string from an offset.\n    '''\n    return base64(PREFIX + str(offset))\n\n\ndef cursor_to_offset(cursor):\n    '''\n    Rederives the offset from the cursor string.\n    '''\n    try:\n        return int(unbase64(cursor)[len(PREFIX):])\n    except:\n        return None\n\n\ndef cursor_for_object_in_connection(data, _object):\n    '''\n    Return the cursor associated with an object in an array.\n    '''\n    if _object not in data:\n        return None\n\n    offset = data.index(_object)\n    return offset_to_cursor(offset)\n\n\ndef get_offset_with_default(cursor=None, default_offset=0):\n    '''\n    Given an optional cursor and a default offset, returns the offset\n    to use; if the cursor contains a valid offset, that will be used,\n    otherwise it will be the default.\n    '''\n    if not is_str(cursor):\n        return default_offset\n\n    offset = cursor_to_offset(cursor)\n    try:\n        return int(offset)\n    except:\n        return default_offset\n", 
    "graphql_relay.connection.connection": "from collections import OrderedDict\n\nfrom graphql.type import (\n    GraphQLArgument,\n    GraphQLBoolean,\n    GraphQLInt,\n    GraphQLNonNull,\n    GraphQLList,\n    GraphQLObjectType,\n    GraphQLString,\n    GraphQLField\n)\nfrom ..utils import resolve_maybe_thunk\n\n\nconnection_args = OrderedDict((\n    ('before', GraphQLArgument(GraphQLString)),\n    ('after', GraphQLArgument(GraphQLString)),\n    ('first', GraphQLArgument(GraphQLInt)),\n    ('last', GraphQLArgument(GraphQLInt)),\n))\n\n\ndef connection_definitions(name, node_type, resolve_node=None, resolve_cursor=None, edge_fields=None, connection_fields=None):\n    edge_fields = edge_fields or OrderedDict()\n    connection_fields = connection_fields or OrderedDict()\n    edge_type = GraphQLObjectType(\n        name + 'Edge',\n        description='An edge in a connection.',\n        fields=lambda: OrderedDict((\n            ('node', GraphQLField(\n                node_type,\n                resolver=resolve_node,\n                description='The item at the end of the edge',\n            )),\n            ('cursor', GraphQLField(\n                GraphQLNonNull(GraphQLString),\n                resolver=resolve_cursor,\n                description='A cursor for use in pagination',\n            )),\n        ), **resolve_maybe_thunk(edge_fields))\n    )\n\n    connection_type = GraphQLObjectType(\n        name + 'Connection',\n        description='A connection to a list of items.',\n        fields=lambda: OrderedDict((\n            ('pageInfo', GraphQLField(\n                GraphQLNonNull(page_info_type),\n                description='The Information to aid in pagination',\n            )),\n            ('edges', GraphQLField(\n                GraphQLList(edge_type),\n                description='A list of edges.',\n            )),\n        ), **resolve_maybe_thunk(connection_fields))\n    )\n\n    return edge_type, connection_type\n\n\n# The common page info type used by all connections.\npage_info_type = GraphQLObjectType(\n    'PageInfo',\n    description='Information about pagination in a connection.',\n    fields=lambda: OrderedDict((\n        ('hasNextPage', GraphQLField(\n            GraphQLNonNull(GraphQLBoolean),\n            description='When paginating forwards, are there more items?',\n        )),\n        ('hasPreviousPage', GraphQLField(\n            GraphQLNonNull(GraphQLBoolean),\n            description='When paginating backwards, are there more items?',\n        )),\n        ('startCursor', GraphQLField(\n            GraphQLString,\n            description='When paginating backwards, the cursor to continue.',\n        )),\n        ('endCursor', GraphQLField(\n            GraphQLString,\n            description='When paginating forwards, the cursor to continue.',\n        )),\n    ))\n)\n", 
    "graphql_relay.connection.connectiontypes": "class Connection(object):\n\n    def __init__(self, edges, page_info):\n        self.edges = edges\n        self.page_info = page_info\n\n    def to_dict(self):\n        return {\n            'edges': [e.to_dict() for e in self.edges],\n            'pageInfo': self.page_info.to_dict(),\n        }\n\n\nclass PageInfo(object):\n\n    def __init__(self, start_cursor=\"\", end_cursor=\"\",\n                 has_previous_page=False, has_next_page=False):\n        self.startCursor = start_cursor\n        self.endCursor = end_cursor\n        self.hasPreviousPage = has_previous_page\n        self.hasNextPage = has_next_page\n\n    def to_dict(self):\n        return {\n            'startCursor': self.startCursor,\n            'endCursor': self.endCursor,\n            'hasPreviousPage': self.hasPreviousPage,\n            'hasNextPage': self.hasNextPage,\n        }\n\n\nclass Edge(object):\n\n    def __init__(self, node, cursor):\n        self.node = node\n        self.cursor = cursor\n\n    def to_dict(self):\n        return {\n            'node': self.node,\n            'cursor': self.cursor,\n        }\n", 
    "graphql_relay.mutation.__init__": "", 
    "graphql_relay.mutation.mutation": "from collections import OrderedDict\nfrom promise import Promise\nfrom graphql.type import (\n    GraphQLArgument,\n    GraphQLInputObjectField,\n    GraphQLInputObjectType,\n    GraphQLNonNull,\n    GraphQLObjectType,\n    GraphQLString,\n    GraphQLField,\n)\nfrom graphql.error import GraphQLError\nfrom ..utils import resolve_maybe_thunk\n\n\ndef mutation_with_client_mutation_id(name, input_fields, output_fields, mutate_and_get_payload):\n    augmented_input_fields = OrderedDict(\n        resolve_maybe_thunk(input_fields),\n        clientMutationId=GraphQLInputObjectField(\n            GraphQLNonNull(GraphQLString)\n        )\n    )\n    augmented_output_fields = OrderedDict(\n        resolve_maybe_thunk(output_fields),\n        clientMutationId=GraphQLField(\n            GraphQLNonNull(GraphQLString)\n        )\n    )\n\n    input_type = GraphQLInputObjectType(\n        name + 'Input',\n        fields=augmented_input_fields,\n    )\n    output_type = GraphQLObjectType(\n        name + 'Payload',\n        fields=augmented_output_fields,\n    )\n\n    def resolver(__, args, *_):\n        input = args.get('input')\n\n        def on_resolve(payload):\n            try:\n                payload.clientMutationId = input['clientMutationId']\n            except:\n                raise GraphQLError('Cannot set clientMutationId in the payload object {}'.format(repr(payload)))\n            return payload\n\n        return Promise.resolve(mutate_and_get_payload(input, *_)).then(on_resolve)\n\n    return GraphQLField(\n        output_type,\n        args=OrderedDict((\n            ('input', GraphQLArgument(GraphQLNonNull(input_type))),\n        )),\n        resolver=resolver\n    )\n", 
    "graphql_relay.node.__init__": "", 
    "graphql_relay.node.node": "from collections import OrderedDict\nfrom graphql_relay.utils import base64, unbase64\n\nfrom graphql.type import (\n    GraphQLArgument,\n    GraphQLNonNull,\n    GraphQLID,\n    GraphQLField,\n    GraphQLInterfaceType,\n)\n\n\ndef node_definitions(id_fetcher, type_resolver=None, id_resolver=None):\n    '''\n    Given a function to map from an ID to an underlying object, and a function\n    to map from an underlying object to the concrete GraphQLObjectType it\n    corresponds to, constructs a `Node` interface that objects can implement,\n    and a field config for a `node` root field.\n\n    If the type_resolver is omitted, object resolution on the interface will be\n    handled with the `isTypeOf` method on object types, as with any GraphQL\n    interface without a provided `resolveType` method.\n    '''\n    node_interface = GraphQLInterfaceType(\n        'Node',\n        description='An object with an ID',\n        fields=lambda: OrderedDict((\n            ('id', GraphQLField(\n                GraphQLNonNull(GraphQLID),\n                description='The id of the object.',\n                resolver=id_resolver,\n            )),\n        )),\n        resolve_type=type_resolver\n    )\n    node_field = GraphQLField(\n        node_interface,\n        description='Fetches an object given its ID',\n        args=OrderedDict((\n            ('id', GraphQLArgument(\n                GraphQLNonNull(GraphQLID),\n                description='The ID of an object'\n            )),\n        )),\n        resolver=lambda obj, args, *_: id_fetcher(args.get('id'), *_)\n    )\n    return node_interface, node_field\n\n\ndef to_global_id(type, id):\n    '''\n    Takes a type name and an ID specific to that type name, and returns a\n    \"global ID\" that is unique among all types.\n    '''\n    return base64(':'.join([type, str(id)]))\n\n\ndef from_global_id(global_id):\n    '''\n    Takes the \"global ID\" created by toGlobalID, and retuns the type name and ID\n    used to create it.\n    '''\n    unbased_global_id = unbase64(global_id)\n    _type, _id = unbased_global_id.split(':', 1)\n    return _type, _id\n\n\ndef global_id_field(type_name, id_fetcher=None):\n    '''\n    Creates the configuration for an id field on a node, using `to_global_id` to\n    construct the ID from the provided typename. The type-specific ID is fetcher\n    by calling id_fetcher on the object, or if not provided, by accessing the `id`\n    property on the object.\n    '''\n    return GraphQLField(\n        GraphQLNonNull(GraphQLID),\n        description='The ID of an object',\n        resolver=lambda obj, args, context, info: to_global_id(\n            type_name or info.parent_type.name,\n            id_fetcher(obj, context, info) if id_fetcher else obj.id\n        )\n    )\n", 
    "graphql_relay.utils": "from base64 import b64encode as _base64, b64decode as _unbase64\n\ntry:\n    str_type = basestring\n    base64 = _base64\n    unbase64 = _unbase64\n\n    def is_str(s):\n        return isinstance(s, basestring)\n\nexcept NameError:\n    def base64(s):\n        return _base64(bytes(s, 'utf-8')).decode('utf-8')\n\n    def unbase64(s):\n        return _unbase64(s).decode('utf-8')\n\n    def is_str(s):\n        return isinstance(s, str)\n\n\ndef resolve_maybe_thunk(f):\n    if callable(f):\n        return f()\n    return f\n", 
    "hashlib": "# $Id$\n#\n#  Copyright (C) 2005   Gregory P. Smith (greg@krypto.org)\n#  Licensed to PSF under a Contributor Agreement.\n#\n\n__doc__ = \"\"\"hashlib module - A common interface to many hash functions.\n\nnew(name, string='') - returns a new hash object implementing the\n                       given hash function; initializing the hash\n                       using the given string data.\n\nNamed constructor functions are also available, these are much faster\nthan using new():\n\nmd5(), sha1(), sha224(), sha256(), sha384(), and sha512()\n\nMore algorithms may be available on your platform but the above are guaranteed\nto exist.  See the algorithms_guaranteed and algorithms_available attributes\nto find out what algorithm names can be passed to new().\n\nNOTE: If you want the adler32 or crc32 hash functions they are available in\nthe zlib module.\n\nChoose your hash function wisely.  Some have known collision weaknesses.\nsha384 and sha512 will be slow on 32 bit platforms.\n\nHash objects have these methods:\n - update(arg): Update the hash object with the string arg. Repeated calls\n                are equivalent to a single call with the concatenation of all\n                the arguments.\n - digest():    Return the digest of the strings passed to the update() method\n                so far. This may contain non-ASCII characters, including\n                NUL bytes.\n - hexdigest(): Like digest() except the digest is returned as a string of\n                double length, containing only hexadecimal digits.\n - copy():      Return a copy (clone) of the hash object. This can be used to\n                efficiently compute the digests of strings that share a common\n                initial substring.\n\nFor example, to obtain the digest of the string 'Nobody inspects the\nspammish repetition':\n\n    >>> import hashlib\n    >>> m = hashlib.md5()\n    >>> m.update(\"Nobody inspects\")\n    >>> m.update(\" the spammish repetition\")\n    >>> m.digest()\n    '\\\\xbbd\\\\x9c\\\\x83\\\\xdd\\\\x1e\\\\xa5\\\\xc9\\\\xd9\\\\xde\\\\xc9\\\\xa1\\\\x8d\\\\xf0\\\\xff\\\\xe9'\n\nMore condensed:\n\n    >>> hashlib.sha224(\"Nobody inspects the spammish repetition\").hexdigest()\n    'a4337bc45a8fc544c03f52dc550cd6e1e87021bc896588bd79e901e2'\n\n\"\"\"\n\n# This tuple and __get_builtin_constructor() must be modified if a new\n# always available algorithm is added.\n__always_supported = ('md5', 'sha1', 'sha224', 'sha256', 'sha384', 'sha512')\n\nalgorithms_guaranteed = set(__always_supported)\nalgorithms_available = set(__always_supported)\n\nalgorithms = __always_supported\n\n__all__ = __always_supported + ('new', 'algorithms_guaranteed',\n                                'algorithms_available', 'algorithms',\n                                'pbkdf2_hmac')\n\n\ndef __get_builtin_constructor(name):\n    try:\n        if name in ('SHA1', 'sha1'):\n            import _sha\n            return _sha.new\n        elif name in ('MD5', 'md5'):\n            import _md5\n            return _md5.new\n        elif name in ('SHA256', 'sha256', 'SHA224', 'sha224'):\n            import _sha256\n            bs = name[3:]\n            if bs == '256':\n                return _sha256.sha256\n            elif bs == '224':\n                return _sha256.sha224\n        elif name in ('SHA512', 'sha512', 'SHA384', 'sha384'):\n            import _sha512\n            bs = name[3:]\n            if bs == '512':\n                return _sha512.sha512\n            elif bs == '384':\n                return _sha512.sha384\n    except ImportError:\n        pass  # no extension module, this hash is unsupported.\n\n    raise ValueError('unsupported hash type ' + name)\n\n\ndef __get_openssl_constructor(name):\n    try:\n        f = getattr(_hashlib, 'openssl_' + name)\n        # Allow the C module to raise ValueError.  The function will be\n        # defined but the hash not actually available thanks to OpenSSL.\n        f()\n        # Use the C function directly (very fast)\n        return f\n    except (AttributeError, ValueError):\n        return __get_builtin_constructor(name)\n\n\ndef __py_new(name, string=''):\n    \"\"\"new(name, string='') - Return a new hashing object using the named algorithm;\n    optionally initialized with a string.\n    \"\"\"\n    return __get_builtin_constructor(name)(string)\n\n\ndef __hash_new(name, string=''):\n    \"\"\"new(name, string='') - Return a new hashing object using the named algorithm;\n    optionally initialized with a string.\n    \"\"\"\n    try:\n        return _hashlib.new(name, string)\n    except ValueError:\n        # If the _hashlib module (OpenSSL) doesn't support the named\n        # hash, try using our builtin implementations.\n        # This allows for SHA224/256 and SHA384/512 support even though\n        # the OpenSSL library prior to 0.9.8 doesn't provide them.\n        return __get_builtin_constructor(name)(string)\n\n\ntry:\n    import _hashlib\n    new = __hash_new\n    __get_hash = __get_openssl_constructor\n    algorithms_available = algorithms_available.union(\n        _hashlib.openssl_md_meth_names)\nexcept ImportError:\n    new = __py_new\n    __get_hash = __get_builtin_constructor\n\nfor __func_name in __always_supported:\n    # try them all, some may not work due to the OpenSSL\n    # version not supporting that algorithm.\n    try:\n        globals()[__func_name] = __get_hash(__func_name)\n    except ValueError:\n        import logging\n        logging.exception('code for hash %s was not found.', __func_name)\n\n\ntry:\n    # OpenSSL's PKCS5_PBKDF2_HMAC requires OpenSSL 1.0+ with HMAC and SHA\n    from _hashlib import pbkdf2_hmac\nexcept ImportError:\n    import binascii\n    import struct\n\n    _trans_5C = b\"\".join(chr(x ^ 0x5C) for x in range(256))\n    _trans_36 = b\"\".join(chr(x ^ 0x36) for x in range(256))\n\n    def pbkdf2_hmac(hash_name, password, salt, iterations, dklen=None):\n        \"\"\"Password based key derivation function 2 (PKCS #5 v2.0)\n\n        This Python implementations based on the hmac module about as fast\n        as OpenSSL's PKCS5_PBKDF2_HMAC for short passwords and much faster\n        for long passwords.\n        \"\"\"\n        if not isinstance(hash_name, str):\n            raise TypeError(hash_name)\n\n        if not isinstance(password, (bytes, bytearray)):\n            password = bytes(buffer(password))\n        if not isinstance(salt, (bytes, bytearray)):\n            salt = bytes(buffer(salt))\n\n        # Fast inline HMAC implementation\n        inner = new(hash_name)\n        outer = new(hash_name)\n        blocksize = getattr(inner, 'block_size', 64)\n        if len(password) > blocksize:\n            password = new(hash_name, password).digest()\n        password = password + b'\\x00' * (blocksize - len(password))\n        inner.update(password.translate(_trans_36))\n        outer.update(password.translate(_trans_5C))\n\n        def prf(msg, inner=inner, outer=outer):\n            # PBKDF2_HMAC uses the password as key. We can re-use the same\n            # digest objects and and just update copies to skip initialization.\n            icpy = inner.copy()\n            ocpy = outer.copy()\n            icpy.update(msg)\n            ocpy.update(icpy.digest())\n            return ocpy.digest()\n\n        if iterations < 1:\n            raise ValueError(iterations)\n        if dklen is None:\n            dklen = outer.digest_size\n        if dklen < 1:\n            raise ValueError(dklen)\n\n        hex_format_string = \"%%0%ix\" % (new(hash_name).digest_size * 2)\n\n        dkey = b''\n        loop = 1\n        while len(dkey) < dklen:\n            prev = prf(salt + struct.pack(b'>I', loop))\n            rkey = int(binascii.hexlify(prev), 16)\n            for i in xrange(iterations - 1):\n                prev = prf(prev)\n                rkey ^= int(binascii.hexlify(prev), 16)\n            loop += 1\n            dkey += binascii.unhexlify(hex_format_string % rkey)\n\n        return dkey[:dklen]\n\n# Cleanup locals()\ndel __always_supported, __func_name, __get_hash\ndel __py_new, __hash_new, __get_openssl_constructor\n", 
    "heapq": "# -*- coding: utf-8 -*-\n\n\"\"\"Heap queue algorithm (a.k.a. priority queue).\n\nHeaps are arrays for which a[k] <= a[2*k+1] and a[k] <= a[2*k+2] for\nall k, counting elements from 0.  For the sake of comparison,\nnon-existing elements are considered to be infinite.  The interesting\nproperty of a heap is that a[0] is always its smallest element.\n\nUsage:\n\nheap = []            # creates an empty heap\nheappush(heap, item) # pushes a new item on the heap\nitem = heappop(heap) # pops the smallest item from the heap\nitem = heap[0]       # smallest item on the heap without popping it\nheapify(x)           # transforms list into a heap, in-place, in linear time\nitem = heapreplace(heap, item) # pops and returns smallest item, and adds\n                               # new item; the heap size is unchanged\n\nOur API differs from textbook heap algorithms as follows:\n\n- We use 0-based indexing.  This makes the relationship between the\n  index for a node and the indexes for its children slightly less\n  obvious, but is more suitable since Python uses 0-based indexing.\n\n- Our heappop() method returns the smallest item, not the largest.\n\nThese two make it possible to view the heap as a regular Python list\nwithout surprises: heap[0] is the smallest item, and heap.sort()\nmaintains the heap invariant!\n\"\"\"\n\n# Original code by Kevin O'Connor, augmented by Tim Peters and Raymond Hettinger\n\n__about__ = \"\"\"Heap queues\n\n[explanation by Fran\u00e7ois Pinard]\n\nHeaps are arrays for which a[k] <= a[2*k+1] and a[k] <= a[2*k+2] for\nall k, counting elements from 0.  For the sake of comparison,\nnon-existing elements are considered to be infinite.  The interesting\nproperty of a heap is that a[0] is always its smallest element.\n\nThe strange invariant above is meant to be an efficient memory\nrepresentation for a tournament.  The numbers below are `k', not a[k]:\n\n                                   0\n\n                  1                                 2\n\n          3               4                5               6\n\n      7       8       9       10      11      12      13      14\n\n    15 16   17 18   19 20   21 22   23 24   25 26   27 28   29 30\n\n\nIn the tree above, each cell `k' is topping `2*k+1' and `2*k+2'.  In\nan usual binary tournament we see in sports, each cell is the winner\nover the two cells it tops, and we can trace the winner down the tree\nto see all opponents s/he had.  However, in many computer applications\nof such tournaments, we do not need to trace the history of a winner.\nTo be more memory efficient, when a winner is promoted, we try to\nreplace it by something else at a lower level, and the rule becomes\nthat a cell and the two cells it tops contain three different items,\nbut the top cell \"wins\" over the two topped cells.\n\nIf this heap invariant is protected at all time, index 0 is clearly\nthe overall winner.  The simplest algorithmic way to remove it and\nfind the \"next\" winner is to move some loser (let's say cell 30 in the\ndiagram above) into the 0 position, and then percolate this new 0 down\nthe tree, exchanging values, until the invariant is re-established.\nThis is clearly logarithmic on the total number of items in the tree.\nBy iterating over all items, you get an O(n ln n) sort.\n\nA nice feature of this sort is that you can efficiently insert new\nitems while the sort is going on, provided that the inserted items are\nnot \"better\" than the last 0'th element you extracted.  This is\nespecially useful in simulation contexts, where the tree holds all\nincoming events, and the \"win\" condition means the smallest scheduled\ntime.  When an event schedule other events for execution, they are\nscheduled into the future, so they can easily go into the heap.  So, a\nheap is a good structure for implementing schedulers (this is what I\nused for my MIDI sequencer :-).\n\nVarious structures for implementing schedulers have been extensively\nstudied, and heaps are good for this, as they are reasonably speedy,\nthe speed is almost constant, and the worst case is not much different\nthan the average case.  However, there are other representations which\nare more efficient overall, yet the worst cases might be terrible.\n\nHeaps are also very useful in big disk sorts.  You most probably all\nknow that a big sort implies producing \"runs\" (which are pre-sorted\nsequences, which size is usually related to the amount of CPU memory),\nfollowed by a merging passes for these runs, which merging is often\nvery cleverly organised[1].  It is very important that the initial\nsort produces the longest runs possible.  Tournaments are a good way\nto that.  If, using all the memory available to hold a tournament, you\nreplace and percolate items that happen to fit the current run, you'll\nproduce runs which are twice the size of the memory for random input,\nand much better for input fuzzily ordered.\n\nMoreover, if you output the 0'th item on disk and get an input which\nmay not fit in the current tournament (because the value \"wins\" over\nthe last output value), it cannot fit in the heap, so the size of the\nheap decreases.  The freed memory could be cleverly reused immediately\nfor progressively building a second heap, which grows at exactly the\nsame rate the first heap is melting.  When the first heap completely\nvanishes, you switch heaps and start a new run.  Clever and quite\neffective!\n\nIn a word, heaps are useful memory structures to know.  I use them in\na few applications, and I think it is good to keep a `heap' module\naround. :-)\n\n--------------------\n[1] The disk balancing algorithms which are current, nowadays, are\nmore annoying than clever, and this is a consequence of the seeking\ncapabilities of the disks.  On devices which cannot seek, like big\ntape drives, the story was quite different, and one had to be very\nclever to ensure (far in advance) that each tape movement will be the\nmost effective possible (that is, will best participate at\n\"progressing\" the merge).  Some tapes were even able to read\nbackwards, and this was also used to avoid the rewinding time.\nBelieve me, real good tape sorts were quite spectacular to watch!\nFrom all times, sorting has always been a Great Art! :-)\n\"\"\"\n\n__all__ = ['heappush', 'heappop', 'heapify', 'heapreplace', 'merge',\n           'nlargest', 'nsmallest', 'heappushpop']\n\nfrom itertools import islice, count, imap, izip, tee, chain\nfrom operator import itemgetter\n\ndef cmp_lt(x, y):\n    # Use __lt__ if available; otherwise, try __le__.\n    # In Py3.x, only __lt__ will be called.\n    return (x < y) if hasattr(x, '__lt__') else (not y <= x)\n\ndef heappush(heap, item):\n    \"\"\"Push item onto heap, maintaining the heap invariant.\"\"\"\n    heap.append(item)\n    _siftdown(heap, 0, len(heap)-1)\n\ndef heappop(heap):\n    \"\"\"Pop the smallest item off the heap, maintaining the heap invariant.\"\"\"\n    lastelt = heap.pop()    # raises appropriate IndexError if heap is empty\n    if heap:\n        returnitem = heap[0]\n        heap[0] = lastelt\n        _siftup(heap, 0)\n    else:\n        returnitem = lastelt\n    return returnitem\n\ndef heapreplace(heap, item):\n    \"\"\"Pop and return the current smallest value, and add the new item.\n\n    This is more efficient than heappop() followed by heappush(), and can be\n    more appropriate when using a fixed-size heap.  Note that the value\n    returned may be larger than item!  That constrains reasonable uses of\n    this routine unless written as part of a conditional replacement:\n\n        if item > heap[0]:\n            item = heapreplace(heap, item)\n    \"\"\"\n    returnitem = heap[0]    # raises appropriate IndexError if heap is empty\n    heap[0] = item\n    _siftup(heap, 0)\n    return returnitem\n\ndef heappushpop(heap, item):\n    \"\"\"Fast version of a heappush followed by a heappop.\"\"\"\n    if heap and cmp_lt(heap[0], item):\n        item, heap[0] = heap[0], item\n        _siftup(heap, 0)\n    return item\n\ndef heapify(x):\n    \"\"\"Transform list into a heap, in-place, in O(len(x)) time.\"\"\"\n    n = len(x)\n    # Transform bottom-up.  The largest index there's any point to looking at\n    # is the largest with a child index in-range, so must have 2*i + 1 < n,\n    # or i < (n-1)/2.  If n is even = 2*j, this is (2*j-1)/2 = j-1/2 so\n    # j-1 is the largest, which is n//2 - 1.  If n is odd = 2*j+1, this is\n    # (2*j+1-1)/2 = j so j-1 is the largest, and that's again n//2-1.\n    for i in reversed(xrange(n//2)):\n        _siftup(x, i)\n\ndef _heappushpop_max(heap, item):\n    \"\"\"Maxheap version of a heappush followed by a heappop.\"\"\"\n    if heap and cmp_lt(item, heap[0]):\n        item, heap[0] = heap[0], item\n        _siftup_max(heap, 0)\n    return item\n\ndef _heapify_max(x):\n    \"\"\"Transform list into a maxheap, in-place, in O(len(x)) time.\"\"\"\n    n = len(x)\n    for i in reversed(range(n//2)):\n        _siftup_max(x, i)\n\ndef nlargest(n, iterable):\n    \"\"\"Find the n largest elements in a dataset.\n\n    Equivalent to:  sorted(iterable, reverse=True)[:n]\n    \"\"\"\n    if n < 0:\n        return []\n    it = iter(iterable)\n    result = list(islice(it, n))\n    if not result:\n        return result\n    heapify(result)\n    _heappushpop = heappushpop\n    for elem in it:\n        _heappushpop(result, elem)\n    result.sort(reverse=True)\n    return result\n\ndef nsmallest(n, iterable):\n    \"\"\"Find the n smallest elements in a dataset.\n\n    Equivalent to:  sorted(iterable)[:n]\n    \"\"\"\n    if n < 0:\n        return []\n    it = iter(iterable)\n    result = list(islice(it, n))\n    if not result:\n        return result\n    _heapify_max(result)\n    _heappushpop = _heappushpop_max\n    for elem in it:\n        _heappushpop(result, elem)\n    result.sort()\n    return result\n\n# 'heap' is a heap at all indices >= startpos, except possibly for pos.  pos\n# is the index of a leaf with a possibly out-of-order value.  Restore the\n# heap invariant.\ndef _siftdown(heap, startpos, pos):\n    newitem = heap[pos]\n    # Follow the path to the root, moving parents down until finding a place\n    # newitem fits.\n    while pos > startpos:\n        parentpos = (pos - 1) >> 1\n        parent = heap[parentpos]\n        if cmp_lt(newitem, parent):\n            heap[pos] = parent\n            pos = parentpos\n            continue\n        break\n    heap[pos] = newitem\n\n# The child indices of heap index pos are already heaps, and we want to make\n# a heap at index pos too.  We do this by bubbling the smaller child of\n# pos up (and so on with that child's children, etc) until hitting a leaf,\n# then using _siftdown to move the oddball originally at index pos into place.\n#\n# We *could* break out of the loop as soon as we find a pos where newitem <=\n# both its children, but turns out that's not a good idea, and despite that\n# many books write the algorithm that way.  During a heap pop, the last array\n# element is sifted in, and that tends to be large, so that comparing it\n# against values starting from the root usually doesn't pay (= usually doesn't\n# get us out of the loop early).  See Knuth, Volume 3, where this is\n# explained and quantified in an exercise.\n#\n# Cutting the # of comparisons is important, since these routines have no\n# way to extract \"the priority\" from an array element, so that intelligence\n# is likely to be hiding in custom __cmp__ methods, or in array elements\n# storing (priority, record) tuples.  Comparisons are thus potentially\n# expensive.\n#\n# On random arrays of length 1000, making this change cut the number of\n# comparisons made by heapify() a little, and those made by exhaustive\n# heappop() a lot, in accord with theory.  Here are typical results from 3\n# runs (3 just to demonstrate how small the variance is):\n#\n# Compares needed by heapify     Compares needed by 1000 heappops\n# --------------------------     --------------------------------\n# 1837 cut to 1663               14996 cut to 8680\n# 1855 cut to 1659               14966 cut to 8678\n# 1847 cut to 1660               15024 cut to 8703\n#\n# Building the heap by using heappush() 1000 times instead required\n# 2198, 2148, and 2219 compares:  heapify() is more efficient, when\n# you can use it.\n#\n# The total compares needed by list.sort() on the same lists were 8627,\n# 8627, and 8632 (this should be compared to the sum of heapify() and\n# heappop() compares):  list.sort() is (unsurprisingly!) more efficient\n# for sorting.\n\ndef _siftup(heap, pos):\n    endpos = len(heap)\n    startpos = pos\n    newitem = heap[pos]\n    # Bubble up the smaller child until hitting a leaf.\n    childpos = 2*pos + 1    # leftmost child position\n    while childpos < endpos:\n        # Set childpos to index of smaller child.\n        rightpos = childpos + 1\n        if rightpos < endpos and not cmp_lt(heap[childpos], heap[rightpos]):\n            childpos = rightpos\n        # Move the smaller child up.\n        heap[pos] = heap[childpos]\n        pos = childpos\n        childpos = 2*pos + 1\n    # The leaf at pos is empty now.  Put newitem there, and bubble it up\n    # to its final resting place (by sifting its parents down).\n    heap[pos] = newitem\n    _siftdown(heap, startpos, pos)\n\ndef _siftdown_max(heap, startpos, pos):\n    'Maxheap variant of _siftdown'\n    newitem = heap[pos]\n    # Follow the path to the root, moving parents down until finding a place\n    # newitem fits.\n    while pos > startpos:\n        parentpos = (pos - 1) >> 1\n        parent = heap[parentpos]\n        if cmp_lt(parent, newitem):\n            heap[pos] = parent\n            pos = parentpos\n            continue\n        break\n    heap[pos] = newitem\n\ndef _siftup_max(heap, pos):\n    'Maxheap variant of _siftup'\n    endpos = len(heap)\n    startpos = pos\n    newitem = heap[pos]\n    # Bubble up the larger child until hitting a leaf.\n    childpos = 2*pos + 1    # leftmost child position\n    while childpos < endpos:\n        # Set childpos to index of larger child.\n        rightpos = childpos + 1\n        if rightpos < endpos and not cmp_lt(heap[rightpos], heap[childpos]):\n            childpos = rightpos\n        # Move the larger child up.\n        heap[pos] = heap[childpos]\n        pos = childpos\n        childpos = 2*pos + 1\n    # The leaf at pos is empty now.  Put newitem there, and bubble it up\n    # to its final resting place (by sifting its parents down).\n    heap[pos] = newitem\n    _siftdown_max(heap, startpos, pos)\n\n# If available, use C implementation\ntry:\n    from _heapq import *\nexcept ImportError:\n    pass\n\ndef merge(*iterables):\n    '''Merge multiple sorted inputs into a single sorted output.\n\n    Similar to sorted(itertools.chain(*iterables)) but returns a generator,\n    does not pull the data into memory all at once, and assumes that each of\n    the input streams is already sorted (smallest to largest).\n\n    >>> list(merge([1,3,5,7], [0,2,4,8], [5,10,15,20], [], [25]))\n    [0, 1, 2, 3, 4, 5, 5, 7, 8, 10, 15, 20, 25]\n\n    '''\n    _heappop, _heapreplace, _StopIteration = heappop, heapreplace, StopIteration\n    _len = len\n\n    h = []\n    h_append = h.append\n    for itnum, it in enumerate(map(iter, iterables)):\n        try:\n            next = it.next\n            h_append([next(), itnum, next])\n        except _StopIteration:\n            pass\n    heapify(h)\n\n    while _len(h) > 1:\n        try:\n            while 1:\n                v, itnum, next = s = h[0]\n                yield v\n                s[0] = next()               # raises StopIteration when exhausted\n                _heapreplace(h, s)          # restore heap condition\n        except _StopIteration:\n            _heappop(h)                     # remove empty iterator\n    if h:\n        # fast case when only a single iterator remains\n        v, itnum, next = h[0]\n        yield v\n        for v in next.__self__:\n            yield v\n\n# Extend the implementations of nsmallest and nlargest to use a key= argument\n_nsmallest = nsmallest\ndef nsmallest(n, iterable, key=None):\n    \"\"\"Find the n smallest elements in a dataset.\n\n    Equivalent to:  sorted(iterable, key=key)[:n]\n    \"\"\"\n    # Short-cut for n==1 is to use min() when len(iterable)>0\n    if n == 1:\n        it = iter(iterable)\n        head = list(islice(it, 1))\n        if not head:\n            return []\n        if key is None:\n            return [min(chain(head, it))]\n        return [min(chain(head, it), key=key)]\n\n    # When n>=size, it's faster to use sorted()\n    try:\n        size = len(iterable)\n    except (TypeError, AttributeError):\n        pass\n    else:\n        if n >= size:\n            return sorted(iterable, key=key)[:n]\n\n    # When key is none, use simpler decoration\n    if key is None:\n        it = izip(iterable, count())                        # decorate\n        result = _nsmallest(n, it)\n        return map(itemgetter(0), result)                   # undecorate\n\n    # General case, slowest method\n    in1, in2 = tee(iterable)\n    it = izip(imap(key, in1), count(), in2)                 # decorate\n    result = _nsmallest(n, it)\n    return map(itemgetter(2), result)                       # undecorate\n\n_nlargest = nlargest\ndef nlargest(n, iterable, key=None):\n    \"\"\"Find the n largest elements in a dataset.\n\n    Equivalent to:  sorted(iterable, key=key, reverse=True)[:n]\n    \"\"\"\n\n    # Short-cut for n==1 is to use max() when len(iterable)>0\n    if n == 1:\n        it = iter(iterable)\n        head = list(islice(it, 1))\n        if not head:\n            return []\n        if key is None:\n            return [max(chain(head, it))]\n        return [max(chain(head, it), key=key)]\n\n    # When n>=size, it's faster to use sorted()\n    try:\n        size = len(iterable)\n    except (TypeError, AttributeError):\n        pass\n    else:\n        if n >= size:\n            return sorted(iterable, key=key, reverse=True)[:n]\n\n    # When key is none, use simpler decoration\n    if key is None:\n        it = izip(iterable, count(0,-1))                    # decorate\n        result = _nlargest(n, it)\n        return map(itemgetter(0), result)                   # undecorate\n\n    # General case, slowest method\n    in1, in2 = tee(iterable)\n    it = izip(imap(key, in1), count(0,-1), in2)             # decorate\n    result = _nlargest(n, it)\n    return map(itemgetter(2), result)                       # undecorate\n\nif __name__ == \"__main__\":\n    # Simple sanity test\n    heap = []\n    data = [1, 3, 5, 7, 9, 2, 4, 6, 8, 0]\n    for item in data:\n        heappush(heap, item)\n    sort = []\n    while heap:\n        sort.append(heappop(heap))\n    print sort\n\n    import doctest\n    doctest.testmod()\n", 
    "inspect": "# -*- coding: utf-8 -*-\n\"\"\"Get useful information from live Python objects.\n\nThis module encapsulates the interface provided by the internal special\nattributes (func_*, co_*, im_*, tb_*, etc.) in a friendlier fashion.\nIt also provides some help for examining source code and class layout.\n\nHere are some of the useful functions provided by this module:\n\n    ismodule(), isclass(), ismethod(), isfunction(), isgeneratorfunction(),\n        isgenerator(), istraceback(), isframe(), iscode(), isbuiltin(),\n        isroutine() - check object types\n    getmembers() - get members of an object that satisfy a given condition\n\n    getfile(), getsourcefile(), getsource() - find an object's source code\n    getdoc(), getcomments() - get documentation on an object\n    getmodule() - determine the module that an object came from\n    getclasstree() - arrange classes so as to represent their hierarchy\n\n    getargspec(), getargvalues(), getcallargs() - get info about function arguments\n    formatargspec(), formatargvalues() - format an argument spec\n    getouterframes(), getinnerframes() - get info about frames\n    currentframe() - get the current stack frame\n    stack(), trace() - get info about frames on the stack or in a traceback\n\"\"\"\n\n# This module is in the public domain.  No warranties.\n\n__author__ = 'Ka-Ping Yee <ping@lfw.org>'\n__date__ = '1 Jan 2001'\n\nimport sys\nimport os\nimport types\nimport string\nimport re\nimport dis\nimport imp\nimport tokenize\nimport linecache\nfrom operator import attrgetter\nfrom collections import namedtuple\n\n# These constants are from Include/code.h.\nCO_OPTIMIZED, CO_NEWLOCALS, CO_VARARGS, CO_VARKEYWORDS = 0x1, 0x2, 0x4, 0x8\nCO_NESTED, CO_GENERATOR, CO_NOFREE = 0x10, 0x20, 0x40\n# See Include/object.h\nTPFLAGS_IS_ABSTRACT = 1 << 20\n\n# ----------------------------------------------------------- type-checking\ndef ismodule(object):\n    \"\"\"Return true if the object is a module.\n\n    Module objects provide these attributes:\n        __doc__         documentation string\n        __file__        filename (missing for built-in modules)\"\"\"\n    return isinstance(object, types.ModuleType)\n\ndef isclass(object):\n    \"\"\"Return true if the object is a class.\n\n    Class objects provide these attributes:\n        __doc__         documentation string\n        __module__      name of module in which this class was defined\"\"\"\n    return isinstance(object, (type, types.ClassType))\n\ndef ismethod(object):\n    \"\"\"Return true if the object is an instance method.\n\n    Instance method objects provide these attributes:\n        __doc__         documentation string\n        __name__        name with which this method was defined\n        im_class        class object in which this method belongs\n        im_func         function object containing implementation of method\n        im_self         instance to which this method is bound, or None\"\"\"\n    return isinstance(object, types.MethodType)\n\ndef ismethoddescriptor(object):\n    \"\"\"Return true if the object is a method descriptor.\n\n    But not if ismethod() or isclass() or isfunction() are true.\n\n    This is new in Python 2.2, and, for example, is true of int.__add__.\n    An object passing this test has a __get__ attribute but not a __set__\n    attribute, but beyond that the set of attributes varies.  __name__ is\n    usually sensible, and __doc__ often is.\n\n    Methods implemented via descriptors that also pass one of the other\n    tests return false from the ismethoddescriptor() test, simply because\n    the other tests promise more -- you can, e.g., count on having the\n    im_func attribute (etc) when an object passes ismethod().\"\"\"\n    return (hasattr(object, \"__get__\")\n            and not hasattr(object, \"__set__\") # else it's a data descriptor\n            and not ismethod(object)           # mutual exclusion\n            and not isfunction(object)\n            and not isclass(object))\n\ndef isdatadescriptor(object):\n    \"\"\"Return true if the object is a data descriptor.\n\n    Data descriptors have both a __get__ and a __set__ attribute.  Examples are\n    properties (defined in Python) and getsets and members (defined in C).\n    Typically, data descriptors will also have __name__ and __doc__ attributes\n    (properties, getsets, and members have both of these attributes), but this\n    is not guaranteed.\"\"\"\n    return (hasattr(object, \"__set__\") and hasattr(object, \"__get__\"))\n\nif hasattr(types, 'MemberDescriptorType'):\n    # CPython and equivalent\n    def ismemberdescriptor(object):\n        \"\"\"Return true if the object is a member descriptor.\n\n        Member descriptors are specialized descriptors defined in extension\n        modules.\"\"\"\n        return isinstance(object, types.MemberDescriptorType)\nelse:\n    # Other implementations\n    def ismemberdescriptor(object):\n        \"\"\"Return true if the object is a member descriptor.\n\n        Member descriptors are specialized descriptors defined in extension\n        modules.\"\"\"\n        return False\n\nif hasattr(types, 'GetSetDescriptorType'):\n    # CPython and equivalent\n    def isgetsetdescriptor(object):\n        \"\"\"Return true if the object is a getset descriptor.\n\n        getset descriptors are specialized descriptors defined in extension\n        modules.\"\"\"\n        return isinstance(object, types.GetSetDescriptorType)\nelse:\n    # Other implementations\n    def isgetsetdescriptor(object):\n        \"\"\"Return true if the object is a getset descriptor.\n\n        getset descriptors are specialized descriptors defined in extension\n        modules.\"\"\"\n        return False\n\ndef isfunction(object):\n    \"\"\"Return true if the object is a user-defined function.\n\n    Function objects provide these attributes:\n        __doc__         documentation string\n        __name__        name with which this function was defined\n        func_code       code object containing compiled function bytecode\n        func_defaults   tuple of any default values for arguments\n        func_doc        (same as __doc__)\n        func_globals    global namespace in which this function was defined\n        func_name       (same as __name__)\"\"\"\n    return isinstance(object, types.FunctionType)\n\ndef isgeneratorfunction(object):\n    \"\"\"Return true if the object is a user-defined generator function.\n\n    Generator function objects provides same attributes as functions.\n\n    See help(isfunction) for attributes listing.\"\"\"\n    return bool((isfunction(object) or ismethod(object)) and\n                object.func_code.co_flags & CO_GENERATOR)\n\ndef isgenerator(object):\n    \"\"\"Return true if the object is a generator.\n\n    Generator objects provide these attributes:\n        __iter__        defined to support iteration over container\n        close           raises a new GeneratorExit exception inside the\n                        generator to terminate the iteration\n        gi_code         code object\n        gi_frame        frame object or possibly None once the generator has\n                        been exhausted\n        gi_running      set to 1 when generator is executing, 0 otherwise\n        next            return the next item from the container\n        send            resumes the generator and \"sends\" a value that becomes\n                        the result of the current yield-expression\n        throw           used to raise an exception inside the generator\"\"\"\n    return isinstance(object, types.GeneratorType)\n\ndef istraceback(object):\n    \"\"\"Return true if the object is a traceback.\n\n    Traceback objects provide these attributes:\n        tb_frame        frame object at this level\n        tb_lasti        index of last attempted instruction in bytecode\n        tb_lineno       current line number in Python source code\n        tb_next         next inner traceback object (called by this level)\"\"\"\n    return isinstance(object, types.TracebackType)\n\ndef isframe(object):\n    \"\"\"Return true if the object is a frame object.\n\n    Frame objects provide these attributes:\n        f_back          next outer frame object (this frame's caller)\n        f_builtins      built-in namespace seen by this frame\n        f_code          code object being executed in this frame\n        f_exc_traceback traceback if raised in this frame, or None\n        f_exc_type      exception type if raised in this frame, or None\n        f_exc_value     exception value if raised in this frame, or None\n        f_globals       global namespace seen by this frame\n        f_lasti         index of last attempted instruction in bytecode\n        f_lineno        current line number in Python source code\n        f_locals        local namespace seen by this frame\n        f_restricted    0 or 1 if frame is in restricted execution mode\n        f_trace         tracing function for this frame, or None\"\"\"\n    return isinstance(object, types.FrameType)\n\ndef iscode(object):\n    \"\"\"Return true if the object is a code object.\n\n    Code objects provide these attributes:\n        co_argcount     number of arguments (not including * or ** args)\n        co_code         string of raw compiled bytecode\n        co_consts       tuple of constants used in the bytecode\n        co_filename     name of file in which this code object was created\n        co_firstlineno  number of first line in Python source code\n        co_flags        bitmap: 1=optimized | 2=newlocals | 4=*arg | 8=**arg\n        co_lnotab       encoded mapping of line numbers to bytecode indices\n        co_name         name with which this code object was defined\n        co_names        tuple of names of local variables\n        co_nlocals      number of local variables\n        co_stacksize    virtual machine stack space required\n        co_varnames     tuple of names of arguments and local variables\"\"\"\n    return isinstance(object, types.CodeType)\n\ndef isbuiltin(object):\n    \"\"\"Return true if the object is a built-in function or method.\n\n    Built-in functions and methods provide these attributes:\n        __doc__         documentation string\n        __name__        original name of this function or method\n        __self__        instance to which a method is bound, or None\"\"\"\n    return isinstance(object, types.BuiltinFunctionType)\n\ndef isroutine(object):\n    \"\"\"Return true if the object is any kind of function or method.\"\"\"\n    return (isbuiltin(object)\n            or isfunction(object)\n            or ismethod(object)\n            or ismethoddescriptor(object))\n\ndef isabstract(object):\n    \"\"\"Return true if the object is an abstract base class (ABC).\"\"\"\n    return bool(isinstance(object, type) and object.__flags__ & TPFLAGS_IS_ABSTRACT)\n\ndef getmembers(object, predicate=None):\n    \"\"\"Return all members of an object as (name, value) pairs sorted by name.\n    Optionally, only return members that satisfy a given predicate.\"\"\"\n    results = []\n    for key in dir(object):\n        try:\n            value = getattr(object, key)\n        except AttributeError:\n            continue\n        if not predicate or predicate(value):\n            results.append((key, value))\n    results.sort()\n    return results\n\nAttribute = namedtuple('Attribute', 'name kind defining_class object')\n\ndef classify_class_attrs(cls):\n    \"\"\"Return list of attribute-descriptor tuples.\n\n    For each name in dir(cls), the return list contains a 4-tuple\n    with these elements:\n\n        0. The name (a string).\n\n        1. The kind of attribute this is, one of these strings:\n               'class method'    created via classmethod()\n               'static method'   created via staticmethod()\n               'property'        created via property()\n               'method'          any other flavor of method\n               'data'            not a method\n\n        2. The class which defined this attribute (a class).\n\n        3. The object as obtained directly from the defining class's\n           __dict__, not via getattr.  This is especially important for\n           data attributes:  C.data is just a data object, but\n           C.__dict__['data'] may be a data descriptor with additional\n           info, like a __doc__ string.\n    \"\"\"\n\n    mro = getmro(cls)\n    names = dir(cls)\n    result = []\n    for name in names:\n        # Get the object associated with the name, and where it was defined.\n        # Getting an obj from the __dict__ sometimes reveals more than\n        # using getattr.  Static and class methods are dramatic examples.\n        # Furthermore, some objects may raise an Exception when fetched with\n        # getattr(). This is the case with some descriptors (bug #1785).\n        # Thus, we only use getattr() as a last resort.\n        homecls = None\n        for base in (cls,) + mro:\n            if name in base.__dict__:\n                obj = base.__dict__[name]\n                homecls = base\n                break\n        else:\n            obj = getattr(cls, name)\n            homecls = getattr(obj, \"__objclass__\", homecls)\n\n        # Classify the object.\n        if isinstance(obj, staticmethod):\n            kind = \"static method\"\n        elif isinstance(obj, classmethod):\n            kind = \"class method\"\n        elif isinstance(obj, property):\n            kind = \"property\"\n        elif ismethoddescriptor(obj):\n            kind = \"method\"\n        elif isdatadescriptor(obj):\n            kind = \"data\"\n        else:\n            obj_via_getattr = getattr(cls, name)\n            if (ismethod(obj_via_getattr) or\n                ismethoddescriptor(obj_via_getattr)):\n                kind = \"method\"\n            else:\n                kind = \"data\"\n            obj = obj_via_getattr\n\n        result.append(Attribute(name, kind, homecls, obj))\n\n    return result\n\n# ----------------------------------------------------------- class helpers\ndef _searchbases(cls, accum):\n    # Simulate the \"classic class\" search order.\n    if cls in accum:\n        return\n    accum.append(cls)\n    for base in cls.__bases__:\n        _searchbases(base, accum)\n\ndef getmro(cls):\n    \"Return tuple of base classes (including cls) in method resolution order.\"\n    if hasattr(cls, \"__mro__\"):\n        return cls.__mro__\n    else:\n        result = []\n        _searchbases(cls, result)\n        return tuple(result)\n\n# -------------------------------------------------- source code extraction\ndef indentsize(line):\n    \"\"\"Return the indent size, in spaces, at the start of a line of text.\"\"\"\n    expline = string.expandtabs(line)\n    return len(expline) - len(string.lstrip(expline))\n\ndef getdoc(object):\n    \"\"\"Get the documentation string for an object.\n\n    All tabs are expanded to spaces.  To clean up docstrings that are\n    indented to line up with blocks of code, any whitespace than can be\n    uniformly removed from the second line onwards is removed.\"\"\"\n    try:\n        doc = object.__doc__\n    except AttributeError:\n        return None\n    if not isinstance(doc, types.StringTypes):\n        return None\n    return cleandoc(doc)\n\ndef cleandoc(doc):\n    \"\"\"Clean up indentation from docstrings.\n\n    Any whitespace that can be uniformly removed from the second line\n    onwards is removed.\"\"\"\n    try:\n        lines = string.split(string.expandtabs(doc), '\\n')\n    except UnicodeError:\n        return None\n    else:\n        # Find minimum indentation of any non-blank lines after first line.\n        margin = sys.maxint\n        for line in lines[1:]:\n            content = len(string.lstrip(line))\n            if content:\n                indent = len(line) - content\n                margin = min(margin, indent)\n        # Remove indentation.\n        if lines:\n            lines[0] = lines[0].lstrip()\n        if margin < sys.maxint:\n            for i in range(1, len(lines)): lines[i] = lines[i][margin:]\n        # Remove any trailing or leading blank lines.\n        while lines and not lines[-1]:\n            lines.pop()\n        while lines and not lines[0]:\n            lines.pop(0)\n        return string.join(lines, '\\n')\n\ndef getfile(object):\n    \"\"\"Work out which source or compiled file an object was defined in.\"\"\"\n    if ismodule(object):\n        if hasattr(object, '__file__'):\n            return object.__file__\n        raise TypeError('{!r} is a built-in module'.format(object))\n    if isclass(object):\n        object = sys.modules.get(object.__module__)\n        if hasattr(object, '__file__'):\n            return object.__file__\n        raise TypeError('{!r} is a built-in class'.format(object))\n    if ismethod(object):\n        object = object.im_func\n    if isfunction(object):\n        object = object.func_code\n    if istraceback(object):\n        object = object.tb_frame\n    if isframe(object):\n        object = object.f_code\n    if iscode(object):\n        return object.co_filename\n    raise TypeError('{!r} is not a module, class, method, '\n                    'function, traceback, frame, or code object'.format(object))\n\nModuleInfo = namedtuple('ModuleInfo', 'name suffix mode module_type')\n\ndef getmoduleinfo(path):\n    \"\"\"Get the module name, suffix, mode, and module type for a given file.\"\"\"\n    filename = os.path.basename(path)\n    suffixes = map(lambda info:\n                   (-len(info[0]), info[0], info[1], info[2]),\n                    imp.get_suffixes())\n    suffixes.sort() # try longest suffixes first, in case they overlap\n    for neglen, suffix, mode, mtype in suffixes:\n        if filename[neglen:] == suffix:\n            return ModuleInfo(filename[:neglen], suffix, mode, mtype)\n\ndef getmodulename(path):\n    \"\"\"Return the module name for a given file, or None.\"\"\"\n    info = getmoduleinfo(path)\n    if info: return info[0]\n\ndef getsourcefile(object):\n    \"\"\"Return the filename that can be used to locate an object's source.\n    Return None if no way can be identified to get the source.\n    \"\"\"\n    filename = getfile(object)\n    if string.lower(filename[-4:]) in ('.pyc', '.pyo'):\n        filename = filename[:-4] + '.py'\n    for suffix, mode, kind in imp.get_suffixes():\n        if 'b' in mode and string.lower(filename[-len(suffix):]) == suffix:\n            # Looks like a binary file.  We want to only return a text file.\n            return None\n    if os.path.exists(filename):\n        return filename\n    # only return a non-existent filename if the module has a PEP 302 loader\n    if hasattr(getmodule(object, filename), '__loader__'):\n        return filename\n    # or it is in the linecache\n    if filename in linecache.cache:\n        return filename\n\ndef getabsfile(object, _filename=None):\n    \"\"\"Return an absolute path to the source or compiled file for an object.\n\n    The idea is for each object to have a unique origin, so this routine\n    normalizes the result as much as possible.\"\"\"\n    if _filename is None:\n        _filename = getsourcefile(object) or getfile(object)\n    return os.path.normcase(os.path.abspath(_filename))\n\nmodulesbyfile = {}\n_filesbymodname = {}\n\ndef getmodule(object, _filename=None):\n    \"\"\"Return the module an object was defined in, or None if not found.\"\"\"\n    if ismodule(object):\n        return object\n    if hasattr(object, '__module__'):\n        return sys.modules.get(object.__module__)\n    # Try the filename to modulename cache\n    if _filename is not None and _filename in modulesbyfile:\n        return sys.modules.get(modulesbyfile[_filename])\n    # Try the cache again with the absolute file name\n    try:\n        file = getabsfile(object, _filename)\n    except TypeError:\n        return None\n    if file in modulesbyfile:\n        return sys.modules.get(modulesbyfile[file])\n    # Update the filename to module name cache and check yet again\n    # Copy sys.modules in order to cope with changes while iterating\n    for modname, module in sys.modules.items():\n        if ismodule(module) and hasattr(module, '__file__'):\n            f = module.__file__\n            if f == _filesbymodname.get(modname, None):\n                # Have already mapped this module, so skip it\n                continue\n            _filesbymodname[modname] = f\n            f = getabsfile(module)\n            # Always map to the name the module knows itself by\n            modulesbyfile[f] = modulesbyfile[\n                os.path.realpath(f)] = module.__name__\n    if file in modulesbyfile:\n        return sys.modules.get(modulesbyfile[file])\n    # Check the main module\n    main = sys.modules['__main__']\n    if not hasattr(object, '__name__'):\n        return None\n    if hasattr(main, object.__name__):\n        mainobject = getattr(main, object.__name__)\n        if mainobject is object:\n            return main\n    # Check builtins\n    builtin = sys.modules['__builtin__']\n    if hasattr(builtin, object.__name__):\n        builtinobject = getattr(builtin, object.__name__)\n        if builtinobject is object:\n            return builtin\n\ndef findsource(object):\n    \"\"\"Return the entire source file and starting line number for an object.\n\n    The argument may be a module, class, method, function, traceback, frame,\n    or code object.  The source code is returned as a list of all the lines\n    in the file and the line number indexes a line in that list.  An IOError\n    is raised if the source code cannot be retrieved.\"\"\"\n\n    file = getfile(object)\n    sourcefile = getsourcefile(object)\n    if not sourcefile and file[:1] + file[-1:] != '<>':\n        raise IOError('source code not available')\n    file = sourcefile if sourcefile else file\n\n    module = getmodule(object, file)\n    if module:\n        lines = linecache.getlines(file, module.__dict__)\n    else:\n        lines = linecache.getlines(file)\n    if not lines:\n        raise IOError('could not get source code')\n\n    if ismodule(object):\n        return lines, 0\n\n    if isclass(object):\n        name = object.__name__\n        pat = re.compile(r'^(\\s*)class\\s*' + name + r'\\b')\n        # make some effort to find the best matching class definition:\n        # use the one with the least indentation, which is the one\n        # that's most probably not inside a function definition.\n        candidates = []\n        for i in range(len(lines)):\n            match = pat.match(lines[i])\n            if match:\n                # if it's at toplevel, it's already the best one\n                if lines[i][0] == 'c':\n                    return lines, i\n                # else add whitespace to candidate list\n                candidates.append((match.group(1), i))\n        if candidates:\n            # this will sort by whitespace, and by line number,\n            # less whitespace first\n            candidates.sort()\n            return lines, candidates[0][1]\n        else:\n            raise IOError('could not find class definition')\n\n    if ismethod(object):\n        object = object.im_func\n    if isfunction(object):\n        object = object.func_code\n    if istraceback(object):\n        object = object.tb_frame\n    if isframe(object):\n        object = object.f_code\n    if iscode(object):\n        if not hasattr(object, 'co_firstlineno'):\n            raise IOError('could not find function definition')\n        lnum = object.co_firstlineno - 1\n        pat = re.compile(r'^(\\s*def\\s)|(.*(?<!\\w)lambda(:|\\s))|^(\\s*@)')\n        while lnum > 0:\n            if pat.match(lines[lnum]): break\n            lnum = lnum - 1\n        return lines, lnum\n    raise IOError('could not find code object')\n\ndef getcomments(object):\n    \"\"\"Get lines of comments immediately preceding an object's source code.\n\n    Returns None when source can't be found.\n    \"\"\"\n    try:\n        lines, lnum = findsource(object)\n    except (IOError, TypeError):\n        return None\n\n    if ismodule(object):\n        # Look for a comment block at the top of the file.\n        start = 0\n        if lines and lines[0][:2] == '#!': start = 1\n        while start < len(lines) and string.strip(lines[start]) in ('', '#'):\n            start = start + 1\n        if start < len(lines) and lines[start][:1] == '#':\n            comments = []\n            end = start\n            while end < len(lines) and lines[end][:1] == '#':\n                comments.append(string.expandtabs(lines[end]))\n                end = end + 1\n            return string.join(comments, '')\n\n    # Look for a preceding block of comments at the same indentation.\n    elif lnum > 0:\n        indent = indentsize(lines[lnum])\n        end = lnum - 1\n        if end >= 0 and string.lstrip(lines[end])[:1] == '#' and \\\n            indentsize(lines[end]) == indent:\n            comments = [string.lstrip(string.expandtabs(lines[end]))]\n            if end > 0:\n                end = end - 1\n                comment = string.lstrip(string.expandtabs(lines[end]))\n                while comment[:1] == '#' and indentsize(lines[end]) == indent:\n                    comments[:0] = [comment]\n                    end = end - 1\n                    if end < 0: break\n                    comment = string.lstrip(string.expandtabs(lines[end]))\n            while comments and string.strip(comments[0]) == '#':\n                comments[:1] = []\n            while comments and string.strip(comments[-1]) == '#':\n                comments[-1:] = []\n            return string.join(comments, '')\n\nclass EndOfBlock(Exception): pass\n\nclass BlockFinder:\n    \"\"\"Provide a tokeneater() method to detect the end of a code block.\"\"\"\n    def __init__(self):\n        self.indent = 0\n        self.islambda = False\n        self.started = False\n        self.passline = False\n        self.last = 1\n\n    def tokeneater(self, type, token, srow_scol, erow_ecol, line):\n        srow, scol = srow_scol\n        erow, ecol = erow_ecol\n        if not self.started:\n            # look for the first \"def\", \"class\" or \"lambda\"\n            if token in (\"def\", \"class\", \"lambda\"):\n                if token == \"lambda\":\n                    self.islambda = True\n                self.started = True\n            self.passline = True    # skip to the end of the line\n        elif type == tokenize.NEWLINE:\n            self.passline = False   # stop skipping when a NEWLINE is seen\n            self.last = srow\n            if self.islambda:       # lambdas always end at the first NEWLINE\n                raise EndOfBlock\n        elif self.passline:\n            pass\n        elif type == tokenize.INDENT:\n            self.indent = self.indent + 1\n            self.passline = True\n        elif type == tokenize.DEDENT:\n            self.indent = self.indent - 1\n            # the end of matching indent/dedent pairs end a block\n            # (note that this only works for \"def\"/\"class\" blocks,\n            #  not e.g. for \"if: else:\" or \"try: finally:\" blocks)\n            if self.indent <= 0:\n                raise EndOfBlock\n        elif self.indent == 0 and type not in (tokenize.COMMENT, tokenize.NL):\n            # any other token on the same indentation level end the previous\n            # block as well, except the pseudo-tokens COMMENT and NL.\n            raise EndOfBlock\n\ndef getblock(lines):\n    \"\"\"Extract the block of code at the top of the given list of lines.\"\"\"\n    blockfinder = BlockFinder()\n    try:\n        tokenize.tokenize(iter(lines).next, blockfinder.tokeneater)\n    except (EndOfBlock, IndentationError):\n        pass\n    return lines[:blockfinder.last]\n\ndef getsourcelines(object):\n    \"\"\"Return a list of source lines and starting line number for an object.\n\n    The argument may be a module, class, method, function, traceback, frame,\n    or code object.  The source code is returned as a list of the lines\n    corresponding to the object and the line number indicates where in the\n    original source file the first line of code was found.  An IOError is\n    raised if the source code cannot be retrieved.\"\"\"\n    lines, lnum = findsource(object)\n\n    if ismodule(object): return lines, 0\n    else: return getblock(lines[lnum:]), lnum + 1\n\ndef getsource(object):\n    \"\"\"Return the text of the source code for an object.\n\n    The argument may be a module, class, method, function, traceback, frame,\n    or code object.  The source code is returned as a single string.  An\n    IOError is raised if the source code cannot be retrieved.\"\"\"\n    lines, lnum = getsourcelines(object)\n    return string.join(lines, '')\n\n# --------------------------------------------------- class tree extraction\ndef walktree(classes, children, parent):\n    \"\"\"Recursive helper function for getclasstree().\"\"\"\n    results = []\n    classes.sort(key=attrgetter('__module__', '__name__'))\n    for c in classes:\n        results.append((c, c.__bases__))\n        if c in children:\n            results.append(walktree(children[c], children, c))\n    return results\n\ndef getclasstree(classes, unique=0):\n    \"\"\"Arrange the given list of classes into a hierarchy of nested lists.\n\n    Where a nested list appears, it contains classes derived from the class\n    whose entry immediately precedes the list.  Each entry is a 2-tuple\n    containing a class and a tuple of its base classes.  If the 'unique'\n    argument is true, exactly one entry appears in the returned structure\n    for each class in the given list.  Otherwise, classes using multiple\n    inheritance and their descendants will appear multiple times.\"\"\"\n    children = {}\n    roots = []\n    for c in classes:\n        if c.__bases__:\n            for parent in c.__bases__:\n                if not parent in children:\n                    children[parent] = []\n                if c not in children[parent]:\n                    children[parent].append(c)\n                if unique and parent in classes: break\n        elif c not in roots:\n            roots.append(c)\n    for parent in children:\n        if parent not in classes:\n            roots.append(parent)\n    return walktree(roots, children, None)\n\n# ------------------------------------------------ argument list extraction\nArguments = namedtuple('Arguments', 'args varargs keywords')\n\ndef getargs(co):\n    \"\"\"Get information about the arguments accepted by a code object.\n\n    Three things are returned: (args, varargs, varkw), where 'args' is\n    a list of argument names (possibly containing nested lists), and\n    'varargs' and 'varkw' are the names of the * and ** arguments or None.\"\"\"\n\n    if not iscode(co):\n        if hasattr(len, 'func_code') and type(co) is type(len.func_code):\n            # PyPy extension: built-in function objects have a func_code too.\n            # There is no co_code on it, but co_argcount and co_varnames and\n            # co_flags are present.\n            pass\n        else:\n            raise TypeError('{!r} is not a code object'.format(co))\n\n    code = getattr(co, 'co_code', '')\n    nargs = co.co_argcount\n    names = co.co_varnames\n    args = list(names[:nargs])\n    step = 0\n\n    # The following acrobatics are for anonymous (tuple) arguments.\n    for i in range(nargs):\n        if args[i][:1] in ('', '.'):\n            stack, remain, count = [], [], []\n            while step < len(code):\n                op = ord(code[step])\n                step = step + 1\n                if op >= dis.HAVE_ARGUMENT:\n                    opname = dis.opname[op]\n                    value = ord(code[step]) + ord(code[step+1])*256\n                    step = step + 2\n                    if opname in ('UNPACK_TUPLE', 'UNPACK_SEQUENCE'):\n                        remain.append(value)\n                        count.append(value)\n                    elif opname == 'STORE_FAST':\n                        stack.append(names[value])\n\n                        # Special case for sublists of length 1: def foo((bar))\n                        # doesn't generate the UNPACK_TUPLE bytecode, so if\n                        # `remain` is empty here, we have such a sublist.\n                        if not remain:\n                            stack[0] = [stack[0]]\n                            break\n                        else:\n                            remain[-1] = remain[-1] - 1\n                            while remain[-1] == 0:\n                                remain.pop()\n                                size = count.pop()\n                                stack[-size:] = [stack[-size:]]\n                                if not remain: break\n                                remain[-1] = remain[-1] - 1\n                            if not remain: break\n            args[i] = stack[0]\n\n    varargs = None\n    if co.co_flags & CO_VARARGS:\n        varargs = co.co_varnames[nargs]\n        nargs = nargs + 1\n    varkw = None\n    if co.co_flags & CO_VARKEYWORDS:\n        varkw = co.co_varnames[nargs]\n    return Arguments(args, varargs, varkw)\n\nArgSpec = namedtuple('ArgSpec', 'args varargs keywords defaults')\n\ndef getargspec(func):\n    \"\"\"Get the names and default values of a function's arguments.\n\n    A tuple of four things is returned: (args, varargs, varkw, defaults).\n    'args' is a list of the argument names (it may contain nested lists).\n    'varargs' and 'varkw' are the names of the * and ** arguments or None.\n    'defaults' is an n-tuple of the default values of the last n arguments.\n    \"\"\"\n\n    if ismethod(func):\n        func = func.im_func\n    if not (isfunction(func) or\n            isbuiltin(func) and hasattr(func, 'func_code')):\n            # PyPy extension: this works for built-in functions too\n        raise TypeError('{!r} is not a Python function'.format(func))\n    args, varargs, varkw = getargs(func.func_code)\n    return ArgSpec(args, varargs, varkw, func.func_defaults)\n\nArgInfo = namedtuple('ArgInfo', 'args varargs keywords locals')\n\ndef getargvalues(frame):\n    \"\"\"Get information about arguments passed into a particular frame.\n\n    A tuple of four things is returned: (args, varargs, varkw, locals).\n    'args' is a list of the argument names (it may contain nested lists).\n    'varargs' and 'varkw' are the names of the * and ** arguments or None.\n    'locals' is the locals dictionary of the given frame.\"\"\"\n    args, varargs, varkw = getargs(frame.f_code)\n    return ArgInfo(args, varargs, varkw, frame.f_locals)\n\ndef joinseq(seq):\n    if len(seq) == 1:\n        return '(' + seq[0] + ',)'\n    else:\n        return '(' + string.join(seq, ', ') + ')'\n\ndef strseq(object, convert, join=joinseq):\n    \"\"\"Recursively walk a sequence, stringifying each element.\"\"\"\n    if type(object) in (list, tuple):\n        return join(map(lambda o, c=convert, j=join: strseq(o, c, j), object))\n    else:\n        return convert(object)\n\ndef formatargspec(args, varargs=None, varkw=None, defaults=None,\n                  formatarg=str,\n                  formatvarargs=lambda name: '*' + name,\n                  formatvarkw=lambda name: '**' + name,\n                  formatvalue=lambda value: '=' + repr(value),\n                  join=joinseq):\n    \"\"\"Format an argument spec from the 4 values returned by getargspec.\n\n    The first four arguments are (args, varargs, varkw, defaults).  The\n    other four arguments are the corresponding optional formatting functions\n    that are called to turn names and values into strings.  The ninth\n    argument is an optional function to format the sequence of arguments.\"\"\"\n    specs = []\n    if defaults:\n        firstdefault = len(args) - len(defaults)\n    for i, arg in enumerate(args):\n        spec = strseq(arg, formatarg, join)\n        if defaults and i >= firstdefault:\n            spec = spec + formatvalue(defaults[i - firstdefault])\n        specs.append(spec)\n    if varargs is not None:\n        specs.append(formatvarargs(varargs))\n    if varkw is not None:\n        specs.append(formatvarkw(varkw))\n    return '(' + string.join(specs, ', ') + ')'\n\ndef formatargvalues(args, varargs, varkw, locals,\n                    formatarg=str,\n                    formatvarargs=lambda name: '*' + name,\n                    formatvarkw=lambda name: '**' + name,\n                    formatvalue=lambda value: '=' + repr(value),\n                    join=joinseq):\n    \"\"\"Format an argument spec from the 4 values returned by getargvalues.\n\n    The first four arguments are (args, varargs, varkw, locals).  The\n    next four arguments are the corresponding optional formatting functions\n    that are called to turn names and values into strings.  The ninth\n    argument is an optional function to format the sequence of arguments.\"\"\"\n    def convert(name, locals=locals,\n                formatarg=formatarg, formatvalue=formatvalue):\n        return formatarg(name) + formatvalue(locals[name])\n    specs = []\n    for i in range(len(args)):\n        specs.append(strseq(args[i], convert, join))\n    if varargs:\n        specs.append(formatvarargs(varargs) + formatvalue(locals[varargs]))\n    if varkw:\n        specs.append(formatvarkw(varkw) + formatvalue(locals[varkw]))\n    return '(' + string.join(specs, ', ') + ')'\n\ndef getcallargs(func, *positional, **named):\n    \"\"\"Get the mapping of arguments to values.\n\n    A dict is returned, with keys the function argument names (including the\n    names of the * and ** arguments, if any), and values the respective bound\n    values from 'positional' and 'named'.\"\"\"\n    args, varargs, varkw, defaults = getargspec(func)\n    f_name = func.__name__\n    arg2value = {}\n\n    # The following closures are basically because of tuple parameter unpacking.\n    assigned_tuple_params = []\n    def assign(arg, value):\n        if isinstance(arg, str):\n            arg2value[arg] = value\n        else:\n            assigned_tuple_params.append(arg)\n            value = iter(value)\n            for i, subarg in enumerate(arg):\n                try:\n                    subvalue = next(value)\n                except StopIteration:\n                    raise ValueError('need more than %d %s to unpack' %\n                                     (i, 'values' if i > 1 else 'value'))\n                assign(subarg,subvalue)\n            try:\n                next(value)\n            except StopIteration:\n                pass\n            else:\n                raise ValueError('too many values to unpack')\n    def is_assigned(arg):\n        if isinstance(arg,str):\n            return arg in arg2value\n        return arg in assigned_tuple_params\n    if ismethod(func) and func.im_self is not None:\n        # implicit 'self' (or 'cls' for classmethods) argument\n        positional = (func.im_self,) + positional\n    num_pos = len(positional)\n    num_total = num_pos + len(named)\n    num_args = len(args)\n    num_defaults = len(defaults) if defaults else 0\n    for arg, value in zip(args, positional):\n        assign(arg, value)\n    if varargs:\n        if num_pos > num_args:\n            assign(varargs, positional[-(num_pos-num_args):])\n        else:\n            assign(varargs, ())\n    elif 0 < num_args < num_pos:\n        raise TypeError('%s() takes %s %d %s (%d given)' % (\n            f_name, 'at most' if defaults else 'exactly', num_args,\n            'arguments' if num_args > 1 else 'argument', num_total))\n    elif num_args == 0 and num_total:\n        if varkw:\n            if num_pos:\n                # XXX: We should use num_pos, but Python also uses num_total:\n                raise TypeError('%s() takes exactly 0 arguments '\n                                '(%d given)' % (f_name, num_total))\n        else:\n            raise TypeError('%s() takes no arguments (%d given)' %\n                            (f_name, num_total))\n    for arg in args:\n        if isinstance(arg, str) and arg in named:\n            if is_assigned(arg):\n                raise TypeError(\"%s() got multiple values for keyword \"\n                                \"argument '%s'\" % (f_name, arg))\n            else:\n                assign(arg, named.pop(arg))\n    if defaults:    # fill in any missing values with the defaults\n        for arg, value in zip(args[-num_defaults:], defaults):\n            if not is_assigned(arg):\n                assign(arg, value)\n    if varkw:\n        assign(varkw, named)\n    elif named:\n        unexpected = next(iter(named))\n        if isinstance(unexpected, unicode):\n            unexpected = unexpected.encode(sys.getdefaultencoding(), 'replace')\n        raise TypeError(\"%s() got an unexpected keyword argument '%s'\" %\n                        (f_name, unexpected))\n    unassigned = num_args - len([arg for arg in args if is_assigned(arg)])\n    if unassigned:\n        num_required = num_args - num_defaults\n        raise TypeError('%s() takes %s %d %s (%d given)' % (\n            f_name, 'at least' if defaults else 'exactly', num_required,\n            'arguments' if num_required > 1 else 'argument', num_total))\n    return arg2value\n\n# -------------------------------------------------- stack frame extraction\n\nTraceback = namedtuple('Traceback', 'filename lineno function code_context index')\n\ndef getframeinfo(frame, context=1):\n    \"\"\"Get information about a frame or traceback object.\n\n    A tuple of five things is returned: the filename, the line number of\n    the current line, the function name, a list of lines of context from\n    the source code, and the index of the current line within that list.\n    The optional second argument specifies the number of lines of context\n    to return, which are centered around the current line.\"\"\"\n    if istraceback(frame):\n        lineno = frame.tb_lineno\n        frame = frame.tb_frame\n    else:\n        lineno = frame.f_lineno\n    if not isframe(frame):\n        raise TypeError('{!r} is not a frame or traceback object'.format(frame))\n\n    filename = getsourcefile(frame) or getfile(frame)\n    if context > 0:\n        start = lineno - 1 - context//2\n        try:\n            lines, lnum = findsource(frame)\n        except IOError:\n            lines = index = None\n        else:\n            start = max(start, 1)\n            start = max(0, min(start, len(lines) - context))\n            lines = lines[start:start+context]\n            index = lineno - 1 - start\n    else:\n        lines = index = None\n\n    return Traceback(filename, lineno, frame.f_code.co_name, lines, index)\n\ndef getlineno(frame):\n    \"\"\"Get the line number from a frame object, allowing for optimization.\"\"\"\n    # FrameType.f_lineno is now a descriptor that grovels co_lnotab\n    return frame.f_lineno\n\ndef getouterframes(frame, context=1):\n    \"\"\"Get a list of records for a frame and all higher (calling) frames.\n\n    Each record contains a frame object, filename, line number, function\n    name, a list of lines of context, and index within the context.\"\"\"\n    framelist = []\n    while frame:\n        framelist.append((frame,) + getframeinfo(frame, context))\n        frame = frame.f_back\n    return framelist\n\ndef getinnerframes(tb, context=1):\n    \"\"\"Get a list of records for a traceback's frame and all lower frames.\n\n    Each record contains a frame object, filename, line number, function\n    name, a list of lines of context, and index within the context.\"\"\"\n    framelist = []\n    while tb:\n        framelist.append((tb.tb_frame,) + getframeinfo(tb, context))\n        tb = tb.tb_next\n    return framelist\n\nif hasattr(sys, '_getframe'):\n    currentframe = sys._getframe\nelse:\n    currentframe = lambda _=None: None\n\ndef stack(context=1):\n    \"\"\"Return a list of records for the stack above the caller's frame.\"\"\"\n    return getouterframes(sys._getframe(1), context)\n\ndef trace(context=1):\n    \"\"\"Return a list of records for the stack below the current exception.\"\"\"\n    return getinnerframes(sys.exc_info()[2], context)\n", 
    "io": "\"\"\"The io module provides the Python interfaces to stream handling. The\nbuiltin open function is defined in this module.\n\nAt the top of the I/O hierarchy is the abstract base class IOBase. It\ndefines the basic interface to a stream. Note, however, that there is no\nseparation between reading and writing to streams; implementations are\nallowed to raise an IOError if they do not support a given operation.\n\nExtending IOBase is RawIOBase which deals simply with the reading and\nwriting of raw bytes to a stream. FileIO subclasses RawIOBase to provide\nan interface to OS files.\n\nBufferedIOBase deals with buffering on a raw byte stream (RawIOBase). Its\nsubclasses, BufferedWriter, BufferedReader, and BufferedRWPair buffer\nstreams that are readable, writable, and both respectively.\nBufferedRandom provides a buffered interface to random access\nstreams. BytesIO is a simple stream of in-memory bytes.\n\nAnother IOBase subclass, TextIOBase, deals with the encoding and decoding\nof streams into text. TextIOWrapper, which extends it, is a buffered text\ninterface to a buffered raw stream (`BufferedIOBase`). Finally, StringIO\nis a in-memory stream for text.\n\nArgument names are not part of the specification, and only the arguments\nof open() are intended to be used as keyword arguments.\n\ndata:\n\nDEFAULT_BUFFER_SIZE\n\n   An int containing the default buffer size used by the module's buffered\n   I/O classes. open() uses the file's blksize (as obtained by os.stat) if\n   possible.\n\"\"\"\n# New I/O library conforming to PEP 3116.\n\n__author__ = (\"Guido van Rossum <guido@python.org>, \"\n              \"Mike Verdone <mike.verdone@gmail.com>, \"\n              \"Mark Russell <mark.russell@zen.co.uk>, \"\n              \"Antoine Pitrou <solipsis@pitrou.net>, \"\n              \"Amaury Forgeot d'Arc <amauryfa@gmail.com>, \"\n              \"Benjamin Peterson <benjamin@python.org>\")\n\n__all__ = [\"BlockingIOError\", \"open\", \"IOBase\", \"RawIOBase\", \"FileIO\",\n           \"BytesIO\", \"StringIO\", \"BufferedIOBase\",\n           \"BufferedReader\", \"BufferedWriter\", \"BufferedRWPair\",\n           \"BufferedRandom\", \"TextIOBase\", \"TextIOWrapper\",\n           \"UnsupportedOperation\", \"SEEK_SET\", \"SEEK_CUR\", \"SEEK_END\"]\n\n\nimport _io\nimport abc\n\nfrom _io import (DEFAULT_BUFFER_SIZE, BlockingIOError, UnsupportedOperation,\n                 open, FileIO, BytesIO, StringIO, BufferedReader,\n                 BufferedWriter, BufferedRWPair, BufferedRandom,\n                 IncrementalNewlineDecoder, TextIOWrapper)\n\nOpenWrapper = _io.open # for compatibility with _pyio\n\n# for seek()\nSEEK_SET = 0\nSEEK_CUR = 1\nSEEK_END = 2\n\n# Declaring ABCs in C is tricky so we do it here.\n# Method descriptions and default implementations are inherited from the C\n# version however.\nclass IOBase(_io._IOBase):\n    __metaclass__ = abc.ABCMeta\n    __doc__ = _io._IOBase.__doc__\n\nclass RawIOBase(_io._RawIOBase, IOBase):\n    __doc__ = _io._RawIOBase.__doc__\n\nclass BufferedIOBase(_io._BufferedIOBase, IOBase):\n    __doc__ = _io._BufferedIOBase.__doc__\n\nclass TextIOBase(_io._TextIOBase, IOBase):\n    __doc__ = _io._TextIOBase.__doc__\n\nRawIOBase.register(FileIO)\n\nfor klass in (BytesIO, BufferedReader, BufferedWriter, BufferedRandom,\n              BufferedRWPair):\n    BufferedIOBase.register(klass)\n\nfor klass in (StringIO, TextIOWrapper):\n    TextIOBase.register(klass)\ndel klass\n", 
    "json.__init__": "r\"\"\"JSON (JavaScript Object Notation) <http://json.org> is a subset of\nJavaScript syntax (ECMA-262 3rd edition) used as a lightweight data\ninterchange format.\n\n:mod:`json` exposes an API familiar to users of the standard library\n:mod:`marshal` and :mod:`pickle` modules. It is the externally maintained\nversion of the :mod:`json` library contained in Python 2.6, but maintains\ncompatibility with Python 2.4 and Python 2.5 and (currently) has\nsignificant performance advantages, even without using the optional C\nextension for speedups.\n\nEncoding basic Python object hierarchies::\n\n    >>> import json\n    >>> json.dumps(['foo', {'bar': ('baz', None, 1.0, 2)}])\n    '[\"foo\", {\"bar\": [\"baz\", null, 1.0, 2]}]'\n    >>> print json.dumps(\"\\\"foo\\bar\")\n    \"\\\"foo\\bar\"\n    >>> print json.dumps(u'\\u1234')\n    \"\\u1234\"\n    >>> print json.dumps('\\\\')\n    \"\\\\\"\n    >>> print json.dumps({\"c\": 0, \"b\": 0, \"a\": 0}, sort_keys=True)\n    {\"a\": 0, \"b\": 0, \"c\": 0}\n    >>> from StringIO import StringIO\n    >>> io = StringIO()\n    >>> json.dump(['streaming API'], io)\n    >>> io.getvalue()\n    '[\"streaming API\"]'\n\nCompact encoding::\n\n    >>> import json\n    >>> json.dumps([1,2,3,{'4': 5, '6': 7}], sort_keys=True, separators=(',',':'))\n    '[1,2,3,{\"4\":5,\"6\":7}]'\n\nPretty printing::\n\n    >>> import json\n    >>> print json.dumps({'4': 5, '6': 7}, sort_keys=True,\n    ...                  indent=4, separators=(',', ': '))\n    {\n        \"4\": 5,\n        \"6\": 7\n    }\n\nDecoding JSON::\n\n    >>> import json\n    >>> obj = [u'foo', {u'bar': [u'baz', None, 1.0, 2]}]\n    >>> json.loads('[\"foo\", {\"bar\":[\"baz\", null, 1.0, 2]}]') == obj\n    True\n    >>> json.loads('\"\\\\\"foo\\\\bar\"') == u'\"foo\\x08ar'\n    True\n    >>> from StringIO import StringIO\n    >>> io = StringIO('[\"streaming API\"]')\n    >>> json.load(io)[0] == 'streaming API'\n    True\n\nSpecializing JSON object decoding::\n\n    >>> import json\n    >>> def as_complex(dct):\n    ...     if '__complex__' in dct:\n    ...         return complex(dct['real'], dct['imag'])\n    ...     return dct\n    ...\n    >>> json.loads('{\"__complex__\": true, \"real\": 1, \"imag\": 2}',\n    ...     object_hook=as_complex)\n    (1+2j)\n    >>> from decimal import Decimal\n    >>> json.loads('1.1', parse_float=Decimal) == Decimal('1.1')\n    True\n\nSpecializing JSON object encoding::\n\n    >>> import json\n    >>> def encode_complex(obj):\n    ...     if isinstance(obj, complex):\n    ...         return [obj.real, obj.imag]\n    ...     raise TypeError(repr(o) + \" is not JSON serializable\")\n    ...\n    >>> json.dumps(2 + 1j, default=encode_complex)\n    '[2.0, 1.0]'\n    >>> json.JSONEncoder(default=encode_complex).encode(2 + 1j)\n    '[2.0, 1.0]'\n    >>> ''.join(json.JSONEncoder(default=encode_complex).iterencode(2 + 1j))\n    '[2.0, 1.0]'\n\n\nUsing json.tool from the shell to validate and pretty-print::\n\n    $ echo '{\"json\":\"obj\"}' | python -m json.tool\n    {\n        \"json\": \"obj\"\n    }\n    $ echo '{ 1.2:3.4}' | python -m json.tool\n    Expecting property name enclosed in double quotes: line 1 column 3 (char 2)\n\"\"\"\n__version__ = '2.0.9'\n__all__ = [\n    'dump', 'dumps', 'load', 'loads',\n    'JSONDecoder', 'JSONEncoder',\n]\n\n__author__ = 'Bob Ippolito <bob@redivi.com>'\n\ntry:\n    # PyPy speedup, the interface is different than CPython's _json\n    import _pypyjson\nexcept ImportError:\n    _pypyjson = None\n\nfrom .decoder import JSONDecoder\nfrom .encoder import JSONEncoder\n\n_default_encoder = JSONEncoder(\n    skipkeys=False,\n    ensure_ascii=True,\n    check_circular=True,\n    allow_nan=True,\n    indent=None,\n    separators=None,\n    encoding='utf-8',\n    default=None,\n)\n\ndef dump(obj, fp, skipkeys=False, ensure_ascii=True, check_circular=True,\n        allow_nan=True, cls=None, indent=None, separators=None,\n        encoding='utf-8', default=None, sort_keys=False, **kw):\n    \"\"\"Serialize ``obj`` as a JSON formatted stream to ``fp`` (a\n    ``.write()``-supporting file-like object).\n\n    If ``skipkeys`` is true then ``dict`` keys that are not basic types\n    (``str``, ``unicode``, ``int``, ``long``, ``float``, ``bool``, ``None``)\n    will be skipped instead of raising a ``TypeError``.\n\n    If ``ensure_ascii`` is true (the default), all non-ASCII characters in the\n    output are escaped with ``\\uXXXX`` sequences, and the result is a ``str``\n    instance consisting of ASCII characters only.  If ``ensure_ascii`` is\n    ``False``, some chunks written to ``fp`` may be ``unicode`` instances.\n    This usually happens because the input contains unicode strings or the\n    ``encoding`` parameter is used. Unless ``fp.write()`` explicitly\n    understands ``unicode`` (as in ``codecs.getwriter``) this is likely to\n    cause an error.\n\n    If ``check_circular`` is false, then the circular reference check\n    for container types will be skipped and a circular reference will\n    result in an ``OverflowError`` (or worse).\n\n    If ``allow_nan`` is false, then it will be a ``ValueError`` to\n    serialize out of range ``float`` values (``nan``, ``inf``, ``-inf``)\n    in strict compliance of the JSON specification, instead of using the\n    JavaScript equivalents (``NaN``, ``Infinity``, ``-Infinity``).\n\n    If ``indent`` is a non-negative integer, then JSON array elements and\n    object members will be pretty-printed with that indent level. An indent\n    level of 0 will only insert newlines. ``None`` is the most compact\n    representation.  Since the default item separator is ``', '``,  the\n    output might include trailing whitespace when ``indent`` is specified.\n    You can use ``separators=(',', ': ')`` to avoid this.\n\n    If ``separators`` is an ``(item_separator, dict_separator)`` tuple\n    then it will be used instead of the default ``(', ', ': ')`` separators.\n    ``(',', ':')`` is the most compact JSON representation.\n\n    ``encoding`` is the character encoding for str instances, default is UTF-8.\n\n    ``default(obj)`` is a function that should return a serializable version\n    of obj or raise TypeError. The default simply raises TypeError.\n\n    If *sort_keys* is ``True`` (default: ``False``), then the output of\n    dictionaries will be sorted by key.\n\n    To use a custom ``JSONEncoder`` subclass (e.g. one that overrides the\n    ``.default()`` method to serialize additional types), specify it with\n    the ``cls`` kwarg; otherwise ``JSONEncoder`` is used.\n\n    \"\"\"\n    # cached encoder\n    if (not skipkeys and ensure_ascii and\n        check_circular and allow_nan and\n        cls is None and indent is None and separators is None and\n        encoding == 'utf-8' and default is None and not sort_keys and not kw):\n        iterable = _default_encoder.iterencode(obj)\n    else:\n        if cls is None:\n            cls = JSONEncoder\n        iterable = cls(skipkeys=skipkeys, ensure_ascii=ensure_ascii,\n            check_circular=check_circular, allow_nan=allow_nan, indent=indent,\n            separators=separators, encoding=encoding,\n            default=default, sort_keys=sort_keys, **kw).iterencode(obj)\n    # could accelerate with writelines in some versions of Python, at\n    # a debuggability cost\n    for chunk in iterable:\n        fp.write(chunk)\n\n\ndef dumps(obj, skipkeys=False, ensure_ascii=True, check_circular=True,\n        allow_nan=True, cls=None, indent=None, separators=None,\n        encoding='utf-8', default=None, sort_keys=False, **kw):\n    \"\"\"Serialize ``obj`` to a JSON formatted ``str``.\n\n    If ``skipkeys`` is false then ``dict`` keys that are not basic types\n    (``str``, ``unicode``, ``int``, ``long``, ``float``, ``bool``, ``None``)\n    will be skipped instead of raising a ``TypeError``.\n\n    If ``ensure_ascii`` is false, all non-ASCII characters are not escaped, and\n    the return value may be a ``unicode`` instance. See ``dump`` for details.\n\n    If ``check_circular`` is false, then the circular reference check\n    for container types will be skipped and a circular reference will\n    result in an ``OverflowError`` (or worse).\n\n    If ``allow_nan`` is false, then it will be a ``ValueError`` to\n    serialize out of range ``float`` values (``nan``, ``inf``, ``-inf``) in\n    strict compliance of the JSON specification, instead of using the\n    JavaScript equivalents (``NaN``, ``Infinity``, ``-Infinity``).\n\n    If ``indent`` is a non-negative integer, then JSON array elements and\n    object members will be pretty-printed with that indent level. An indent\n    level of 0 will only insert newlines. ``None`` is the most compact\n    representation.  Since the default item separator is ``', '``,  the\n    output might include trailing whitespace when ``indent`` is specified.\n    You can use ``separators=(',', ': ')`` to avoid this.\n\n    If ``separators`` is an ``(item_separator, dict_separator)`` tuple\n    then it will be used instead of the default ``(', ', ': ')`` separators.\n    ``(',', ':')`` is the most compact JSON representation.\n\n    ``encoding`` is the character encoding for str instances, default is UTF-8.\n\n    ``default(obj)`` is a function that should return a serializable version\n    of obj or raise TypeError. The default simply raises TypeError.\n\n    If *sort_keys* is ``True`` (default: ``False``), then the output of\n    dictionaries will be sorted by key.\n\n    To use a custom ``JSONEncoder`` subclass (e.g. one that overrides the\n    ``.default()`` method to serialize additional types), specify it with\n    the ``cls`` kwarg; otherwise ``JSONEncoder`` is used.\n\n    \"\"\"\n    # cached encoder\n    if (not skipkeys and ensure_ascii and\n        check_circular and allow_nan and\n        cls is None and indent is None and separators is None and\n        encoding == 'utf-8' and default is None and not sort_keys and not kw):\n        return _default_encoder.encode(obj)\n    if cls is None:\n        cls = JSONEncoder\n    return cls(\n        skipkeys=skipkeys, ensure_ascii=ensure_ascii,\n        check_circular=check_circular, allow_nan=allow_nan, indent=indent,\n        separators=separators, encoding=encoding, default=default,\n        sort_keys=sort_keys, **kw).encode(obj)\n\n\n_default_decoder = JSONDecoder(encoding=None, object_hook=None,\n                               object_pairs_hook=None)\n\ndef load(fp, encoding=None, cls=None, object_hook=None, parse_float=None,\n        parse_int=None, parse_constant=None, object_pairs_hook=None, **kw):\n    \"\"\"Deserialize ``fp`` (a ``.read()``-supporting file-like object containing\n    a JSON document) to a Python object.\n\n    If the contents of ``fp`` is encoded with an ASCII based encoding other\n    than utf-8 (e.g. latin-1), then an appropriate ``encoding`` name must\n    be specified. Encodings that are not ASCII based (such as UCS-2) are\n    not allowed, and should be wrapped with\n    ``codecs.getreader(fp)(encoding)``, or simply decoded to a ``unicode``\n    object and passed to ``loads()``\n\n    ``object_hook`` is an optional function that will be called with the\n    result of any object literal decode (a ``dict``). The return value of\n    ``object_hook`` will be used instead of the ``dict``. This feature\n    can be used to implement custom decoders (e.g. JSON-RPC class hinting).\n\n    ``object_pairs_hook`` is an optional function that will be called with the\n    result of any object literal decoded with an ordered list of pairs.  The\n    return value of ``object_pairs_hook`` will be used instead of the ``dict``.\n    This feature can be used to implement custom decoders that rely on the\n    order that the key and value pairs are decoded (for example,\n    collections.OrderedDict will remember the order of insertion). If\n    ``object_hook`` is also defined, the ``object_pairs_hook`` takes priority.\n\n    To use a custom ``JSONDecoder`` subclass, specify it with the ``cls``\n    kwarg; otherwise ``JSONDecoder`` is used.\n\n    \"\"\"\n    return loads(fp.read(),\n        encoding=encoding, cls=cls, object_hook=object_hook,\n        parse_float=parse_float, parse_int=parse_int,\n        parse_constant=parse_constant, object_pairs_hook=object_pairs_hook,\n        **kw)\n\n\ndef loads(s, encoding=None, cls=None, object_hook=None, parse_float=None,\n        parse_int=None, parse_constant=None, object_pairs_hook=None, **kw):\n    \"\"\"Deserialize ``s`` (a ``str`` or ``unicode`` instance containing a JSON\n    document) to a Python object.\n\n    If ``s`` is a ``str`` instance and is encoded with an ASCII based encoding\n    other than utf-8 (e.g. latin-1) then an appropriate ``encoding`` name\n    must be specified. Encodings that are not ASCII based (such as UCS-2)\n    are not allowed and should be decoded to ``unicode`` first.\n\n    ``object_hook`` is an optional function that will be called with the\n    result of any object literal decode (a ``dict``). The return value of\n    ``object_hook`` will be used instead of the ``dict``. This feature\n    can be used to implement custom decoders (e.g. JSON-RPC class hinting).\n\n    ``object_pairs_hook`` is an optional function that will be called with the\n    result of any object literal decoded with an ordered list of pairs.  The\n    return value of ``object_pairs_hook`` will be used instead of the ``dict``.\n    This feature can be used to implement custom decoders that rely on the\n    order that the key and value pairs are decoded (for example,\n    collections.OrderedDict will remember the order of insertion). If\n    ``object_hook`` is also defined, the ``object_pairs_hook`` takes priority.\n\n    ``parse_float``, if specified, will be called with the string\n    of every JSON float to be decoded. By default this is equivalent to\n    float(num_str). This can be used to use another datatype or parser\n    for JSON floats (e.g. decimal.Decimal).\n\n    ``parse_int``, if specified, will be called with the string\n    of every JSON int to be decoded. By default this is equivalent to\n    int(num_str). This can be used to use another datatype or parser\n    for JSON integers (e.g. float).\n\n    ``parse_constant``, if specified, will be called with one of the\n    following strings: -Infinity, Infinity, NaN, null, true, false.\n    This can be used to raise an exception if invalid JSON numbers\n    are encountered.\n\n    To use a custom ``JSONDecoder`` subclass, specify it with the ``cls``\n    kwarg; otherwise ``JSONDecoder`` is used.\n\n    \"\"\"\n    if (cls is None and encoding is None and object_hook is None and\n            parse_int is None and parse_float is None and\n            parse_constant is None and object_pairs_hook is None and not kw):\n        if _pypyjson and not isinstance(s, unicode):\n            return _pypyjson.loads(s)\n        else:\n            return _default_decoder.decode(s)\n    if cls is None:\n        cls = JSONDecoder\n    if object_hook is not None:\n        kw['object_hook'] = object_hook\n    if object_pairs_hook is not None:\n        kw['object_pairs_hook'] = object_pairs_hook\n    if parse_float is not None:\n        kw['parse_float'] = parse_float\n    if parse_int is not None:\n        kw['parse_int'] = parse_int\n    if parse_constant is not None:\n        kw['parse_constant'] = parse_constant\n    return cls(encoding=encoding, **kw).decode(s)\n", 
    "json.decoder": "\"\"\"Implementation of JSONDecoder\n\"\"\"\nimport re\nimport sys\nimport struct\n\nfrom json import scanner\ntry:\n    from _json import scanstring as c_scanstring\nexcept ImportError:\n    c_scanstring = None\n\n__all__ = ['JSONDecoder']\n\nFLAGS = re.VERBOSE | re.MULTILINE | re.DOTALL\n\ndef _floatconstants():\n    _BYTES = '7FF80000000000007FF0000000000000'.decode('hex')\n    if sys.byteorder != 'big':\n        _BYTES = _BYTES[:8][::-1] + _BYTES[8:][::-1]\n    nan, inf = struct.unpack('dd', _BYTES)\n    return nan, inf, -inf\n\nNaN, PosInf, NegInf = _floatconstants()\n\n\ndef linecol(doc, pos):\n    lineno = doc.count('\\n', 0, pos) + 1\n    if lineno == 1:\n        colno = pos + 1\n    else:\n        colno = pos - doc.rindex('\\n', 0, pos)\n    return lineno, colno\n\n\ndef errmsg(msg, doc, pos, end=None):\n    # Note that this function is called from _json\n    lineno, colno = linecol(doc, pos)\n    if end is None:\n        fmt = '{0}: line {1} column {2} (char {3})'\n        return fmt.format(msg, lineno, colno, pos)\n        #fmt = '%s: line %d column %d (char %d)'\n        #return fmt % (msg, lineno, colno, pos)\n    endlineno, endcolno = linecol(doc, end)\n    fmt = '{0}: line {1} column {2} - line {3} column {4} (char {5} - {6})'\n    return fmt.format(msg, lineno, colno, endlineno, endcolno, pos, end)\n    #fmt = '%s: line %d column %d - line %d column %d (char %d - %d)'\n    #return fmt % (msg, lineno, colno, endlineno, endcolno, pos, end)\n\n\n_CONSTANTS = {\n    '-Infinity': NegInf,\n    'Infinity': PosInf,\n    'NaN': NaN,\n}\n\nSTRINGCHUNK = re.compile(r'(.*?)([\"\\\\\\x00-\\x1f])', FLAGS)\nBACKSLASH = {\n    '\"': u'\"', '\\\\': u'\\\\', '/': u'/',\n    'b': u'\\b', 'f': u'\\f', 'n': u'\\n', 'r': u'\\r', 't': u'\\t',\n}\n\nDEFAULT_ENCODING = \"utf-8\"\n\ndef _decode_uXXXX(s, pos):\n    esc = s[pos + 1:pos + 5]\n    if len(esc) == 4 and esc[1] not in 'xX':\n        try:\n            return int(esc, 16)\n        except ValueError:\n            pass\n    msg = \"Invalid \\\\uXXXX escape\"\n    raise ValueError(errmsg(msg, s, pos))\n\ndef py_scanstring(s, end, encoding=None, strict=True):\n    \"\"\"Scan the string s for a JSON string. End is the index of the\n    character in s after the quote that started the JSON string.\n    Unescapes all valid JSON string escape sequences and raises ValueError\n    on attempt to decode an invalid string. If strict is False then literal\n    control characters are allowed in the string.\n\n    Returns a tuple of the decoded string and the index of the character in s\n    after the end quote.\"\"\"\n    if encoding is None:\n        encoding = DEFAULT_ENCODING\n    chunks = []\n    _append = chunks.append\n    begin = end - 1\n    while 1:\n        chunk = STRINGCHUNK.match(s, end)\n        if chunk is None:\n            raise ValueError(\n                errmsg(\"Unterminated string starting at\", s, begin))\n        end = chunk.end()\n        content, terminator = chunk.groups()\n        # Content is contains zero or more unescaped string characters\n        if content:\n            if not isinstance(content, unicode):\n                content = unicode(content, encoding)\n            _append(content)\n        # Terminator is the end of string, a literal control character,\n        # or a backslash denoting that an escape sequence follows\n        if terminator == '\"':\n            break\n        elif terminator != '\\\\':\n            if strict:\n                #msg = \"Invalid control character %r at\" % (terminator,)\n                msg = \"Invalid control character {0!r} at\".format(terminator)\n                raise ValueError(errmsg(msg, s, end))\n            else:\n                _append(terminator)\n                continue\n        try:\n            esc = s[end]\n        except IndexError:\n            raise ValueError(\n                errmsg(\"Unterminated string starting at\", s, begin))\n        # If not a unicode escape sequence, must be in the lookup table\n        if esc != 'u':\n            try:\n                char = BACKSLASH[esc]\n            except KeyError:\n                msg = \"Invalid \\\\escape: \" + repr(esc)\n                raise ValueError(errmsg(msg, s, end))\n            end += 1\n        else:\n            # Unicode escape sequence\n            uni = _decode_uXXXX(s, end)\n            end += 5\n            # Check for surrogate pair on UCS-4 systems\n            if sys.maxunicode > 65535 and \\\n               0xd800 <= uni <= 0xdbff and s[end:end + 2] == '\\\\u':\n                uni2 = _decode_uXXXX(s, end + 1)\n                if 0xdc00 <= uni2 <= 0xdfff:\n                    uni = 0x10000 + (((uni - 0xd800) << 10) | (uni2 - 0xdc00))\n                    end += 6\n            char = unichr(uni)\n        # Append the unescaped character\n        _append(char)\n    return u''.join(chunks), end\n\n\n# Use speedup if available\nscanstring = c_scanstring or py_scanstring\n\nWHITESPACE = re.compile(r'[ \\t\\n\\r]*', FLAGS)\nWHITESPACE_STR = ' \\t\\n\\r'\n\ndef JSONObject(s_and_end, encoding, strict, scan_once, object_hook,\n               object_pairs_hook):\n    s, end = s_and_end\n    pairs = []\n    pairs_append = pairs.append\n    # Use a slice to prevent IndexError from being raised, the following\n    # check will raise a more specific ValueError if the string is empty\n    nextchar = s[end:end + 1]\n    # Normally we expect nextchar == '\"'\n    if nextchar != '\"':\n        if nextchar in WHITESPACE_STR:\n            end = WHITESPACE.match(s, end).end()\n            nextchar = s[end:end + 1]\n        # Trivial empty object\n        if nextchar == '}':\n            if object_pairs_hook is not None:\n                result = object_pairs_hook(pairs)\n                return result, end + 1\n            pairs = {}\n            if object_hook is not None:\n                pairs = object_hook(pairs)\n            return pairs, end + 1\n        elif nextchar != '\"':\n            raise ValueError(errmsg(\n                \"Expecting property name enclosed in double quotes\", s, end))\n    end += 1\n    while True:\n        key, end = scanstring(s, end, encoding, strict)\n\n        # To skip some function call overhead we optimize the fast paths where\n        # the JSON key separator is \": \" or just \":\".\n        if s[end:end + 1] != ':':\n            end = WHITESPACE.match(s, end).end()\n            if s[end:end + 1] != ':':\n                raise ValueError(errmsg(\"Expecting ':' delimiter\", s, end))\n        end += 1\n\n        try:\n            if s[end] in WHITESPACE_STR:\n                end += 1\n                if s[end] in WHITESPACE_STR:\n                    end = WHITESPACE.match(s, end + 1).end()\n        except IndexError:\n            pass\n\n        try:\n            value, end = scan_once(s, end)\n        except StopIteration:\n            raise ValueError(errmsg(\"Expecting object\", s, end))\n        pairs_append((key, value))\n\n        try:\n            nextchar = s[end]\n            if nextchar in WHITESPACE_STR:\n                end = WHITESPACE.match(s, end + 1).end()\n                nextchar = s[end]\n        except IndexError:\n            nextchar = ''\n        end += 1\n\n        if nextchar == '}':\n            break\n        elif nextchar != ',':\n            raise ValueError(errmsg(\"Expecting ',' delimiter\", s, end - 1))\n\n        try:\n            nextchar = s[end]\n            if nextchar in WHITESPACE_STR:\n                end += 1\n                nextchar = s[end]\n                if nextchar in WHITESPACE_STR:\n                    end = WHITESPACE.match(s, end + 1).end()\n                    nextchar = s[end]\n        except IndexError:\n            nextchar = ''\n\n        end += 1\n        if nextchar != '\"':\n            raise ValueError(errmsg(\n                \"Expecting property name enclosed in double quotes\", s, end - 1))\n    if object_pairs_hook is not None:\n        result = object_pairs_hook(pairs)\n        return result, end\n    pairs = dict(pairs)\n    if object_hook is not None:\n        pairs = object_hook(pairs)\n    return pairs, end\n\ndef JSONArray(s_and_end, scan_once):\n    s, end = s_and_end\n    values = []\n    nextchar = s[end:end + 1]\n    if nextchar in WHITESPACE_STR:\n        end = WHITESPACE.match(s, end + 1).end()\n        nextchar = s[end:end + 1]\n    # Look-ahead for trivial empty array\n    if nextchar == ']':\n        return values, end + 1\n    _append = values.append\n    while True:\n        try:\n            value, end = scan_once(s, end)\n        except StopIteration:\n            raise ValueError(errmsg(\"Expecting object\", s, end))\n        _append(value)\n        nextchar = s[end:end + 1]\n        if nextchar in WHITESPACE_STR:\n            end = WHITESPACE.match(s, end + 1).end()\n            nextchar = s[end:end + 1]\n        end += 1\n        if nextchar == ']':\n            break\n        elif nextchar != ',':\n            raise ValueError(errmsg(\"Expecting ',' delimiter\", s, end))\n        try:\n            if s[end] in WHITESPACE_STR:\n                end += 1\n                if s[end] in WHITESPACE_STR:\n                    end = WHITESPACE.match(s, end + 1).end()\n        except IndexError:\n            pass\n\n    return values, end\n\nclass JSONDecoder(object):\n    \"\"\"Simple JSON <http://json.org> decoder\n\n    Performs the following translations in decoding by default:\n\n    +---------------+-------------------+\n    | JSON          | Python            |\n    +===============+===================+\n    | object        | dict              |\n    +---------------+-------------------+\n    | array         | list              |\n    +---------------+-------------------+\n    | string        | unicode           |\n    +---------------+-------------------+\n    | number (int)  | int, long         |\n    +---------------+-------------------+\n    | number (real) | float             |\n    +---------------+-------------------+\n    | true          | True              |\n    +---------------+-------------------+\n    | false         | False             |\n    +---------------+-------------------+\n    | null          | None              |\n    +---------------+-------------------+\n\n    It also understands ``NaN``, ``Infinity``, and ``-Infinity`` as\n    their corresponding ``float`` values, which is outside the JSON spec.\n\n    \"\"\"\n\n    def __init__(self, encoding=None, object_hook=None, parse_float=None,\n            parse_int=None, parse_constant=None, strict=True,\n            object_pairs_hook=None):\n        \"\"\"``encoding`` determines the encoding used to interpret any ``str``\n        objects decoded by this instance (utf-8 by default).  It has no\n        effect when decoding ``unicode`` objects.\n\n        Note that currently only encodings that are a superset of ASCII work,\n        strings of other encodings should be passed in as ``unicode``.\n\n        ``object_hook``, if specified, will be called with the result\n        of every JSON object decoded and its return value will be used in\n        place of the given ``dict``.  This can be used to provide custom\n        deserializations (e.g. to support JSON-RPC class hinting).\n\n        ``object_pairs_hook``, if specified will be called with the result of\n        every JSON object decoded with an ordered list of pairs.  The return\n        value of ``object_pairs_hook`` will be used instead of the ``dict``.\n        This feature can be used to implement custom decoders that rely on the\n        order that the key and value pairs are decoded (for example,\n        collections.OrderedDict will remember the order of insertion). If\n        ``object_hook`` is also defined, the ``object_pairs_hook`` takes\n        priority.\n\n        ``parse_float``, if specified, will be called with the string\n        of every JSON float to be decoded. By default this is equivalent to\n        float(num_str). This can be used to use another datatype or parser\n        for JSON floats (e.g. decimal.Decimal).\n\n        ``parse_int``, if specified, will be called with the string\n        of every JSON int to be decoded. By default this is equivalent to\n        int(num_str). This can be used to use another datatype or parser\n        for JSON integers (e.g. float).\n\n        ``parse_constant``, if specified, will be called with one of the\n        following strings: -Infinity, Infinity, NaN.\n        This can be used to raise an exception if invalid JSON numbers\n        are encountered.\n\n        If ``strict`` is false (true is the default), then control\n        characters will be allowed inside strings.  Control characters in\n        this context are those with character codes in the 0-31 range,\n        including ``'\\\\t'`` (tab), ``'\\\\n'``, ``'\\\\r'`` and ``'\\\\0'``.\n\n        \"\"\"\n        self.encoding = encoding\n        self.object_hook = object_hook\n        self.object_pairs_hook = object_pairs_hook\n        self.parse_float = parse_float or float\n        self.parse_int = parse_int or int\n        self.parse_constant = parse_constant or _CONSTANTS.__getitem__\n        self.strict = strict\n        self.parse_object = JSONObject\n        self.parse_array = JSONArray\n        self.parse_string = scanstring\n        self.scan_once = scanner.make_scanner(self)\n\n    def decode(self, s):\n        \"\"\"Return the Python representation of ``s`` (a ``str`` or ``unicode``\n        instance containing a JSON document)\n\n        \"\"\"\n        obj, end = self.raw_decode(s, idx=WHITESPACE.match(s, 0).end())\n        end = WHITESPACE.match(s, end).end()\n        if end != len(s):\n            raise ValueError(errmsg(\"Extra data\", s, end, len(s)))\n        return obj\n\n    def raw_decode(self, s, idx=0):\n        \"\"\"Decode a JSON document from ``s`` (a ``str`` or ``unicode``\n        beginning with a JSON document) and return a 2-tuple of the Python\n        representation and the index in ``s`` where the document ended.\n\n        This can be used to decode a JSON document from a string that may\n        have extraneous data at the end.\n\n        \"\"\"\n        try:\n            obj, end = self.scan_once(s, idx)\n        except StopIteration:\n            raise ValueError(\"No JSON object could be decoded\")\n        return obj, end\n", 
    "json.encoder": "\"\"\"Implementation of JSONEncoder\n\"\"\"\nimport re\n\nfrom __pypy__.builders import StringBuilder, UnicodeBuilder\n\nclass StringOrUnicodeBuilder(object):\n    def __init__(self):\n        self._builder = StringBuilder()\n    def append(self, string):\n        try:\n            self._builder.append(string)\n        except UnicodeEncodeError:\n            ub = UnicodeBuilder()\n            ub.append(self._builder.build())\n            self._builder = ub\n            ub.append(string)\n    def build(self):\n        return self._builder.build()\n\n\nESCAPE = re.compile(r'[\\x00-\\x1f\\\\\"\\b\\f\\n\\r\\t]')\nESCAPE_ASCII = re.compile(r'([\\\\\"]|[^\\ -~])')\nHAS_UTF8 = re.compile(r'[\\x80-\\xff]')\nESCAPE_DCT = {\n    '\\\\': '\\\\\\\\',\n    '\"': '\\\\\"',\n    '\\b': '\\\\b',\n    '\\f': '\\\\f',\n    '\\n': '\\\\n',\n    '\\r': '\\\\r',\n    '\\t': '\\\\t',\n}\nfor i in range(0x20):\n    ESCAPE_DCT.setdefault(chr(i), '\\\\u%04x' % (i,))\n\nINFINITY = float('inf')\nFLOAT_REPR = repr\n\ndef raw_encode_basestring(s):\n    \"\"\"Return a JSON representation of a Python string\n\n    \"\"\"\n    def replace(match):\n        return ESCAPE_DCT[match.group(0)]\n    return ESCAPE.sub(replace, s)\nencode_basestring = lambda s: '\"' + raw_encode_basestring(s) + '\"'\n\ndef raw_encode_basestring_ascii(s):\n    \"\"\"Return an ASCII-only JSON representation of a Python string\n\n    \"\"\"\n    if isinstance(s, str) and HAS_UTF8.search(s) is not None:\n        s = s.decode('utf-8')\n    def replace(match):\n        s = match.group(0)\n        try:\n            return ESCAPE_DCT[s]\n        except KeyError:\n            n = ord(s)\n            if n < 0x10000:\n                return '\\\\u%04x' % (n,)\n            else:\n                # surrogate pair\n                n -= 0x10000\n                s1 = 0xd800 | ((n >> 10) & 0x3ff)\n                s2 = 0xdc00 | (n & 0x3ff)\n                return '\\\\u%04x\\\\u%04x' % (s1, s2)\n    if ESCAPE_ASCII.search(s):\n        return str(ESCAPE_ASCII.sub(replace, s))\n    return s\nencode_basestring_ascii = lambda s: '\"' + raw_encode_basestring_ascii(s) + '\"'\n\n\nclass JSONEncoder(object):\n    \"\"\"Extensible JSON <http://json.org> encoder for Python data structures.\n\n    Supports the following objects and types by default:\n\n    +-------------------+---------------+\n    | Python            | JSON          |\n    +===================+===============+\n    | dict              | object        |\n    +-------------------+---------------+\n    | list, tuple       | array         |\n    +-------------------+---------------+\n    | str, unicode      | string        |\n    +-------------------+---------------+\n    | int, long, float  | number        |\n    +-------------------+---------------+\n    | True              | true          |\n    +-------------------+---------------+\n    | False             | false         |\n    +-------------------+---------------+\n    | None              | null          |\n    +-------------------+---------------+\n\n    To extend this to recognize other objects, subclass and implement a\n    ``.default()`` method with another method that returns a serializable\n    object for ``o`` if possible, otherwise it should call the superclass\n    implementation (to raise ``TypeError``).\n\n    \"\"\"\n    item_separator = ', '\n    key_separator = ': '\n    def __init__(self, skipkeys=False, ensure_ascii=True,\n            check_circular=True, allow_nan=True, sort_keys=False,\n            indent=None, separators=None, encoding='utf-8', default=None):\n        \"\"\"Constructor for JSONEncoder, with sensible defaults.\n\n        If skipkeys is false, then it is a TypeError to attempt\n        encoding of keys that are not str, int, long, float or None.  If\n        skipkeys is True, such items are simply skipped.\n\n        If *ensure_ascii* is true (the default), all non-ASCII\n        characters in the output are escaped with \\uXXXX sequences,\n        and the results are str instances consisting of ASCII\n        characters only.  If ensure_ascii is False, a result may be a\n        unicode instance.  This usually happens if the input contains\n        unicode strings or the *encoding* parameter is used.\n\n        If check_circular is true, then lists, dicts, and custom encoded\n        objects will be checked for circular references during encoding to\n        prevent an infinite recursion (which would cause an OverflowError).\n        Otherwise, no such check takes place.\n\n        If allow_nan is true, then NaN, Infinity, and -Infinity will be\n        encoded as such.  This behavior is not JSON specification compliant,\n        but is consistent with most JavaScript based encoders and decoders.\n        Otherwise, it will be a ValueError to encode such floats.\n\n        If sort_keys is true, then the output of dictionaries will be\n        sorted by key; this is useful for regression tests to ensure\n        that JSON serializations can be compared on a day-to-day basis.\n\n        If indent is a non-negative integer, then JSON array\n        elements and object members will be pretty-printed with that\n        indent level.  An indent level of 0 will only insert newlines.\n        None is the most compact representation.  Since the default\n        item separator is ', ',  the output might include trailing\n        whitespace when indent is specified.  You can use\n        separators=(',', ': ') to avoid this.\n\n        If specified, separators should be a (item_separator, key_separator)\n        tuple.  The default is (', ', ': ').  To get the most compact JSON\n        representation you should specify (',', ':') to eliminate whitespace.\n\n        If specified, default is a function that gets called for objects\n        that can't otherwise be serialized.  It should return a JSON encodable\n        version of the object or raise a ``TypeError``.\n\n        If encoding is not None, then all input strings will be\n        transformed into unicode using that encoding prior to JSON-encoding.\n        The default is UTF-8.\n\n        \"\"\"\n\n        self.skipkeys = skipkeys\n        self.ensure_ascii = ensure_ascii\n        if ensure_ascii:\n            self.__encoder = raw_encode_basestring_ascii\n        else:\n            self.__encoder = raw_encode_basestring\n        if encoding != 'utf-8':\n            orig_encoder = self.__encoder\n            def encoder(o):\n                if isinstance(o, str):\n                    o = o.decode(encoding)\n                return orig_encoder(o)\n            self.__encoder = encoder\n        self.check_circular = check_circular\n        self.allow_nan = allow_nan\n        self.sort_keys = sort_keys\n        self.indent = indent\n        if separators is not None:\n            self.item_separator, self.key_separator = separators\n        if default is not None:\n            self.default = default\n        self.encoding = encoding\n\n    def default(self, o):\n        \"\"\"Implement this method in a subclass such that it returns\n        a serializable object for ``o``, or calls the base implementation\n        (to raise a ``TypeError``).\n\n        For example, to support arbitrary iterators, you could\n        implement default like this::\n\n            def default(self, o):\n                try:\n                    iterable = iter(o)\n                except TypeError:\n                    pass\n                else:\n                    return list(iterable)\n                # Let the base class default method raise the TypeError\n                return JSONEncoder.default(self, o)\n\n        \"\"\"\n        raise TypeError(repr(o) + \" is not JSON serializable\")\n\n    def encode(self, o):\n        \"\"\"Return a JSON string representation of a Python data structure.\n\n        >>> JSONEncoder().encode({\"foo\": [\"bar\", \"baz\"]})\n        '{\"foo\": [\"bar\", \"baz\"]}'\n\n        \"\"\"\n        if self.check_circular:\n            markers = {}\n        else:\n            markers = None\n        if self.ensure_ascii:\n            builder = StringBuilder()\n        else:\n            builder = StringOrUnicodeBuilder()\n        self.__encode(o, markers, builder, 0)\n        return builder.build()\n\n    def __emit_indent(self, builder, _current_indent_level):\n        if self.indent is not None:\n            _current_indent_level += 1\n            newline_indent = '\\n' + (' ' * (self.indent *\n                                            _current_indent_level))\n            separator = self.item_separator + newline_indent\n            builder.append(newline_indent)\n        else:\n            separator = self.item_separator\n        return separator, _current_indent_level\n\n    def __emit_unindent(self, builder, _current_indent_level):\n        if self.indent is not None:\n            builder.append('\\n')\n            builder.append(' ' * (self.indent * (_current_indent_level - 1)))\n\n    def __encode(self, o, markers, builder, _current_indent_level):\n        if isinstance(o, basestring):\n            builder.append('\"')\n            builder.append(self.__encoder(o))\n            builder.append('\"')\n        elif o is None:\n            builder.append('null')\n        elif o is True:\n            builder.append('true')\n        elif o is False:\n            builder.append('false')\n        elif isinstance(o, (int, long)):\n            builder.append(str(o))\n        elif isinstance(o, float):\n            builder.append(self.__floatstr(o))\n        elif isinstance(o, (list, tuple)):\n            if not o:\n                builder.append('[]')\n                return\n            self.__encode_list(o, markers, builder, _current_indent_level)\n        elif isinstance(o, dict):\n            if not o:\n                builder.append('{}')\n                return\n            self.__encode_dict(o, markers, builder, _current_indent_level)\n        else:\n            self.__mark_markers(markers, o)\n            res = self.default(o)\n            self.__encode(res, markers, builder, _current_indent_level)\n            self.__remove_markers(markers, o)\n            return res\n\n    def __encode_list(self, l, markers, builder, _current_indent_level):\n        self.__mark_markers(markers, l)\n        builder.append('[')\n        first = True\n        separator, _current_indent_level = self.__emit_indent(builder,\n                                                      _current_indent_level)\n        for elem in l:\n            if first:\n                first = False\n            else:\n                builder.append(separator)\n            self.__encode(elem, markers, builder, _current_indent_level)\n            del elem # XXX grumble\n        self.__emit_unindent(builder, _current_indent_level)\n        builder.append(']')\n        self.__remove_markers(markers, l)\n\n    def __encode_dict(self, d, markers, builder, _current_indent_level):\n        self.__mark_markers(markers, d)\n        first = True\n        builder.append('{')\n        separator, _current_indent_level = self.__emit_indent(builder,\n                                                         _current_indent_level)\n        if self.sort_keys:\n            items = sorted(d.items(), key=lambda kv: kv[0])\n        else:\n            items = d.iteritems()\n\n        for key, v in items:\n            if first:\n                first = False\n            else:\n                builder.append(separator)\n            if isinstance(key, basestring):\n                pass\n            # JavaScript is weakly typed for these, so it makes sense to\n            # also allow them.  Many encoders seem to do something like this.\n            elif isinstance(key, float):\n                key = self.__floatstr(key)\n            elif key is True:\n                key = 'true'\n            elif key is False:\n                key = 'false'\n            elif key is None:\n                key = 'null'\n            elif isinstance(key, (int, long)):\n                key = str(key)\n            elif self.skipkeys:\n                continue\n            else:\n                raise TypeError(\"key \" + repr(key) + \" is not a string\")\n            builder.append('\"')\n            builder.append(self.__encoder(key))\n            builder.append('\"')\n            builder.append(self.key_separator)\n            self.__encode(v, markers, builder, _current_indent_level)\n            del key\n            del v # XXX grumble\n        self.__emit_unindent(builder, _current_indent_level)\n        builder.append('}')\n        self.__remove_markers(markers, d)\n\n    def iterencode(self, o, _one_shot=False):\n        \"\"\"Encode the given object and yield each string\n        representation as available.\n\n        For example::\n\n            for chunk in JSONEncoder().iterencode(bigobject):\n                mysocket.write(chunk)\n\n        \"\"\"\n        if self.check_circular:\n            markers = {}\n        else:\n            markers = None\n        return self.__iterencode(o, markers, 0)\n\n    def __floatstr(self, o):\n        # Check for specials.  Note that this type of test is processor\n        # and/or platform-specific, so do tests which don't depend on the\n        # internals.\n\n        if o != o:\n            text = 'NaN'\n        elif o == INFINITY:\n            text = 'Infinity'\n        elif o == -INFINITY:\n            text = '-Infinity'\n        else:\n            return FLOAT_REPR(o)\n\n        if not self.allow_nan:\n            raise ValueError(\n                \"Out of range float values are not JSON compliant: \" +\n                repr(o))\n\n        return text\n\n    def __mark_markers(self, markers, o):\n        if markers is not None:\n            if id(o) in markers:\n                raise ValueError(\"Circular reference detected\")\n            markers[id(o)] = None\n\n    def __remove_markers(self, markers, o):\n        if markers is not None:\n            del markers[id(o)]\n\n    def __iterencode_list(self, lst, markers, _current_indent_level):\n        if not lst:\n            yield '[]'\n            return\n        self.__mark_markers(markers, lst)\n        buf = '['\n        if self.indent is not None:\n            _current_indent_level += 1\n            newline_indent = '\\n' + (' ' * (self.indent *\n                                            _current_indent_level))\n            separator = self.item_separator + newline_indent\n            buf += newline_indent\n        else:\n            newline_indent = None\n            separator = self.item_separator\n        first = True\n        for value in lst:\n            if first:\n                first = False\n            else:\n                buf = separator\n            if isinstance(value, basestring):\n                yield buf + '\"' + self.__encoder(value) + '\"'\n            elif value is None:\n                yield buf + 'null'\n            elif value is True:\n                yield buf + 'true'\n            elif value is False:\n                yield buf + 'false'\n            elif isinstance(value, (int, long)):\n                yield buf + str(value)\n            elif isinstance(value, float):\n                yield buf + self.__floatstr(value)\n            else:\n                yield buf\n                if isinstance(value, (list, tuple)):\n                    chunks = self.__iterencode_list(value, markers,\n                                                   _current_indent_level)\n                elif isinstance(value, dict):\n                    chunks = self.__iterencode_dict(value, markers,\n                                                   _current_indent_level)\n                else:\n                    chunks = self.__iterencode(value, markers,\n                                              _current_indent_level)\n                for chunk in chunks:\n                    yield chunk\n        if newline_indent is not None:\n            _current_indent_level -= 1\n            yield '\\n' + (' ' * (self.indent * _current_indent_level))\n        yield ']'\n        self.__remove_markers(markers, lst)\n\n    def __iterencode_dict(self, dct, markers, _current_indent_level):\n        if not dct:\n            yield '{}'\n            return\n        self.__mark_markers(markers, dct)\n        yield '{'\n        if self.indent is not None:\n            _current_indent_level += 1\n            newline_indent = '\\n' + (' ' * (self.indent *\n                                            _current_indent_level))\n            item_separator = self.item_separator + newline_indent\n            yield newline_indent\n        else:\n            newline_indent = None\n            item_separator = self.item_separator\n        first = True\n        if self.sort_keys:\n            items = sorted(dct.items(), key=lambda kv: kv[0])\n        else:\n            items = dct.iteritems()\n        for key, value in items:\n            if isinstance(key, basestring):\n                pass\n            # JavaScript is weakly typed for these, so it makes sense to\n            # also allow them.  Many encoders seem to do something like this.\n            elif isinstance(key, float):\n                key = self.__floatstr(key)\n            elif key is True:\n                key = 'true'\n            elif key is False:\n                key = 'false'\n            elif key is None:\n                key = 'null'\n            elif isinstance(key, (int, long)):\n                key = str(key)\n            elif self.skipkeys:\n                continue\n            else:\n                raise TypeError(\"key \" + repr(key) + \" is not a string\")\n            if first:\n                first = False\n            else:\n                yield item_separator\n            yield '\"' + self.__encoder(key) + '\"'\n            yield self.key_separator\n            if isinstance(value, basestring):\n                yield '\"' + self.__encoder(value) + '\"'\n            elif value is None:\n                yield 'null'\n            elif value is True:\n                yield 'true'\n            elif value is False:\n                yield 'false'\n            elif isinstance(value, (int, long)):\n                yield str(value)\n            elif isinstance(value, float):\n                yield self.__floatstr(value)\n            else:\n                if isinstance(value, (list, tuple)):\n                    chunks = self.__iterencode_list(value, markers,\n                                                   _current_indent_level)\n                elif isinstance(value, dict):\n                    chunks = self.__iterencode_dict(value, markers,\n                                                   _current_indent_level)\n                else:\n                    chunks = self.__iterencode(value, markers,\n                                              _current_indent_level)\n                for chunk in chunks:\n                    yield chunk\n        if newline_indent is not None:\n            _current_indent_level -= 1\n            yield '\\n' + (' ' * (self.indent * _current_indent_level))\n        yield '}'\n        self.__remove_markers(markers, dct)\n\n    def __iterencode(self, o, markers, _current_indent_level):\n        if isinstance(o, basestring):\n            yield '\"' + self.__encoder(o) + '\"'\n        elif o is None:\n            yield 'null'\n        elif o is True:\n            yield 'true'\n        elif o is False:\n            yield 'false'\n        elif isinstance(o, (int, long)):\n            yield str(o)\n        elif isinstance(o, float):\n            yield self.__floatstr(o)\n        elif isinstance(o, (list, tuple)):\n            for chunk in self.__iterencode_list(o, markers,\n                                               _current_indent_level):\n                yield chunk\n        elif isinstance(o, dict):\n            for chunk in self.__iterencode_dict(o, markers,\n                                               _current_indent_level):\n                yield chunk\n        else:\n            self.__mark_markers(markers, o)\n            obj = self.default(o)\n            for chunk in self.__iterencode(obj, markers,\n                                          _current_indent_level):\n                yield chunk\n            self.__remove_markers(markers, o)\n\n\n# overwrite some helpers here with more efficient versions\ntry:\n    from _pypyjson import raw_encode_basestring_ascii\nexcept ImportError:\n    pass\n", 
    "json.scanner": "\"\"\"JSON token scanner\n\"\"\"\nimport re\ntry:\n    from _json import make_scanner as c_make_scanner\nexcept ImportError:\n    c_make_scanner = None\n\n__all__ = ['make_scanner']\n\nNUMBER_RE = re.compile(\n    r'(-?(?:0|[1-9]\\d*))(\\.\\d+)?([eE][-+]?\\d+)?',\n    (re.VERBOSE | re.MULTILINE | re.DOTALL))\n\ndef py_make_scanner(context):\n    parse_object = context.parse_object\n    parse_array = context.parse_array\n    parse_string = context.parse_string\n    match_number = NUMBER_RE.match\n    encoding = context.encoding\n    strict = context.strict\n    parse_float = context.parse_float\n    parse_int = context.parse_int\n    parse_constant = context.parse_constant\n    object_hook = context.object_hook\n    object_pairs_hook = context.object_pairs_hook\n\n    def _scan_once(string, idx):\n        try:\n            nextchar = string[idx]\n        except IndexError:\n            raise StopIteration\n\n        if nextchar == '\"':\n            return parse_string(string, idx + 1, encoding, strict)\n        elif nextchar == '{':\n            return parse_object((string, idx + 1), encoding, strict,\n                _scan_once, object_hook, object_pairs_hook)\n        elif nextchar == '[':\n            return parse_array((string, idx + 1), _scan_once)\n        elif nextchar == 'n' and string[idx:idx + 4] == 'null':\n            return None, idx + 4\n        elif nextchar == 't' and string[idx:idx + 4] == 'true':\n            return True, idx + 4\n        elif nextchar == 'f' and string[idx:idx + 5] == 'false':\n            return False, idx + 5\n\n        m = match_number(string, idx)\n        if m is not None:\n            integer, frac, exp = m.groups()\n            if frac or exp:\n                res = parse_float(integer + (frac or '') + (exp or ''))\n            else:\n                res = parse_int(integer)\n            return res, m.end()\n        elif nextchar == 'N' and string[idx:idx + 3] == 'NaN':\n            return parse_constant('NaN'), idx + 3\n        elif nextchar == 'I' and string[idx:idx + 8] == 'Infinity':\n            return parse_constant('Infinity'), idx + 8\n        elif nextchar == '-' and string[idx:idx + 9] == '-Infinity':\n            return parse_constant('-Infinity'), idx + 9\n        else:\n            raise StopIteration\n\n    return _scan_once\n\nmake_scanner = c_make_scanner or py_make_scanner\n", 
    "keyword": "#! /usr/bin/env python\n\n\"\"\"Keywords (from \"graminit.c\")\n\nThis file is automatically generated; please don't muck it up!\n\nTo update the symbols in this file, 'cd' to the top directory of\nthe python source tree after building the interpreter and run:\n\n    ./python Lib/keyword.py\n\"\"\"\n\n__all__ = [\"iskeyword\", \"kwlist\"]\n\nkwlist = [\n#--start keywords--\n        'and',\n        'as',\n        'assert',\n        'break',\n        'class',\n        'continue',\n        'def',\n        'del',\n        'elif',\n        'else',\n        'except',\n        'exec',\n        'finally',\n        'for',\n        'from',\n        'global',\n        'if',\n        'import',\n        'in',\n        'is',\n        'lambda',\n        'not',\n        'or',\n        'pass',\n        'print',\n        'raise',\n        'return',\n        'try',\n        'while',\n        'with',\n        'yield',\n#--end keywords--\n        ]\n\niskeyword = frozenset(kwlist).__contains__\n\ndef main():\n    import sys, re\n\n    args = sys.argv[1:]\n    iptfile = args and args[0] or \"Python/graminit.c\"\n    if len(args) > 1: optfile = args[1]\n    else: optfile = \"Lib/keyword.py\"\n\n    # scan the source file for keywords\n    fp = open(iptfile)\n    strprog = re.compile('\"([^\"]+)\"')\n    lines = []\n    for line in fp:\n        if '{1, \"' in line:\n            match = strprog.search(line)\n            if match:\n                lines.append(\"        '\" + match.group(1) + \"',\\n\")\n    fp.close()\n    lines.sort()\n\n    # load the output skeleton from the target\n    fp = open(optfile)\n    format = fp.readlines()\n    fp.close()\n\n    # insert the lines of keywords\n    try:\n        start = format.index(\"#--start keywords--\\n\") + 1\n        end = format.index(\"#--end keywords--\\n\")\n        format[start:end] = lines\n    except ValueError:\n        sys.stderr.write(\"target does not contain format markers\\n\")\n        sys.exit(1)\n\n    # write the output file\n    fp = open(optfile, 'w')\n    fp.write(''.join(format))\n    fp.close()\n\nif __name__ == \"__main__\":\n    main()\n", 
    "linecache": "\"\"\"Cache lines from files.\n\nThis is intended to read lines from modules imported -- hence if a filename\nis not found, it will look down the module search path for a file by\nthat name.\n\"\"\"\n\nimport sys\nimport os\n\n__all__ = [\"getline\", \"clearcache\", \"checkcache\"]\n\ndef getline(filename, lineno, module_globals=None):\n    lines = getlines(filename, module_globals)\n    if 1 <= lineno <= len(lines):\n        return lines[lineno-1]\n    else:\n        return ''\n\n\n# The cache\n\ncache = {} # The cache\n\n\ndef clearcache():\n    \"\"\"Clear the cache entirely.\"\"\"\n\n    global cache\n    cache = {}\n\n\ndef getlines(filename, module_globals=None):\n    \"\"\"Get the lines for a file from the cache.\n    Update the cache if it doesn't contain an entry for this file already.\"\"\"\n\n    if filename in cache:\n        return cache[filename][2]\n    else:\n        return updatecache(filename, module_globals)\n\n\ndef checkcache(filename=None):\n    \"\"\"Discard cache entries that are out of date.\n    (This is not checked upon each call!)\"\"\"\n\n    if filename is None:\n        filenames = cache.keys()\n    else:\n        if filename in cache:\n            filenames = [filename]\n        else:\n            return\n\n    for filename in filenames:\n        size, mtime, lines, fullname = cache[filename]\n        if mtime is None:\n            continue   # no-op for files loaded via a __loader__\n        try:\n            stat = os.stat(fullname)\n        except os.error:\n            del cache[filename]\n            continue\n        if size != stat.st_size or mtime != stat.st_mtime:\n            del cache[filename]\n\n\ndef updatecache(filename, module_globals=None):\n    \"\"\"Update a cache entry and return its list of lines.\n    If something's wrong, print a message, discard the cache entry,\n    and return an empty list.\"\"\"\n\n    if filename in cache:\n        del cache[filename]\n    if not filename or (filename.startswith('<') and filename.endswith('>')):\n        return []\n\n    fullname = filename\n    try:\n        stat = os.stat(fullname)\n    except OSError:\n        basename = filename\n\n        # Try for a __loader__, if available\n        if module_globals and '__loader__' in module_globals:\n            name = module_globals.get('__name__')\n            loader = module_globals['__loader__']\n            get_source = getattr(loader, 'get_source', None)\n\n            if name and get_source:\n                try:\n                    data = get_source(name)\n                except (ImportError, IOError):\n                    pass\n                else:\n                    if data is None:\n                        # No luck, the PEP302 loader cannot find the source\n                        # for this module.\n                        return []\n                    cache[filename] = (\n                        len(data), None,\n                        [line+'\\n' for line in data.splitlines()], fullname\n                    )\n                    return cache[filename][2]\n\n        # Try looking through the module search path, which is only useful\n        # when handling a relative filename.\n        if os.path.isabs(filename):\n            return []\n\n        for dirname in sys.path:\n            # When using imputil, sys.path may contain things other than\n            # strings; ignore them when it happens.\n            try:\n                fullname = os.path.join(dirname, basename)\n            except (TypeError, AttributeError):\n                # Not sufficiently string-like to do anything useful with.\n                continue\n            try:\n                stat = os.stat(fullname)\n                break\n            except os.error:\n                pass\n        else:\n            return []\n    try:\n        with open(fullname, 'rU') as fp:\n            lines = fp.readlines()\n    except IOError:\n        return []\n    if lines and not lines[-1].endswith('\\n'):\n        lines[-1] += '\\n'\n    size, mtime = stat.st_size, stat.st_mtime\n    cache[filename] = size, mtime, lines, fullname\n    return lines\n", 
    "locale": "\"\"\" Locale support.\n\n    The module provides low-level access to the C lib's locale APIs\n    and adds high level number formatting APIs as well as a locale\n    aliasing engine to complement these.\n\n    The aliasing engine includes support for many commonly used locale\n    names and maps them to values suitable for passing to the C lib's\n    setlocale() function. It also includes default encodings for all\n    supported locale names.\n\n\"\"\"\n\nimport sys\nimport encodings\nimport encodings.aliases\nimport re\nimport operator\nimport functools\n\ntry:\n    _unicode = unicode\nexcept NameError:\n    # If Python is built without Unicode support, the unicode type\n    # will not exist. Fake one.\n    class _unicode(object):\n        pass\n\n# Try importing the _locale module.\n#\n# If this fails, fall back on a basic 'C' locale emulation.\n\n# Yuck:  LC_MESSAGES is non-standard:  can't tell whether it exists before\n# trying the import.  So __all__ is also fiddled at the end of the file.\n__all__ = [\"getlocale\", \"getdefaultlocale\", \"getpreferredencoding\", \"Error\",\n           \"setlocale\", \"resetlocale\", \"localeconv\", \"strcoll\", \"strxfrm\",\n           \"str\", \"atof\", \"atoi\", \"format\", \"format_string\", \"currency\",\n           \"normalize\", \"LC_CTYPE\", \"LC_COLLATE\", \"LC_TIME\", \"LC_MONETARY\",\n           \"LC_NUMERIC\", \"LC_ALL\", \"CHAR_MAX\"]\n\ntry:\n\n    from _locale import *\n\nexcept ImportError:\n\n    # Locale emulation\n\n    CHAR_MAX = 127\n    LC_ALL = 6\n    LC_COLLATE = 3\n    LC_CTYPE = 0\n    LC_MESSAGES = 5\n    LC_MONETARY = 4\n    LC_NUMERIC = 1\n    LC_TIME = 2\n    Error = ValueError\n\n    def localeconv():\n        \"\"\" localeconv() -> dict.\n            Returns numeric and monetary locale-specific parameters.\n        \"\"\"\n        # 'C' locale default values\n        return {'grouping': [127],\n                'currency_symbol': '',\n                'n_sign_posn': 127,\n                'p_cs_precedes': 127,\n                'n_cs_precedes': 127,\n                'mon_grouping': [],\n                'n_sep_by_space': 127,\n                'decimal_point': '.',\n                'negative_sign': '',\n                'positive_sign': '',\n                'p_sep_by_space': 127,\n                'int_curr_symbol': '',\n                'p_sign_posn': 127,\n                'thousands_sep': '',\n                'mon_thousands_sep': '',\n                'frac_digits': 127,\n                'mon_decimal_point': '',\n                'int_frac_digits': 127}\n\n    def setlocale(category, value=None):\n        \"\"\" setlocale(integer,string=None) -> string.\n            Activates/queries locale processing.\n        \"\"\"\n        if value not in (None, '', 'C'):\n            raise Error, '_locale emulation only supports \"C\" locale'\n        return 'C'\n\n    def strcoll(a,b):\n        \"\"\" strcoll(string,string) -> int.\n            Compares two strings according to the locale.\n        \"\"\"\n        return cmp(a,b)\n\n    def strxfrm(s):\n        \"\"\" strxfrm(string) -> string.\n            Returns a string that behaves for cmp locale-aware.\n        \"\"\"\n        return s\n\n\n_localeconv = localeconv\n\n# With this dict, you can override some items of localeconv's return value.\n# This is useful for testing purposes.\n_override_localeconv = {}\n\n@functools.wraps(_localeconv)\ndef localeconv():\n    d = _localeconv()\n    if _override_localeconv:\n        d.update(_override_localeconv)\n    return d\n\n\n### Number formatting APIs\n\n# Author: Martin von Loewis\n# improved by Georg Brandl\n\n# Iterate over grouping intervals\ndef _grouping_intervals(grouping):\n    last_interval = None\n    for interval in grouping:\n        # if grouping is -1, we are done\n        if interval == CHAR_MAX:\n            return\n        # 0: re-use last group ad infinitum\n        if interval == 0:\n            if last_interval is None:\n                raise ValueError(\"invalid grouping\")\n            while True:\n                yield last_interval\n        yield interval\n        last_interval = interval\n\n#perform the grouping from right to left\ndef _group(s, monetary=False):\n    conv = localeconv()\n    thousands_sep = conv[monetary and 'mon_thousands_sep' or 'thousands_sep']\n    grouping = conv[monetary and 'mon_grouping' or 'grouping']\n    if not grouping:\n        return (s, 0)\n    if s[-1] == ' ':\n        stripped = s.rstrip()\n        right_spaces = s[len(stripped):]\n        s = stripped\n    else:\n        right_spaces = ''\n    left_spaces = ''\n    groups = []\n    for interval in _grouping_intervals(grouping):\n        if not s or s[-1] not in \"0123456789\":\n            # only non-digit characters remain (sign, spaces)\n            left_spaces = s\n            s = ''\n            break\n        groups.append(s[-interval:])\n        s = s[:-interval]\n    if s:\n        groups.append(s)\n    groups.reverse()\n    return (\n        left_spaces + thousands_sep.join(groups) + right_spaces,\n        len(thousands_sep) * (len(groups) - 1)\n    )\n\n# Strip a given amount of excess padding from the given string\ndef _strip_padding(s, amount):\n    lpos = 0\n    while amount and s[lpos] == ' ':\n        lpos += 1\n        amount -= 1\n    rpos = len(s) - 1\n    while amount and s[rpos] == ' ':\n        rpos -= 1\n        amount -= 1\n    return s[lpos:rpos+1]\n\n_percent_re = re.compile(r'%(?:\\((?P<key>.*?)\\))?'\n                         r'(?P<modifiers>[-#0-9 +*.hlL]*?)[eEfFgGdiouxXcrs%]')\n\ndef format(percent, value, grouping=False, monetary=False, *additional):\n    \"\"\"Returns the locale-aware substitution of a %? specifier\n    (percent).\n\n    additional is for format strings which contain one or more\n    '*' modifiers.\"\"\"\n    # this is only for one-percent-specifier strings and this should be checked\n    match = _percent_re.match(percent)\n    if not match or len(match.group())!= len(percent):\n        raise ValueError((\"format() must be given exactly one %%char \"\n                         \"format specifier, %s not valid\") % repr(percent))\n    return _format(percent, value, grouping, monetary, *additional)\n\ndef _format(percent, value, grouping=False, monetary=False, *additional):\n    if additional:\n        formatted = percent % ((value,) + additional)\n    else:\n        formatted = percent % value\n    # floats and decimal ints need special action!\n    if percent[-1] in 'eEfFgG':\n        seps = 0\n        parts = formatted.split('.')\n        if grouping:\n            parts[0], seps = _group(parts[0], monetary=monetary)\n        decimal_point = localeconv()[monetary and 'mon_decimal_point'\n                                              or 'decimal_point']\n        formatted = decimal_point.join(parts)\n        if seps:\n            formatted = _strip_padding(formatted, seps)\n    elif percent[-1] in 'diu':\n        seps = 0\n        if grouping:\n            formatted, seps = _group(formatted, monetary=monetary)\n        if seps:\n            formatted = _strip_padding(formatted, seps)\n    return formatted\n\ndef format_string(f, val, grouping=False):\n    \"\"\"Formats a string in the same way that the % formatting would use,\n    but takes the current locale into account.\n    Grouping is applied if the third parameter is true.\"\"\"\n    percents = list(_percent_re.finditer(f))\n    new_f = _percent_re.sub('%s', f)\n\n    if operator.isMappingType(val):\n        new_val = []\n        for perc in percents:\n            if perc.group()[-1]=='%':\n                new_val.append('%')\n            else:\n                new_val.append(format(perc.group(), val, grouping))\n    else:\n        if not isinstance(val, tuple):\n            val = (val,)\n        new_val = []\n        i = 0\n        for perc in percents:\n            if perc.group()[-1]=='%':\n                new_val.append('%')\n            else:\n                starcount = perc.group('modifiers').count('*')\n                new_val.append(_format(perc.group(),\n                                      val[i],\n                                      grouping,\n                                      False,\n                                      *val[i+1:i+1+starcount]))\n                i += (1 + starcount)\n    val = tuple(new_val)\n\n    return new_f % val\n\ndef currency(val, symbol=True, grouping=False, international=False):\n    \"\"\"Formats val according to the currency settings\n    in the current locale.\"\"\"\n    conv = localeconv()\n\n    # check for illegal values\n    digits = conv[international and 'int_frac_digits' or 'frac_digits']\n    if digits == 127:\n        raise ValueError(\"Currency formatting is not possible using \"\n                         \"the 'C' locale.\")\n\n    s = format('%%.%if' % digits, abs(val), grouping, monetary=True)\n    # '<' and '>' are markers if the sign must be inserted between symbol and value\n    s = '<' + s + '>'\n\n    if symbol:\n        smb = conv[international and 'int_curr_symbol' or 'currency_symbol']\n        precedes = conv[val<0 and 'n_cs_precedes' or 'p_cs_precedes']\n        separated = conv[val<0 and 'n_sep_by_space' or 'p_sep_by_space']\n\n        if precedes:\n            s = smb + (separated and ' ' or '') + s\n        else:\n            s = s + (separated and ' ' or '') + smb\n\n    sign_pos = conv[val<0 and 'n_sign_posn' or 'p_sign_posn']\n    sign = conv[val<0 and 'negative_sign' or 'positive_sign']\n\n    if sign_pos == 0:\n        s = '(' + s + ')'\n    elif sign_pos == 1:\n        s = sign + s\n    elif sign_pos == 2:\n        s = s + sign\n    elif sign_pos == 3:\n        s = s.replace('<', sign)\n    elif sign_pos == 4:\n        s = s.replace('>', sign)\n    else:\n        # the default if nothing specified;\n        # this should be the most fitting sign position\n        s = sign + s\n\n    return s.replace('<', '').replace('>', '')\n\ndef str(val):\n    \"\"\"Convert float to integer, taking the locale into account.\"\"\"\n    return format(\"%.12g\", val)\n\ndef atof(string, func=float):\n    \"Parses a string as a float according to the locale settings.\"\n    #First, get rid of the grouping\n    ts = localeconv()['thousands_sep']\n    if ts:\n        string = string.replace(ts, '')\n    #next, replace the decimal point with a dot\n    dd = localeconv()['decimal_point']\n    if dd:\n        string = string.replace(dd, '.')\n    #finally, parse the string\n    return func(string)\n\ndef atoi(str):\n    \"Converts a string to an integer according to the locale settings.\"\n    return atof(str, int)\n\ndef _test():\n    setlocale(LC_ALL, \"\")\n    #do grouping\n    s1 = format(\"%d\", 123456789,1)\n    print s1, \"is\", atoi(s1)\n    #standard formatting\n    s1 = str(3.14)\n    print s1, \"is\", atof(s1)\n\n### Locale name aliasing engine\n\n# Author: Marc-Andre Lemburg, mal@lemburg.com\n# Various tweaks by Fredrik Lundh <fredrik@pythonware.com>\n\n# store away the low-level version of setlocale (it's\n# overridden below)\n_setlocale = setlocale\n\n# Avoid relying on the locale-dependent .lower() method\n# (see issue #1813).\n_ascii_lower_map = ''.join(\n    chr(x + 32 if x >= ord('A') and x <= ord('Z') else x)\n    for x in range(256)\n)\n\ndef _replace_encoding(code, encoding):\n    if '.' in code:\n        langname = code[:code.index('.')]\n    else:\n        langname = code\n    # Convert the encoding to a C lib compatible encoding string\n    norm_encoding = encodings.normalize_encoding(encoding)\n    #print('norm encoding: %r' % norm_encoding)\n    norm_encoding = encodings.aliases.aliases.get(norm_encoding,\n                                                  norm_encoding)\n    #print('aliased encoding: %r' % norm_encoding)\n    encoding = locale_encoding_alias.get(norm_encoding,\n                                         norm_encoding)\n    #print('found encoding %r' % encoding)\n    return langname + '.' + encoding\n\ndef normalize(localename):\n\n    \"\"\" Returns a normalized locale code for the given locale\n        name.\n\n        The returned locale code is formatted for use with\n        setlocale().\n\n        If normalization fails, the original name is returned\n        unchanged.\n\n        If the given encoding is not known, the function defaults to\n        the default encoding for the locale code just like setlocale()\n        does.\n\n    \"\"\"\n    # Normalize the locale name and extract the encoding and modifier\n    if isinstance(localename, _unicode):\n        localename = localename.encode('ascii')\n    code = localename.translate(_ascii_lower_map)\n    if ':' in code:\n        # ':' is sometimes used as encoding delimiter.\n        code = code.replace(':', '.')\n    if '@' in code:\n        code, modifier = code.split('@', 1)\n    else:\n        modifier = ''\n    if '.' in code:\n        langname, encoding = code.split('.')[:2]\n    else:\n        langname = code\n        encoding = ''\n\n    # First lookup: fullname (possibly with encoding and modifier)\n    lang_enc = langname\n    if encoding:\n        norm_encoding = encoding.replace('-', '')\n        norm_encoding = norm_encoding.replace('_', '')\n        lang_enc += '.' + norm_encoding\n    lookup_name = lang_enc\n    if modifier:\n        lookup_name += '@' + modifier\n    code = locale_alias.get(lookup_name, None)\n    if code is not None:\n        return code\n    #print('first lookup failed')\n\n    if modifier:\n        # Second try: fullname without modifier (possibly with encoding)\n        code = locale_alias.get(lang_enc, None)\n        if code is not None:\n            #print('lookup without modifier succeeded')\n            if '@' not in code:\n                return code + '@' + modifier\n            if code.split('@', 1)[1].translate(_ascii_lower_map) == modifier:\n                return code\n        #print('second lookup failed')\n\n    if encoding:\n        # Third try: langname (without encoding, possibly with modifier)\n        lookup_name = langname\n        if modifier:\n            lookup_name += '@' + modifier\n        code = locale_alias.get(lookup_name, None)\n        if code is not None:\n            #print('lookup without encoding succeeded')\n            if '@' not in code:\n                return _replace_encoding(code, encoding)\n            code, modifier = code.split('@', 1)\n            return _replace_encoding(code, encoding) + '@' + modifier\n\n        if modifier:\n            # Fourth try: langname (without encoding and modifier)\n            code = locale_alias.get(langname, None)\n            if code is not None:\n                #print('lookup without modifier and encoding succeeded')\n                if '@' not in code:\n                    return _replace_encoding(code, encoding) + '@' + modifier\n                code, defmod = code.split('@', 1)\n                if defmod.translate(_ascii_lower_map) == modifier:\n                    return _replace_encoding(code, encoding) + '@' + defmod\n\n    return localename\n\ndef _parse_localename(localename):\n\n    \"\"\" Parses the locale code for localename and returns the\n        result as tuple (language code, encoding).\n\n        The localename is normalized and passed through the locale\n        alias engine. A ValueError is raised in case the locale name\n        cannot be parsed.\n\n        The language code corresponds to RFC 1766.  code and encoding\n        can be None in case the values cannot be determined or are\n        unknown to this implementation.\n\n    \"\"\"\n    code = normalize(localename)\n    if '@' in code:\n        # Deal with locale modifiers\n        code, modifier = code.split('@', 1)\n        if modifier == 'euro' and '.' not in code:\n            # Assume Latin-9 for @euro locales. This is bogus,\n            # since some systems may use other encodings for these\n            # locales. Also, we ignore other modifiers.\n            return code, 'iso-8859-15'\n\n    if '.' in code:\n        return tuple(code.split('.')[:2])\n    elif code == 'C':\n        return None, None\n    raise ValueError, 'unknown locale: %s' % localename\n\ndef _build_localename(localetuple):\n\n    \"\"\" Builds a locale code from the given tuple (language code,\n        encoding).\n\n        No aliasing or normalizing takes place.\n\n    \"\"\"\n    language, encoding = localetuple\n    if language is None:\n        language = 'C'\n    if encoding is None:\n        return language\n    else:\n        return language + '.' + encoding\n\ndef getdefaultlocale(envvars=('LC_ALL', 'LC_CTYPE', 'LANG', 'LANGUAGE')):\n\n    \"\"\" Tries to determine the default locale settings and returns\n        them as tuple (language code, encoding).\n\n        According to POSIX, a program which has not called\n        setlocale(LC_ALL, \"\") runs using the portable 'C' locale.\n        Calling setlocale(LC_ALL, \"\") lets it use the default locale as\n        defined by the LANG variable. Since we don't want to interfere\n        with the current locale setting we thus emulate the behavior\n        in the way described above.\n\n        To maintain compatibility with other platforms, not only the\n        LANG variable is tested, but a list of variables given as\n        envvars parameter. The first found to be defined will be\n        used. envvars defaults to the search path used in GNU gettext;\n        it must always contain the variable name 'LANG'.\n\n        Except for the code 'C', the language code corresponds to RFC\n        1766.  code and encoding can be None in case the values cannot\n        be determined.\n\n    \"\"\"\n\n    try:\n        # check if it's supported by the _locale module\n        import _locale\n        code, encoding = _locale._getdefaultlocale()\n    except (ImportError, AttributeError):\n        pass\n    else:\n        # make sure the code/encoding values are valid\n        if sys.platform == \"win32\" and code and code[:2] == \"0x\":\n            # map windows language identifier to language name\n            code = windows_locale.get(int(code, 0))\n        # ...add other platform-specific processing here, if\n        # necessary...\n        return code, encoding\n\n    # fall back on POSIX behaviour\n    import os\n    lookup = os.environ.get\n    for variable in envvars:\n        localename = lookup(variable,None)\n        if localename:\n            if variable == 'LANGUAGE':\n                localename = localename.split(':')[0]\n            break\n    else:\n        localename = 'C'\n    return _parse_localename(localename)\n\n\ndef getlocale(category=LC_CTYPE):\n\n    \"\"\" Returns the current setting for the given locale category as\n        tuple (language code, encoding).\n\n        category may be one of the LC_* value except LC_ALL. It\n        defaults to LC_CTYPE.\n\n        Except for the code 'C', the language code corresponds to RFC\n        1766.  code and encoding can be None in case the values cannot\n        be determined.\n\n    \"\"\"\n    localename = _setlocale(category)\n    if category == LC_ALL and ';' in localename:\n        raise TypeError, 'category LC_ALL is not supported'\n    return _parse_localename(localename)\n\ndef setlocale(category, locale=None):\n\n    \"\"\" Set the locale for the given category.  The locale can be\n        a string, an iterable of two strings (language code and encoding),\n        or None.\n\n        Iterables are converted to strings using the locale aliasing\n        engine.  Locale strings are passed directly to the C lib.\n\n        category may be given as one of the LC_* values.\n\n    \"\"\"\n    if locale and type(locale) is not type(\"\"):\n        # convert to string\n        locale = normalize(_build_localename(locale))\n    return _setlocale(category, locale)\n\ndef resetlocale(category=LC_ALL):\n\n    \"\"\" Sets the locale for category to the default setting.\n\n        The default setting is determined by calling\n        getdefaultlocale(). category defaults to LC_ALL.\n\n    \"\"\"\n    _setlocale(category, _build_localename(getdefaultlocale()))\n\nif sys.platform.startswith(\"win\"):\n    # On Win32, this will return the ANSI code page\n    def getpreferredencoding(do_setlocale = True):\n        \"\"\"Return the charset that the user is likely using.\"\"\"\n        import _locale\n        return _locale._getdefaultlocale()[1]\nelse:\n    # On Unix, if CODESET is available, use that.\n    try:\n        CODESET\n    except NameError:\n        # Fall back to parsing environment variables :-(\n        def getpreferredencoding(do_setlocale = True):\n            \"\"\"Return the charset that the user is likely using,\n            by looking at environment variables.\"\"\"\n            return getdefaultlocale()[1]\n    else:\n        def getpreferredencoding(do_setlocale = True):\n            \"\"\"Return the charset that the user is likely using,\n            according to the system configuration.\"\"\"\n            if do_setlocale:\n                oldloc = setlocale(LC_CTYPE)\n                try:\n                    setlocale(LC_CTYPE, \"\")\n                except Error:\n                    pass\n                result = nl_langinfo(CODESET)\n                setlocale(LC_CTYPE, oldloc)\n                return result\n            else:\n                return nl_langinfo(CODESET)\n\n\n### Database\n#\n# The following data was extracted from the locale.alias file which\n# comes with X11 and then hand edited removing the explicit encoding\n# definitions and adding some more aliases. The file is usually\n# available as /usr/lib/X11/locale/locale.alias.\n#\n\n#\n# The local_encoding_alias table maps lowercase encoding alias names\n# to C locale encoding names (case-sensitive). Note that normalize()\n# first looks up the encoding in the encodings.aliases dictionary and\n# then applies this mapping to find the correct C lib name for the\n# encoding.\n#\nlocale_encoding_alias = {\n\n    # Mappings for non-standard encoding names used in locale names\n    '437':                          'C',\n    'c':                            'C',\n    'en':                           'ISO8859-1',\n    'jis':                          'JIS7',\n    'jis7':                         'JIS7',\n    'ajec':                         'eucJP',\n\n    # Mappings from Python codec names to C lib encoding names\n    'ascii':                        'ISO8859-1',\n    'latin_1':                      'ISO8859-1',\n    'iso8859_1':                    'ISO8859-1',\n    'iso8859_10':                   'ISO8859-10',\n    'iso8859_11':                   'ISO8859-11',\n    'iso8859_13':                   'ISO8859-13',\n    'iso8859_14':                   'ISO8859-14',\n    'iso8859_15':                   'ISO8859-15',\n    'iso8859_16':                   'ISO8859-16',\n    'iso8859_2':                    'ISO8859-2',\n    'iso8859_3':                    'ISO8859-3',\n    'iso8859_4':                    'ISO8859-4',\n    'iso8859_5':                    'ISO8859-5',\n    'iso8859_6':                    'ISO8859-6',\n    'iso8859_7':                    'ISO8859-7',\n    'iso8859_8':                    'ISO8859-8',\n    'iso8859_9':                    'ISO8859-9',\n    'iso2022_jp':                   'JIS7',\n    'shift_jis':                    'SJIS',\n    'tactis':                       'TACTIS',\n    'euc_jp':                       'eucJP',\n    'euc_kr':                       'eucKR',\n    'utf_8':                        'UTF-8',\n    'koi8_r':                       'KOI8-R',\n    'koi8_u':                       'KOI8-U',\n    # XXX This list is still incomplete. If you know more\n    # mappings, please file a bug report. Thanks.\n}\n\n#\n# The locale_alias table maps lowercase alias names to C locale names\n# (case-sensitive). Encodings are always separated from the locale\n# name using a dot ('.'); they should only be given in case the\n# language name is needed to interpret the given encoding alias\n# correctly (CJK codes often have this need).\n#\n# Note that the normalize() function which uses this tables\n# removes '_' and '-' characters from the encoding part of the\n# locale name before doing the lookup. This saves a lot of\n# space in the table.\n#\n# MAL 2004-12-10:\n# Updated alias mapping to most recent locale.alias file\n# from X.org distribution using makelocalealias.py.\n#\n# These are the differences compared to the old mapping (Python 2.4\n# and older):\n#\n#    updated 'bg' -> 'bg_BG.ISO8859-5' to 'bg_BG.CP1251'\n#    updated 'bg_bg' -> 'bg_BG.ISO8859-5' to 'bg_BG.CP1251'\n#    updated 'bulgarian' -> 'bg_BG.ISO8859-5' to 'bg_BG.CP1251'\n#    updated 'cz' -> 'cz_CZ.ISO8859-2' to 'cs_CZ.ISO8859-2'\n#    updated 'cz_cz' -> 'cz_CZ.ISO8859-2' to 'cs_CZ.ISO8859-2'\n#    updated 'czech' -> 'cs_CS.ISO8859-2' to 'cs_CZ.ISO8859-2'\n#    updated 'dutch' -> 'nl_BE.ISO8859-1' to 'nl_NL.ISO8859-1'\n#    updated 'et' -> 'et_EE.ISO8859-4' to 'et_EE.ISO8859-15'\n#    updated 'et_ee' -> 'et_EE.ISO8859-4' to 'et_EE.ISO8859-15'\n#    updated 'fi' -> 'fi_FI.ISO8859-1' to 'fi_FI.ISO8859-15'\n#    updated 'fi_fi' -> 'fi_FI.ISO8859-1' to 'fi_FI.ISO8859-15'\n#    updated 'iw' -> 'iw_IL.ISO8859-8' to 'he_IL.ISO8859-8'\n#    updated 'iw_il' -> 'iw_IL.ISO8859-8' to 'he_IL.ISO8859-8'\n#    updated 'japanese' -> 'ja_JP.SJIS' to 'ja_JP.eucJP'\n#    updated 'lt' -> 'lt_LT.ISO8859-4' to 'lt_LT.ISO8859-13'\n#    updated 'lv' -> 'lv_LV.ISO8859-4' to 'lv_LV.ISO8859-13'\n#    updated 'sl' -> 'sl_CS.ISO8859-2' to 'sl_SI.ISO8859-2'\n#    updated 'slovene' -> 'sl_CS.ISO8859-2' to 'sl_SI.ISO8859-2'\n#    updated 'th_th' -> 'th_TH.TACTIS' to 'th_TH.ISO8859-11'\n#    updated 'zh_cn' -> 'zh_CN.eucCN' to 'zh_CN.gb2312'\n#    updated 'zh_cn.big5' -> 'zh_TW.eucTW' to 'zh_TW.big5'\n#    updated 'zh_tw' -> 'zh_TW.eucTW' to 'zh_TW.big5'\n#\n# MAL 2008-05-30:\n# Updated alias mapping to most recent locale.alias file\n# from X.org distribution using makelocalealias.py.\n#\n# These are the differences compared to the old mapping (Python 2.5\n# and older):\n#\n#    updated 'cs_cs.iso88592' -> 'cs_CZ.ISO8859-2' to 'cs_CS.ISO8859-2'\n#    updated 'serbocroatian' -> 'sh_YU.ISO8859-2' to 'sr_CS.ISO8859-2'\n#    updated 'sh' -> 'sh_YU.ISO8859-2' to 'sr_CS.ISO8859-2'\n#    updated 'sh_hr.iso88592' -> 'sh_HR.ISO8859-2' to 'hr_HR.ISO8859-2'\n#    updated 'sh_sp' -> 'sh_YU.ISO8859-2' to 'sr_CS.ISO8859-2'\n#    updated 'sh_yu' -> 'sh_YU.ISO8859-2' to 'sr_CS.ISO8859-2'\n#    updated 'sp' -> 'sp_YU.ISO8859-5' to 'sr_CS.ISO8859-5'\n#    updated 'sp_yu' -> 'sp_YU.ISO8859-5' to 'sr_CS.ISO8859-5'\n#    updated 'sr' -> 'sr_YU.ISO8859-5' to 'sr_CS.ISO8859-5'\n#    updated 'sr@cyrillic' -> 'sr_YU.ISO8859-5' to 'sr_CS.ISO8859-5'\n#    updated 'sr_sp' -> 'sr_SP.ISO8859-2' to 'sr_CS.ISO8859-2'\n#    updated 'sr_yu' -> 'sr_YU.ISO8859-5' to 'sr_CS.ISO8859-5'\n#    updated 'sr_yu.cp1251@cyrillic' -> 'sr_YU.CP1251' to 'sr_CS.CP1251'\n#    updated 'sr_yu.iso88592' -> 'sr_YU.ISO8859-2' to 'sr_CS.ISO8859-2'\n#    updated 'sr_yu.iso88595' -> 'sr_YU.ISO8859-5' to 'sr_CS.ISO8859-5'\n#    updated 'sr_yu.iso88595@cyrillic' -> 'sr_YU.ISO8859-5' to 'sr_CS.ISO8859-5'\n#    updated 'sr_yu.microsoftcp1251@cyrillic' -> 'sr_YU.CP1251' to 'sr_CS.CP1251'\n#    updated 'sr_yu.utf8@cyrillic' -> 'sr_YU.UTF-8' to 'sr_CS.UTF-8'\n#    updated 'sr_yu@cyrillic' -> 'sr_YU.ISO8859-5' to 'sr_CS.ISO8859-5'\n#\n# AP 2010-04-12:\n# Updated alias mapping to most recent locale.alias file\n# from X.org distribution using makelocalealias.py.\n#\n# These are the differences compared to the old mapping (Python 2.6.5\n# and older):\n#\n#    updated 'ru' -> 'ru_RU.ISO8859-5' to 'ru_RU.UTF-8'\n#    updated 'ru_ru' -> 'ru_RU.ISO8859-5' to 'ru_RU.UTF-8'\n#    updated 'serbocroatian' -> 'sr_CS.ISO8859-2' to 'sr_RS.UTF-8@latin'\n#    updated 'sh' -> 'sr_CS.ISO8859-2' to 'sr_RS.UTF-8@latin'\n#    updated 'sh_yu' -> 'sr_CS.ISO8859-2' to 'sr_RS.UTF-8@latin'\n#    updated 'sr' -> 'sr_CS.ISO8859-5' to 'sr_RS.UTF-8'\n#    updated 'sr@cyrillic' -> 'sr_CS.ISO8859-5' to 'sr_RS.UTF-8'\n#    updated 'sr@latn' -> 'sr_CS.ISO8859-2' to 'sr_RS.UTF-8@latin'\n#    updated 'sr_cs.utf8@latn' -> 'sr_CS.UTF-8' to 'sr_RS.UTF-8@latin'\n#    updated 'sr_cs@latn' -> 'sr_CS.ISO8859-2' to 'sr_RS.UTF-8@latin'\n#    updated 'sr_yu' -> 'sr_CS.ISO8859-5' to 'sr_RS.UTF-8@latin'\n#    updated 'sr_yu.utf8@cyrillic' -> 'sr_CS.UTF-8' to 'sr_RS.UTF-8'\n#    updated 'sr_yu@cyrillic' -> 'sr_CS.ISO8859-5' to 'sr_RS.UTF-8'\n#\n# SS 2013-12-20:\n# Updated alias mapping to most recent locale.alias file\n# from X.org distribution using makelocalealias.py.\n#\n# These are the differences compared to the old mapping (Python 2.7.6\n# and older):\n#\n#    updated 'a3' -> 'a3_AZ.KOI8-C' to 'az_AZ.KOI8-C'\n#    updated 'a3_az' -> 'a3_AZ.KOI8-C' to 'az_AZ.KOI8-C'\n#    updated 'a3_az.koi8c' -> 'a3_AZ.KOI8-C' to 'az_AZ.KOI8-C'\n#    updated 'cs_cs.iso88592' -> 'cs_CS.ISO8859-2' to 'cs_CZ.ISO8859-2'\n#    updated 'hebrew' -> 'iw_IL.ISO8859-8' to 'he_IL.ISO8859-8'\n#    updated 'hebrew.iso88598' -> 'iw_IL.ISO8859-8' to 'he_IL.ISO8859-8'\n#    updated 'sd' -> 'sd_IN@devanagari.UTF-8' to 'sd_IN.UTF-8'\n#    updated 'sr@latn' -> 'sr_RS.UTF-8@latin' to 'sr_CS.UTF-8@latin'\n#    updated 'sr_cs' -> 'sr_RS.UTF-8' to 'sr_CS.UTF-8'\n#    updated 'sr_cs.utf8@latn' -> 'sr_RS.UTF-8@latin' to 'sr_CS.UTF-8@latin'\n#    updated 'sr_cs@latn' -> 'sr_RS.UTF-8@latin' to 'sr_CS.UTF-8@latin'\n#\n# SS 2014-10-01:\n# Updated alias mapping with glibc 2.19 supported locales.\n\nlocale_alias = {\n    'a3':                                   'az_AZ.KOI8-C',\n    'a3_az':                                'az_AZ.KOI8-C',\n    'a3_az.koi8c':                          'az_AZ.KOI8-C',\n    'a3_az.koic':                           'az_AZ.KOI8-C',\n    'aa_dj':                                'aa_DJ.ISO8859-1',\n    'aa_er':                                'aa_ER.UTF-8',\n    'aa_et':                                'aa_ET.UTF-8',\n    'af':                                   'af_ZA.ISO8859-1',\n    'af_za':                                'af_ZA.ISO8859-1',\n    'af_za.iso88591':                       'af_ZA.ISO8859-1',\n    'am':                                   'am_ET.UTF-8',\n    'am_et':                                'am_ET.UTF-8',\n    'american':                             'en_US.ISO8859-1',\n    'american.iso88591':                    'en_US.ISO8859-1',\n    'an_es':                                'an_ES.ISO8859-15',\n    'ar':                                   'ar_AA.ISO8859-6',\n    'ar_aa':                                'ar_AA.ISO8859-6',\n    'ar_aa.iso88596':                       'ar_AA.ISO8859-6',\n    'ar_ae':                                'ar_AE.ISO8859-6',\n    'ar_ae.iso88596':                       'ar_AE.ISO8859-6',\n    'ar_bh':                                'ar_BH.ISO8859-6',\n    'ar_bh.iso88596':                       'ar_BH.ISO8859-6',\n    'ar_dz':                                'ar_DZ.ISO8859-6',\n    'ar_dz.iso88596':                       'ar_DZ.ISO8859-6',\n    'ar_eg':                                'ar_EG.ISO8859-6',\n    'ar_eg.iso88596':                       'ar_EG.ISO8859-6',\n    'ar_in':                                'ar_IN.UTF-8',\n    'ar_iq':                                'ar_IQ.ISO8859-6',\n    'ar_iq.iso88596':                       'ar_IQ.ISO8859-6',\n    'ar_jo':                                'ar_JO.ISO8859-6',\n    'ar_jo.iso88596':                       'ar_JO.ISO8859-6',\n    'ar_kw':                                'ar_KW.ISO8859-6',\n    'ar_kw.iso88596':                       'ar_KW.ISO8859-6',\n    'ar_lb':                                'ar_LB.ISO8859-6',\n    'ar_lb.iso88596':                       'ar_LB.ISO8859-6',\n    'ar_ly':                                'ar_LY.ISO8859-6',\n    'ar_ly.iso88596':                       'ar_LY.ISO8859-6',\n    'ar_ma':                                'ar_MA.ISO8859-6',\n    'ar_ma.iso88596':                       'ar_MA.ISO8859-6',\n    'ar_om':                                'ar_OM.ISO8859-6',\n    'ar_om.iso88596':                       'ar_OM.ISO8859-6',\n    'ar_qa':                                'ar_QA.ISO8859-6',\n    'ar_qa.iso88596':                       'ar_QA.ISO8859-6',\n    'ar_sa':                                'ar_SA.ISO8859-6',\n    'ar_sa.iso88596':                       'ar_SA.ISO8859-6',\n    'ar_sd':                                'ar_SD.ISO8859-6',\n    'ar_sd.iso88596':                       'ar_SD.ISO8859-6',\n    'ar_sy':                                'ar_SY.ISO8859-6',\n    'ar_sy.iso88596':                       'ar_SY.ISO8859-6',\n    'ar_tn':                                'ar_TN.ISO8859-6',\n    'ar_tn.iso88596':                       'ar_TN.ISO8859-6',\n    'ar_ye':                                'ar_YE.ISO8859-6',\n    'ar_ye.iso88596':                       'ar_YE.ISO8859-6',\n    'arabic':                               'ar_AA.ISO8859-6',\n    'arabic.iso88596':                      'ar_AA.ISO8859-6',\n    'as':                                   'as_IN.UTF-8',\n    'as_in':                                'as_IN.UTF-8',\n    'ast_es':                               'ast_ES.ISO8859-15',\n    'ayc_pe':                               'ayc_PE.UTF-8',\n    'az':                                   'az_AZ.ISO8859-9E',\n    'az_az':                                'az_AZ.ISO8859-9E',\n    'az_az.iso88599e':                      'az_AZ.ISO8859-9E',\n    'be':                                   'be_BY.CP1251',\n    'be@latin':                             'be_BY.UTF-8@latin',\n    'be_bg.utf8':                           'bg_BG.UTF-8',\n    'be_by':                                'be_BY.CP1251',\n    'be_by.cp1251':                         'be_BY.CP1251',\n    'be_by.microsoftcp1251':                'be_BY.CP1251',\n    'be_by.utf8@latin':                     'be_BY.UTF-8@latin',\n    'be_by@latin':                          'be_BY.UTF-8@latin',\n    'bem_zm':                               'bem_ZM.UTF-8',\n    'ber_dz':                               'ber_DZ.UTF-8',\n    'ber_ma':                               'ber_MA.UTF-8',\n    'bg':                                   'bg_BG.CP1251',\n    'bg_bg':                                'bg_BG.CP1251',\n    'bg_bg.cp1251':                         'bg_BG.CP1251',\n    'bg_bg.iso88595':                       'bg_BG.ISO8859-5',\n    'bg_bg.koi8r':                          'bg_BG.KOI8-R',\n    'bg_bg.microsoftcp1251':                'bg_BG.CP1251',\n    'bho_in':                               'bho_IN.UTF-8',\n    'bn_bd':                                'bn_BD.UTF-8',\n    'bn_in':                                'bn_IN.UTF-8',\n    'bo_cn':                                'bo_CN.UTF-8',\n    'bo_in':                                'bo_IN.UTF-8',\n    'bokmal':                               'nb_NO.ISO8859-1',\n    'bokm\\xe5l':                            'nb_NO.ISO8859-1',\n    'br':                                   'br_FR.ISO8859-1',\n    'br_fr':                                'br_FR.ISO8859-1',\n    'br_fr.iso88591':                       'br_FR.ISO8859-1',\n    'br_fr.iso885914':                      'br_FR.ISO8859-14',\n    'br_fr.iso885915':                      'br_FR.ISO8859-15',\n    'br_fr.iso885915@euro':                 'br_FR.ISO8859-15',\n    'br_fr.utf8@euro':                      'br_FR.UTF-8',\n    'br_fr@euro':                           'br_FR.ISO8859-15',\n    'brx_in':                               'brx_IN.UTF-8',\n    'bs':                                   'bs_BA.ISO8859-2',\n    'bs_ba':                                'bs_BA.ISO8859-2',\n    'bs_ba.iso88592':                       'bs_BA.ISO8859-2',\n    'bulgarian':                            'bg_BG.CP1251',\n    'byn_er':                               'byn_ER.UTF-8',\n    'c':                                    'C',\n    'c-french':                             'fr_CA.ISO8859-1',\n    'c-french.iso88591':                    'fr_CA.ISO8859-1',\n    'c.ascii':                              'C',\n    'c.en':                                 'C',\n    'c.iso88591':                           'en_US.ISO8859-1',\n    'c.utf8':                               'en_US.UTF-8',\n    'c_c':                                  'C',\n    'c_c.c':                                'C',\n    'ca':                                   'ca_ES.ISO8859-1',\n    'ca_ad':                                'ca_AD.ISO8859-1',\n    'ca_ad.iso88591':                       'ca_AD.ISO8859-1',\n    'ca_ad.iso885915':                      'ca_AD.ISO8859-15',\n    'ca_ad.iso885915@euro':                 'ca_AD.ISO8859-15',\n    'ca_ad.utf8@euro':                      'ca_AD.UTF-8',\n    'ca_ad@euro':                           'ca_AD.ISO8859-15',\n    'ca_es':                                'ca_ES.ISO8859-1',\n    'ca_es.iso88591':                       'ca_ES.ISO8859-1',\n    'ca_es.iso885915':                      'ca_ES.ISO8859-15',\n    'ca_es.iso885915@euro':                 'ca_ES.ISO8859-15',\n    'ca_es.utf8@euro':                      'ca_ES.UTF-8',\n    'ca_es@valencia':                       'ca_ES.ISO8859-15@valencia',\n    'ca_es@euro':                           'ca_ES.ISO8859-15',\n    'ca_fr':                                'ca_FR.ISO8859-1',\n    'ca_fr.iso88591':                       'ca_FR.ISO8859-1',\n    'ca_fr.iso885915':                      'ca_FR.ISO8859-15',\n    'ca_fr.iso885915@euro':                 'ca_FR.ISO8859-15',\n    'ca_fr.utf8@euro':                      'ca_FR.UTF-8',\n    'ca_fr@euro':                           'ca_FR.ISO8859-15',\n    'ca_it':                                'ca_IT.ISO8859-1',\n    'ca_it.iso88591':                       'ca_IT.ISO8859-1',\n    'ca_it.iso885915':                      'ca_IT.ISO8859-15',\n    'ca_it.iso885915@euro':                 'ca_IT.ISO8859-15',\n    'ca_it.utf8@euro':                      'ca_IT.UTF-8',\n    'ca_it@euro':                           'ca_IT.ISO8859-15',\n    'catalan':                              'ca_ES.ISO8859-1',\n    'cextend':                              'en_US.ISO8859-1',\n    'cextend.en':                           'en_US.ISO8859-1',\n    'chinese-s':                            'zh_CN.eucCN',\n    'chinese-t':                            'zh_TW.eucTW',\n    'crh_ua':                               'crh_UA.UTF-8',\n    'croatian':                             'hr_HR.ISO8859-2',\n    'cs':                                   'cs_CZ.ISO8859-2',\n    'cs_cs':                                'cs_CZ.ISO8859-2',\n    'cs_cs.iso88592':                       'cs_CZ.ISO8859-2',\n    'cs_cz':                                'cs_CZ.ISO8859-2',\n    'cs_cz.iso88592':                       'cs_CZ.ISO8859-2',\n    'csb_pl':                               'csb_PL.UTF-8',\n    'cv_ru':                                'cv_RU.UTF-8',\n    'cy':                                   'cy_GB.ISO8859-1',\n    'cy_gb':                                'cy_GB.ISO8859-1',\n    'cy_gb.iso88591':                       'cy_GB.ISO8859-1',\n    'cy_gb.iso885914':                      'cy_GB.ISO8859-14',\n    'cy_gb.iso885915':                      'cy_GB.ISO8859-15',\n    'cy_gb@euro':                           'cy_GB.ISO8859-15',\n    'cz':                                   'cs_CZ.ISO8859-2',\n    'cz_cz':                                'cs_CZ.ISO8859-2',\n    'czech':                                'cs_CZ.ISO8859-2',\n    'da':                                   'da_DK.ISO8859-1',\n    'da.iso885915':                         'da_DK.ISO8859-15',\n    'da_dk':                                'da_DK.ISO8859-1',\n    'da_dk.88591':                          'da_DK.ISO8859-1',\n    'da_dk.885915':                         'da_DK.ISO8859-15',\n    'da_dk.iso88591':                       'da_DK.ISO8859-1',\n    'da_dk.iso885915':                      'da_DK.ISO8859-15',\n    'da_dk@euro':                           'da_DK.ISO8859-15',\n    'danish':                               'da_DK.ISO8859-1',\n    'danish.iso88591':                      'da_DK.ISO8859-1',\n    'dansk':                                'da_DK.ISO8859-1',\n    'de':                                   'de_DE.ISO8859-1',\n    'de.iso885915':                         'de_DE.ISO8859-15',\n    'de_at':                                'de_AT.ISO8859-1',\n    'de_at.iso88591':                       'de_AT.ISO8859-1',\n    'de_at.iso885915':                      'de_AT.ISO8859-15',\n    'de_at.iso885915@euro':                 'de_AT.ISO8859-15',\n    'de_at.utf8@euro':                      'de_AT.UTF-8',\n    'de_at@euro':                           'de_AT.ISO8859-15',\n    'de_be':                                'de_BE.ISO8859-1',\n    'de_be.iso88591':                       'de_BE.ISO8859-1',\n    'de_be.iso885915':                      'de_BE.ISO8859-15',\n    'de_be.iso885915@euro':                 'de_BE.ISO8859-15',\n    'de_be.utf8@euro':                      'de_BE.UTF-8',\n    'de_be@euro':                           'de_BE.ISO8859-15',\n    'de_ch':                                'de_CH.ISO8859-1',\n    'de_ch.iso88591':                       'de_CH.ISO8859-1',\n    'de_ch.iso885915':                      'de_CH.ISO8859-15',\n    'de_ch@euro':                           'de_CH.ISO8859-15',\n    'de_de':                                'de_DE.ISO8859-1',\n    'de_de.88591':                          'de_DE.ISO8859-1',\n    'de_de.885915':                         'de_DE.ISO8859-15',\n    'de_de.885915@euro':                    'de_DE.ISO8859-15',\n    'de_de.iso88591':                       'de_DE.ISO8859-1',\n    'de_de.iso885915':                      'de_DE.ISO8859-15',\n    'de_de.iso885915@euro':                 'de_DE.ISO8859-15',\n    'de_de.utf8@euro':                      'de_DE.UTF-8',\n    'de_de@euro':                           'de_DE.ISO8859-15',\n    'de_li.utf8':                           'de_LI.UTF-8',\n    'de_lu':                                'de_LU.ISO8859-1',\n    'de_lu.iso88591':                       'de_LU.ISO8859-1',\n    'de_lu.iso885915':                      'de_LU.ISO8859-15',\n    'de_lu.iso885915@euro':                 'de_LU.ISO8859-15',\n    'de_lu.utf8@euro':                      'de_LU.UTF-8',\n    'de_lu@euro':                           'de_LU.ISO8859-15',\n    'deutsch':                              'de_DE.ISO8859-1',\n    'doi_in':                               'doi_IN.UTF-8',\n    'dutch':                                'nl_NL.ISO8859-1',\n    'dutch.iso88591':                       'nl_BE.ISO8859-1',\n    'dv_mv':                                'dv_MV.UTF-8',\n    'dz_bt':                                'dz_BT.UTF-8',\n    'ee':                                   'ee_EE.ISO8859-4',\n    'ee_ee':                                'ee_EE.ISO8859-4',\n    'ee_ee.iso88594':                       'ee_EE.ISO8859-4',\n    'eesti':                                'et_EE.ISO8859-1',\n    'el':                                   'el_GR.ISO8859-7',\n    'el_cy':                                'el_CY.ISO8859-7',\n    'el_gr':                                'el_GR.ISO8859-7',\n    'el_gr.iso88597':                       'el_GR.ISO8859-7',\n    'el_gr@euro':                           'el_GR.ISO8859-15',\n    'en':                                   'en_US.ISO8859-1',\n    'en.iso88591':                          'en_US.ISO8859-1',\n    'en_ag':                                'en_AG.UTF-8',\n    'en_au':                                'en_AU.ISO8859-1',\n    'en_au.iso88591':                       'en_AU.ISO8859-1',\n    'en_be':                                'en_BE.ISO8859-1',\n    'en_be@euro':                           'en_BE.ISO8859-15',\n    'en_bw':                                'en_BW.ISO8859-1',\n    'en_bw.iso88591':                       'en_BW.ISO8859-1',\n    'en_ca':                                'en_CA.ISO8859-1',\n    'en_ca.iso88591':                       'en_CA.ISO8859-1',\n    'en_dk':                                'en_DK.ISO8859-1',\n    'en_dl.utf8':                           'en_DL.UTF-8',\n    'en_gb':                                'en_GB.ISO8859-1',\n    'en_gb.88591':                          'en_GB.ISO8859-1',\n    'en_gb.iso88591':                       'en_GB.ISO8859-1',\n    'en_gb.iso885915':                      'en_GB.ISO8859-15',\n    'en_gb@euro':                           'en_GB.ISO8859-15',\n    'en_hk':                                'en_HK.ISO8859-1',\n    'en_hk.iso88591':                       'en_HK.ISO8859-1',\n    'en_ie':                                'en_IE.ISO8859-1',\n    'en_ie.iso88591':                       'en_IE.ISO8859-1',\n    'en_ie.iso885915':                      'en_IE.ISO8859-15',\n    'en_ie.iso885915@euro':                 'en_IE.ISO8859-15',\n    'en_ie.utf8@euro':                      'en_IE.UTF-8',\n    'en_ie@euro':                           'en_IE.ISO8859-15',\n    'en_in':                                'en_IN.ISO8859-1',\n    'en_ng':                                'en_NG.UTF-8',\n    'en_nz':                                'en_NZ.ISO8859-1',\n    'en_nz.iso88591':                       'en_NZ.ISO8859-1',\n    'en_ph':                                'en_PH.ISO8859-1',\n    'en_ph.iso88591':                       'en_PH.ISO8859-1',\n    'en_sg':                                'en_SG.ISO8859-1',\n    'en_sg.iso88591':                       'en_SG.ISO8859-1',\n    'en_uk':                                'en_GB.ISO8859-1',\n    'en_us':                                'en_US.ISO8859-1',\n    'en_us.88591':                          'en_US.ISO8859-1',\n    'en_us.885915':                         'en_US.ISO8859-15',\n    'en_us.iso88591':                       'en_US.ISO8859-1',\n    'en_us.iso885915':                      'en_US.ISO8859-15',\n    'en_us.iso885915@euro':                 'en_US.ISO8859-15',\n    'en_us@euro':                           'en_US.ISO8859-15',\n    'en_us@euro@euro':                      'en_US.ISO8859-15',\n    'en_za':                                'en_ZA.ISO8859-1',\n    'en_za.88591':                          'en_ZA.ISO8859-1',\n    'en_za.iso88591':                       'en_ZA.ISO8859-1',\n    'en_za.iso885915':                      'en_ZA.ISO8859-15',\n    'en_za@euro':                           'en_ZA.ISO8859-15',\n    'en_zm':                                'en_ZM.UTF-8',\n    'en_zw':                                'en_ZW.ISO8859-1',\n    'en_zw.iso88591':                       'en_ZW.ISO8859-1',\n    'en_zw.utf8':                           'en_ZS.UTF-8',\n    'eng_gb':                               'en_GB.ISO8859-1',\n    'eng_gb.8859':                          'en_GB.ISO8859-1',\n    'english':                              'en_EN.ISO8859-1',\n    'english.iso88591':                     'en_EN.ISO8859-1',\n    'english_uk':                           'en_GB.ISO8859-1',\n    'english_uk.8859':                      'en_GB.ISO8859-1',\n    'english_united-states':                'en_US.ISO8859-1',\n    'english_united-states.437':            'C',\n    'english_us':                           'en_US.ISO8859-1',\n    'english_us.8859':                      'en_US.ISO8859-1',\n    'english_us.ascii':                     'en_US.ISO8859-1',\n    'eo':                                   'eo_XX.ISO8859-3',\n    'eo.utf8':                              'eo.UTF-8',\n    'eo_eo':                                'eo_EO.ISO8859-3',\n    'eo_eo.iso88593':                       'eo_EO.ISO8859-3',\n    'eo_us.utf8':                           'eo_US.UTF-8',\n    'eo_xx':                                'eo_XX.ISO8859-3',\n    'eo_xx.iso88593':                       'eo_XX.ISO8859-3',\n    'es':                                   'es_ES.ISO8859-1',\n    'es_ar':                                'es_AR.ISO8859-1',\n    'es_ar.iso88591':                       'es_AR.ISO8859-1',\n    'es_bo':                                'es_BO.ISO8859-1',\n    'es_bo.iso88591':                       'es_BO.ISO8859-1',\n    'es_cl':                                'es_CL.ISO8859-1',\n    'es_cl.iso88591':                       'es_CL.ISO8859-1',\n    'es_co':                                'es_CO.ISO8859-1',\n    'es_co.iso88591':                       'es_CO.ISO8859-1',\n    'es_cr':                                'es_CR.ISO8859-1',\n    'es_cr.iso88591':                       'es_CR.ISO8859-1',\n    'es_cu':                                'es_CU.UTF-8',\n    'es_do':                                'es_DO.ISO8859-1',\n    'es_do.iso88591':                       'es_DO.ISO8859-1',\n    'es_ec':                                'es_EC.ISO8859-1',\n    'es_ec.iso88591':                       'es_EC.ISO8859-1',\n    'es_es':                                'es_ES.ISO8859-1',\n    'es_es.88591':                          'es_ES.ISO8859-1',\n    'es_es.iso88591':                       'es_ES.ISO8859-1',\n    'es_es.iso885915':                      'es_ES.ISO8859-15',\n    'es_es.iso885915@euro':                 'es_ES.ISO8859-15',\n    'es_es.utf8@euro':                      'es_ES.UTF-8',\n    'es_es@euro':                           'es_ES.ISO8859-15',\n    'es_gt':                                'es_GT.ISO8859-1',\n    'es_gt.iso88591':                       'es_GT.ISO8859-1',\n    'es_hn':                                'es_HN.ISO8859-1',\n    'es_hn.iso88591':                       'es_HN.ISO8859-1',\n    'es_mx':                                'es_MX.ISO8859-1',\n    'es_mx.iso88591':                       'es_MX.ISO8859-1',\n    'es_ni':                                'es_NI.ISO8859-1',\n    'es_ni.iso88591':                       'es_NI.ISO8859-1',\n    'es_pa':                                'es_PA.ISO8859-1',\n    'es_pa.iso88591':                       'es_PA.ISO8859-1',\n    'es_pa.iso885915':                      'es_PA.ISO8859-15',\n    'es_pa@euro':                           'es_PA.ISO8859-15',\n    'es_pe':                                'es_PE.ISO8859-1',\n    'es_pe.iso88591':                       'es_PE.ISO8859-1',\n    'es_pe.iso885915':                      'es_PE.ISO8859-15',\n    'es_pe@euro':                           'es_PE.ISO8859-15',\n    'es_pr':                                'es_PR.ISO8859-1',\n    'es_pr.iso88591':                       'es_PR.ISO8859-1',\n    'es_py':                                'es_PY.ISO8859-1',\n    'es_py.iso88591':                       'es_PY.ISO8859-1',\n    'es_py.iso885915':                      'es_PY.ISO8859-15',\n    'es_py@euro':                           'es_PY.ISO8859-15',\n    'es_sv':                                'es_SV.ISO8859-1',\n    'es_sv.iso88591':                       'es_SV.ISO8859-1',\n    'es_sv.iso885915':                      'es_SV.ISO8859-15',\n    'es_sv@euro':                           'es_SV.ISO8859-15',\n    'es_us':                                'es_US.ISO8859-1',\n    'es_us.iso88591':                       'es_US.ISO8859-1',\n    'es_uy':                                'es_UY.ISO8859-1',\n    'es_uy.iso88591':                       'es_UY.ISO8859-1',\n    'es_uy.iso885915':                      'es_UY.ISO8859-15',\n    'es_uy@euro':                           'es_UY.ISO8859-15',\n    'es_ve':                                'es_VE.ISO8859-1',\n    'es_ve.iso88591':                       'es_VE.ISO8859-1',\n    'es_ve.iso885915':                      'es_VE.ISO8859-15',\n    'es_ve@euro':                           'es_VE.ISO8859-15',\n    'estonian':                             'et_EE.ISO8859-1',\n    'et':                                   'et_EE.ISO8859-15',\n    'et_ee':                                'et_EE.ISO8859-15',\n    'et_ee.iso88591':                       'et_EE.ISO8859-1',\n    'et_ee.iso885913':                      'et_EE.ISO8859-13',\n    'et_ee.iso885915':                      'et_EE.ISO8859-15',\n    'et_ee.iso88594':                       'et_EE.ISO8859-4',\n    'et_ee@euro':                           'et_EE.ISO8859-15',\n    'eu':                                   'eu_ES.ISO8859-1',\n    'eu_es':                                'eu_ES.ISO8859-1',\n    'eu_es.iso88591':                       'eu_ES.ISO8859-1',\n    'eu_es.iso885915':                      'eu_ES.ISO8859-15',\n    'eu_es.iso885915@euro':                 'eu_ES.ISO8859-15',\n    'eu_es.utf8@euro':                      'eu_ES.UTF-8',\n    'eu_es@euro':                           'eu_ES.ISO8859-15',\n    'eu_fr':                                'eu_FR.ISO8859-1',\n    'fa':                                   'fa_IR.UTF-8',\n    'fa_ir':                                'fa_IR.UTF-8',\n    'fa_ir.isiri3342':                      'fa_IR.ISIRI-3342',\n    'ff_sn':                                'ff_SN.UTF-8',\n    'fi':                                   'fi_FI.ISO8859-15',\n    'fi.iso885915':                         'fi_FI.ISO8859-15',\n    'fi_fi':                                'fi_FI.ISO8859-15',\n    'fi_fi.88591':                          'fi_FI.ISO8859-1',\n    'fi_fi.iso88591':                       'fi_FI.ISO8859-1',\n    'fi_fi.iso885915':                      'fi_FI.ISO8859-15',\n    'fi_fi.iso885915@euro':                 'fi_FI.ISO8859-15',\n    'fi_fi.utf8@euro':                      'fi_FI.UTF-8',\n    'fi_fi@euro':                           'fi_FI.ISO8859-15',\n    'fil_ph':                               'fil_PH.UTF-8',\n    'finnish':                              'fi_FI.ISO8859-1',\n    'finnish.iso88591':                     'fi_FI.ISO8859-1',\n    'fo':                                   'fo_FO.ISO8859-1',\n    'fo_fo':                                'fo_FO.ISO8859-1',\n    'fo_fo.iso88591':                       'fo_FO.ISO8859-1',\n    'fo_fo.iso885915':                      'fo_FO.ISO8859-15',\n    'fo_fo@euro':                           'fo_FO.ISO8859-15',\n    'fr':                                   'fr_FR.ISO8859-1',\n    'fr.iso885915':                         'fr_FR.ISO8859-15',\n    'fr_be':                                'fr_BE.ISO8859-1',\n    'fr_be.88591':                          'fr_BE.ISO8859-1',\n    'fr_be.iso88591':                       'fr_BE.ISO8859-1',\n    'fr_be.iso885915':                      'fr_BE.ISO8859-15',\n    'fr_be.iso885915@euro':                 'fr_BE.ISO8859-15',\n    'fr_be.utf8@euro':                      'fr_BE.UTF-8',\n    'fr_be@euro':                           'fr_BE.ISO8859-15',\n    'fr_ca':                                'fr_CA.ISO8859-1',\n    'fr_ca.88591':                          'fr_CA.ISO8859-1',\n    'fr_ca.iso88591':                       'fr_CA.ISO8859-1',\n    'fr_ca.iso885915':                      'fr_CA.ISO8859-15',\n    'fr_ca@euro':                           'fr_CA.ISO8859-15',\n    'fr_ch':                                'fr_CH.ISO8859-1',\n    'fr_ch.88591':                          'fr_CH.ISO8859-1',\n    'fr_ch.iso88591':                       'fr_CH.ISO8859-1',\n    'fr_ch.iso885915':                      'fr_CH.ISO8859-15',\n    'fr_ch@euro':                           'fr_CH.ISO8859-15',\n    'fr_fr':                                'fr_FR.ISO8859-1',\n    'fr_fr.88591':                          'fr_FR.ISO8859-1',\n    'fr_fr.iso88591':                       'fr_FR.ISO8859-1',\n    'fr_fr.iso885915':                      'fr_FR.ISO8859-15',\n    'fr_fr.iso885915@euro':                 'fr_FR.ISO8859-15',\n    'fr_fr.utf8@euro':                      'fr_FR.UTF-8',\n    'fr_fr@euro':                           'fr_FR.ISO8859-15',\n    'fr_lu':                                'fr_LU.ISO8859-1',\n    'fr_lu.88591':                          'fr_LU.ISO8859-1',\n    'fr_lu.iso88591':                       'fr_LU.ISO8859-1',\n    'fr_lu.iso885915':                      'fr_LU.ISO8859-15',\n    'fr_lu.iso885915@euro':                 'fr_LU.ISO8859-15',\n    'fr_lu.utf8@euro':                      'fr_LU.UTF-8',\n    'fr_lu@euro':                           'fr_LU.ISO8859-15',\n    'fran\\xe7ais':                          'fr_FR.ISO8859-1',\n    'fre_fr':                               'fr_FR.ISO8859-1',\n    'fre_fr.8859':                          'fr_FR.ISO8859-1',\n    'french':                               'fr_FR.ISO8859-1',\n    'french.iso88591':                      'fr_CH.ISO8859-1',\n    'french_france':                        'fr_FR.ISO8859-1',\n    'french_france.8859':                   'fr_FR.ISO8859-1',\n    'fur_it':                               'fur_IT.UTF-8',\n    'fy_de':                                'fy_DE.UTF-8',\n    'fy_nl':                                'fy_NL.UTF-8',\n    'ga':                                   'ga_IE.ISO8859-1',\n    'ga_ie':                                'ga_IE.ISO8859-1',\n    'ga_ie.iso88591':                       'ga_IE.ISO8859-1',\n    'ga_ie.iso885914':                      'ga_IE.ISO8859-14',\n    'ga_ie.iso885915':                      'ga_IE.ISO8859-15',\n    'ga_ie.iso885915@euro':                 'ga_IE.ISO8859-15',\n    'ga_ie.utf8@euro':                      'ga_IE.UTF-8',\n    'ga_ie@euro':                           'ga_IE.ISO8859-15',\n    'galego':                               'gl_ES.ISO8859-1',\n    'galician':                             'gl_ES.ISO8859-1',\n    'gd':                                   'gd_GB.ISO8859-1',\n    'gd_gb':                                'gd_GB.ISO8859-1',\n    'gd_gb.iso88591':                       'gd_GB.ISO8859-1',\n    'gd_gb.iso885914':                      'gd_GB.ISO8859-14',\n    'gd_gb.iso885915':                      'gd_GB.ISO8859-15',\n    'gd_gb@euro':                           'gd_GB.ISO8859-15',\n    'ger_de':                               'de_DE.ISO8859-1',\n    'ger_de.8859':                          'de_DE.ISO8859-1',\n    'german':                               'de_DE.ISO8859-1',\n    'german.iso88591':                      'de_CH.ISO8859-1',\n    'german_germany':                       'de_DE.ISO8859-1',\n    'german_germany.8859':                  'de_DE.ISO8859-1',\n    'gez_er':                               'gez_ER.UTF-8',\n    'gez_et':                               'gez_ET.UTF-8',\n    'gl':                                   'gl_ES.ISO8859-1',\n    'gl_es':                                'gl_ES.ISO8859-1',\n    'gl_es.iso88591':                       'gl_ES.ISO8859-1',\n    'gl_es.iso885915':                      'gl_ES.ISO8859-15',\n    'gl_es.iso885915@euro':                 'gl_ES.ISO8859-15',\n    'gl_es.utf8@euro':                      'gl_ES.UTF-8',\n    'gl_es@euro':                           'gl_ES.ISO8859-15',\n    'greek':                                'el_GR.ISO8859-7',\n    'greek.iso88597':                       'el_GR.ISO8859-7',\n    'gu_in':                                'gu_IN.UTF-8',\n    'gv':                                   'gv_GB.ISO8859-1',\n    'gv_gb':                                'gv_GB.ISO8859-1',\n    'gv_gb.iso88591':                       'gv_GB.ISO8859-1',\n    'gv_gb.iso885914':                      'gv_GB.ISO8859-14',\n    'gv_gb.iso885915':                      'gv_GB.ISO8859-15',\n    'gv_gb@euro':                           'gv_GB.ISO8859-15',\n    'ha_ng':                                'ha_NG.UTF-8',\n    'he':                                   'he_IL.ISO8859-8',\n    'he_il':                                'he_IL.ISO8859-8',\n    'he_il.cp1255':                         'he_IL.CP1255',\n    'he_il.iso88598':                       'he_IL.ISO8859-8',\n    'he_il.microsoftcp1255':                'he_IL.CP1255',\n    'hebrew':                               'he_IL.ISO8859-8',\n    'hebrew.iso88598':                      'he_IL.ISO8859-8',\n    'hi':                                   'hi_IN.ISCII-DEV',\n    'hi_in':                                'hi_IN.ISCII-DEV',\n    'hi_in.isciidev':                       'hi_IN.ISCII-DEV',\n    'hne':                                  'hne_IN.UTF-8',\n    'hne_in':                               'hne_IN.UTF-8',\n    'hr':                                   'hr_HR.ISO8859-2',\n    'hr_hr':                                'hr_HR.ISO8859-2',\n    'hr_hr.iso88592':                       'hr_HR.ISO8859-2',\n    'hrvatski':                             'hr_HR.ISO8859-2',\n    'hsb_de':                               'hsb_DE.ISO8859-2',\n    'ht_ht':                                'ht_HT.UTF-8',\n    'hu':                                   'hu_HU.ISO8859-2',\n    'hu_hu':                                'hu_HU.ISO8859-2',\n    'hu_hu.iso88592':                       'hu_HU.ISO8859-2',\n    'hungarian':                            'hu_HU.ISO8859-2',\n    'hy_am':                                'hy_AM.UTF-8',\n    'hy_am.armscii8':                       'hy_AM.ARMSCII_8',\n    'ia':                                   'ia.UTF-8',\n    'ia_fr':                                'ia_FR.UTF-8',\n    'icelandic':                            'is_IS.ISO8859-1',\n    'icelandic.iso88591':                   'is_IS.ISO8859-1',\n    'id':                                   'id_ID.ISO8859-1',\n    'id_id':                                'id_ID.ISO8859-1',\n    'ig_ng':                                'ig_NG.UTF-8',\n    'ik_ca':                                'ik_CA.UTF-8',\n    'in':                                   'id_ID.ISO8859-1',\n    'in_id':                                'id_ID.ISO8859-1',\n    'is':                                   'is_IS.ISO8859-1',\n    'is_is':                                'is_IS.ISO8859-1',\n    'is_is.iso88591':                       'is_IS.ISO8859-1',\n    'is_is.iso885915':                      'is_IS.ISO8859-15',\n    'is_is@euro':                           'is_IS.ISO8859-15',\n    'iso-8859-1':                           'en_US.ISO8859-1',\n    'iso-8859-15':                          'en_US.ISO8859-15',\n    'iso8859-1':                            'en_US.ISO8859-1',\n    'iso8859-15':                           'en_US.ISO8859-15',\n    'iso_8859_1':                           'en_US.ISO8859-1',\n    'iso_8859_15':                          'en_US.ISO8859-15',\n    'it':                                   'it_IT.ISO8859-1',\n    'it.iso885915':                         'it_IT.ISO8859-15',\n    'it_ch':                                'it_CH.ISO8859-1',\n    'it_ch.iso88591':                       'it_CH.ISO8859-1',\n    'it_ch.iso885915':                      'it_CH.ISO8859-15',\n    'it_ch@euro':                           'it_CH.ISO8859-15',\n    'it_it':                                'it_IT.ISO8859-1',\n    'it_it.88591':                          'it_IT.ISO8859-1',\n    'it_it.iso88591':                       'it_IT.ISO8859-1',\n    'it_it.iso885915':                      'it_IT.ISO8859-15',\n    'it_it.iso885915@euro':                 'it_IT.ISO8859-15',\n    'it_it.utf8@euro':                      'it_IT.UTF-8',\n    'it_it@euro':                           'it_IT.ISO8859-15',\n    'italian':                              'it_IT.ISO8859-1',\n    'italian.iso88591':                     'it_IT.ISO8859-1',\n    'iu':                                   'iu_CA.NUNACOM-8',\n    'iu_ca':                                'iu_CA.NUNACOM-8',\n    'iu_ca.nunacom8':                       'iu_CA.NUNACOM-8',\n    'iw':                                   'he_IL.ISO8859-8',\n    'iw_il':                                'he_IL.ISO8859-8',\n    'iw_il.iso88598':                       'he_IL.ISO8859-8',\n    'iw_il.utf8':                           'iw_IL.UTF-8',\n    'ja':                                   'ja_JP.eucJP',\n    'ja.jis':                               'ja_JP.JIS7',\n    'ja.sjis':                              'ja_JP.SJIS',\n    'ja_jp':                                'ja_JP.eucJP',\n    'ja_jp.ajec':                           'ja_JP.eucJP',\n    'ja_jp.euc':                            'ja_JP.eucJP',\n    'ja_jp.eucjp':                          'ja_JP.eucJP',\n    'ja_jp.iso-2022-jp':                    'ja_JP.JIS7',\n    'ja_jp.iso2022jp':                      'ja_JP.JIS7',\n    'ja_jp.jis':                            'ja_JP.JIS7',\n    'ja_jp.jis7':                           'ja_JP.JIS7',\n    'ja_jp.mscode':                         'ja_JP.SJIS',\n    'ja_jp.pck':                            'ja_JP.SJIS',\n    'ja_jp.sjis':                           'ja_JP.SJIS',\n    'ja_jp.ujis':                           'ja_JP.eucJP',\n    'japan':                                'ja_JP.eucJP',\n    'japanese':                             'ja_JP.eucJP',\n    'japanese-euc':                         'ja_JP.eucJP',\n    'japanese.euc':                         'ja_JP.eucJP',\n    'japanese.sjis':                        'ja_JP.SJIS',\n    'jp_jp':                                'ja_JP.eucJP',\n    'ka':                                   'ka_GE.GEORGIAN-ACADEMY',\n    'ka_ge':                                'ka_GE.GEORGIAN-ACADEMY',\n    'ka_ge.georgianacademy':                'ka_GE.GEORGIAN-ACADEMY',\n    'ka_ge.georgianps':                     'ka_GE.GEORGIAN-PS',\n    'ka_ge.georgianrs':                     'ka_GE.GEORGIAN-ACADEMY',\n    'kk_kz':                                'kk_KZ.RK1048',\n    'kl':                                   'kl_GL.ISO8859-1',\n    'kl_gl':                                'kl_GL.ISO8859-1',\n    'kl_gl.iso88591':                       'kl_GL.ISO8859-1',\n    'kl_gl.iso885915':                      'kl_GL.ISO8859-15',\n    'kl_gl@euro':                           'kl_GL.ISO8859-15',\n    'km_kh':                                'km_KH.UTF-8',\n    'kn':                                   'kn_IN.UTF-8',\n    'kn_in':                                'kn_IN.UTF-8',\n    'ko':                                   'ko_KR.eucKR',\n    'ko_kr':                                'ko_KR.eucKR',\n    'ko_kr.euc':                            'ko_KR.eucKR',\n    'ko_kr.euckr':                          'ko_KR.eucKR',\n    'kok_in':                               'kok_IN.UTF-8',\n    'korean':                               'ko_KR.eucKR',\n    'korean.euc':                           'ko_KR.eucKR',\n    'ks':                                   'ks_IN.UTF-8',\n    'ks_in':                                'ks_IN.UTF-8',\n    'ks_in@devanagari':                     'ks_IN.UTF-8@devanagari',\n    'ks_in@devanagari.utf8':                'ks_IN.UTF-8@devanagari',\n    'ku_tr':                                'ku_TR.ISO8859-9',\n    'kw':                                   'kw_GB.ISO8859-1',\n    'kw_gb':                                'kw_GB.ISO8859-1',\n    'kw_gb.iso88591':                       'kw_GB.ISO8859-1',\n    'kw_gb.iso885914':                      'kw_GB.ISO8859-14',\n    'kw_gb.iso885915':                      'kw_GB.ISO8859-15',\n    'kw_gb@euro':                           'kw_GB.ISO8859-15',\n    'ky':                                   'ky_KG.UTF-8',\n    'ky_kg':                                'ky_KG.UTF-8',\n    'lb_lu':                                'lb_LU.UTF-8',\n    'lg_ug':                                'lg_UG.ISO8859-10',\n    'li_be':                                'li_BE.UTF-8',\n    'li_nl':                                'li_NL.UTF-8',\n    'lij_it':                               'lij_IT.UTF-8',\n    'lithuanian':                           'lt_LT.ISO8859-13',\n    'lo':                                   'lo_LA.MULELAO-1',\n    'lo_la':                                'lo_LA.MULELAO-1',\n    'lo_la.cp1133':                         'lo_LA.IBM-CP1133',\n    'lo_la.ibmcp1133':                      'lo_LA.IBM-CP1133',\n    'lo_la.mulelao1':                       'lo_LA.MULELAO-1',\n    'lt':                                   'lt_LT.ISO8859-13',\n    'lt_lt':                                'lt_LT.ISO8859-13',\n    'lt_lt.iso885913':                      'lt_LT.ISO8859-13',\n    'lt_lt.iso88594':                       'lt_LT.ISO8859-4',\n    'lv':                                   'lv_LV.ISO8859-13',\n    'lv_lv':                                'lv_LV.ISO8859-13',\n    'lv_lv.iso885913':                      'lv_LV.ISO8859-13',\n    'lv_lv.iso88594':                       'lv_LV.ISO8859-4',\n    'mag_in':                               'mag_IN.UTF-8',\n    'mai':                                  'mai_IN.UTF-8',\n    'mai_in':                               'mai_IN.UTF-8',\n    'mg_mg':                                'mg_MG.ISO8859-15',\n    'mhr_ru':                               'mhr_RU.UTF-8',\n    'mi':                                   'mi_NZ.ISO8859-1',\n    'mi_nz':                                'mi_NZ.ISO8859-1',\n    'mi_nz.iso88591':                       'mi_NZ.ISO8859-1',\n    'mk':                                   'mk_MK.ISO8859-5',\n    'mk_mk':                                'mk_MK.ISO8859-5',\n    'mk_mk.cp1251':                         'mk_MK.CP1251',\n    'mk_mk.iso88595':                       'mk_MK.ISO8859-5',\n    'mk_mk.microsoftcp1251':                'mk_MK.CP1251',\n    'ml':                                   'ml_IN.UTF-8',\n    'ml_in':                                'ml_IN.UTF-8',\n    'mn_mn':                                'mn_MN.UTF-8',\n    'mni_in':                               'mni_IN.UTF-8',\n    'mr':                                   'mr_IN.UTF-8',\n    'mr_in':                                'mr_IN.UTF-8',\n    'ms':                                   'ms_MY.ISO8859-1',\n    'ms_my':                                'ms_MY.ISO8859-1',\n    'ms_my.iso88591':                       'ms_MY.ISO8859-1',\n    'mt':                                   'mt_MT.ISO8859-3',\n    'mt_mt':                                'mt_MT.ISO8859-3',\n    'mt_mt.iso88593':                       'mt_MT.ISO8859-3',\n    'my_mm':                                'my_MM.UTF-8',\n    'nan_tw@latin':                         'nan_TW.UTF-8@latin',\n    'nb':                                   'nb_NO.ISO8859-1',\n    'nb_no':                                'nb_NO.ISO8859-1',\n    'nb_no.88591':                          'nb_NO.ISO8859-1',\n    'nb_no.iso88591':                       'nb_NO.ISO8859-1',\n    'nb_no.iso885915':                      'nb_NO.ISO8859-15',\n    'nb_no@euro':                           'nb_NO.ISO8859-15',\n    'nds_de':                               'nds_DE.UTF-8',\n    'nds_nl':                               'nds_NL.UTF-8',\n    'ne_np':                                'ne_NP.UTF-8',\n    'nhn_mx':                               'nhn_MX.UTF-8',\n    'niu_nu':                               'niu_NU.UTF-8',\n    'niu_nz':                               'niu_NZ.UTF-8',\n    'nl':                                   'nl_NL.ISO8859-1',\n    'nl.iso885915':                         'nl_NL.ISO8859-15',\n    'nl_aw':                                'nl_AW.UTF-8',\n    'nl_be':                                'nl_BE.ISO8859-1',\n    'nl_be.88591':                          'nl_BE.ISO8859-1',\n    'nl_be.iso88591':                       'nl_BE.ISO8859-1',\n    'nl_be.iso885915':                      'nl_BE.ISO8859-15',\n    'nl_be.iso885915@euro':                 'nl_BE.ISO8859-15',\n    'nl_be.utf8@euro':                      'nl_BE.UTF-8',\n    'nl_be@euro':                           'nl_BE.ISO8859-15',\n    'nl_nl':                                'nl_NL.ISO8859-1',\n    'nl_nl.88591':                          'nl_NL.ISO8859-1',\n    'nl_nl.iso88591':                       'nl_NL.ISO8859-1',\n    'nl_nl.iso885915':                      'nl_NL.ISO8859-15',\n    'nl_nl.iso885915@euro':                 'nl_NL.ISO8859-15',\n    'nl_nl.utf8@euro':                      'nl_NL.UTF-8',\n    'nl_nl@euro':                           'nl_NL.ISO8859-15',\n    'nn':                                   'nn_NO.ISO8859-1',\n    'nn_no':                                'nn_NO.ISO8859-1',\n    'nn_no.88591':                          'nn_NO.ISO8859-1',\n    'nn_no.iso88591':                       'nn_NO.ISO8859-1',\n    'nn_no.iso885915':                      'nn_NO.ISO8859-15',\n    'nn_no@euro':                           'nn_NO.ISO8859-15',\n    'no':                                   'no_NO.ISO8859-1',\n    'no@nynorsk':                           'ny_NO.ISO8859-1',\n    'no_no':                                'no_NO.ISO8859-1',\n    'no_no.88591':                          'no_NO.ISO8859-1',\n    'no_no.iso88591':                       'no_NO.ISO8859-1',\n    'no_no.iso885915':                      'no_NO.ISO8859-15',\n    'no_no.iso88591@bokmal':                'no_NO.ISO8859-1',\n    'no_no.iso88591@nynorsk':               'no_NO.ISO8859-1',\n    'no_no@euro':                           'no_NO.ISO8859-15',\n    'norwegian':                            'no_NO.ISO8859-1',\n    'norwegian.iso88591':                   'no_NO.ISO8859-1',\n    'nr':                                   'nr_ZA.ISO8859-1',\n    'nr_za':                                'nr_ZA.ISO8859-1',\n    'nr_za.iso88591':                       'nr_ZA.ISO8859-1',\n    'nso':                                  'nso_ZA.ISO8859-15',\n    'nso_za':                               'nso_ZA.ISO8859-15',\n    'nso_za.iso885915':                     'nso_ZA.ISO8859-15',\n    'ny':                                   'ny_NO.ISO8859-1',\n    'ny_no':                                'ny_NO.ISO8859-1',\n    'ny_no.88591':                          'ny_NO.ISO8859-1',\n    'ny_no.iso88591':                       'ny_NO.ISO8859-1',\n    'ny_no.iso885915':                      'ny_NO.ISO8859-15',\n    'ny_no@euro':                           'ny_NO.ISO8859-15',\n    'nynorsk':                              'nn_NO.ISO8859-1',\n    'oc':                                   'oc_FR.ISO8859-1',\n    'oc_fr':                                'oc_FR.ISO8859-1',\n    'oc_fr.iso88591':                       'oc_FR.ISO8859-1',\n    'oc_fr.iso885915':                      'oc_FR.ISO8859-15',\n    'oc_fr@euro':                           'oc_FR.ISO8859-15',\n    'om_et':                                'om_ET.UTF-8',\n    'om_ke':                                'om_KE.ISO8859-1',\n    'or':                                   'or_IN.UTF-8',\n    'or_in':                                'or_IN.UTF-8',\n    'os_ru':                                'os_RU.UTF-8',\n    'pa':                                   'pa_IN.UTF-8',\n    'pa_in':                                'pa_IN.UTF-8',\n    'pa_pk':                                'pa_PK.UTF-8',\n    'pap_an':                               'pap_AN.UTF-8',\n    'pd':                                   'pd_US.ISO8859-1',\n    'pd_de':                                'pd_DE.ISO8859-1',\n    'pd_de.iso88591':                       'pd_DE.ISO8859-1',\n    'pd_de.iso885915':                      'pd_DE.ISO8859-15',\n    'pd_de@euro':                           'pd_DE.ISO8859-15',\n    'pd_us':                                'pd_US.ISO8859-1',\n    'pd_us.iso88591':                       'pd_US.ISO8859-1',\n    'pd_us.iso885915':                      'pd_US.ISO8859-15',\n    'pd_us@euro':                           'pd_US.ISO8859-15',\n    'ph':                                   'ph_PH.ISO8859-1',\n    'ph_ph':                                'ph_PH.ISO8859-1',\n    'ph_ph.iso88591':                       'ph_PH.ISO8859-1',\n    'pl':                                   'pl_PL.ISO8859-2',\n    'pl_pl':                                'pl_PL.ISO8859-2',\n    'pl_pl.iso88592':                       'pl_PL.ISO8859-2',\n    'polish':                               'pl_PL.ISO8859-2',\n    'portuguese':                           'pt_PT.ISO8859-1',\n    'portuguese.iso88591':                  'pt_PT.ISO8859-1',\n    'portuguese_brazil':                    'pt_BR.ISO8859-1',\n    'portuguese_brazil.8859':               'pt_BR.ISO8859-1',\n    'posix':                                'C',\n    'posix-utf2':                           'C',\n    'pp':                                   'pp_AN.ISO8859-1',\n    'pp_an':                                'pp_AN.ISO8859-1',\n    'pp_an.iso88591':                       'pp_AN.ISO8859-1',\n    'ps_af':                                'ps_AF.UTF-8',\n    'pt':                                   'pt_PT.ISO8859-1',\n    'pt.iso885915':                         'pt_PT.ISO8859-15',\n    'pt_br':                                'pt_BR.ISO8859-1',\n    'pt_br.88591':                          'pt_BR.ISO8859-1',\n    'pt_br.iso88591':                       'pt_BR.ISO8859-1',\n    'pt_br.iso885915':                      'pt_BR.ISO8859-15',\n    'pt_br@euro':                           'pt_BR.ISO8859-15',\n    'pt_pt':                                'pt_PT.ISO8859-1',\n    'pt_pt.88591':                          'pt_PT.ISO8859-1',\n    'pt_pt.iso88591':                       'pt_PT.ISO8859-1',\n    'pt_pt.iso885915':                      'pt_PT.ISO8859-15',\n    'pt_pt.iso885915@euro':                 'pt_PT.ISO8859-15',\n    'pt_pt.utf8@euro':                      'pt_PT.UTF-8',\n    'pt_pt@euro':                           'pt_PT.ISO8859-15',\n    'ro':                                   'ro_RO.ISO8859-2',\n    'ro_ro':                                'ro_RO.ISO8859-2',\n    'ro_ro.iso88592':                       'ro_RO.ISO8859-2',\n    'romanian':                             'ro_RO.ISO8859-2',\n    'ru':                                   'ru_RU.UTF-8',\n    'ru.koi8r':                             'ru_RU.KOI8-R',\n    'ru_ru':                                'ru_RU.UTF-8',\n    'ru_ru.cp1251':                         'ru_RU.CP1251',\n    'ru_ru.iso88595':                       'ru_RU.ISO8859-5',\n    'ru_ru.koi8r':                          'ru_RU.KOI8-R',\n    'ru_ru.microsoftcp1251':                'ru_RU.CP1251',\n    'ru_ua':                                'ru_UA.KOI8-U',\n    'ru_ua.cp1251':                         'ru_UA.CP1251',\n    'ru_ua.koi8u':                          'ru_UA.KOI8-U',\n    'ru_ua.microsoftcp1251':                'ru_UA.CP1251',\n    'rumanian':                             'ro_RO.ISO8859-2',\n    'russian':                              'ru_RU.ISO8859-5',\n    'rw':                                   'rw_RW.ISO8859-1',\n    'rw_rw':                                'rw_RW.ISO8859-1',\n    'rw_rw.iso88591':                       'rw_RW.ISO8859-1',\n    'sa_in':                                'sa_IN.UTF-8',\n    'sat_in':                               'sat_IN.UTF-8',\n    'sc_it':                                'sc_IT.UTF-8',\n    'sd':                                   'sd_IN.UTF-8',\n    'sd@devanagari':                        'sd_IN.UTF-8@devanagari',\n    'sd_in':                                'sd_IN.UTF-8',\n    'sd_in@devanagari':                     'sd_IN.UTF-8@devanagari',\n    'sd_in@devanagari.utf8':                'sd_IN.UTF-8@devanagari',\n    'sd_pk':                                'sd_PK.UTF-8',\n    'se_no':                                'se_NO.UTF-8',\n    'serbocroatian':                        'sr_RS.UTF-8@latin',\n    'sh':                                   'sr_RS.UTF-8@latin',\n    'sh_ba.iso88592@bosnia':                'sr_CS.ISO8859-2',\n    'sh_hr':                                'sh_HR.ISO8859-2',\n    'sh_hr.iso88592':                       'hr_HR.ISO8859-2',\n    'sh_sp':                                'sr_CS.ISO8859-2',\n    'sh_yu':                                'sr_RS.UTF-8@latin',\n    'shs_ca':                               'shs_CA.UTF-8',\n    'si':                                   'si_LK.UTF-8',\n    'si_lk':                                'si_LK.UTF-8',\n    'sid_et':                               'sid_ET.UTF-8',\n    'sinhala':                              'si_LK.UTF-8',\n    'sk':                                   'sk_SK.ISO8859-2',\n    'sk_sk':                                'sk_SK.ISO8859-2',\n    'sk_sk.iso88592':                       'sk_SK.ISO8859-2',\n    'sl':                                   'sl_SI.ISO8859-2',\n    'sl_cs':                                'sl_CS.ISO8859-2',\n    'sl_si':                                'sl_SI.ISO8859-2',\n    'sl_si.iso88592':                       'sl_SI.ISO8859-2',\n    'slovak':                               'sk_SK.ISO8859-2',\n    'slovene':                              'sl_SI.ISO8859-2',\n    'slovenian':                            'sl_SI.ISO8859-2',\n    'so_dj':                                'so_DJ.ISO8859-1',\n    'so_et':                                'so_ET.UTF-8',\n    'so_ke':                                'so_KE.ISO8859-1',\n    'so_so':                                'so_SO.ISO8859-1',\n    'sp':                                   'sr_CS.ISO8859-5',\n    'sp_yu':                                'sr_CS.ISO8859-5',\n    'spanish':                              'es_ES.ISO8859-1',\n    'spanish.iso88591':                     'es_ES.ISO8859-1',\n    'spanish_spain':                        'es_ES.ISO8859-1',\n    'spanish_spain.8859':                   'es_ES.ISO8859-1',\n    'sq':                                   'sq_AL.ISO8859-2',\n    'sq_al':                                'sq_AL.ISO8859-2',\n    'sq_al.iso88592':                       'sq_AL.ISO8859-2',\n    'sq_mk':                                'sq_MK.UTF-8',\n    'sr':                                   'sr_RS.UTF-8',\n    'sr@cyrillic':                          'sr_RS.UTF-8',\n    'sr@latin':                             'sr_RS.UTF-8@latin',\n    'sr@latn':                              'sr_CS.UTF-8@latin',\n    'sr_cs':                                'sr_CS.UTF-8',\n    'sr_cs.iso88592':                       'sr_CS.ISO8859-2',\n    'sr_cs.iso88592@latn':                  'sr_CS.ISO8859-2',\n    'sr_cs.iso88595':                       'sr_CS.ISO8859-5',\n    'sr_cs.utf8@latn':                      'sr_CS.UTF-8@latin',\n    'sr_cs@latn':                           'sr_CS.UTF-8@latin',\n    'sr_me':                                'sr_ME.UTF-8',\n    'sr_rs':                                'sr_RS.UTF-8',\n    'sr_rs@latin':                          'sr_RS.UTF-8@latin',\n    'sr_rs@latn':                           'sr_RS.UTF-8@latin',\n    'sr_sp':                                'sr_CS.ISO8859-2',\n    'sr_yu':                                'sr_RS.UTF-8@latin',\n    'sr_yu.cp1251@cyrillic':                'sr_CS.CP1251',\n    'sr_yu.iso88592':                       'sr_CS.ISO8859-2',\n    'sr_yu.iso88595':                       'sr_CS.ISO8859-5',\n    'sr_yu.iso88595@cyrillic':              'sr_CS.ISO8859-5',\n    'sr_yu.microsoftcp1251@cyrillic':       'sr_CS.CP1251',\n    'sr_yu.utf8':                           'sr_RS.UTF-8',\n    'sr_yu.utf8@cyrillic':                  'sr_RS.UTF-8',\n    'sr_yu@cyrillic':                       'sr_RS.UTF-8',\n    'ss':                                   'ss_ZA.ISO8859-1',\n    'ss_za':                                'ss_ZA.ISO8859-1',\n    'ss_za.iso88591':                       'ss_ZA.ISO8859-1',\n    'st':                                   'st_ZA.ISO8859-1',\n    'st_za':                                'st_ZA.ISO8859-1',\n    'st_za.iso88591':                       'st_ZA.ISO8859-1',\n    'sv':                                   'sv_SE.ISO8859-1',\n    'sv.iso885915':                         'sv_SE.ISO8859-15',\n    'sv_fi':                                'sv_FI.ISO8859-1',\n    'sv_fi.iso88591':                       'sv_FI.ISO8859-1',\n    'sv_fi.iso885915':                      'sv_FI.ISO8859-15',\n    'sv_fi.iso885915@euro':                 'sv_FI.ISO8859-15',\n    'sv_fi.utf8@euro':                      'sv_FI.UTF-8',\n    'sv_fi@euro':                           'sv_FI.ISO8859-15',\n    'sv_se':                                'sv_SE.ISO8859-1',\n    'sv_se.88591':                          'sv_SE.ISO8859-1',\n    'sv_se.iso88591':                       'sv_SE.ISO8859-1',\n    'sv_se.iso885915':                      'sv_SE.ISO8859-15',\n    'sv_se@euro':                           'sv_SE.ISO8859-15',\n    'sw_ke':                                'sw_KE.UTF-8',\n    'sw_tz':                                'sw_TZ.UTF-8',\n    'swedish':                              'sv_SE.ISO8859-1',\n    'swedish.iso88591':                     'sv_SE.ISO8859-1',\n    'szl_pl':                               'szl_PL.UTF-8',\n    'ta':                                   'ta_IN.TSCII-0',\n    'ta_in':                                'ta_IN.TSCII-0',\n    'ta_in.tscii':                          'ta_IN.TSCII-0',\n    'ta_in.tscii0':                         'ta_IN.TSCII-0',\n    'ta_lk':                                'ta_LK.UTF-8',\n    'te':                                   'te_IN.UTF-8',\n    'te_in':                                'te_IN.UTF-8',\n    'tg':                                   'tg_TJ.KOI8-C',\n    'tg_tj':                                'tg_TJ.KOI8-C',\n    'tg_tj.koi8c':                          'tg_TJ.KOI8-C',\n    'th':                                   'th_TH.ISO8859-11',\n    'th_th':                                'th_TH.ISO8859-11',\n    'th_th.iso885911':                      'th_TH.ISO8859-11',\n    'th_th.tactis':                         'th_TH.TIS620',\n    'th_th.tis620':                         'th_TH.TIS620',\n    'thai':                                 'th_TH.ISO8859-11',\n    'ti_er':                                'ti_ER.UTF-8',\n    'ti_et':                                'ti_ET.UTF-8',\n    'tig_er':                               'tig_ER.UTF-8',\n    'tk_tm':                                'tk_TM.UTF-8',\n    'tl':                                   'tl_PH.ISO8859-1',\n    'tl_ph':                                'tl_PH.ISO8859-1',\n    'tl_ph.iso88591':                       'tl_PH.ISO8859-1',\n    'tn':                                   'tn_ZA.ISO8859-15',\n    'tn_za':                                'tn_ZA.ISO8859-15',\n    'tn_za.iso885915':                      'tn_ZA.ISO8859-15',\n    'tr':                                   'tr_TR.ISO8859-9',\n    'tr_cy':                                'tr_CY.ISO8859-9',\n    'tr_tr':                                'tr_TR.ISO8859-9',\n    'tr_tr.iso88599':                       'tr_TR.ISO8859-9',\n    'ts':                                   'ts_ZA.ISO8859-1',\n    'ts_za':                                'ts_ZA.ISO8859-1',\n    'ts_za.iso88591':                       'ts_ZA.ISO8859-1',\n    'tt':                                   'tt_RU.TATAR-CYR',\n    'tt_ru':                                'tt_RU.TATAR-CYR',\n    'tt_ru.koi8c':                          'tt_RU.KOI8-C',\n    'tt_ru.tatarcyr':                       'tt_RU.TATAR-CYR',\n    'tt_ru@iqtelif':                        'tt_RU.UTF-8@iqtelif',\n    'turkish':                              'tr_TR.ISO8859-9',\n    'turkish.iso88599':                     'tr_TR.ISO8859-9',\n    'ug_cn':                                'ug_CN.UTF-8',\n    'uk':                                   'uk_UA.KOI8-U',\n    'uk_ua':                                'uk_UA.KOI8-U',\n    'uk_ua.cp1251':                         'uk_UA.CP1251',\n    'uk_ua.iso88595':                       'uk_UA.ISO8859-5',\n    'uk_ua.koi8u':                          'uk_UA.KOI8-U',\n    'uk_ua.microsoftcp1251':                'uk_UA.CP1251',\n    'univ':                                 'en_US.utf',\n    'universal':                            'en_US.utf',\n    'universal.utf8@ucs4':                  'en_US.UTF-8',\n    'unm_us':                               'unm_US.UTF-8',\n    'ur':                                   'ur_PK.CP1256',\n    'ur_in':                                'ur_IN.UTF-8',\n    'ur_pk':                                'ur_PK.CP1256',\n    'ur_pk.cp1256':                         'ur_PK.CP1256',\n    'ur_pk.microsoftcp1256':                'ur_PK.CP1256',\n    'uz':                                   'uz_UZ.UTF-8',\n    'uz_uz':                                'uz_UZ.UTF-8',\n    'uz_uz.iso88591':                       'uz_UZ.ISO8859-1',\n    'uz_uz.utf8@cyrillic':                  'uz_UZ.UTF-8',\n    'uz_uz@cyrillic':                       'uz_UZ.UTF-8',\n    've':                                   've_ZA.UTF-8',\n    've_za':                                've_ZA.UTF-8',\n    'vi':                                   'vi_VN.TCVN',\n    'vi_vn':                                'vi_VN.TCVN',\n    'vi_vn.tcvn':                           'vi_VN.TCVN',\n    'vi_vn.tcvn5712':                       'vi_VN.TCVN',\n    'vi_vn.viscii':                         'vi_VN.VISCII',\n    'vi_vn.viscii111':                      'vi_VN.VISCII',\n    'wa':                                   'wa_BE.ISO8859-1',\n    'wa_be':                                'wa_BE.ISO8859-1',\n    'wa_be.iso88591':                       'wa_BE.ISO8859-1',\n    'wa_be.iso885915':                      'wa_BE.ISO8859-15',\n    'wa_be.iso885915@euro':                 'wa_BE.ISO8859-15',\n    'wa_be@euro':                           'wa_BE.ISO8859-15',\n    'wae_ch':                               'wae_CH.UTF-8',\n    'wal_et':                               'wal_ET.UTF-8',\n    'wo_sn':                                'wo_SN.UTF-8',\n    'xh':                                   'xh_ZA.ISO8859-1',\n    'xh_za':                                'xh_ZA.ISO8859-1',\n    'xh_za.iso88591':                       'xh_ZA.ISO8859-1',\n    'yi':                                   'yi_US.CP1255',\n    'yi_us':                                'yi_US.CP1255',\n    'yi_us.cp1255':                         'yi_US.CP1255',\n    'yi_us.microsoftcp1255':                'yi_US.CP1255',\n    'yo_ng':                                'yo_NG.UTF-8',\n    'yue_hk':                               'yue_HK.UTF-8',\n    'zh':                                   'zh_CN.eucCN',\n    'zh_cn':                                'zh_CN.gb2312',\n    'zh_cn.big5':                           'zh_TW.big5',\n    'zh_cn.euc':                            'zh_CN.eucCN',\n    'zh_cn.gb18030':                        'zh_CN.gb18030',\n    'zh_cn.gb2312':                         'zh_CN.gb2312',\n    'zh_cn.gbk':                            'zh_CN.gbk',\n    'zh_hk':                                'zh_HK.big5hkscs',\n    'zh_hk.big5':                           'zh_HK.big5',\n    'zh_hk.big5hk':                         'zh_HK.big5hkscs',\n    'zh_hk.big5hkscs':                      'zh_HK.big5hkscs',\n    'zh_sg':                                'zh_SG.GB2312',\n    'zh_sg.gbk':                            'zh_SG.GBK',\n    'zh_tw':                                'zh_TW.big5',\n    'zh_tw.big5':                           'zh_TW.big5',\n    'zh_tw.euc':                            'zh_TW.eucTW',\n    'zh_tw.euctw':                          'zh_TW.eucTW',\n    'zu':                                   'zu_ZA.ISO8859-1',\n    'zu_za':                                'zu_ZA.ISO8859-1',\n    'zu_za.iso88591':                       'zu_ZA.ISO8859-1',\n}\n\n#\n# This maps Windows language identifiers to locale strings.\n#\n# This list has been updated from\n# http://msdn.microsoft.com/library/default.asp?url=/library/en-us/intl/nls_238z.asp\n# to include every locale up to Windows Vista.\n#\n# NOTE: this mapping is incomplete.  If your language is missing, please\n# submit a bug report to the Python bug tracker at http://bugs.python.org/\n# Make sure you include the missing language identifier and the suggested\n# locale code.\n#\n\nwindows_locale = {\n    0x0436: \"af_ZA\", # Afrikaans\n    0x041c: \"sq_AL\", # Albanian\n    0x0484: \"gsw_FR\",# Alsatian - France\n    0x045e: \"am_ET\", # Amharic - Ethiopia\n    0x0401: \"ar_SA\", # Arabic - Saudi Arabia\n    0x0801: \"ar_IQ\", # Arabic - Iraq\n    0x0c01: \"ar_EG\", # Arabic - Egypt\n    0x1001: \"ar_LY\", # Arabic - Libya\n    0x1401: \"ar_DZ\", # Arabic - Algeria\n    0x1801: \"ar_MA\", # Arabic - Morocco\n    0x1c01: \"ar_TN\", # Arabic - Tunisia\n    0x2001: \"ar_OM\", # Arabic - Oman\n    0x2401: \"ar_YE\", # Arabic - Yemen\n    0x2801: \"ar_SY\", # Arabic - Syria\n    0x2c01: \"ar_JO\", # Arabic - Jordan\n    0x3001: \"ar_LB\", # Arabic - Lebanon\n    0x3401: \"ar_KW\", # Arabic - Kuwait\n    0x3801: \"ar_AE\", # Arabic - United Arab Emirates\n    0x3c01: \"ar_BH\", # Arabic - Bahrain\n    0x4001: \"ar_QA\", # Arabic - Qatar\n    0x042b: \"hy_AM\", # Armenian\n    0x044d: \"as_IN\", # Assamese - India\n    0x042c: \"az_AZ\", # Azeri - Latin\n    0x082c: \"az_AZ\", # Azeri - Cyrillic\n    0x046d: \"ba_RU\", # Bashkir\n    0x042d: \"eu_ES\", # Basque - Russia\n    0x0423: \"be_BY\", # Belarusian\n    0x0445: \"bn_IN\", # Begali\n    0x201a: \"bs_BA\", # Bosnian - Cyrillic\n    0x141a: \"bs_BA\", # Bosnian - Latin\n    0x047e: \"br_FR\", # Breton - France\n    0x0402: \"bg_BG\", # Bulgarian\n#    0x0455: \"my_MM\", # Burmese - Not supported\n    0x0403: \"ca_ES\", # Catalan\n    0x0004: \"zh_CHS\",# Chinese - Simplified\n    0x0404: \"zh_TW\", # Chinese - Taiwan\n    0x0804: \"zh_CN\", # Chinese - PRC\n    0x0c04: \"zh_HK\", # Chinese - Hong Kong S.A.R.\n    0x1004: \"zh_SG\", # Chinese - Singapore\n    0x1404: \"zh_MO\", # Chinese - Macao S.A.R.\n    0x7c04: \"zh_CHT\",# Chinese - Traditional\n    0x0483: \"co_FR\", # Corsican - France\n    0x041a: \"hr_HR\", # Croatian\n    0x101a: \"hr_BA\", # Croatian - Bosnia\n    0x0405: \"cs_CZ\", # Czech\n    0x0406: \"da_DK\", # Danish\n    0x048c: \"gbz_AF\",# Dari - Afghanistan\n    0x0465: \"div_MV\",# Divehi - Maldives\n    0x0413: \"nl_NL\", # Dutch - The Netherlands\n    0x0813: \"nl_BE\", # Dutch - Belgium\n    0x0409: \"en_US\", # English - United States\n    0x0809: \"en_GB\", # English - United Kingdom\n    0x0c09: \"en_AU\", # English - Australia\n    0x1009: \"en_CA\", # English - Canada\n    0x1409: \"en_NZ\", # English - New Zealand\n    0x1809: \"en_IE\", # English - Ireland\n    0x1c09: \"en_ZA\", # English - South Africa\n    0x2009: \"en_JA\", # English - Jamaica\n    0x2409: \"en_CB\", # English - Carribbean\n    0x2809: \"en_BZ\", # English - Belize\n    0x2c09: \"en_TT\", # English - Trinidad\n    0x3009: \"en_ZW\", # English - Zimbabwe\n    0x3409: \"en_PH\", # English - Philippines\n    0x4009: \"en_IN\", # English - India\n    0x4409: \"en_MY\", # English - Malaysia\n    0x4809: \"en_IN\", # English - Singapore\n    0x0425: \"et_EE\", # Estonian\n    0x0438: \"fo_FO\", # Faroese\n    0x0464: \"fil_PH\",# Filipino\n    0x040b: \"fi_FI\", # Finnish\n    0x040c: \"fr_FR\", # French - France\n    0x080c: \"fr_BE\", # French - Belgium\n    0x0c0c: \"fr_CA\", # French - Canada\n    0x100c: \"fr_CH\", # French - Switzerland\n    0x140c: \"fr_LU\", # French - Luxembourg\n    0x180c: \"fr_MC\", # French - Monaco\n    0x0462: \"fy_NL\", # Frisian - Netherlands\n    0x0456: \"gl_ES\", # Galician\n    0x0437: \"ka_GE\", # Georgian\n    0x0407: \"de_DE\", # German - Germany\n    0x0807: \"de_CH\", # German - Switzerland\n    0x0c07: \"de_AT\", # German - Austria\n    0x1007: \"de_LU\", # German - Luxembourg\n    0x1407: \"de_LI\", # German - Liechtenstein\n    0x0408: \"el_GR\", # Greek\n    0x046f: \"kl_GL\", # Greenlandic - Greenland\n    0x0447: \"gu_IN\", # Gujarati\n    0x0468: \"ha_NG\", # Hausa - Latin\n    0x040d: \"he_IL\", # Hebrew\n    0x0439: \"hi_IN\", # Hindi\n    0x040e: \"hu_HU\", # Hungarian\n    0x040f: \"is_IS\", # Icelandic\n    0x0421: \"id_ID\", # Indonesian\n    0x045d: \"iu_CA\", # Inuktitut - Syllabics\n    0x085d: \"iu_CA\", # Inuktitut - Latin\n    0x083c: \"ga_IE\", # Irish - Ireland\n    0x0410: \"it_IT\", # Italian - Italy\n    0x0810: \"it_CH\", # Italian - Switzerland\n    0x0411: \"ja_JP\", # Japanese\n    0x044b: \"kn_IN\", # Kannada - India\n    0x043f: \"kk_KZ\", # Kazakh\n    0x0453: \"kh_KH\", # Khmer - Cambodia\n    0x0486: \"qut_GT\",# K'iche - Guatemala\n    0x0487: \"rw_RW\", # Kinyarwanda - Rwanda\n    0x0457: \"kok_IN\",# Konkani\n    0x0412: \"ko_KR\", # Korean\n    0x0440: \"ky_KG\", # Kyrgyz\n    0x0454: \"lo_LA\", # Lao - Lao PDR\n    0x0426: \"lv_LV\", # Latvian\n    0x0427: \"lt_LT\", # Lithuanian\n    0x082e: \"dsb_DE\",# Lower Sorbian - Germany\n    0x046e: \"lb_LU\", # Luxembourgish\n    0x042f: \"mk_MK\", # FYROM Macedonian\n    0x043e: \"ms_MY\", # Malay - Malaysia\n    0x083e: \"ms_BN\", # Malay - Brunei Darussalam\n    0x044c: \"ml_IN\", # Malayalam - India\n    0x043a: \"mt_MT\", # Maltese\n    0x0481: \"mi_NZ\", # Maori\n    0x047a: \"arn_CL\",# Mapudungun\n    0x044e: \"mr_IN\", # Marathi\n    0x047c: \"moh_CA\",# Mohawk - Canada\n    0x0450: \"mn_MN\", # Mongolian - Cyrillic\n    0x0850: \"mn_CN\", # Mongolian - PRC\n    0x0461: \"ne_NP\", # Nepali\n    0x0414: \"nb_NO\", # Norwegian - Bokmal\n    0x0814: \"nn_NO\", # Norwegian - Nynorsk\n    0x0482: \"oc_FR\", # Occitan - France\n    0x0448: \"or_IN\", # Oriya - India\n    0x0463: \"ps_AF\", # Pashto - Afghanistan\n    0x0429: \"fa_IR\", # Persian\n    0x0415: \"pl_PL\", # Polish\n    0x0416: \"pt_BR\", # Portuguese - Brazil\n    0x0816: \"pt_PT\", # Portuguese - Portugal\n    0x0446: \"pa_IN\", # Punjabi\n    0x046b: \"quz_BO\",# Quechua (Bolivia)\n    0x086b: \"quz_EC\",# Quechua (Ecuador)\n    0x0c6b: \"quz_PE\",# Quechua (Peru)\n    0x0418: \"ro_RO\", # Romanian - Romania\n    0x0417: \"rm_CH\", # Romansh\n    0x0419: \"ru_RU\", # Russian\n    0x243b: \"smn_FI\",# Sami Finland\n    0x103b: \"smj_NO\",# Sami Norway\n    0x143b: \"smj_SE\",# Sami Sweden\n    0x043b: \"se_NO\", # Sami Northern Norway\n    0x083b: \"se_SE\", # Sami Northern Sweden\n    0x0c3b: \"se_FI\", # Sami Northern Finland\n    0x203b: \"sms_FI\",# Sami Skolt\n    0x183b: \"sma_NO\",# Sami Southern Norway\n    0x1c3b: \"sma_SE\",# Sami Southern Sweden\n    0x044f: \"sa_IN\", # Sanskrit\n    0x0c1a: \"sr_SP\", # Serbian - Cyrillic\n    0x1c1a: \"sr_BA\", # Serbian - Bosnia Cyrillic\n    0x081a: \"sr_SP\", # Serbian - Latin\n    0x181a: \"sr_BA\", # Serbian - Bosnia Latin\n    0x045b: \"si_LK\", # Sinhala - Sri Lanka\n    0x046c: \"ns_ZA\", # Northern Sotho\n    0x0432: \"tn_ZA\", # Setswana - Southern Africa\n    0x041b: \"sk_SK\", # Slovak\n    0x0424: \"sl_SI\", # Slovenian\n    0x040a: \"es_ES\", # Spanish - Spain\n    0x080a: \"es_MX\", # Spanish - Mexico\n    0x0c0a: \"es_ES\", # Spanish - Spain (Modern)\n    0x100a: \"es_GT\", # Spanish - Guatemala\n    0x140a: \"es_CR\", # Spanish - Costa Rica\n    0x180a: \"es_PA\", # Spanish - Panama\n    0x1c0a: \"es_DO\", # Spanish - Dominican Republic\n    0x200a: \"es_VE\", # Spanish - Venezuela\n    0x240a: \"es_CO\", # Spanish - Colombia\n    0x280a: \"es_PE\", # Spanish - Peru\n    0x2c0a: \"es_AR\", # Spanish - Argentina\n    0x300a: \"es_EC\", # Spanish - Ecuador\n    0x340a: \"es_CL\", # Spanish - Chile\n    0x380a: \"es_UR\", # Spanish - Uruguay\n    0x3c0a: \"es_PY\", # Spanish - Paraguay\n    0x400a: \"es_BO\", # Spanish - Bolivia\n    0x440a: \"es_SV\", # Spanish - El Salvador\n    0x480a: \"es_HN\", # Spanish - Honduras\n    0x4c0a: \"es_NI\", # Spanish - Nicaragua\n    0x500a: \"es_PR\", # Spanish - Puerto Rico\n    0x540a: \"es_US\", # Spanish - United States\n#    0x0430: \"\", # Sutu - Not supported\n    0x0441: \"sw_KE\", # Swahili\n    0x041d: \"sv_SE\", # Swedish - Sweden\n    0x081d: \"sv_FI\", # Swedish - Finland\n    0x045a: \"syr_SY\",# Syriac\n    0x0428: \"tg_TJ\", # Tajik - Cyrillic\n    0x085f: \"tmz_DZ\",# Tamazight - Latin\n    0x0449: \"ta_IN\", # Tamil\n    0x0444: \"tt_RU\", # Tatar\n    0x044a: \"te_IN\", # Telugu\n    0x041e: \"th_TH\", # Thai\n    0x0851: \"bo_BT\", # Tibetan - Bhutan\n    0x0451: \"bo_CN\", # Tibetan - PRC\n    0x041f: \"tr_TR\", # Turkish\n    0x0442: \"tk_TM\", # Turkmen - Cyrillic\n    0x0480: \"ug_CN\", # Uighur - Arabic\n    0x0422: \"uk_UA\", # Ukrainian\n    0x042e: \"wen_DE\",# Upper Sorbian - Germany\n    0x0420: \"ur_PK\", # Urdu\n    0x0820: \"ur_IN\", # Urdu - India\n    0x0443: \"uz_UZ\", # Uzbek - Latin\n    0x0843: \"uz_UZ\", # Uzbek - Cyrillic\n    0x042a: \"vi_VN\", # Vietnamese\n    0x0452: \"cy_GB\", # Welsh\n    0x0488: \"wo_SN\", # Wolof - Senegal\n    0x0434: \"xh_ZA\", # Xhosa - South Africa\n    0x0485: \"sah_RU\",# Yakut - Cyrillic\n    0x0478: \"ii_CN\", # Yi - PRC\n    0x046a: \"yo_NG\", # Yoruba - Nigeria\n    0x0435: \"zu_ZA\", # Zulu\n}\n\ndef _print_locale():\n\n    \"\"\" Test function.\n    \"\"\"\n    categories = {}\n    def _init_categories(categories=categories):\n        for k,v in globals().items():\n            if k[:3] == 'LC_':\n                categories[k] = v\n    _init_categories()\n    del categories['LC_ALL']\n\n    print 'Locale defaults as determined by getdefaultlocale():'\n    print '-'*72\n    lang, enc = getdefaultlocale()\n    print 'Language: ', lang or '(undefined)'\n    print 'Encoding: ', enc or '(undefined)'\n    print\n\n    print 'Locale settings on startup:'\n    print '-'*72\n    for name,category in categories.items():\n        print name, '...'\n        lang, enc = getlocale(category)\n        print '   Language: ', lang or '(undefined)'\n        print '   Encoding: ', enc or '(undefined)'\n        print\n\n    print\n    print 'Locale settings after calling resetlocale():'\n    print '-'*72\n    resetlocale()\n    for name,category in categories.items():\n        print name, '...'\n        lang, enc = getlocale(category)\n        print '   Language: ', lang or '(undefined)'\n        print '   Encoding: ', enc or '(undefined)'\n        print\n\n    try:\n        setlocale(LC_ALL, \"\")\n    except:\n        print 'NOTE:'\n        print 'setlocale(LC_ALL, \"\") does not support the default locale'\n        print 'given in the OS environment variables.'\n    else:\n        print\n        print 'Locale settings after calling setlocale(LC_ALL, \"\"):'\n        print '-'*72\n        for name,category in categories.items():\n            print name, '...'\n            lang, enc = getlocale(category)\n            print '   Language: ', lang or '(undefined)'\n            print '   Encoding: ', enc or '(undefined)'\n            print\n\n###\n\ntry:\n    LC_MESSAGES\nexcept NameError:\n    pass\nelse:\n    __all__.append(\"LC_MESSAGES\")\n\nif __name__=='__main__':\n    print 'Locale aliasing:'\n    print\n    _print_locale()\n    print\n    print 'Number formatting:'\n    print\n    _test()\n", 
    "logging.__init__": "# Copyright 2001-2014 by Vinay Sajip. All Rights Reserved.\n#\n# Permission to use, copy, modify, and distribute this software and its\n# documentation for any purpose and without fee is hereby granted,\n# provided that the above copyright notice appear in all copies and that\n# both that copyright notice and this permission notice appear in\n# supporting documentation, and that the name of Vinay Sajip\n# not be used in advertising or publicity pertaining to distribution\n# of the software without specific, written prior permission.\n# VINAY SAJIP DISCLAIMS ALL WARRANTIES WITH REGARD TO THIS SOFTWARE, INCLUDING\n# ALL IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS. IN NO EVENT SHALL\n# VINAY SAJIP BE LIABLE FOR ANY SPECIAL, INDIRECT OR CONSEQUENTIAL DAMAGES OR\n# ANY DAMAGES WHATSOEVER RESULTING FROM LOSS OF USE, DATA OR PROFITS, WHETHER\n# IN AN ACTION OF CONTRACT, NEGLIGENCE OR OTHER TORTIOUS ACTION, ARISING OUT\n# OF OR IN CONNECTION WITH THE USE OR PERFORMANCE OF THIS SOFTWARE.\n\n\"\"\"\nLogging package for Python. Based on PEP 282 and comments thereto in\ncomp.lang.python.\n\nCopyright (C) 2001-2014 Vinay Sajip. All Rights Reserved.\n\nTo use, simply 'import logging' and log away!\n\"\"\"\n\nimport sys, os, time, cStringIO, traceback, warnings, weakref, collections\n\n__all__ = ['BASIC_FORMAT', 'BufferingFormatter', 'CRITICAL', 'DEBUG', 'ERROR',\n           'FATAL', 'FileHandler', 'Filter', 'Formatter', 'Handler', 'INFO',\n           'LogRecord', 'Logger', 'LoggerAdapter', 'NOTSET', 'NullHandler',\n           'StreamHandler', 'WARN', 'WARNING', 'addLevelName', 'basicConfig',\n           'captureWarnings', 'critical', 'debug', 'disable', 'error',\n           'exception', 'fatal', 'getLevelName', 'getLogger', 'getLoggerClass',\n           'info', 'log', 'makeLogRecord', 'setLoggerClass', 'warn', 'warning']\n\ntry:\n    import codecs\nexcept ImportError:\n    codecs = None\n\ntry:\n    import thread\n    import threading\nexcept ImportError:\n    thread = None\n\n__author__  = \"Vinay Sajip <vinay_sajip@red-dove.com>\"\n__status__  = \"production\"\n# Note: the attributes below are no longer maintained.\n__version__ = \"0.5.1.2\"\n__date__    = \"07 February 2010\"\n\n#---------------------------------------------------------------------------\n#   Miscellaneous module data\n#---------------------------------------------------------------------------\ntry:\n    unicode\n    _unicode = True\nexcept NameError:\n    _unicode = False\n\n#\n# _srcfile is used when walking the stack to check when we've got the first\n# caller stack frame.\n#\nif hasattr(sys, 'frozen'): #support for py2exe\n    _srcfile = \"logging%s__init__%s\" % (os.sep, __file__[-4:])\nelif __file__[-4:].lower() in ['.pyc', '.pyo']:\n    _srcfile = __file__[:-4] + '.py'\nelse:\n    _srcfile = __file__\n_srcfile = os.path.normcase(_srcfile)\n\n# next bit filched from 1.5.2's inspect.py\ndef currentframe():\n    \"\"\"Return the frame object for the caller's stack frame.\"\"\"\n    try:\n        raise Exception\n    except:\n        return sys.exc_info()[2].tb_frame.f_back\n\nif hasattr(sys, '_getframe'): currentframe = lambda: sys._getframe(3)\n# done filching\n\n# _srcfile is only used in conjunction with sys._getframe().\n# To provide compatibility with older versions of Python, set _srcfile\n# to None if _getframe() is not available; this value will prevent\n# findCaller() from being called.\n#if not hasattr(sys, \"_getframe\"):\n#    _srcfile = None\n\n#\n#_startTime is used as the base when calculating the relative time of events\n#\n_startTime = time.time()\n\n#\n#raiseExceptions is used to see if exceptions during handling should be\n#propagated\n#\nraiseExceptions = 1\n\n#\n# If you don't want threading information in the log, set this to zero\n#\nlogThreads = 1\n\n#\n# If you don't want multiprocessing information in the log, set this to zero\n#\nlogMultiprocessing = 1\n\n#\n# If you don't want process information in the log, set this to zero\n#\nlogProcesses = 1\n\n#---------------------------------------------------------------------------\n#   Level related stuff\n#---------------------------------------------------------------------------\n#\n# Default levels and level names, these can be replaced with any positive set\n# of values having corresponding names. There is a pseudo-level, NOTSET, which\n# is only really there as a lower limit for user-defined levels. Handlers and\n# loggers are initialized with NOTSET so that they will log all messages, even\n# at user-defined levels.\n#\n\nCRITICAL = 50\nFATAL = CRITICAL\nERROR = 40\nWARNING = 30\nWARN = WARNING\nINFO = 20\nDEBUG = 10\nNOTSET = 0\n\n# NOTE(flaper87): This is different from\n# python's stdlib module since pypy's\n# dicts are much faster when their\n# keys are all of the same type.\n# Introduced in commit 9de7b40c586f\n_levelToName = {\n    CRITICAL: 'CRITICAL',\n    ERROR: 'ERROR',\n    WARNING: 'WARNING',\n    INFO: 'INFO',\n    DEBUG: 'DEBUG',\n    NOTSET: 'NOTSET',\n}\n_nameToLevel = {\n    'CRITICAL': CRITICAL,\n    'ERROR': ERROR,\n    'WARN': WARNING,\n    'WARNING': WARNING,\n    'INFO': INFO,\n    'DEBUG': DEBUG,\n    'NOTSET': NOTSET,\n}\n_levelNames = dict(_levelToName)\n_levelNames.update(_nameToLevel)   # backward compatibility\n\ndef getLevelName(level):\n    \"\"\"\n    Return the textual representation of logging level 'level'.\n\n    If the level is one of the predefined levels (CRITICAL, ERROR, WARNING,\n    INFO, DEBUG) then you get the corresponding string. If you have\n    associated levels with names using addLevelName then the name you have\n    associated with 'level' is returned.\n\n    If a numeric value corresponding to one of the defined levels is passed\n    in, the corresponding string representation is returned.\n\n    Otherwise, the string \"Level %s\" % level is returned.\n    \"\"\"\n\n    # NOTE(flaper87): Check also in _nameToLevel\n    # if value is None.\n    return (_levelToName.get(level) or\n            _nameToLevel.get(level, (\"Level %s\" % level)))\n\ndef addLevelName(level, levelName):\n    \"\"\"\n    Associate 'levelName' with 'level'.\n\n    This is used when converting levels to text during message formatting.\n    \"\"\"\n    _acquireLock()\n    try:    #unlikely to cause an exception, but you never know...\n        _levelToName[level] = levelName\n        _nameToLevel[levelName] = level\n    finally:\n        _releaseLock()\n\ndef _checkLevel(level):\n    if isinstance(level, (int, long)):\n        rv = level\n    elif str(level) == level:\n        if level not in _nameToLevel:\n            raise ValueError(\"Unknown level: %r\" % level)\n        rv = _nameToLevel[level]\n    else:\n        raise TypeError(\"Level not an integer or a valid string: %r\" % level)\n    return rv\n\n#---------------------------------------------------------------------------\n#   Thread-related stuff\n#---------------------------------------------------------------------------\n\n#\n#_lock is used to serialize access to shared data structures in this module.\n#This needs to be an RLock because fileConfig() creates and configures\n#Handlers, and so might arbitrary user threads. Since Handler code updates the\n#shared dictionary _handlers, it needs to acquire the lock. But if configuring,\n#the lock would already have been acquired - so we need an RLock.\n#The same argument applies to Loggers and Manager.loggerDict.\n#\nif thread:\n    _lock = threading.RLock()\nelse:\n    _lock = None\n\ndef _acquireLock():\n    \"\"\"\n    Acquire the module-level lock for serializing access to shared data.\n\n    This should be released with _releaseLock().\n    \"\"\"\n    if _lock:\n        _lock.acquire()\n\ndef _releaseLock():\n    \"\"\"\n    Release the module-level lock acquired by calling _acquireLock().\n    \"\"\"\n    if _lock:\n        _lock.release()\n\n#---------------------------------------------------------------------------\n#   The logging record\n#---------------------------------------------------------------------------\n\nclass LogRecord(object):\n    \"\"\"\n    A LogRecord instance represents an event being logged.\n\n    LogRecord instances are created every time something is logged. They\n    contain all the information pertinent to the event being logged. The\n    main information passed in is in msg and args, which are combined\n    using str(msg) % args to create the message field of the record. The\n    record also includes information such as when the record was created,\n    the source line where the logging call was made, and any exception\n    information to be logged.\n    \"\"\"\n    def __init__(self, name, level, pathname, lineno,\n                 msg, args, exc_info, func=None):\n        \"\"\"\n        Initialize a logging record with interesting information.\n        \"\"\"\n        ct = time.time()\n        self.name = name\n        self.msg = msg\n        #\n        # The following statement allows passing of a dictionary as a sole\n        # argument, so that you can do something like\n        #  logging.debug(\"a %(a)d b %(b)s\", {'a':1, 'b':2})\n        # Suggested by Stefan Behnel.\n        # Note that without the test for args[0], we get a problem because\n        # during formatting, we test to see if the arg is present using\n        # 'if self.args:'. If the event being logged is e.g. 'Value is %d'\n        # and if the passed arg fails 'if self.args:' then no formatting\n        # is done. For example, logger.warn('Value is %d', 0) would log\n        # 'Value is %d' instead of 'Value is 0'.\n        # For the use case of passing a dictionary, this should not be a\n        # problem.\n        # Issue #21172: a request was made to relax the isinstance check\n        # to hasattr(args[0], '__getitem__'). However, the docs on string\n        # formatting still seem to suggest a mapping object is required.\n        # Thus, while not removing the isinstance check, it does now look\n        # for collections.Mapping rather than, as before, dict.\n        if (args and len(args) == 1 and isinstance(args[0], collections.Mapping)\n            and args[0]):\n            args = args[0]\n        self.args = args\n        self.levelname = getLevelName(level)\n        self.levelno = level\n        self.pathname = pathname\n        try:\n            self.filename = os.path.basename(pathname)\n            self.module = os.path.splitext(self.filename)[0]\n        except (TypeError, ValueError, AttributeError):\n            self.filename = pathname\n            self.module = \"Unknown module\"\n        self.exc_info = exc_info\n        self.exc_text = None      # used to cache the traceback text\n        self.lineno = lineno\n        self.funcName = func\n        self.created = ct\n        self.msecs = (ct - int(ct)) * 1000\n        self.relativeCreated = (self.created - _startTime) * 1000\n        if logThreads and thread:\n            self.thread = thread.get_ident()\n            self.threadName = threading.current_thread().name\n        else:\n            self.thread = None\n            self.threadName = None\n        if not logMultiprocessing:\n            self.processName = None\n        else:\n            self.processName = 'MainProcess'\n            mp = sys.modules.get('multiprocessing')\n            if mp is not None:\n                # Errors may occur if multiprocessing has not finished loading\n                # yet - e.g. if a custom import hook causes third-party code\n                # to run when multiprocessing calls import. See issue 8200\n                # for an example\n                try:\n                    self.processName = mp.current_process().name\n                except StandardError:\n                    pass\n        if logProcesses and hasattr(os, 'getpid'):\n            self.process = os.getpid()\n        else:\n            self.process = None\n\n    def __str__(self):\n        return '<LogRecord: %s, %s, %s, %s, \"%s\">'%(self.name, self.levelno,\n            self.pathname, self.lineno, self.msg)\n\n    def getMessage(self):\n        \"\"\"\n        Return the message for this LogRecord.\n\n        Return the message for this LogRecord after merging any user-supplied\n        arguments with the message.\n        \"\"\"\n        if not _unicode: #if no unicode support...\n            msg = str(self.msg)\n        else:\n            msg = self.msg\n            if not isinstance(msg, basestring):\n                try:\n                    msg = str(self.msg)\n                except UnicodeError:\n                    msg = self.msg      #Defer encoding till later\n        if self.args:\n            msg = msg % self.args\n        return msg\n\ndef makeLogRecord(dict):\n    \"\"\"\n    Make a LogRecord whose attributes are defined by the specified dictionary,\n    This function is useful for converting a logging event received over\n    a socket connection (which is sent as a dictionary) into a LogRecord\n    instance.\n    \"\"\"\n    rv = LogRecord(None, None, \"\", 0, \"\", (), None, None)\n    rv.__dict__.update(dict)\n    return rv\n\n#---------------------------------------------------------------------------\n#   Formatter classes and functions\n#---------------------------------------------------------------------------\n\nclass Formatter(object):\n    \"\"\"\n    Formatter instances are used to convert a LogRecord to text.\n\n    Formatters need to know how a LogRecord is constructed. They are\n    responsible for converting a LogRecord to (usually) a string which can\n    be interpreted by either a human or an external system. The base Formatter\n    allows a formatting string to be specified. If none is supplied, the\n    default value of \"%s(message)\\\\n\" is used.\n\n    The Formatter can be initialized with a format string which makes use of\n    knowledge of the LogRecord attributes - e.g. the default value mentioned\n    above makes use of the fact that the user's message and arguments are pre-\n    formatted into a LogRecord's message attribute. Currently, the useful\n    attributes in a LogRecord are described by:\n\n    %(name)s            Name of the logger (logging channel)\n    %(levelno)s         Numeric logging level for the message (DEBUG, INFO,\n                        WARNING, ERROR, CRITICAL)\n    %(levelname)s       Text logging level for the message (\"DEBUG\", \"INFO\",\n                        \"WARNING\", \"ERROR\", \"CRITICAL\")\n    %(pathname)s        Full pathname of the source file where the logging\n                        call was issued (if available)\n    %(filename)s        Filename portion of pathname\n    %(module)s          Module (name portion of filename)\n    %(lineno)d          Source line number where the logging call was issued\n                        (if available)\n    %(funcName)s        Function name\n    %(created)f         Time when the LogRecord was created (time.time()\n                        return value)\n    %(asctime)s         Textual time when the LogRecord was created\n    %(msecs)d           Millisecond portion of the creation time\n    %(relativeCreated)d Time in milliseconds when the LogRecord was created,\n                        relative to the time the logging module was loaded\n                        (typically at application startup time)\n    %(thread)d          Thread ID (if available)\n    %(threadName)s      Thread name (if available)\n    %(process)d         Process ID (if available)\n    %(message)s         The result of record.getMessage(), computed just as\n                        the record is emitted\n    \"\"\"\n\n    converter = time.localtime\n\n    def __init__(self, fmt=None, datefmt=None):\n        \"\"\"\n        Initialize the formatter with specified format strings.\n\n        Initialize the formatter either with the specified format string, or a\n        default as described above. Allow for specialized date formatting with\n        the optional datefmt argument (if omitted, you get the ISO8601 format).\n        \"\"\"\n        if fmt:\n            self._fmt = fmt\n        else:\n            self._fmt = \"%(message)s\"\n        self.datefmt = datefmt\n\n    def formatTime(self, record, datefmt=None):\n        \"\"\"\n        Return the creation time of the specified LogRecord as formatted text.\n\n        This method should be called from format() by a formatter which\n        wants to make use of a formatted time. This method can be overridden\n        in formatters to provide for any specific requirement, but the\n        basic behaviour is as follows: if datefmt (a string) is specified,\n        it is used with time.strftime() to format the creation time of the\n        record. Otherwise, the ISO8601 format is used. The resulting\n        string is returned. This function uses a user-configurable function\n        to convert the creation time to a tuple. By default, time.localtime()\n        is used; to change this for a particular formatter instance, set the\n        'converter' attribute to a function with the same signature as\n        time.localtime() or time.gmtime(). To change it for all formatters,\n        for example if you want all logging times to be shown in GMT,\n        set the 'converter' attribute in the Formatter class.\n        \"\"\"\n        ct = self.converter(record.created)\n        if datefmt:\n            s = time.strftime(datefmt, ct)\n        else:\n            t = time.strftime(\"%Y-%m-%d %H:%M:%S\", ct)\n            s = \"%s,%03d\" % (t, record.msecs)\n        return s\n\n    def formatException(self, ei):\n        \"\"\"\n        Format and return the specified exception information as a string.\n\n        This default implementation just uses\n        traceback.print_exception()\n        \"\"\"\n        sio = cStringIO.StringIO()\n        traceback.print_exception(ei[0], ei[1], ei[2], None, sio)\n        s = sio.getvalue()\n        sio.close()\n        if s[-1:] == \"\\n\":\n            s = s[:-1]\n        return s\n\n    def usesTime(self):\n        \"\"\"\n        Check if the format uses the creation time of the record.\n        \"\"\"\n        return self._fmt.find(\"%(asctime)\") >= 0\n\n    def format(self, record):\n        \"\"\"\n        Format the specified record as text.\n\n        The record's attribute dictionary is used as the operand to a\n        string formatting operation which yields the returned string.\n        Before formatting the dictionary, a couple of preparatory steps\n        are carried out. The message attribute of the record is computed\n        using LogRecord.getMessage(). If the formatting string uses the\n        time (as determined by a call to usesTime(), formatTime() is\n        called to format the event time. If there is exception information,\n        it is formatted using formatException() and appended to the message.\n        \"\"\"\n        record.message = record.getMessage()\n        if self.usesTime():\n            record.asctime = self.formatTime(record, self.datefmt)\n        s = self._fmt % record.__dict__\n        if record.exc_info:\n            # Cache the traceback text to avoid converting it multiple times\n            # (it's constant anyway)\n            if not record.exc_text:\n                record.exc_text = self.formatException(record.exc_info)\n        if record.exc_text:\n            if s[-1:] != \"\\n\":\n                s = s + \"\\n\"\n            try:\n                s = s + record.exc_text\n            except UnicodeError:\n                # Sometimes filenames have non-ASCII chars, which can lead\n                # to errors when s is Unicode and record.exc_text is str\n                # See issue 8924.\n                # We also use replace for when there are multiple\n                # encodings, e.g. UTF-8 for the filesystem and latin-1\n                # for a script. See issue 13232.\n                s = s + record.exc_text.decode(sys.getfilesystemencoding(),\n                                               'replace')\n        return s\n\n#\n#   The default formatter to use when no other is specified\n#\n_defaultFormatter = Formatter()\n\nclass BufferingFormatter(object):\n    \"\"\"\n    A formatter suitable for formatting a number of records.\n    \"\"\"\n    def __init__(self, linefmt=None):\n        \"\"\"\n        Optionally specify a formatter which will be used to format each\n        individual record.\n        \"\"\"\n        if linefmt:\n            self.linefmt = linefmt\n        else:\n            self.linefmt = _defaultFormatter\n\n    def formatHeader(self, records):\n        \"\"\"\n        Return the header string for the specified records.\n        \"\"\"\n        return \"\"\n\n    def formatFooter(self, records):\n        \"\"\"\n        Return the footer string for the specified records.\n        \"\"\"\n        return \"\"\n\n    def format(self, records):\n        \"\"\"\n        Format the specified records and return the result as a string.\n        \"\"\"\n        rv = \"\"\n        if len(records) > 0:\n            rv = rv + self.formatHeader(records)\n            for record in records:\n                rv = rv + self.linefmt.format(record)\n            rv = rv + self.formatFooter(records)\n        return rv\n\n#---------------------------------------------------------------------------\n#   Filter classes and functions\n#---------------------------------------------------------------------------\n\nclass Filter(object):\n    \"\"\"\n    Filter instances are used to perform arbitrary filtering of LogRecords.\n\n    Loggers and Handlers can optionally use Filter instances to filter\n    records as desired. The base filter class only allows events which are\n    below a certain point in the logger hierarchy. For example, a filter\n    initialized with \"A.B\" will allow events logged by loggers \"A.B\",\n    \"A.B.C\", \"A.B.C.D\", \"A.B.D\" etc. but not \"A.BB\", \"B.A.B\" etc. If\n    initialized with the empty string, all events are passed.\n    \"\"\"\n    def __init__(self, name=''):\n        \"\"\"\n        Initialize a filter.\n\n        Initialize with the name of the logger which, together with its\n        children, will have its events allowed through the filter. If no\n        name is specified, allow every event.\n        \"\"\"\n        self.name = name\n        self.nlen = len(name)\n\n    def filter(self, record):\n        \"\"\"\n        Determine if the specified record is to be logged.\n\n        Is the specified record to be logged? Returns 0 for no, nonzero for\n        yes. If deemed appropriate, the record may be modified in-place.\n        \"\"\"\n        if self.nlen == 0:\n            return 1\n        elif self.name == record.name:\n            return 1\n        elif record.name.find(self.name, 0, self.nlen) != 0:\n            return 0\n        return (record.name[self.nlen] == \".\")\n\nclass Filterer(object):\n    \"\"\"\n    A base class for loggers and handlers which allows them to share\n    common code.\n    \"\"\"\n    def __init__(self):\n        \"\"\"\n        Initialize the list of filters to be an empty list.\n        \"\"\"\n        self.filters = []\n\n    def addFilter(self, filter):\n        \"\"\"\n        Add the specified filter to this handler.\n        \"\"\"\n        if not (filter in self.filters):\n            self.filters.append(filter)\n\n    def removeFilter(self, filter):\n        \"\"\"\n        Remove the specified filter from this handler.\n        \"\"\"\n        if filter in self.filters:\n            self.filters.remove(filter)\n\n    def filter(self, record):\n        \"\"\"\n        Determine if a record is loggable by consulting all the filters.\n\n        The default is to allow the record to be logged; any filter can veto\n        this and the record is then dropped. Returns a zero value if a record\n        is to be dropped, else non-zero.\n        \"\"\"\n        rv = 1\n        for f in self.filters:\n            if not f.filter(record):\n                rv = 0\n                break\n        return rv\n\n#---------------------------------------------------------------------------\n#   Handler classes and functions\n#---------------------------------------------------------------------------\n\n_handlers = weakref.WeakValueDictionary()  #map of handler names to handlers\n_handlerList = [] # added to allow handlers to be removed in reverse of order initialized\n\ndef _removeHandlerRef(wr):\n    \"\"\"\n    Remove a handler reference from the internal cleanup list.\n    \"\"\"\n    # This function can be called during module teardown, when globals are\n    # set to None. It can also be called from another thread. So we need to\n    # pre-emptively grab the necessary globals and check if they're None,\n    # to prevent race conditions and failures during interpreter shutdown.\n    acquire, release, handlers = _acquireLock, _releaseLock, _handlerList\n    if acquire and release and handlers:\n        acquire()\n        try:\n            if wr in handlers:\n                handlers.remove(wr)\n        finally:\n            release()\n\ndef _addHandlerRef(handler):\n    \"\"\"\n    Add a handler to the internal cleanup list using a weak reference.\n    \"\"\"\n    _acquireLock()\n    try:\n        _handlerList.append(weakref.ref(handler, _removeHandlerRef))\n    finally:\n        _releaseLock()\n\nclass Handler(Filterer):\n    \"\"\"\n    Handler instances dispatch logging events to specific destinations.\n\n    The base handler class. Acts as a placeholder which defines the Handler\n    interface. Handlers can optionally use Formatter instances to format\n    records as desired. By default, no formatter is specified; in this case,\n    the 'raw' message as determined by record.message is logged.\n    \"\"\"\n    def __init__(self, level=NOTSET):\n        \"\"\"\n        Initializes the instance - basically setting the formatter to None\n        and the filter list to empty.\n        \"\"\"\n        Filterer.__init__(self)\n        self._name = None\n        self.level = _checkLevel(level)\n        self.formatter = None\n        # Add the handler to the global _handlerList (for cleanup on shutdown)\n        _addHandlerRef(self)\n        self.createLock()\n\n    def get_name(self):\n        return self._name\n\n    def set_name(self, name):\n        _acquireLock()\n        try:\n            if self._name in _handlers:\n                del _handlers[self._name]\n            self._name = name\n            if name:\n                _handlers[name] = self\n        finally:\n            _releaseLock()\n\n    name = property(get_name, set_name)\n\n    def createLock(self):\n        \"\"\"\n        Acquire a thread lock for serializing access to the underlying I/O.\n        \"\"\"\n        if thread:\n            self.lock = threading.RLock()\n        else:\n            self.lock = None\n\n    def acquire(self):\n        \"\"\"\n        Acquire the I/O thread lock.\n        \"\"\"\n        if self.lock:\n            self.lock.acquire()\n\n    def release(self):\n        \"\"\"\n        Release the I/O thread lock.\n        \"\"\"\n        if self.lock:\n            self.lock.release()\n\n    def setLevel(self, level):\n        \"\"\"\n        Set the logging level of this handler.\n        \"\"\"\n        self.level = _checkLevel(level)\n\n    def format(self, record):\n        \"\"\"\n        Format the specified record.\n\n        If a formatter is set, use it. Otherwise, use the default formatter\n        for the module.\n        \"\"\"\n        if self.formatter:\n            fmt = self.formatter\n        else:\n            fmt = _defaultFormatter\n        return fmt.format(record)\n\n    def emit(self, record):\n        \"\"\"\n        Do whatever it takes to actually log the specified logging record.\n\n        This version is intended to be implemented by subclasses and so\n        raises a NotImplementedError.\n        \"\"\"\n        raise NotImplementedError('emit must be implemented '\n                                  'by Handler subclasses')\n\n    def handle(self, record):\n        \"\"\"\n        Conditionally emit the specified logging record.\n\n        Emission depends on filters which may have been added to the handler.\n        Wrap the actual emission of the record with acquisition/release of\n        the I/O thread lock. Returns whether the filter passed the record for\n        emission.\n        \"\"\"\n        rv = self.filter(record)\n        if rv:\n            self.acquire()\n            try:\n                self.emit(record)\n            finally:\n                self.release()\n        return rv\n\n    def setFormatter(self, fmt):\n        \"\"\"\n        Set the formatter for this handler.\n        \"\"\"\n        self.formatter = fmt\n\n    def flush(self):\n        \"\"\"\n        Ensure all logging output has been flushed.\n\n        This version does nothing and is intended to be implemented by\n        subclasses.\n        \"\"\"\n        pass\n\n    def close(self):\n        \"\"\"\n        Tidy up any resources used by the handler.\n\n        This version removes the handler from an internal map of handlers,\n        _handlers, which is used for handler lookup by name. Subclasses\n        should ensure that this gets called from overridden close()\n        methods.\n        \"\"\"\n        #get the module data lock, as we're updating a shared structure.\n        _acquireLock()\n        try:    #unlikely to raise an exception, but you never know...\n            if self._name and self._name in _handlers:\n                del _handlers[self._name]\n        finally:\n            _releaseLock()\n\n    def handleError(self, record):\n        \"\"\"\n        Handle errors which occur during an emit() call.\n\n        This method should be called from handlers when an exception is\n        encountered during an emit() call. If raiseExceptions is false,\n        exceptions get silently ignored. This is what is mostly wanted\n        for a logging system - most users will not care about errors in\n        the logging system, they are more interested in application errors.\n        You could, however, replace this with a custom handler if you wish.\n        The record which was being processed is passed in to this method.\n        \"\"\"\n        if raiseExceptions and sys.stderr:  # see issue 13807\n            ei = sys.exc_info()\n            try:\n                traceback.print_exception(ei[0], ei[1], ei[2],\n                                          None, sys.stderr)\n                sys.stderr.write('Logged from file %s, line %s\\n' % (\n                                 record.filename, record.lineno))\n            except IOError:\n                pass    # see issue 5971\n            finally:\n                del ei\n\nclass StreamHandler(Handler):\n    \"\"\"\n    A handler class which writes logging records, appropriately formatted,\n    to a stream. Note that this class does not close the stream, as\n    sys.stdout or sys.stderr may be used.\n    \"\"\"\n\n    def __init__(self, stream=None):\n        \"\"\"\n        Initialize the handler.\n\n        If stream is not specified, sys.stderr is used.\n        \"\"\"\n        Handler.__init__(self)\n        if stream is None:\n            stream = sys.stderr\n        self.stream = stream\n\n    def flush(self):\n        \"\"\"\n        Flushes the stream.\n        \"\"\"\n        self.acquire()\n        try:\n            if self.stream and hasattr(self.stream, \"flush\"):\n                self.stream.flush()\n        finally:\n            self.release()\n\n    def emit(self, record):\n        \"\"\"\n        Emit a record.\n\n        If a formatter is specified, it is used to format the record.\n        The record is then written to the stream with a trailing newline.  If\n        exception information is present, it is formatted using\n        traceback.print_exception and appended to the stream.  If the stream\n        has an 'encoding' attribute, it is used to determine how to do the\n        output to the stream.\n        \"\"\"\n        try:\n            msg = self.format(record)\n            stream = self.stream\n            fs = \"%s\\n\"\n            if not _unicode: #if no unicode support...\n                stream.write(fs % msg)\n            else:\n                try:\n                    if (isinstance(msg, unicode) and\n                        getattr(stream, 'encoding', None)):\n                        ufs = u'%s\\n'\n                        try:\n                            stream.write(ufs % msg)\n                        except UnicodeEncodeError:\n                            #Printing to terminals sometimes fails. For example,\n                            #with an encoding of 'cp1251', the above write will\n                            #work if written to a stream opened or wrapped by\n                            #the codecs module, but fail when writing to a\n                            #terminal even when the codepage is set to cp1251.\n                            #An extra encoding step seems to be needed.\n                            stream.write((ufs % msg).encode(stream.encoding))\n                    else:\n                        stream.write(fs % msg)\n                except UnicodeError:\n                    stream.write(fs % msg.encode(\"UTF-8\"))\n            self.flush()\n        except (KeyboardInterrupt, SystemExit):\n            raise\n        except:\n            self.handleError(record)\n\nclass FileHandler(StreamHandler):\n    \"\"\"\n    A handler class which writes formatted logging records to disk files.\n    \"\"\"\n    def __init__(self, filename, mode='a', encoding=None, delay=0):\n        \"\"\"\n        Open the specified file and use it as the stream for logging.\n        \"\"\"\n        #keep the absolute path, otherwise derived classes which use this\n        #may come a cropper when the current directory changes\n        if codecs is None:\n            encoding = None\n        self.baseFilename = os.path.abspath(filename)\n        self.mode = mode\n        self.encoding = encoding\n        self.delay = delay\n        if delay:\n            #We don't open the stream, but we still need to call the\n            #Handler constructor to set level, formatter, lock etc.\n            Handler.__init__(self)\n            self.stream = None\n        else:\n            StreamHandler.__init__(self, self._open())\n\n    def close(self):\n        \"\"\"\n        Closes the stream.\n        \"\"\"\n        self.acquire()\n        try:\n            if self.stream:\n                self.flush()\n                if hasattr(self.stream, \"close\"):\n                    self.stream.close()\n                self.stream = None\n            # Issue #19523: call unconditionally to\n            # prevent a handler leak when delay is set\n            StreamHandler.close(self)\n        finally:\n            self.release()\n\n    def _open(self):\n        \"\"\"\n        Open the current base file with the (original) mode and encoding.\n        Return the resulting stream.\n        \"\"\"\n        if self.encoding is None:\n            stream = open(self.baseFilename, self.mode)\n        else:\n            stream = codecs.open(self.baseFilename, self.mode, self.encoding)\n        return stream\n\n    def emit(self, record):\n        \"\"\"\n        Emit a record.\n\n        If the stream was not opened because 'delay' was specified in the\n        constructor, open it before calling the superclass's emit.\n        \"\"\"\n        if self.stream is None:\n            self.stream = self._open()\n        StreamHandler.emit(self, record)\n\n#---------------------------------------------------------------------------\n#   Manager classes and functions\n#---------------------------------------------------------------------------\n\nclass PlaceHolder(object):\n    \"\"\"\n    PlaceHolder instances are used in the Manager logger hierarchy to take\n    the place of nodes for which no loggers have been defined. This class is\n    intended for internal use only and not as part of the public API.\n    \"\"\"\n    def __init__(self, alogger):\n        \"\"\"\n        Initialize with the specified logger being a child of this placeholder.\n        \"\"\"\n        #self.loggers = [alogger]\n        self.loggerMap = { alogger : None }\n\n    def append(self, alogger):\n        \"\"\"\n        Add the specified logger as a child of this placeholder.\n        \"\"\"\n        #if alogger not in self.loggers:\n        if alogger not in self.loggerMap:\n            #self.loggers.append(alogger)\n            self.loggerMap[alogger] = None\n\n#\n#   Determine which class to use when instantiating loggers.\n#\n_loggerClass = None\n\ndef setLoggerClass(klass):\n    \"\"\"\n    Set the class to be used when instantiating a logger. The class should\n    define __init__() such that only a name argument is required, and the\n    __init__() should call Logger.__init__()\n    \"\"\"\n    if klass != Logger:\n        if not issubclass(klass, Logger):\n            raise TypeError(\"logger not derived from logging.Logger: \"\n                            + klass.__name__)\n    global _loggerClass\n    _loggerClass = klass\n\ndef getLoggerClass():\n    \"\"\"\n    Return the class to be used when instantiating a logger.\n    \"\"\"\n\n    return _loggerClass\n\nclass Manager(object):\n    \"\"\"\n    There is [under normal circumstances] just one Manager instance, which\n    holds the hierarchy of loggers.\n    \"\"\"\n    def __init__(self, rootnode):\n        \"\"\"\n        Initialize the manager with the root node of the logger hierarchy.\n        \"\"\"\n        self.root = rootnode\n        self.disable = 0\n        self.emittedNoHandlerWarning = 0\n        self.loggerDict = {}\n        self.loggerClass = None\n\n    def getLogger(self, name):\n        \"\"\"\n        Get a logger with the specified name (channel name), creating it\n        if it doesn't yet exist. This name is a dot-separated hierarchical\n        name, such as \"a\", \"a.b\", \"a.b.c\" or similar.\n\n        If a PlaceHolder existed for the specified name [i.e. the logger\n        didn't exist but a child of it did], replace it with the created\n        logger and fix up the parent/child references which pointed to the\n        placeholder to now point to the logger.\n        \"\"\"\n        rv = None\n        if not isinstance(name, basestring):\n            raise TypeError('A logger name must be string or Unicode')\n        if isinstance(name, unicode):\n            name = name.encode('utf-8')\n        _acquireLock()\n        try:\n            if name in self.loggerDict:\n                rv = self.loggerDict[name]\n                if isinstance(rv, PlaceHolder):\n                    ph = rv\n                    rv = (self.loggerClass or _loggerClass)(name)\n                    rv.manager = self\n                    self.loggerDict[name] = rv\n                    self._fixupChildren(ph, rv)\n                    self._fixupParents(rv)\n            else:\n                rv = (self.loggerClass or _loggerClass)(name)\n                rv.manager = self\n                self.loggerDict[name] = rv\n                self._fixupParents(rv)\n        finally:\n            _releaseLock()\n        return rv\n\n    def setLoggerClass(self, klass):\n        \"\"\"\n        Set the class to be used when instantiating a logger with this Manager.\n        \"\"\"\n        if klass != Logger:\n            if not issubclass(klass, Logger):\n                raise TypeError(\"logger not derived from logging.Logger: \"\n                                + klass.__name__)\n        self.loggerClass = klass\n\n    def _fixupParents(self, alogger):\n        \"\"\"\n        Ensure that there are either loggers or placeholders all the way\n        from the specified logger to the root of the logger hierarchy.\n        \"\"\"\n        name = alogger.name\n        i = name.rfind(\".\")\n        rv = None\n        while (i > 0) and not rv:\n            substr = name[:i]\n            if substr not in self.loggerDict:\n                self.loggerDict[substr] = PlaceHolder(alogger)\n            else:\n                obj = self.loggerDict[substr]\n                if isinstance(obj, Logger):\n                    rv = obj\n                else:\n                    assert isinstance(obj, PlaceHolder)\n                    obj.append(alogger)\n            i = name.rfind(\".\", 0, i - 1)\n        if not rv:\n            rv = self.root\n        alogger.parent = rv\n\n    def _fixupChildren(self, ph, alogger):\n        \"\"\"\n        Ensure that children of the placeholder ph are connected to the\n        specified logger.\n        \"\"\"\n        name = alogger.name\n        namelen = len(name)\n        for c in ph.loggerMap.keys():\n            #The if means ... if not c.parent.name.startswith(nm)\n            if c.parent.name[:namelen] != name:\n                alogger.parent = c.parent\n                c.parent = alogger\n\n#---------------------------------------------------------------------------\n#   Logger classes and functions\n#---------------------------------------------------------------------------\n\nclass Logger(Filterer):\n    \"\"\"\n    Instances of the Logger class represent a single logging channel. A\n    \"logging channel\" indicates an area of an application. Exactly how an\n    \"area\" is defined is up to the application developer. Since an\n    application can have any number of areas, logging channels are identified\n    by a unique string. Application areas can be nested (e.g. an area\n    of \"input processing\" might include sub-areas \"read CSV files\", \"read\n    XLS files\" and \"read Gnumeric files\"). To cater for this natural nesting,\n    channel names are organized into a namespace hierarchy where levels are\n    separated by periods, much like the Java or Python package namespace. So\n    in the instance given above, channel names might be \"input\" for the upper\n    level, and \"input.csv\", \"input.xls\" and \"input.gnu\" for the sub-levels.\n    There is no arbitrary limit to the depth of nesting.\n    \"\"\"\n    def __init__(self, name, level=NOTSET):\n        \"\"\"\n        Initialize the logger with a name and an optional level.\n        \"\"\"\n        Filterer.__init__(self)\n        self.name = name\n        self.level = _checkLevel(level)\n        self.parent = None\n        self.propagate = 1\n        self.handlers = []\n        self.disabled = 0\n\n    def setLevel(self, level):\n        \"\"\"\n        Set the logging level of this logger.\n        \"\"\"\n        self.level = _checkLevel(level)\n\n    def debug(self, msg, *args, **kwargs):\n        \"\"\"\n        Log 'msg % args' with severity 'DEBUG'.\n\n        To pass exception information, use the keyword argument exc_info with\n        a true value, e.g.\n\n        logger.debug(\"Houston, we have a %s\", \"thorny problem\", exc_info=1)\n        \"\"\"\n        if self.isEnabledFor(DEBUG):\n            self._log(DEBUG, msg, args, **kwargs)\n\n    def info(self, msg, *args, **kwargs):\n        \"\"\"\n        Log 'msg % args' with severity 'INFO'.\n\n        To pass exception information, use the keyword argument exc_info with\n        a true value, e.g.\n\n        logger.info(\"Houston, we have a %s\", \"interesting problem\", exc_info=1)\n        \"\"\"\n        if self.isEnabledFor(INFO):\n            self._log(INFO, msg, args, **kwargs)\n\n    def warning(self, msg, *args, **kwargs):\n        \"\"\"\n        Log 'msg % args' with severity 'WARNING'.\n\n        To pass exception information, use the keyword argument exc_info with\n        a true value, e.g.\n\n        logger.warning(\"Houston, we have a %s\", \"bit of a problem\", exc_info=1)\n        \"\"\"\n        if self.isEnabledFor(WARNING):\n            self._log(WARNING, msg, args, **kwargs)\n\n    warn = warning\n\n    def error(self, msg, *args, **kwargs):\n        \"\"\"\n        Log 'msg % args' with severity 'ERROR'.\n\n        To pass exception information, use the keyword argument exc_info with\n        a true value, e.g.\n\n        logger.error(\"Houston, we have a %s\", \"major problem\", exc_info=1)\n        \"\"\"\n        if self.isEnabledFor(ERROR):\n            self._log(ERROR, msg, args, **kwargs)\n\n    def exception(self, msg, *args, **kwargs):\n        \"\"\"\n        Convenience method for logging an ERROR with exception information.\n        \"\"\"\n        kwargs['exc_info'] = 1\n        self.error(msg, *args, **kwargs)\n\n    def critical(self, msg, *args, **kwargs):\n        \"\"\"\n        Log 'msg % args' with severity 'CRITICAL'.\n\n        To pass exception information, use the keyword argument exc_info with\n        a true value, e.g.\n\n        logger.critical(\"Houston, we have a %s\", \"major disaster\", exc_info=1)\n        \"\"\"\n        if self.isEnabledFor(CRITICAL):\n            self._log(CRITICAL, msg, args, **kwargs)\n\n    fatal = critical\n\n    def log(self, level, msg, *args, **kwargs):\n        \"\"\"\n        Log 'msg % args' with the integer severity 'level'.\n\n        To pass exception information, use the keyword argument exc_info with\n        a true value, e.g.\n\n        logger.log(level, \"We have a %s\", \"mysterious problem\", exc_info=1)\n        \"\"\"\n        if not isinstance(level, int):\n            if raiseExceptions:\n                raise TypeError(\"level must be an integer\")\n            else:\n                return\n        if self.isEnabledFor(level):\n            self._log(level, msg, args, **kwargs)\n\n    def findCaller(self):\n        \"\"\"\n        Find the stack frame of the caller so that we can note the source\n        file name, line number and function name.\n        \"\"\"\n        f = currentframe()\n        #On some versions of IronPython, currentframe() returns None if\n        #IronPython isn't run with -X:Frames.\n        if f is not None:\n            f = f.f_back\n        rv = \"(unknown file)\", 0, \"(unknown function)\"\n        while hasattr(f, \"f_code\"):\n            co = f.f_code\n            filename = os.path.normcase(co.co_filename)\n            if filename == _srcfile:\n                f = f.f_back\n                continue\n            rv = (co.co_filename, f.f_lineno, co.co_name)\n            break\n        return rv\n\n    def makeRecord(self, name, level, fn, lno, msg, args, exc_info, func=None, extra=None):\n        \"\"\"\n        A factory method which can be overridden in subclasses to create\n        specialized LogRecords.\n        \"\"\"\n        rv = LogRecord(name, level, fn, lno, msg, args, exc_info, func)\n        if extra is not None:\n            for key in extra:\n                if (key in [\"message\", \"asctime\"]) or (key in rv.__dict__):\n                    raise KeyError(\"Attempt to overwrite %r in LogRecord\" % key)\n                rv.__dict__[key] = extra[key]\n        return rv\n\n    def _log(self, level, msg, args, exc_info=None, extra=None):\n        \"\"\"\n        Low-level logging routine which creates a LogRecord and then calls\n        all the handlers of this logger to handle the record.\n        \"\"\"\n        if _srcfile:\n            #IronPython doesn't track Python frames, so findCaller raises an\n            #exception on some versions of IronPython. We trap it here so that\n            #IronPython can use logging.\n            try:\n                fn, lno, func = self.findCaller()\n            except ValueError:\n                fn, lno, func = \"(unknown file)\", 0, \"(unknown function)\"\n        else:\n            fn, lno, func = \"(unknown file)\", 0, \"(unknown function)\"\n        if exc_info:\n            if not isinstance(exc_info, tuple):\n                exc_info = sys.exc_info()\n        record = self.makeRecord(self.name, level, fn, lno, msg, args, exc_info, func, extra)\n        self.handle(record)\n\n    def handle(self, record):\n        \"\"\"\n        Call the handlers for the specified record.\n\n        This method is used for unpickled records received from a socket, as\n        well as those created locally. Logger-level filtering is applied.\n        \"\"\"\n        if (not self.disabled) and self.filter(record):\n            self.callHandlers(record)\n\n    def addHandler(self, hdlr):\n        \"\"\"\n        Add the specified handler to this logger.\n        \"\"\"\n        _acquireLock()\n        try:\n            if not (hdlr in self.handlers):\n                self.handlers.append(hdlr)\n        finally:\n            _releaseLock()\n\n    def removeHandler(self, hdlr):\n        \"\"\"\n        Remove the specified handler from this logger.\n        \"\"\"\n        _acquireLock()\n        try:\n            if hdlr in self.handlers:\n                self.handlers.remove(hdlr)\n        finally:\n            _releaseLock()\n\n    def callHandlers(self, record):\n        \"\"\"\n        Pass a record to all relevant handlers.\n\n        Loop through all handlers for this logger and its parents in the\n        logger hierarchy. If no handler was found, output a one-off error\n        message to sys.stderr. Stop searching up the hierarchy whenever a\n        logger with the \"propagate\" attribute set to zero is found - that\n        will be the last logger whose handlers are called.\n        \"\"\"\n        c = self\n        found = 0\n        while c:\n            for hdlr in c.handlers:\n                found = found + 1\n                if record.levelno >= hdlr.level:\n                    hdlr.handle(record)\n            if not c.propagate:\n                c = None    #break out\n            else:\n                c = c.parent\n        if (found == 0) and raiseExceptions and not self.manager.emittedNoHandlerWarning:\n            sys.stderr.write(\"No handlers could be found for logger\"\n                             \" \\\"%s\\\"\\n\" % self.name)\n            self.manager.emittedNoHandlerWarning = 1\n\n    def getEffectiveLevel(self):\n        \"\"\"\n        Get the effective level for this logger.\n\n        Loop through this logger and its parents in the logger hierarchy,\n        looking for a non-zero logging level. Return the first one found.\n        \"\"\"\n        logger = self\n        while logger:\n            if logger.level:\n                return logger.level\n            logger = logger.parent\n        return NOTSET\n\n    def isEnabledFor(self, level):\n        \"\"\"\n        Is this logger enabled for level 'level'?\n        \"\"\"\n        if self.manager.disable >= level:\n            return 0\n        return level >= self.getEffectiveLevel()\n\n    def getChild(self, suffix):\n        \"\"\"\n        Get a logger which is a descendant to this one.\n\n        This is a convenience method, such that\n\n        logging.getLogger('abc').getChild('def.ghi')\n\n        is the same as\n\n        logging.getLogger('abc.def.ghi')\n\n        It's useful, for example, when the parent logger is named using\n        __name__ rather than a literal string.\n        \"\"\"\n        if self.root is not self:\n            suffix = '.'.join((self.name, suffix))\n        return self.manager.getLogger(suffix)\n\nclass RootLogger(Logger):\n    \"\"\"\n    A root logger is not that different to any other logger, except that\n    it must have a logging level and there is only one instance of it in\n    the hierarchy.\n    \"\"\"\n    def __init__(self, level):\n        \"\"\"\n        Initialize the logger with the name \"root\".\n        \"\"\"\n        Logger.__init__(self, \"root\", level)\n\n_loggerClass = Logger\n\nclass LoggerAdapter(object):\n    \"\"\"\n    An adapter for loggers which makes it easier to specify contextual\n    information in logging output.\n    \"\"\"\n\n    def __init__(self, logger, extra):\n        \"\"\"\n        Initialize the adapter with a logger and a dict-like object which\n        provides contextual information. This constructor signature allows\n        easy stacking of LoggerAdapters, if so desired.\n\n        You can effectively pass keyword arguments as shown in the\n        following example:\n\n        adapter = LoggerAdapter(someLogger, dict(p1=v1, p2=\"v2\"))\n        \"\"\"\n        self.logger = logger\n        self.extra = extra\n\n    def process(self, msg, kwargs):\n        \"\"\"\n        Process the logging message and keyword arguments passed in to\n        a logging call to insert contextual information. You can either\n        manipulate the message itself, the keyword args or both. Return\n        the message and kwargs modified (or not) to suit your needs.\n\n        Normally, you'll only need to override this one method in a\n        LoggerAdapter subclass for your specific needs.\n        \"\"\"\n        kwargs[\"extra\"] = self.extra\n        return msg, kwargs\n\n    def debug(self, msg, *args, **kwargs):\n        \"\"\"\n        Delegate a debug call to the underlying logger, after adding\n        contextual information from this adapter instance.\n        \"\"\"\n        msg, kwargs = self.process(msg, kwargs)\n        self.logger.debug(msg, *args, **kwargs)\n\n    def info(self, msg, *args, **kwargs):\n        \"\"\"\n        Delegate an info call to the underlying logger, after adding\n        contextual information from this adapter instance.\n        \"\"\"\n        msg, kwargs = self.process(msg, kwargs)\n        self.logger.info(msg, *args, **kwargs)\n\n    def warning(self, msg, *args, **kwargs):\n        \"\"\"\n        Delegate a warning call to the underlying logger, after adding\n        contextual information from this adapter instance.\n        \"\"\"\n        msg, kwargs = self.process(msg, kwargs)\n        self.logger.warning(msg, *args, **kwargs)\n\n    def error(self, msg, *args, **kwargs):\n        \"\"\"\n        Delegate an error call to the underlying logger, after adding\n        contextual information from this adapter instance.\n        \"\"\"\n        msg, kwargs = self.process(msg, kwargs)\n        self.logger.error(msg, *args, **kwargs)\n\n    def exception(self, msg, *args, **kwargs):\n        \"\"\"\n        Delegate an exception call to the underlying logger, after adding\n        contextual information from this adapter instance.\n        \"\"\"\n        msg, kwargs = self.process(msg, kwargs)\n        kwargs[\"exc_info\"] = 1\n        self.logger.error(msg, *args, **kwargs)\n\n    def critical(self, msg, *args, **kwargs):\n        \"\"\"\n        Delegate a critical call to the underlying logger, after adding\n        contextual information from this adapter instance.\n        \"\"\"\n        msg, kwargs = self.process(msg, kwargs)\n        self.logger.critical(msg, *args, **kwargs)\n\n    def log(self, level, msg, *args, **kwargs):\n        \"\"\"\n        Delegate a log call to the underlying logger, after adding\n        contextual information from this adapter instance.\n        \"\"\"\n        msg, kwargs = self.process(msg, kwargs)\n        self.logger.log(level, msg, *args, **kwargs)\n\n    def isEnabledFor(self, level):\n        \"\"\"\n        See if the underlying logger is enabled for the specified level.\n        \"\"\"\n        return self.logger.isEnabledFor(level)\n\nroot = RootLogger(WARNING)\nLogger.root = root\nLogger.manager = Manager(Logger.root)\n\n#---------------------------------------------------------------------------\n# Configuration classes and functions\n#---------------------------------------------------------------------------\n\nBASIC_FORMAT = \"%(levelname)s:%(name)s:%(message)s\"\n\ndef basicConfig(**kwargs):\n    \"\"\"\n    Do basic configuration for the logging system.\n\n    This function does nothing if the root logger already has handlers\n    configured. It is a convenience method intended for use by simple scripts\n    to do one-shot configuration of the logging package.\n\n    The default behaviour is to create a StreamHandler which writes to\n    sys.stderr, set a formatter using the BASIC_FORMAT format string, and\n    add the handler to the root logger.\n\n    A number of optional keyword arguments may be specified, which can alter\n    the default behaviour.\n\n    filename  Specifies that a FileHandler be created, using the specified\n              filename, rather than a StreamHandler.\n    filemode  Specifies the mode to open the file, if filename is specified\n              (if filemode is unspecified, it defaults to 'a').\n    format    Use the specified format string for the handler.\n    datefmt   Use the specified date/time format.\n    level     Set the root logger level to the specified level.\n    stream    Use the specified stream to initialize the StreamHandler. Note\n              that this argument is incompatible with 'filename' - if both\n              are present, 'stream' is ignored.\n\n    Note that you could specify a stream created using open(filename, mode)\n    rather than passing the filename and mode in. However, it should be\n    remembered that StreamHandler does not close its stream (since it may be\n    using sys.stdout or sys.stderr), whereas FileHandler closes its stream\n    when the handler is closed.\n    \"\"\"\n    # Add thread safety in case someone mistakenly calls\n    # basicConfig() from multiple threads\n    _acquireLock()\n    try:\n        if len(root.handlers) == 0:\n            filename = kwargs.get(\"filename\")\n            if filename:\n                mode = kwargs.get(\"filemode\", 'a')\n                hdlr = FileHandler(filename, mode)\n            else:\n                stream = kwargs.get(\"stream\")\n                hdlr = StreamHandler(stream)\n            fs = kwargs.get(\"format\", BASIC_FORMAT)\n            dfs = kwargs.get(\"datefmt\", None)\n            fmt = Formatter(fs, dfs)\n            hdlr.setFormatter(fmt)\n            root.addHandler(hdlr)\n            level = kwargs.get(\"level\")\n            if level is not None:\n                root.setLevel(level)\n    finally:\n        _releaseLock()\n\n#---------------------------------------------------------------------------\n# Utility functions at module level.\n# Basically delegate everything to the root logger.\n#---------------------------------------------------------------------------\n\ndef getLogger(name=None):\n    \"\"\"\n    Return a logger with the specified name, creating it if necessary.\n\n    If no name is specified, return the root logger.\n    \"\"\"\n    if name:\n        return Logger.manager.getLogger(name)\n    else:\n        return root\n\n#def getRootLogger():\n#    \"\"\"\n#    Return the root logger.\n#\n#    Note that getLogger('') now does the same thing, so this function is\n#    deprecated and may disappear in the future.\n#    \"\"\"\n#    return root\n\ndef critical(msg, *args, **kwargs):\n    \"\"\"\n    Log a message with severity 'CRITICAL' on the root logger.\n    \"\"\"\n    if len(root.handlers) == 0:\n        basicConfig()\n    root.critical(msg, *args, **kwargs)\n\nfatal = critical\n\ndef error(msg, *args, **kwargs):\n    \"\"\"\n    Log a message with severity 'ERROR' on the root logger.\n    \"\"\"\n    if len(root.handlers) == 0:\n        basicConfig()\n    root.error(msg, *args, **kwargs)\n\ndef exception(msg, *args, **kwargs):\n    \"\"\"\n    Log a message with severity 'ERROR' on the root logger,\n    with exception information.\n    \"\"\"\n    kwargs['exc_info'] = 1\n    error(msg, *args, **kwargs)\n\ndef warning(msg, *args, **kwargs):\n    \"\"\"\n    Log a message with severity 'WARNING' on the root logger.\n    \"\"\"\n    if len(root.handlers) == 0:\n        basicConfig()\n    root.warning(msg, *args, **kwargs)\n\nwarn = warning\n\ndef info(msg, *args, **kwargs):\n    \"\"\"\n    Log a message with severity 'INFO' on the root logger.\n    \"\"\"\n    if len(root.handlers) == 0:\n        basicConfig()\n    root.info(msg, *args, **kwargs)\n\ndef debug(msg, *args, **kwargs):\n    \"\"\"\n    Log a message with severity 'DEBUG' on the root logger.\n    \"\"\"\n    if len(root.handlers) == 0:\n        basicConfig()\n    root.debug(msg, *args, **kwargs)\n\ndef log(level, msg, *args, **kwargs):\n    \"\"\"\n    Log 'msg % args' with the integer severity 'level' on the root logger.\n    \"\"\"\n    if len(root.handlers) == 0:\n        basicConfig()\n    root.log(level, msg, *args, **kwargs)\n\ndef disable(level):\n    \"\"\"\n    Disable all logging calls of severity 'level' and below.\n    \"\"\"\n    root.manager.disable = level\n\ndef shutdown(handlerList=_handlerList):\n    \"\"\"\n    Perform any cleanup actions in the logging system (e.g. flushing\n    buffers).\n\n    Should be called at application exit.\n    \"\"\"\n    for wr in reversed(handlerList[:]):\n        #errors might occur, for example, if files are locked\n        #we just ignore them if raiseExceptions is not set\n        try:\n            h = wr()\n            if h:\n                try:\n                    h.acquire()\n                    h.flush()\n                    h.close()\n                except (IOError, ValueError):\n                    # Ignore errors which might be caused\n                    # because handlers have been closed but\n                    # references to them are still around at\n                    # application exit.\n                    pass\n                finally:\n                    h.release()\n        except:\n            if raiseExceptions:\n                raise\n            #else, swallow\n\n#Let's try and shutdown automatically on application exit...\nimport atexit\natexit.register(shutdown)\n\n# Null handler\n\nclass NullHandler(Handler):\n    \"\"\"\n    This handler does nothing. It's intended to be used to avoid the\n    \"No handlers could be found for logger XXX\" one-off warning. This is\n    important for library code, which may contain code to log events. If a user\n    of the library does not configure logging, the one-off warning might be\n    produced; to avoid this, the library developer simply needs to instantiate\n    a NullHandler and add it to the top-level logger of the library module or\n    package.\n    \"\"\"\n    def handle(self, record):\n        pass\n\n    def emit(self, record):\n        pass\n\n    def createLock(self):\n        self.lock = None\n\n# Warnings integration\n\n_warnings_showwarning = None\n\ndef _showwarning(message, category, filename, lineno, file=None, line=None):\n    \"\"\"\n    Implementation of showwarnings which redirects to logging, which will first\n    check to see if the file parameter is None. If a file is specified, it will\n    delegate to the original warnings implementation of showwarning. Otherwise,\n    it will call warnings.formatwarning and will log the resulting string to a\n    warnings logger named \"py.warnings\" with level logging.WARNING.\n    \"\"\"\n    if file is not None:\n        if _warnings_showwarning is not None:\n            _warnings_showwarning(message, category, filename, lineno, file, line)\n    else:\n        s = warnings.formatwarning(message, category, filename, lineno, line)\n        logger = getLogger(\"py.warnings\")\n        if not logger.handlers:\n            logger.addHandler(NullHandler())\n        logger.warning(\"%s\", s)\n\ndef captureWarnings(capture):\n    \"\"\"\n    If capture is true, redirect all warnings to the logging package.\n    If capture is False, ensure that warnings are not redirected to logging\n    but to their original destinations.\n    \"\"\"\n    global _warnings_showwarning\n    if capture:\n        if _warnings_showwarning is None:\n            _warnings_showwarning = warnings.showwarning\n            warnings.showwarning = _showwarning\n    else:\n        if _warnings_showwarning is not None:\n            warnings.showwarning = _warnings_showwarning\n            _warnings_showwarning = None\n", 
    "marshal": "# Note that PyPy contains also a built-in module 'marshal' which will\n# hide this one if compiled in.\n\nfrom _marshal import __doc__\nfrom _marshal import *\n", 
    "opcode": "\"\"\"\nopcode module - potentially shared between dis and other modules which\noperate on bytecodes (e.g. peephole optimizers).\n\"\"\"\n\n__all__ = [\"cmp_op\", \"hasconst\", \"hasname\", \"hasjrel\", \"hasjabs\",\n           \"haslocal\", \"hascompare\", \"hasfree\", \"opname\", \"opmap\",\n           \"HAVE_ARGUMENT\", \"EXTENDED_ARG\"]\n\ncmp_op = ('<', '<=', '==', '!=', '>', '>=', 'in', 'not in', 'is',\n        'is not', 'exception match', 'BAD')\n\nhasconst = []\nhasname = []\nhasjrel = []\nhasjabs = []\nhaslocal = []\nhascompare = []\nhasfree = []\n\nopmap = {}\nopname = [''] * 256\nfor op in range(256): opname[op] = '<%r>' % (op,)\ndel op\n\ndef def_op(name, op):\n    opname[op] = name\n    opmap[name] = op\n\ndef name_op(name, op):\n    def_op(name, op)\n    hasname.append(op)\n\ndef jrel_op(name, op):\n    def_op(name, op)\n    hasjrel.append(op)\n\ndef jabs_op(name, op):\n    def_op(name, op)\n    hasjabs.append(op)\n\n# Instruction opcodes for compiled code\n# Blank lines correspond to available opcodes\n\ndef_op('STOP_CODE', 0)\ndef_op('POP_TOP', 1)\ndef_op('ROT_TWO', 2)\ndef_op('ROT_THREE', 3)\ndef_op('DUP_TOP', 4)\ndef_op('ROT_FOUR', 5)\n\ndef_op('NOP', 9)\ndef_op('UNARY_POSITIVE', 10)\ndef_op('UNARY_NEGATIVE', 11)\ndef_op('UNARY_NOT', 12)\ndef_op('UNARY_CONVERT', 13)\n\ndef_op('UNARY_INVERT', 15)\n\ndef_op('BINARY_POWER', 19)\ndef_op('BINARY_MULTIPLY', 20)\ndef_op('BINARY_DIVIDE', 21)\ndef_op('BINARY_MODULO', 22)\ndef_op('BINARY_ADD', 23)\ndef_op('BINARY_SUBTRACT', 24)\ndef_op('BINARY_SUBSCR', 25)\ndef_op('BINARY_FLOOR_DIVIDE', 26)\ndef_op('BINARY_TRUE_DIVIDE', 27)\ndef_op('INPLACE_FLOOR_DIVIDE', 28)\ndef_op('INPLACE_TRUE_DIVIDE', 29)\ndef_op('SLICE+0', 30)\ndef_op('SLICE+1', 31)\ndef_op('SLICE+2', 32)\ndef_op('SLICE+3', 33)\n\ndef_op('STORE_SLICE+0', 40)\ndef_op('STORE_SLICE+1', 41)\ndef_op('STORE_SLICE+2', 42)\ndef_op('STORE_SLICE+3', 43)\n\ndef_op('DELETE_SLICE+0', 50)\ndef_op('DELETE_SLICE+1', 51)\ndef_op('DELETE_SLICE+2', 52)\ndef_op('DELETE_SLICE+3', 53)\n\ndef_op('STORE_MAP', 54)\ndef_op('INPLACE_ADD', 55)\ndef_op('INPLACE_SUBTRACT', 56)\ndef_op('INPLACE_MULTIPLY', 57)\ndef_op('INPLACE_DIVIDE', 58)\ndef_op('INPLACE_MODULO', 59)\ndef_op('STORE_SUBSCR', 60)\ndef_op('DELETE_SUBSCR', 61)\ndef_op('BINARY_LSHIFT', 62)\ndef_op('BINARY_RSHIFT', 63)\ndef_op('BINARY_AND', 64)\ndef_op('BINARY_XOR', 65)\ndef_op('BINARY_OR', 66)\ndef_op('INPLACE_POWER', 67)\ndef_op('GET_ITER', 68)\n\ndef_op('PRINT_EXPR', 70)\ndef_op('PRINT_ITEM', 71)\ndef_op('PRINT_NEWLINE', 72)\ndef_op('PRINT_ITEM_TO', 73)\ndef_op('PRINT_NEWLINE_TO', 74)\ndef_op('INPLACE_LSHIFT', 75)\ndef_op('INPLACE_RSHIFT', 76)\ndef_op('INPLACE_AND', 77)\ndef_op('INPLACE_XOR', 78)\ndef_op('INPLACE_OR', 79)\ndef_op('BREAK_LOOP', 80)\ndef_op('WITH_CLEANUP', 81)\ndef_op('LOAD_LOCALS', 82)\ndef_op('RETURN_VALUE', 83)\ndef_op('IMPORT_STAR', 84)\ndef_op('EXEC_STMT', 85)\ndef_op('YIELD_VALUE', 86)\ndef_op('POP_BLOCK', 87)\ndef_op('END_FINALLY', 88)\ndef_op('BUILD_CLASS', 89)\n\nHAVE_ARGUMENT = 90              # Opcodes from here have an argument:\n\nname_op('STORE_NAME', 90)       # Index in name list\nname_op('DELETE_NAME', 91)      # \"\"\ndef_op('UNPACK_SEQUENCE', 92)   # Number of tuple items\njrel_op('FOR_ITER', 93)\ndef_op('LIST_APPEND', 94)\nname_op('STORE_ATTR', 95)       # Index in name list\nname_op('DELETE_ATTR', 96)      # \"\"\nname_op('STORE_GLOBAL', 97)     # \"\"\nname_op('DELETE_GLOBAL', 98)    # \"\"\ndef_op('DUP_TOPX', 99)          # number of items to duplicate\ndef_op('LOAD_CONST', 100)       # Index in const list\nhasconst.append(100)\nname_op('LOAD_NAME', 101)       # Index in name list\ndef_op('BUILD_TUPLE', 102)      # Number of tuple items\ndef_op('BUILD_LIST', 103)       # Number of list items\ndef_op('BUILD_SET', 104)        # Number of set items\ndef_op('BUILD_MAP', 105)        # Number of dict entries (upto 255)\nname_op('LOAD_ATTR', 106)       # Index in name list\ndef_op('COMPARE_OP', 107)       # Comparison operator\nhascompare.append(107)\nname_op('IMPORT_NAME', 108)     # Index in name list\nname_op('IMPORT_FROM', 109)     # Index in name list\njrel_op('JUMP_FORWARD', 110)    # Number of bytes to skip\njabs_op('JUMP_IF_FALSE_OR_POP', 111) # Target byte offset from beginning of code\njabs_op('JUMP_IF_TRUE_OR_POP', 112)  # \"\"\njabs_op('JUMP_ABSOLUTE', 113)        # \"\"\njabs_op('POP_JUMP_IF_FALSE', 114)    # \"\"\njabs_op('POP_JUMP_IF_TRUE', 115)     # \"\"\n\nname_op('LOAD_GLOBAL', 116)     # Index in name list\n\njabs_op('CONTINUE_LOOP', 119)   # Target address\njrel_op('SETUP_LOOP', 120)      # Distance to target address\njrel_op('SETUP_EXCEPT', 121)    # \"\"\njrel_op('SETUP_FINALLY', 122)   # \"\"\n\ndef_op('LOAD_FAST', 124)        # Local variable number\nhaslocal.append(124)\ndef_op('STORE_FAST', 125)       # Local variable number\nhaslocal.append(125)\ndef_op('DELETE_FAST', 126)      # Local variable number\nhaslocal.append(126)\n\ndef_op('RAISE_VARARGS', 130)    # Number of raise arguments (1, 2, or 3)\ndef_op('CALL_FUNCTION', 131)    # #args + (#kwargs << 8)\ndef_op('MAKE_FUNCTION', 132)    # Number of args with default values\ndef_op('BUILD_SLICE', 133)      # Number of items\ndef_op('MAKE_CLOSURE', 134)\ndef_op('LOAD_CLOSURE', 135)\nhasfree.append(135)\ndef_op('LOAD_DEREF', 136)\nhasfree.append(136)\ndef_op('STORE_DEREF', 137)\nhasfree.append(137)\n\ndef_op('CALL_FUNCTION_VAR', 140)     # #args + (#kwargs << 8)\ndef_op('CALL_FUNCTION_KW', 141)      # #args + (#kwargs << 8)\ndef_op('CALL_FUNCTION_VAR_KW', 142)  # #args + (#kwargs << 8)\n\njrel_op('SETUP_WITH', 143)\n\ndef_op('EXTENDED_ARG', 145)\nEXTENDED_ARG = 145\ndef_op('SET_ADD', 146)\ndef_op('MAP_ADD', 147)\n\n# pypy modification, experimental bytecode\ndef_op('LOOKUP_METHOD', 201)          # Index in name list\nhasname.append(201)\ndef_op('CALL_METHOD', 202)            # #args not including 'self'\ndef_op('BUILD_LIST_FROM_ARG', 203)\njrel_op('JUMP_IF_NOT_DEBUG', 204)     # jump over assert statements\n\ndel def_op, name_op, jrel_op, jabs_op\n", 
    "optparse": "\"\"\"A powerful, extensible, and easy-to-use option parser.\n\nBy Greg Ward <gward@python.net>\n\nOriginally distributed as Optik.\n\nFor support, use the optik-users@lists.sourceforge.net mailing list\n(http://lists.sourceforge.net/lists/listinfo/optik-users).\n\nSimple usage example:\n\n   from optparse import OptionParser\n\n   parser = OptionParser()\n   parser.add_option(\"-f\", \"--file\", dest=\"filename\",\n                     help=\"write report to FILE\", metavar=\"FILE\")\n   parser.add_option(\"-q\", \"--quiet\",\n                     action=\"store_false\", dest=\"verbose\", default=True,\n                     help=\"don't print status messages to stdout\")\n\n   (options, args) = parser.parse_args()\n\"\"\"\n\n__version__ = \"1.5.3\"\n\n__all__ = ['Option',\n           'make_option',\n           'SUPPRESS_HELP',\n           'SUPPRESS_USAGE',\n           'Values',\n           'OptionContainer',\n           'OptionGroup',\n           'OptionParser',\n           'HelpFormatter',\n           'IndentedHelpFormatter',\n           'TitledHelpFormatter',\n           'OptParseError',\n           'OptionError',\n           'OptionConflictError',\n           'OptionValueError',\n           'BadOptionError']\n\n__copyright__ = \"\"\"\nCopyright (c) 2001-2006 Gregory P. Ward.  All rights reserved.\nCopyright (c) 2002-2006 Python Software Foundation.  All rights reserved.\n\nRedistribution and use in source and binary forms, with or without\nmodification, are permitted provided that the following conditions are\nmet:\n\n  * Redistributions of source code must retain the above copyright\n    notice, this list of conditions and the following disclaimer.\n\n  * Redistributions in binary form must reproduce the above copyright\n    notice, this list of conditions and the following disclaimer in the\n    documentation and/or other materials provided with the distribution.\n\n  * Neither the name of the author nor the names of its\n    contributors may be used to endorse or promote products derived from\n    this software without specific prior written permission.\n\nTHIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS\nIS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED\nTO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A\nPARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE AUTHOR OR\nCONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\nEXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,\nPROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR\nPROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF\nLIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING\nNEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS\nSOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n\"\"\"\n\nimport sys, os\nimport types\nimport textwrap\n\ndef _repr(self):\n    return \"<%s at 0x%x: %s>\" % (self.__class__.__name__, id(self), self)\n\n\n# This file was generated from:\n#   Id: option_parser.py 527 2006-07-23 15:21:30Z greg\n#   Id: option.py 522 2006-06-11 16:22:03Z gward\n#   Id: help.py 527 2006-07-23 15:21:30Z greg\n#   Id: errors.py 509 2006-04-20 00:58:24Z gward\n\ntry:\n    from gettext import gettext\nexcept ImportError:\n    def gettext(message):\n        return message\n_ = gettext\n\n\nclass OptParseError (Exception):\n    def __init__(self, msg):\n        self.msg = msg\n\n    def __str__(self):\n        return self.msg\n\n\nclass OptionError (OptParseError):\n    \"\"\"\n    Raised if an Option instance is created with invalid or\n    inconsistent arguments.\n    \"\"\"\n\n    def __init__(self, msg, option):\n        self.msg = msg\n        self.option_id = str(option)\n\n    def __str__(self):\n        if self.option_id:\n            return \"option %s: %s\" % (self.option_id, self.msg)\n        else:\n            return self.msg\n\nclass OptionConflictError (OptionError):\n    \"\"\"\n    Raised if conflicting options are added to an OptionParser.\n    \"\"\"\n\nclass OptionValueError (OptParseError):\n    \"\"\"\n    Raised if an invalid option value is encountered on the command\n    line.\n    \"\"\"\n\nclass BadOptionError (OptParseError):\n    \"\"\"\n    Raised if an invalid option is seen on the command line.\n    \"\"\"\n    def __init__(self, opt_str):\n        self.opt_str = opt_str\n\n    def __str__(self):\n        return _(\"no such option: %s\") % self.opt_str\n\nclass AmbiguousOptionError (BadOptionError):\n    \"\"\"\n    Raised if an ambiguous option is seen on the command line.\n    \"\"\"\n    def __init__(self, opt_str, possibilities):\n        BadOptionError.__init__(self, opt_str)\n        self.possibilities = possibilities\n\n    def __str__(self):\n        return (_(\"ambiguous option: %s (%s?)\")\n                % (self.opt_str, \", \".join(self.possibilities)))\n\n\nclass HelpFormatter:\n\n    \"\"\"\n    Abstract base class for formatting option help.  OptionParser\n    instances should use one of the HelpFormatter subclasses for\n    formatting help; by default IndentedHelpFormatter is used.\n\n    Instance attributes:\n      parser : OptionParser\n        the controlling OptionParser instance\n      indent_increment : int\n        the number of columns to indent per nesting level\n      max_help_position : int\n        the maximum starting column for option help text\n      help_position : int\n        the calculated starting column for option help text;\n        initially the same as the maximum\n      width : int\n        total number of columns for output (pass None to constructor for\n        this value to be taken from the $COLUMNS environment variable)\n      level : int\n        current indentation level\n      current_indent : int\n        current indentation level (in columns)\n      help_width : int\n        number of columns available for option help text (calculated)\n      default_tag : str\n        text to replace with each option's default value, \"%default\"\n        by default.  Set to false value to disable default value expansion.\n      option_strings : { Option : str }\n        maps Option instances to the snippet of help text explaining\n        the syntax of that option, e.g. \"-h, --help\" or\n        \"-fFILE, --file=FILE\"\n      _short_opt_fmt : str\n        format string controlling how short options with values are\n        printed in help text.  Must be either \"%s%s\" (\"-fFILE\") or\n        \"%s %s\" (\"-f FILE\"), because those are the two syntaxes that\n        Optik supports.\n      _long_opt_fmt : str\n        similar but for long options; must be either \"%s %s\" (\"--file FILE\")\n        or \"%s=%s\" (\"--file=FILE\").\n    \"\"\"\n\n    NO_DEFAULT_VALUE = \"none\"\n\n    def __init__(self,\n                 indent_increment,\n                 max_help_position,\n                 width,\n                 short_first):\n        self.parser = None\n        self.indent_increment = indent_increment\n        if width is None:\n            try:\n                width = int(os.environ['COLUMNS'])\n            except (KeyError, ValueError):\n                width = 80\n            width -= 2\n        self.width = width\n        self.help_position = self.max_help_position = \\\n                min(max_help_position, max(width - 20, indent_increment * 2))\n        self.current_indent = 0\n        self.level = 0\n        self.help_width = None          # computed later\n        self.short_first = short_first\n        self.default_tag = \"%default\"\n        self.option_strings = {}\n        self._short_opt_fmt = \"%s %s\"\n        self._long_opt_fmt = \"%s=%s\"\n\n    def set_parser(self, parser):\n        self.parser = parser\n\n    def set_short_opt_delimiter(self, delim):\n        if delim not in (\"\", \" \"):\n            raise ValueError(\n                \"invalid metavar delimiter for short options: %r\" % delim)\n        self._short_opt_fmt = \"%s\" + delim + \"%s\"\n\n    def set_long_opt_delimiter(self, delim):\n        if delim not in (\"=\", \" \"):\n            raise ValueError(\n                \"invalid metavar delimiter for long options: %r\" % delim)\n        self._long_opt_fmt = \"%s\" + delim + \"%s\"\n\n    def indent(self):\n        self.current_indent += self.indent_increment\n        self.level += 1\n\n    def dedent(self):\n        self.current_indent -= self.indent_increment\n        assert self.current_indent >= 0, \"Indent decreased below 0.\"\n        self.level -= 1\n\n    def format_usage(self, usage):\n        raise NotImplementedError, \"subclasses must implement\"\n\n    def format_heading(self, heading):\n        raise NotImplementedError, \"subclasses must implement\"\n\n    def _format_text(self, text):\n        \"\"\"\n        Format a paragraph of free-form text for inclusion in the\n        help output at the current indentation level.\n        \"\"\"\n        text_width = max(self.width - self.current_indent, 11)\n        indent = \" \"*self.current_indent\n        return textwrap.fill(text,\n                             text_width,\n                             initial_indent=indent,\n                             subsequent_indent=indent)\n\n    def format_description(self, description):\n        if description:\n            return self._format_text(description) + \"\\n\"\n        else:\n            return \"\"\n\n    def format_epilog(self, epilog):\n        if epilog:\n            return \"\\n\" + self._format_text(epilog) + \"\\n\"\n        else:\n            return \"\"\n\n\n    def expand_default(self, option):\n        if self.parser is None or not self.default_tag:\n            return option.help\n\n        default_value = self.parser.defaults.get(option.dest)\n        if default_value is NO_DEFAULT or default_value is None:\n            default_value = self.NO_DEFAULT_VALUE\n\n        return option.help.replace(self.default_tag, str(default_value))\n\n    def format_option(self, option):\n        # The help for each option consists of two parts:\n        #   * the opt strings and metavars\n        #     eg. (\"-x\", or \"-fFILENAME, --file=FILENAME\")\n        #   * the user-supplied help string\n        #     eg. (\"turn on expert mode\", \"read data from FILENAME\")\n        #\n        # If possible, we write both of these on the same line:\n        #   -x      turn on expert mode\n        #\n        # But if the opt string list is too long, we put the help\n        # string on a second line, indented to the same column it would\n        # start in if it fit on the first line.\n        #   -fFILENAME, --file=FILENAME\n        #           read data from FILENAME\n        result = []\n        opts = self.option_strings[option]\n        opt_width = self.help_position - self.current_indent - 2\n        if len(opts) > opt_width:\n            opts = \"%*s%s\\n\" % (self.current_indent, \"\", opts)\n            indent_first = self.help_position\n        else:                       # start help on same line as opts\n            opts = \"%*s%-*s  \" % (self.current_indent, \"\", opt_width, opts)\n            indent_first = 0\n        result.append(opts)\n        if option.help:\n            help_text = self.expand_default(option)\n            help_lines = textwrap.wrap(help_text, self.help_width)\n            result.append(\"%*s%s\\n\" % (indent_first, \"\", help_lines[0]))\n            result.extend([\"%*s%s\\n\" % (self.help_position, \"\", line)\n                           for line in help_lines[1:]])\n        elif opts[-1] != \"\\n\":\n            result.append(\"\\n\")\n        return \"\".join(result)\n\n    def store_option_strings(self, parser):\n        self.indent()\n        max_len = 0\n        for opt in parser.option_list:\n            strings = self.format_option_strings(opt)\n            self.option_strings[opt] = strings\n            max_len = max(max_len, len(strings) + self.current_indent)\n        self.indent()\n        for group in parser.option_groups:\n            for opt in group.option_list:\n                strings = self.format_option_strings(opt)\n                self.option_strings[opt] = strings\n                max_len = max(max_len, len(strings) + self.current_indent)\n        self.dedent()\n        self.dedent()\n        self.help_position = min(max_len + 2, self.max_help_position)\n        self.help_width = max(self.width - self.help_position, 11)\n\n    def format_option_strings(self, option):\n        \"\"\"Return a comma-separated list of option strings & metavariables.\"\"\"\n        if option.takes_value():\n            metavar = option.metavar or option.dest.upper()\n            short_opts = [self._short_opt_fmt % (sopt, metavar)\n                          for sopt in option._short_opts]\n            long_opts = [self._long_opt_fmt % (lopt, metavar)\n                         for lopt in option._long_opts]\n        else:\n            short_opts = option._short_opts\n            long_opts = option._long_opts\n\n        if self.short_first:\n            opts = short_opts + long_opts\n        else:\n            opts = long_opts + short_opts\n\n        return \", \".join(opts)\n\nclass IndentedHelpFormatter (HelpFormatter):\n    \"\"\"Format help with indented section bodies.\n    \"\"\"\n\n    def __init__(self,\n                 indent_increment=2,\n                 max_help_position=24,\n                 width=None,\n                 short_first=1):\n        HelpFormatter.__init__(\n            self, indent_increment, max_help_position, width, short_first)\n\n    def format_usage(self, usage):\n        return _(\"Usage: %s\\n\") % usage\n\n    def format_heading(self, heading):\n        return \"%*s%s:\\n\" % (self.current_indent, \"\", heading)\n\n\nclass TitledHelpFormatter (HelpFormatter):\n    \"\"\"Format help with underlined section headers.\n    \"\"\"\n\n    def __init__(self,\n                 indent_increment=0,\n                 max_help_position=24,\n                 width=None,\n                 short_first=0):\n        HelpFormatter.__init__ (\n            self, indent_increment, max_help_position, width, short_first)\n\n    def format_usage(self, usage):\n        return \"%s  %s\\n\" % (self.format_heading(_(\"Usage\")), usage)\n\n    def format_heading(self, heading):\n        return \"%s\\n%s\\n\" % (heading, \"=-\"[self.level] * len(heading))\n\n\ndef _parse_num(val, type):\n    if val[:2].lower() == \"0x\":         # hexadecimal\n        radix = 16\n    elif val[:2].lower() == \"0b\":       # binary\n        radix = 2\n        val = val[2:] or \"0\"            # have to remove \"0b\" prefix\n    elif val[:1] == \"0\":                # octal\n        radix = 8\n    else:                               # decimal\n        radix = 10\n\n    return type(val, radix)\n\ndef _parse_int(val):\n    return _parse_num(val, int)\n\ndef _parse_long(val):\n    return _parse_num(val, long)\n\n_builtin_cvt = { \"int\" : (_parse_int, _(\"integer\")),\n                 \"long\" : (_parse_long, _(\"long integer\")),\n                 \"float\" : (float, _(\"floating-point\")),\n                 \"complex\" : (complex, _(\"complex\")) }\n\ndef check_builtin(option, opt, value):\n    (cvt, what) = _builtin_cvt[option.type]\n    try:\n        return cvt(value)\n    except ValueError:\n        raise OptionValueError(\n            _(\"option %s: invalid %s value: %r\") % (opt, what, value))\n\ndef check_choice(option, opt, value):\n    if value in option.choices:\n        return value\n    else:\n        choices = \", \".join(map(repr, option.choices))\n        raise OptionValueError(\n            _(\"option %s: invalid choice: %r (choose from %s)\")\n            % (opt, value, choices))\n\n# Not supplying a default is different from a default of None,\n# so we need an explicit \"not supplied\" value.\nNO_DEFAULT = (\"NO\", \"DEFAULT\")\n\n\nclass Option:\n    \"\"\"\n    Instance attributes:\n      _short_opts : [string]\n      _long_opts : [string]\n\n      action : string\n      type : string\n      dest : string\n      default : any\n      nargs : int\n      const : any\n      choices : [string]\n      callback : function\n      callback_args : (any*)\n      callback_kwargs : { string : any }\n      help : string\n      metavar : string\n    \"\"\"\n\n    # The list of instance attributes that may be set through\n    # keyword args to the constructor.\n    ATTRS = ['action',\n             'type',\n             'dest',\n             'default',\n             'nargs',\n             'const',\n             'choices',\n             'callback',\n             'callback_args',\n             'callback_kwargs',\n             'help',\n             'metavar']\n\n    # The set of actions allowed by option parsers.  Explicitly listed\n    # here so the constructor can validate its arguments.\n    ACTIONS = (\"store\",\n               \"store_const\",\n               \"store_true\",\n               \"store_false\",\n               \"append\",\n               \"append_const\",\n               \"count\",\n               \"callback\",\n               \"help\",\n               \"version\")\n\n    # The set of actions that involve storing a value somewhere;\n    # also listed just for constructor argument validation.  (If\n    # the action is one of these, there must be a destination.)\n    STORE_ACTIONS = (\"store\",\n                     \"store_const\",\n                     \"store_true\",\n                     \"store_false\",\n                     \"append\",\n                     \"append_const\",\n                     \"count\")\n\n    # The set of actions for which it makes sense to supply a value\n    # type, ie. which may consume an argument from the command line.\n    TYPED_ACTIONS = (\"store\",\n                     \"append\",\n                     \"callback\")\n\n    # The set of actions which *require* a value type, ie. that\n    # always consume an argument from the command line.\n    ALWAYS_TYPED_ACTIONS = (\"store\",\n                            \"append\")\n\n    # The set of actions which take a 'const' attribute.\n    CONST_ACTIONS = (\"store_const\",\n                     \"append_const\")\n\n    # The set of known types for option parsers.  Again, listed here for\n    # constructor argument validation.\n    TYPES = (\"string\", \"int\", \"long\", \"float\", \"complex\", \"choice\")\n\n    # Dictionary of argument checking functions, which convert and\n    # validate option arguments according to the option type.\n    #\n    # Signature of checking functions is:\n    #   check(option : Option, opt : string, value : string) -> any\n    # where\n    #   option is the Option instance calling the checker\n    #   opt is the actual option seen on the command-line\n    #     (eg. \"-a\", \"--file\")\n    #   value is the option argument seen on the command-line\n    #\n    # The return value should be in the appropriate Python type\n    # for option.type -- eg. an integer if option.type == \"int\".\n    #\n    # If no checker is defined for a type, arguments will be\n    # unchecked and remain strings.\n    TYPE_CHECKER = { \"int\"    : check_builtin,\n                     \"long\"   : check_builtin,\n                     \"float\"  : check_builtin,\n                     \"complex\": check_builtin,\n                     \"choice\" : check_choice,\n                   }\n\n\n    # CHECK_METHODS is a list of unbound method objects; they are called\n    # by the constructor, in order, after all attributes are\n    # initialized.  The list is created and filled in later, after all\n    # the methods are actually defined.  (I just put it here because I\n    # like to define and document all class attributes in the same\n    # place.)  Subclasses that add another _check_*() method should\n    # define their own CHECK_METHODS list that adds their check method\n    # to those from this class.\n    CHECK_METHODS = None\n\n\n    # -- Constructor/initialization methods ----------------------------\n\n    def __init__(self, *opts, **attrs):\n        # Set _short_opts, _long_opts attrs from 'opts' tuple.\n        # Have to be set now, in case no option strings are supplied.\n        self._short_opts = []\n        self._long_opts = []\n        opts = self._check_opt_strings(opts)\n        self._set_opt_strings(opts)\n\n        # Set all other attrs (action, type, etc.) from 'attrs' dict\n        self._set_attrs(attrs)\n\n        # Check all the attributes we just set.  There are lots of\n        # complicated interdependencies, but luckily they can be farmed\n        # out to the _check_*() methods listed in CHECK_METHODS -- which\n        # could be handy for subclasses!  The one thing these all share\n        # is that they raise OptionError if they discover a problem.\n        for checker in self.CHECK_METHODS:\n            checker(self)\n\n    def _check_opt_strings(self, opts):\n        # Filter out None because early versions of Optik had exactly\n        # one short option and one long option, either of which\n        # could be None.\n        opts = filter(None, opts)\n        if not opts:\n            raise TypeError(\"at least one option string must be supplied\")\n        return opts\n\n    def _set_opt_strings(self, opts):\n        for opt in opts:\n            if len(opt) < 2:\n                raise OptionError(\n                    \"invalid option string %r: \"\n                    \"must be at least two characters long\" % opt, self)\n            elif len(opt) == 2:\n                if not (opt[0] == \"-\" and opt[1] != \"-\"):\n                    raise OptionError(\n                        \"invalid short option string %r: \"\n                        \"must be of the form -x, (x any non-dash char)\" % opt,\n                        self)\n                self._short_opts.append(opt)\n            else:\n                if not (opt[0:2] == \"--\" and opt[2] != \"-\"):\n                    raise OptionError(\n                        \"invalid long option string %r: \"\n                        \"must start with --, followed by non-dash\" % opt,\n                        self)\n                self._long_opts.append(opt)\n\n    def _set_attrs(self, attrs):\n        for attr in self.ATTRS:\n            if attr in attrs:\n                setattr(self, attr, attrs[attr])\n                del attrs[attr]\n            else:\n                if attr == 'default':\n                    setattr(self, attr, NO_DEFAULT)\n                else:\n                    setattr(self, attr, None)\n        if attrs:\n            attrs = attrs.keys()\n            attrs.sort()\n            raise OptionError(\n                \"invalid keyword arguments: %s\" % \", \".join(attrs),\n                self)\n\n\n    # -- Constructor validation methods --------------------------------\n\n    def _check_action(self):\n        if self.action is None:\n            self.action = \"store\"\n        elif self.action not in self.ACTIONS:\n            raise OptionError(\"invalid action: %r\" % self.action, self)\n\n    def _check_type(self):\n        if self.type is None:\n            if self.action in self.ALWAYS_TYPED_ACTIONS:\n                if self.choices is not None:\n                    # The \"choices\" attribute implies \"choice\" type.\n                    self.type = \"choice\"\n                else:\n                    # No type given?  \"string\" is the most sensible default.\n                    self.type = \"string\"\n        else:\n            # Allow type objects or builtin type conversion functions\n            # (int, str, etc.) as an alternative to their names.  (The\n            # complicated check of __builtin__ is only necessary for\n            # Python 2.1 and earlier, and is short-circuited by the\n            # first check on modern Pythons.)\n            import __builtin__\n            if ( type(self.type) is types.TypeType or\n                 (hasattr(self.type, \"__name__\") and\n                  getattr(__builtin__, self.type.__name__, None) is self.type) ):\n                self.type = self.type.__name__\n\n            if self.type == \"str\":\n                self.type = \"string\"\n\n            if self.type not in self.TYPES:\n                raise OptionError(\"invalid option type: %r\" % self.type, self)\n            if self.action not in self.TYPED_ACTIONS:\n                raise OptionError(\n                    \"must not supply a type for action %r\" % self.action, self)\n\n    def _check_choice(self):\n        if self.type == \"choice\":\n            if self.choices is None:\n                raise OptionError(\n                    \"must supply a list of choices for type 'choice'\", self)\n            elif type(self.choices) not in (types.TupleType, types.ListType):\n                raise OptionError(\n                    \"choices must be a list of strings ('%s' supplied)\"\n                    % str(type(self.choices)).split(\"'\")[1], self)\n        elif self.choices is not None:\n            raise OptionError(\n                \"must not supply choices for type %r\" % self.type, self)\n\n    def _check_dest(self):\n        # No destination given, and we need one for this action.  The\n        # self.type check is for callbacks that take a value.\n        takes_value = (self.action in self.STORE_ACTIONS or\n                       self.type is not None)\n        if self.dest is None and takes_value:\n\n            # Glean a destination from the first long option string,\n            # or from the first short option string if no long options.\n            if self._long_opts:\n                # eg. \"--foo-bar\" -> \"foo_bar\"\n                self.dest = self._long_opts[0][2:].replace('-', '_')\n            else:\n                self.dest = self._short_opts[0][1]\n\n    def _check_const(self):\n        if self.action not in self.CONST_ACTIONS and self.const is not None:\n            raise OptionError(\n                \"'const' must not be supplied for action %r\" % self.action,\n                self)\n\n    def _check_nargs(self):\n        if self.action in self.TYPED_ACTIONS:\n            if self.nargs is None:\n                self.nargs = 1\n        elif self.nargs is not None:\n            raise OptionError(\n                \"'nargs' must not be supplied for action %r\" % self.action,\n                self)\n\n    def _check_callback(self):\n        if self.action == \"callback\":\n            if not hasattr(self.callback, '__call__'):\n                raise OptionError(\n                    \"callback not callable: %r\" % self.callback, self)\n            if (self.callback_args is not None and\n                type(self.callback_args) is not types.TupleType):\n                raise OptionError(\n                    \"callback_args, if supplied, must be a tuple: not %r\"\n                    % self.callback_args, self)\n            if (self.callback_kwargs is not None and\n                type(self.callback_kwargs) is not types.DictType):\n                raise OptionError(\n                    \"callback_kwargs, if supplied, must be a dict: not %r\"\n                    % self.callback_kwargs, self)\n        else:\n            if self.callback is not None:\n                raise OptionError(\n                    \"callback supplied (%r) for non-callback option\"\n                    % self.callback, self)\n            if self.callback_args is not None:\n                raise OptionError(\n                    \"callback_args supplied for non-callback option\", self)\n            if self.callback_kwargs is not None:\n                raise OptionError(\n                    \"callback_kwargs supplied for non-callback option\", self)\n\n\n    CHECK_METHODS = [_check_action,\n                     _check_type,\n                     _check_choice,\n                     _check_dest,\n                     _check_const,\n                     _check_nargs,\n                     _check_callback]\n\n\n    # -- Miscellaneous methods -----------------------------------------\n\n    def __str__(self):\n        return \"/\".join(self._short_opts + self._long_opts)\n\n    __repr__ = _repr\n\n    def takes_value(self):\n        return self.type is not None\n\n    def get_opt_string(self):\n        if self._long_opts:\n            return self._long_opts[0]\n        else:\n            return self._short_opts[0]\n\n\n    # -- Processing methods --------------------------------------------\n\n    def check_value(self, opt, value):\n        checker = self.TYPE_CHECKER.get(self.type)\n        if checker is None:\n            return value\n        else:\n            return checker(self, opt, value)\n\n    def convert_value(self, opt, value):\n        if value is not None:\n            if self.nargs == 1:\n                return self.check_value(opt, value)\n            else:\n                return tuple([self.check_value(opt, v) for v in value])\n\n    def process(self, opt, value, values, parser):\n\n        # First, convert the value(s) to the right type.  Howl if any\n        # value(s) are bogus.\n        value = self.convert_value(opt, value)\n\n        # And then take whatever action is expected of us.\n        # This is a separate method to make life easier for\n        # subclasses to add new actions.\n        return self.take_action(\n            self.action, self.dest, opt, value, values, parser)\n\n    def take_action(self, action, dest, opt, value, values, parser):\n        if action == \"store\":\n            setattr(values, dest, value)\n        elif action == \"store_const\":\n            setattr(values, dest, self.const)\n        elif action == \"store_true\":\n            setattr(values, dest, True)\n        elif action == \"store_false\":\n            setattr(values, dest, False)\n        elif action == \"append\":\n            values.ensure_value(dest, []).append(value)\n        elif action == \"append_const\":\n            values.ensure_value(dest, []).append(self.const)\n        elif action == \"count\":\n            setattr(values, dest, values.ensure_value(dest, 0) + 1)\n        elif action == \"callback\":\n            args = self.callback_args or ()\n            kwargs = self.callback_kwargs or {}\n            self.callback(self, opt, value, parser, *args, **kwargs)\n        elif action == \"help\":\n            parser.print_help()\n            parser.exit()\n        elif action == \"version\":\n            parser.print_version()\n            parser.exit()\n        else:\n            raise ValueError(\"unknown action %r\" % self.action)\n\n        return 1\n\n# class Option\n\n\nSUPPRESS_HELP = \"SUPPRESS\"+\"HELP\"\nSUPPRESS_USAGE = \"SUPPRESS\"+\"USAGE\"\n\ntry:\n    basestring\nexcept NameError:\n    def isbasestring(x):\n        return isinstance(x, (types.StringType, types.UnicodeType))\nelse:\n    def isbasestring(x):\n        return isinstance(x, basestring)\n\nclass Values:\n\n    def __init__(self, defaults=None):\n        if defaults:\n            for (attr, val) in defaults.items():\n                setattr(self, attr, val)\n\n    def __str__(self):\n        return str(self.__dict__)\n\n    __repr__ = _repr\n\n    def __cmp__(self, other):\n        if isinstance(other, Values):\n            return cmp(self.__dict__, other.__dict__)\n        elif isinstance(other, types.DictType):\n            return cmp(self.__dict__, other)\n        else:\n            return -1\n\n    def _update_careful(self, dict):\n        \"\"\"\n        Update the option values from an arbitrary dictionary, but only\n        use keys from dict that already have a corresponding attribute\n        in self.  Any keys in dict without a corresponding attribute\n        are silently ignored.\n        \"\"\"\n        for attr in dir(self):\n            if attr in dict:\n                dval = dict[attr]\n                if dval is not None:\n                    setattr(self, attr, dval)\n\n    def _update_loose(self, dict):\n        \"\"\"\n        Update the option values from an arbitrary dictionary,\n        using all keys from the dictionary regardless of whether\n        they have a corresponding attribute in self or not.\n        \"\"\"\n        self.__dict__.update(dict)\n\n    def _update(self, dict, mode):\n        if mode == \"careful\":\n            self._update_careful(dict)\n        elif mode == \"loose\":\n            self._update_loose(dict)\n        else:\n            raise ValueError, \"invalid update mode: %r\" % mode\n\n    def read_module(self, modname, mode=\"careful\"):\n        __import__(modname)\n        mod = sys.modules[modname]\n        self._update(vars(mod), mode)\n\n    def read_file(self, filename, mode=\"careful\"):\n        vars = {}\n        execfile(filename, vars)\n        self._update(vars, mode)\n\n    def ensure_value(self, attr, value):\n        if not hasattr(self, attr) or getattr(self, attr) is None:\n            setattr(self, attr, value)\n        return getattr(self, attr)\n\n\nclass OptionContainer:\n\n    \"\"\"\n    Abstract base class.\n\n    Class attributes:\n      standard_option_list : [Option]\n        list of standard options that will be accepted by all instances\n        of this parser class (intended to be overridden by subclasses).\n\n    Instance attributes:\n      option_list : [Option]\n        the list of Option objects contained by this OptionContainer\n      _short_opt : { string : Option }\n        dictionary mapping short option strings, eg. \"-f\" or \"-X\",\n        to the Option instances that implement them.  If an Option\n        has multiple short option strings, it will appears in this\n        dictionary multiple times. [1]\n      _long_opt : { string : Option }\n        dictionary mapping long option strings, eg. \"--file\" or\n        \"--exclude\", to the Option instances that implement them.\n        Again, a given Option can occur multiple times in this\n        dictionary. [1]\n      defaults : { string : any }\n        dictionary mapping option destination names to default\n        values for each destination [1]\n\n    [1] These mappings are common to (shared by) all components of the\n        controlling OptionParser, where they are initially created.\n\n    \"\"\"\n\n    def __init__(self, option_class, conflict_handler, description):\n        # Initialize the option list and related data structures.\n        # This method must be provided by subclasses, and it must\n        # initialize at least the following instance attributes:\n        # option_list, _short_opt, _long_opt, defaults.\n        self._create_option_list()\n\n        self.option_class = option_class\n        self.set_conflict_handler(conflict_handler)\n        self.set_description(description)\n\n    def _create_option_mappings(self):\n        # For use by OptionParser constructor -- create the master\n        # option mappings used by this OptionParser and all\n        # OptionGroups that it owns.\n        self._short_opt = {}            # single letter -> Option instance\n        self._long_opt = {}             # long option -> Option instance\n        self.defaults = {}              # maps option dest -> default value\n\n\n    def _share_option_mappings(self, parser):\n        # For use by OptionGroup constructor -- use shared option\n        # mappings from the OptionParser that owns this OptionGroup.\n        self._short_opt = parser._short_opt\n        self._long_opt = parser._long_opt\n        self.defaults = parser.defaults\n\n    def set_conflict_handler(self, handler):\n        if handler not in (\"error\", \"resolve\"):\n            raise ValueError, \"invalid conflict_resolution value %r\" % handler\n        self.conflict_handler = handler\n\n    def set_description(self, description):\n        self.description = description\n\n    def get_description(self):\n        return self.description\n\n\n    def destroy(self):\n        \"\"\"see OptionParser.destroy().\"\"\"\n        del self._short_opt\n        del self._long_opt\n        del self.defaults\n\n\n    # -- Option-adding methods -----------------------------------------\n\n    def _check_conflict(self, option):\n        conflict_opts = []\n        for opt in option._short_opts:\n            if opt in self._short_opt:\n                conflict_opts.append((opt, self._short_opt[opt]))\n        for opt in option._long_opts:\n            if opt in self._long_opt:\n                conflict_opts.append((opt, self._long_opt[opt]))\n\n        if conflict_opts:\n            handler = self.conflict_handler\n            if handler == \"error\":\n                raise OptionConflictError(\n                    \"conflicting option string(s): %s\"\n                    % \", \".join([co[0] for co in conflict_opts]),\n                    option)\n            elif handler == \"resolve\":\n                for (opt, c_option) in conflict_opts:\n                    if opt.startswith(\"--\"):\n                        c_option._long_opts.remove(opt)\n                        del self._long_opt[opt]\n                    else:\n                        c_option._short_opts.remove(opt)\n                        del self._short_opt[opt]\n                    if not (c_option._short_opts or c_option._long_opts):\n                        c_option.container.option_list.remove(c_option)\n\n    def add_option(self, *args, **kwargs):\n        \"\"\"add_option(Option)\n           add_option(opt_str, ..., kwarg=val, ...)\n        \"\"\"\n        if type(args[0]) in types.StringTypes:\n            option = self.option_class(*args, **kwargs)\n        elif len(args) == 1 and not kwargs:\n            option = args[0]\n            if not isinstance(option, Option):\n                raise TypeError, \"not an Option instance: %r\" % option\n        else:\n            raise TypeError, \"invalid arguments\"\n\n        self._check_conflict(option)\n\n        self.option_list.append(option)\n        option.container = self\n        for opt in option._short_opts:\n            self._short_opt[opt] = option\n        for opt in option._long_opts:\n            self._long_opt[opt] = option\n\n        if option.dest is not None:     # option has a dest, we need a default\n            if option.default is not NO_DEFAULT:\n                self.defaults[option.dest] = option.default\n            elif option.dest not in self.defaults:\n                self.defaults[option.dest] = None\n\n        return option\n\n    def add_options(self, option_list):\n        for option in option_list:\n            self.add_option(option)\n\n    # -- Option query/removal methods ----------------------------------\n\n    def get_option(self, opt_str):\n        return (self._short_opt.get(opt_str) or\n                self._long_opt.get(opt_str))\n\n    def has_option(self, opt_str):\n        return (opt_str in self._short_opt or\n                opt_str in self._long_opt)\n\n    def remove_option(self, opt_str):\n        option = self._short_opt.get(opt_str)\n        if option is None:\n            option = self._long_opt.get(opt_str)\n        if option is None:\n            raise ValueError(\"no such option %r\" % opt_str)\n\n        for opt in option._short_opts:\n            del self._short_opt[opt]\n        for opt in option._long_opts:\n            del self._long_opt[opt]\n        option.container.option_list.remove(option)\n\n\n    # -- Help-formatting methods ---------------------------------------\n\n    def format_option_help(self, formatter):\n        if not self.option_list:\n            return \"\"\n        result = []\n        for option in self.option_list:\n            if not option.help is SUPPRESS_HELP:\n                result.append(formatter.format_option(option))\n        return \"\".join(result)\n\n    def format_description(self, formatter):\n        return formatter.format_description(self.get_description())\n\n    def format_help(self, formatter):\n        result = []\n        if self.description:\n            result.append(self.format_description(formatter))\n        if self.option_list:\n            result.append(self.format_option_help(formatter))\n        return \"\\n\".join(result)\n\n\nclass OptionGroup (OptionContainer):\n\n    def __init__(self, parser, title, description=None):\n        self.parser = parser\n        OptionContainer.__init__(\n            self, parser.option_class, parser.conflict_handler, description)\n        self.title = title\n\n    def _create_option_list(self):\n        self.option_list = []\n        self._share_option_mappings(self.parser)\n\n    def set_title(self, title):\n        self.title = title\n\n    def destroy(self):\n        \"\"\"see OptionParser.destroy().\"\"\"\n        OptionContainer.destroy(self)\n        del self.option_list\n\n    # -- Help-formatting methods ---------------------------------------\n\n    def format_help(self, formatter):\n        result = formatter.format_heading(self.title)\n        formatter.indent()\n        result += OptionContainer.format_help(self, formatter)\n        formatter.dedent()\n        return result\n\n\nclass OptionParser (OptionContainer):\n\n    \"\"\"\n    Class attributes:\n      standard_option_list : [Option]\n        list of standard options that will be accepted by all instances\n        of this parser class (intended to be overridden by subclasses).\n\n    Instance attributes:\n      usage : string\n        a usage string for your program.  Before it is displayed\n        to the user, \"%prog\" will be expanded to the name of\n        your program (self.prog or os.path.basename(sys.argv[0])).\n      prog : string\n        the name of the current program (to override\n        os.path.basename(sys.argv[0])).\n      description : string\n        A paragraph of text giving a brief overview of your program.\n        optparse reformats this paragraph to fit the current terminal\n        width and prints it when the user requests help (after usage,\n        but before the list of options).\n      epilog : string\n        paragraph of help text to print after option help\n\n      option_groups : [OptionGroup]\n        list of option groups in this parser (option groups are\n        irrelevant for parsing the command-line, but very useful\n        for generating help)\n\n      allow_interspersed_args : bool = true\n        if true, positional arguments may be interspersed with options.\n        Assuming -a and -b each take a single argument, the command-line\n          -ablah foo bar -bboo baz\n        will be interpreted the same as\n          -ablah -bboo -- foo bar baz\n        If this flag were false, that command line would be interpreted as\n          -ablah -- foo bar -bboo baz\n        -- ie. we stop processing options as soon as we see the first\n        non-option argument.  (This is the tradition followed by\n        Python's getopt module, Perl's Getopt::Std, and other argument-\n        parsing libraries, but it is generally annoying to users.)\n\n      process_default_values : bool = true\n        if true, option default values are processed similarly to option\n        values from the command line: that is, they are passed to the\n        type-checking function for the option's type (as long as the\n        default value is a string).  (This really only matters if you\n        have defined custom types; see SF bug #955889.)  Set it to false\n        to restore the behaviour of Optik 1.4.1 and earlier.\n\n      rargs : [string]\n        the argument list currently being parsed.  Only set when\n        parse_args() is active, and continually trimmed down as\n        we consume arguments.  Mainly there for the benefit of\n        callback options.\n      largs : [string]\n        the list of leftover arguments that we have skipped while\n        parsing options.  If allow_interspersed_args is false, this\n        list is always empty.\n      values : Values\n        the set of option values currently being accumulated.  Only\n        set when parse_args() is active.  Also mainly for callbacks.\n\n    Because of the 'rargs', 'largs', and 'values' attributes,\n    OptionParser is not thread-safe.  If, for some perverse reason, you\n    need to parse command-line arguments simultaneously in different\n    threads, use different OptionParser instances.\n\n    \"\"\"\n\n    standard_option_list = []\n\n    def __init__(self,\n                 usage=None,\n                 option_list=None,\n                 option_class=Option,\n                 version=None,\n                 conflict_handler=\"error\",\n                 description=None,\n                 formatter=None,\n                 add_help_option=True,\n                 prog=None,\n                 epilog=None):\n        OptionContainer.__init__(\n            self, option_class, conflict_handler, description)\n        self.set_usage(usage)\n        self.prog = prog\n        self.version = version\n        self.allow_interspersed_args = True\n        self.process_default_values = True\n        if formatter is None:\n            formatter = IndentedHelpFormatter()\n        self.formatter = formatter\n        self.formatter.set_parser(self)\n        self.epilog = epilog\n\n        # Populate the option list; initial sources are the\n        # standard_option_list class attribute, the 'option_list'\n        # argument, and (if applicable) the _add_version_option() and\n        # _add_help_option() methods.\n        self._populate_option_list(option_list,\n                                   add_help=add_help_option)\n\n        self._init_parsing_state()\n\n\n    def destroy(self):\n        \"\"\"\n        Declare that you are done with this OptionParser.  This cleans up\n        reference cycles so the OptionParser (and all objects referenced by\n        it) can be garbage-collected promptly.  After calling destroy(), the\n        OptionParser is unusable.\n        \"\"\"\n        OptionContainer.destroy(self)\n        for group in self.option_groups:\n            group.destroy()\n        del self.option_list\n        del self.option_groups\n        del self.formatter\n\n\n    # -- Private methods -----------------------------------------------\n    # (used by our or OptionContainer's constructor)\n\n    def _create_option_list(self):\n        self.option_list = []\n        self.option_groups = []\n        self._create_option_mappings()\n\n    def _add_help_option(self):\n        self.add_option(\"-h\", \"--help\",\n                        action=\"help\",\n                        help=_(\"show this help message and exit\"))\n\n    def _add_version_option(self):\n        self.add_option(\"--version\",\n                        action=\"version\",\n                        help=_(\"show program's version number and exit\"))\n\n    def _populate_option_list(self, option_list, add_help=True):\n        if self.standard_option_list:\n            self.add_options(self.standard_option_list)\n        if option_list:\n            self.add_options(option_list)\n        if self.version:\n            self._add_version_option()\n        if add_help:\n            self._add_help_option()\n\n    def _init_parsing_state(self):\n        # These are set in parse_args() for the convenience of callbacks.\n        self.rargs = None\n        self.largs = None\n        self.values = None\n\n\n    # -- Simple modifier methods ---------------------------------------\n\n    def set_usage(self, usage):\n        if usage is None:\n            self.usage = _(\"%prog [options]\")\n        elif usage is SUPPRESS_USAGE:\n            self.usage = None\n        # For backwards compatibility with Optik 1.3 and earlier.\n        elif usage.lower().startswith(\"usage: \"):\n            self.usage = usage[7:]\n        else:\n            self.usage = usage\n\n    def enable_interspersed_args(self):\n        \"\"\"Set parsing to not stop on the first non-option, allowing\n        interspersing switches with command arguments. This is the\n        default behavior. See also disable_interspersed_args() and the\n        class documentation description of the attribute\n        allow_interspersed_args.\"\"\"\n        self.allow_interspersed_args = True\n\n    def disable_interspersed_args(self):\n        \"\"\"Set parsing to stop on the first non-option. Use this if\n        you have a command processor which runs another command that\n        has options of its own and you want to make sure these options\n        don't get confused.\n        \"\"\"\n        self.allow_interspersed_args = False\n\n    def set_process_default_values(self, process):\n        self.process_default_values = process\n\n    def set_default(self, dest, value):\n        self.defaults[dest] = value\n\n    def set_defaults(self, **kwargs):\n        self.defaults.update(kwargs)\n\n    def _get_all_options(self):\n        options = self.option_list[:]\n        for group in self.option_groups:\n            options.extend(group.option_list)\n        return options\n\n    def get_default_values(self):\n        if not self.process_default_values:\n            # Old, pre-Optik 1.5 behaviour.\n            return Values(self.defaults)\n\n        defaults = self.defaults.copy()\n        for option in self._get_all_options():\n            default = defaults.get(option.dest)\n            if isbasestring(default):\n                opt_str = option.get_opt_string()\n                defaults[option.dest] = option.check_value(opt_str, default)\n\n        return Values(defaults)\n\n\n    # -- OptionGroup methods -------------------------------------------\n\n    def add_option_group(self, *args, **kwargs):\n        # XXX lots of overlap with OptionContainer.add_option()\n        if type(args[0]) is types.StringType:\n            group = OptionGroup(self, *args, **kwargs)\n        elif len(args) == 1 and not kwargs:\n            group = args[0]\n            if not isinstance(group, OptionGroup):\n                raise TypeError, \"not an OptionGroup instance: %r\" % group\n            if group.parser is not self:\n                raise ValueError, \"invalid OptionGroup (wrong parser)\"\n        else:\n            raise TypeError, \"invalid arguments\"\n\n        self.option_groups.append(group)\n        return group\n\n    def get_option_group(self, opt_str):\n        option = (self._short_opt.get(opt_str) or\n                  self._long_opt.get(opt_str))\n        if option and option.container is not self:\n            return option.container\n        return None\n\n\n    # -- Option-parsing methods ----------------------------------------\n\n    def _get_args(self, args):\n        if args is None:\n            return sys.argv[1:]\n        else:\n            return args[:]              # don't modify caller's list\n\n    def parse_args(self, args=None, values=None):\n        \"\"\"\n        parse_args(args : [string] = sys.argv[1:],\n                   values : Values = None)\n        -> (values : Values, args : [string])\n\n        Parse the command-line options found in 'args' (default:\n        sys.argv[1:]).  Any errors result in a call to 'error()', which\n        by default prints the usage message to stderr and calls\n        sys.exit() with an error message.  On success returns a pair\n        (values, args) where 'values' is an Values instance (with all\n        your option values) and 'args' is the list of arguments left\n        over after parsing options.\n        \"\"\"\n        rargs = self._get_args(args)\n        if values is None:\n            values = self.get_default_values()\n\n        # Store the halves of the argument list as attributes for the\n        # convenience of callbacks:\n        #   rargs\n        #     the rest of the command-line (the \"r\" stands for\n        #     \"remaining\" or \"right-hand\")\n        #   largs\n        #     the leftover arguments -- ie. what's left after removing\n        #     options and their arguments (the \"l\" stands for \"leftover\"\n        #     or \"left-hand\")\n        self.rargs = rargs\n        self.largs = largs = []\n        self.values = values\n\n        try:\n            stop = self._process_args(largs, rargs, values)\n        except (BadOptionError, OptionValueError), err:\n            self.error(str(err))\n\n        args = largs + rargs\n        return self.check_values(values, args)\n\n    def check_values(self, values, args):\n        \"\"\"\n        check_values(values : Values, args : [string])\n        -> (values : Values, args : [string])\n\n        Check that the supplied option values and leftover arguments are\n        valid.  Returns the option values and leftover arguments\n        (possibly adjusted, possibly completely new -- whatever you\n        like).  Default implementation just returns the passed-in\n        values; subclasses may override as desired.\n        \"\"\"\n        return (values, args)\n\n    def _process_args(self, largs, rargs, values):\n        \"\"\"_process_args(largs : [string],\n                         rargs : [string],\n                         values : Values)\n\n        Process command-line arguments and populate 'values', consuming\n        options and arguments from 'rargs'.  If 'allow_interspersed_args' is\n        false, stop at the first non-option argument.  If true, accumulate any\n        interspersed non-option arguments in 'largs'.\n        \"\"\"\n        while rargs:\n            arg = rargs[0]\n            # We handle bare \"--\" explicitly, and bare \"-\" is handled by the\n            # standard arg handler since the short arg case ensures that the\n            # len of the opt string is greater than 1.\n            if arg == \"--\":\n                del rargs[0]\n                return\n            elif arg[0:2] == \"--\":\n                # process a single long option (possibly with value(s))\n                self._process_long_opt(rargs, values)\n            elif arg[:1] == \"-\" and len(arg) > 1:\n                # process a cluster of short options (possibly with\n                # value(s) for the last one only)\n                self._process_short_opts(rargs, values)\n            elif self.allow_interspersed_args:\n                largs.append(arg)\n                del rargs[0]\n            else:\n                return                  # stop now, leave this arg in rargs\n\n        # Say this is the original argument list:\n        # [arg0, arg1, ..., arg(i-1), arg(i), arg(i+1), ..., arg(N-1)]\n        #                            ^\n        # (we are about to process arg(i)).\n        #\n        # Then rargs is [arg(i), ..., arg(N-1)] and largs is a *subset* of\n        # [arg0, ..., arg(i-1)] (any options and their arguments will have\n        # been removed from largs).\n        #\n        # The while loop will usually consume 1 or more arguments per pass.\n        # If it consumes 1 (eg. arg is an option that takes no arguments),\n        # then after _process_arg() is done the situation is:\n        #\n        #   largs = subset of [arg0, ..., arg(i)]\n        #   rargs = [arg(i+1), ..., arg(N-1)]\n        #\n        # If allow_interspersed_args is false, largs will always be\n        # *empty* -- still a subset of [arg0, ..., arg(i-1)], but\n        # not a very interesting subset!\n\n    def _match_long_opt(self, opt):\n        \"\"\"_match_long_opt(opt : string) -> string\n\n        Determine which long option string 'opt' matches, ie. which one\n        it is an unambiguous abbreviation for.  Raises BadOptionError if\n        'opt' doesn't unambiguously match any long option string.\n        \"\"\"\n        return _match_abbrev(opt, self._long_opt)\n\n    def _process_long_opt(self, rargs, values):\n        arg = rargs.pop(0)\n\n        # Value explicitly attached to arg?  Pretend it's the next\n        # argument.\n        if \"=\" in arg:\n            (opt, next_arg) = arg.split(\"=\", 1)\n            rargs.insert(0, next_arg)\n            had_explicit_value = True\n        else:\n            opt = arg\n            had_explicit_value = False\n\n        opt = self._match_long_opt(opt)\n        option = self._long_opt[opt]\n        if option.takes_value():\n            nargs = option.nargs\n            if len(rargs) < nargs:\n                if nargs == 1:\n                    self.error(_(\"%s option requires an argument\") % opt)\n                else:\n                    self.error(_(\"%s option requires %d arguments\")\n                               % (opt, nargs))\n            elif nargs == 1:\n                value = rargs.pop(0)\n            else:\n                value = tuple(rargs[0:nargs])\n                del rargs[0:nargs]\n\n        elif had_explicit_value:\n            self.error(_(\"%s option does not take a value\") % opt)\n\n        else:\n            value = None\n\n        option.process(opt, value, values, self)\n\n    def _process_short_opts(self, rargs, values):\n        arg = rargs.pop(0)\n        stop = False\n        i = 1\n        for ch in arg[1:]:\n            opt = \"-\" + ch\n            option = self._short_opt.get(opt)\n            i += 1                      # we have consumed a character\n\n            if not option:\n                raise BadOptionError(opt)\n            if option.takes_value():\n                # Any characters left in arg?  Pretend they're the\n                # next arg, and stop consuming characters of arg.\n                if i < len(arg):\n                    rargs.insert(0, arg[i:])\n                    stop = True\n\n                nargs = option.nargs\n                if len(rargs) < nargs:\n                    if nargs == 1:\n                        self.error(_(\"%s option requires an argument\") % opt)\n                    else:\n                        self.error(_(\"%s option requires %d arguments\")\n                                   % (opt, nargs))\n                elif nargs == 1:\n                    value = rargs.pop(0)\n                else:\n                    value = tuple(rargs[0:nargs])\n                    del rargs[0:nargs]\n\n            else:                       # option doesn't take a value\n                value = None\n\n            option.process(opt, value, values, self)\n\n            if stop:\n                break\n\n\n    # -- Feedback methods ----------------------------------------------\n\n    def get_prog_name(self):\n        if self.prog is None:\n            return os.path.basename(sys.argv[0])\n        else:\n            return self.prog\n\n    def expand_prog_name(self, s):\n        return s.replace(\"%prog\", self.get_prog_name())\n\n    def get_description(self):\n        return self.expand_prog_name(self.description)\n\n    def exit(self, status=0, msg=None):\n        if msg:\n            sys.stderr.write(msg)\n        sys.exit(status)\n\n    def error(self, msg):\n        \"\"\"error(msg : string)\n\n        Print a usage message incorporating 'msg' to stderr and exit.\n        If you override this in a subclass, it should not return -- it\n        should either exit or raise an exception.\n        \"\"\"\n        self.print_usage(sys.stderr)\n        self.exit(2, \"%s: error: %s\\n\" % (self.get_prog_name(), msg))\n\n    def get_usage(self):\n        if self.usage:\n            return self.formatter.format_usage(\n                self.expand_prog_name(self.usage))\n        else:\n            return \"\"\n\n    def print_usage(self, file=None):\n        \"\"\"print_usage(file : file = stdout)\n\n        Print the usage message for the current program (self.usage) to\n        'file' (default stdout).  Any occurrence of the string \"%prog\" in\n        self.usage is replaced with the name of the current program\n        (basename of sys.argv[0]).  Does nothing if self.usage is empty\n        or not defined.\n        \"\"\"\n        if self.usage:\n            print >>file, self.get_usage()\n\n    def get_version(self):\n        if self.version:\n            return self.expand_prog_name(self.version)\n        else:\n            return \"\"\n\n    def print_version(self, file=None):\n        \"\"\"print_version(file : file = stdout)\n\n        Print the version message for this program (self.version) to\n        'file' (default stdout).  As with print_usage(), any occurrence\n        of \"%prog\" in self.version is replaced by the current program's\n        name.  Does nothing if self.version is empty or undefined.\n        \"\"\"\n        if self.version:\n            print >>file, self.get_version()\n\n    def format_option_help(self, formatter=None):\n        if formatter is None:\n            formatter = self.formatter\n        formatter.store_option_strings(self)\n        result = []\n        result.append(formatter.format_heading(_(\"Options\")))\n        formatter.indent()\n        if self.option_list:\n            result.append(OptionContainer.format_option_help(self, formatter))\n            result.append(\"\\n\")\n        for group in self.option_groups:\n            result.append(group.format_help(formatter))\n            result.append(\"\\n\")\n        formatter.dedent()\n        # Drop the last \"\\n\", or the header if no options or option groups:\n        return \"\".join(result[:-1])\n\n    def format_epilog(self, formatter):\n        return formatter.format_epilog(self.epilog)\n\n    def format_help(self, formatter=None):\n        if formatter is None:\n            formatter = self.formatter\n        result = []\n        if self.usage:\n            result.append(self.get_usage() + \"\\n\")\n        if self.description:\n            result.append(self.format_description(formatter) + \"\\n\")\n        result.append(self.format_option_help(formatter))\n        result.append(self.format_epilog(formatter))\n        return \"\".join(result)\n\n    # used by test suite\n    def _get_encoding(self, file):\n        encoding = getattr(file, \"encoding\", None)\n        if not encoding:\n            encoding = sys.getdefaultencoding()\n        return encoding\n\n    def print_help(self, file=None):\n        \"\"\"print_help(file : file = stdout)\n\n        Print an extended help message, listing all options and any\n        help text provided with them, to 'file' (default stdout).\n        \"\"\"\n        if file is None:\n            file = sys.stdout\n        encoding = self._get_encoding(file)\n        file.write(self.format_help().encode(encoding, \"replace\"))\n\n# class OptionParser\n\n\ndef _match_abbrev(s, wordmap):\n    \"\"\"_match_abbrev(s : string, wordmap : {string : Option}) -> string\n\n    Return the string key in 'wordmap' for which 's' is an unambiguous\n    abbreviation.  If 's' is found to be ambiguous or doesn't match any of\n    'words', raise BadOptionError.\n    \"\"\"\n    # Is there an exact match?\n    if s in wordmap:\n        return s\n    else:\n        # Isolate all words with s as a prefix.\n        possibilities = [word for word in wordmap.keys()\n                         if word.startswith(s)]\n        # No exact match, so there had better be just one possibility.\n        if len(possibilities) == 1:\n            return possibilities[0]\n        elif not possibilities:\n            raise BadOptionError(s)\n        else:\n            # More than one possible completion: ambiguous prefix.\n            possibilities.sort()\n            raise AmbiguousOptionError(s, possibilities)\n\n\n# Some day, there might be many Option classes.  As of Optik 1.3, the\n# preferred way to instantiate Options is indirectly, via make_option(),\n# which will become a factory function when there are many Option\n# classes.\nmake_option = Option\n", 
    "os": "r\"\"\"OS routines for NT or Posix depending on what system we're on.\n\nThis exports:\n  - all functions from posix, nt, os2, or ce, e.g. unlink, stat, etc.\n  - os.path is one of the modules posixpath, or ntpath\n  - os.name is 'posix', 'nt', 'os2', 'ce' or 'riscos'\n  - os.curdir is a string representing the current directory ('.' or ':')\n  - os.pardir is a string representing the parent directory ('..' or '::')\n  - os.sep is the (or a most common) pathname separator ('/' or ':' or '\\\\')\n  - os.extsep is the extension separator ('.' or '/')\n  - os.altsep is the alternate pathname separator (None or '/')\n  - os.pathsep is the component separator used in $PATH etc\n  - os.linesep is the line separator in text files ('\\r' or '\\n' or '\\r\\n')\n  - os.defpath is the default search path for executables\n  - os.devnull is the file path of the null device ('/dev/null', etc.)\n\nPrograms that import and use 'os' stand a better chance of being\nportable between different platforms.  Of course, they must then\nonly use functions that are defined by all platforms (e.g., unlink\nand opendir), and leave all pathname manipulation to os.path\n(e.g., split and join).\n\"\"\"\n\n#'\n\nimport sys, errno\n\n_names = sys.builtin_module_names\n\n# Note:  more names are added to __all__ later.\n__all__ = [\"altsep\", \"curdir\", \"pardir\", \"sep\", \"extsep\", \"pathsep\", \"linesep\",\n           \"defpath\", \"name\", \"path\", \"devnull\",\n           \"SEEK_SET\", \"SEEK_CUR\", \"SEEK_END\"]\n\ndef _get_exports_list(module):\n    try:\n        return list(module.__all__)\n    except AttributeError:\n        return [n for n in dir(module) if n[0] != '_']\n\nif 'posix' in _names:\n    name = 'posix'\n    linesep = '\\n'\n    from posix import *\n    try:\n        from posix import _exit\n    except ImportError:\n        pass\n    import posixpath as path\n\n    import posix\n    __all__.extend(_get_exports_list(posix))\n    del posix\n\nelif 'nt' in _names:\n    name = 'nt'\n    linesep = '\\r\\n'\n    from nt import *\n    try:\n        from nt import _exit\n    except ImportError:\n        pass\n    import ntpath as path\n\n    import nt\n    __all__.extend(_get_exports_list(nt))\n    del nt\n\nelif 'os2' in _names:\n    name = 'os2'\n    linesep = '\\r\\n'\n    from os2 import *\n    try:\n        from os2 import _exit\n    except ImportError:\n        pass\n    if sys.version.find('EMX GCC') == -1:\n        import ntpath as path\n    else:\n        import os2emxpath as path\n        from _emx_link import link\n\n    import os2\n    __all__.extend(_get_exports_list(os2))\n    del os2\n\nelif 'ce' in _names:\n    name = 'ce'\n    linesep = '\\r\\n'\n    from ce import *\n    try:\n        from ce import _exit\n    except ImportError:\n        pass\n    # We can use the standard Windows path.\n    import ntpath as path\n\n    import ce\n    __all__.extend(_get_exports_list(ce))\n    del ce\n\nelif 'riscos' in _names:\n    name = 'riscos'\n    linesep = '\\n'\n    from riscos import *\n    try:\n        from riscos import _exit\n    except ImportError:\n        pass\n    import riscospath as path\n\n    import riscos\n    __all__.extend(_get_exports_list(riscos))\n    del riscos\n\nelse:\n    raise ImportError, 'no os specific module found'\n\nsys.modules['os.path'] = path\nfrom os.path import (curdir, pardir, sep, pathsep, defpath, extsep, altsep,\n    devnull)\n\ndel _names\n\n# Python uses fixed values for the SEEK_ constants; they are mapped\n# to native constants if necessary in posixmodule.c\nSEEK_SET = 0\nSEEK_CUR = 1\nSEEK_END = 2\n\n#'\n\n# Super directory utilities.\n# (Inspired by Eric Raymond; the doc strings are mostly his)\n\ndef makedirs(name, mode=0777):\n    \"\"\"makedirs(path [, mode=0777])\n\n    Super-mkdir; create a leaf directory and all intermediate ones.\n    Works like mkdir, except that any intermediate path segment (not\n    just the rightmost) will be created if it does not exist.  This is\n    recursive.\n\n    \"\"\"\n    head, tail = path.split(name)\n    if not tail:\n        head, tail = path.split(head)\n    if head and tail and not path.exists(head):\n        try:\n            makedirs(head, mode)\n        except OSError, e:\n            # be happy if someone already created the path\n            if e.errno != errno.EEXIST:\n                raise\n        if tail == curdir:           # xxx/newdir/. exists if xxx/newdir exists\n            return\n    mkdir(name, mode)\n\ndef removedirs(name):\n    \"\"\"removedirs(path)\n\n    Super-rmdir; remove a leaf directory and all empty intermediate\n    ones.  Works like rmdir except that, if the leaf directory is\n    successfully removed, directories corresponding to rightmost path\n    segments will be pruned away until either the whole path is\n    consumed or an error occurs.  Errors during this latter phase are\n    ignored -- they generally mean that a directory was not empty.\n\n    \"\"\"\n    rmdir(name)\n    head, tail = path.split(name)\n    if not tail:\n        head, tail = path.split(head)\n    while head and tail:\n        try:\n            rmdir(head)\n        except error:\n            break\n        head, tail = path.split(head)\n\ndef renames(old, new):\n    \"\"\"renames(old, new)\n\n    Super-rename; create directories as necessary and delete any left\n    empty.  Works like rename, except creation of any intermediate\n    directories needed to make the new pathname good is attempted\n    first.  After the rename, directories corresponding to rightmost\n    path segments of the old name will be pruned way until either the\n    whole path is consumed or a nonempty directory is found.\n\n    Note: this function can fail with the new directory structure made\n    if you lack permissions needed to unlink the leaf directory or\n    file.\n\n    \"\"\"\n    head, tail = path.split(new)\n    if head and tail and not path.exists(head):\n        makedirs(head)\n    rename(old, new)\n    head, tail = path.split(old)\n    if head and tail:\n        try:\n            removedirs(head)\n        except error:\n            pass\n\n__all__.extend([\"makedirs\", \"removedirs\", \"renames\"])\n\ndef walk(top, topdown=True, onerror=None, followlinks=False):\n    \"\"\"Directory tree generator.\n\n    For each directory in the directory tree rooted at top (including top\n    itself, but excluding '.' and '..'), yields a 3-tuple\n\n        dirpath, dirnames, filenames\n\n    dirpath is a string, the path to the directory.  dirnames is a list of\n    the names of the subdirectories in dirpath (excluding '.' and '..').\n    filenames is a list of the names of the non-directory files in dirpath.\n    Note that the names in the lists are just names, with no path components.\n    To get a full path (which begins with top) to a file or directory in\n    dirpath, do os.path.join(dirpath, name).\n\n    If optional arg 'topdown' is true or not specified, the triple for a\n    directory is generated before the triples for any of its subdirectories\n    (directories are generated top down).  If topdown is false, the triple\n    for a directory is generated after the triples for all of its\n    subdirectories (directories are generated bottom up).\n\n    When topdown is true, the caller can modify the dirnames list in-place\n    (e.g., via del or slice assignment), and walk will only recurse into the\n    subdirectories whose names remain in dirnames; this can be used to prune the\n    search, or to impose a specific order of visiting.  Modifying dirnames when\n    topdown is false is ineffective, since the directories in dirnames have\n    already been generated by the time dirnames itself is generated. No matter\n    the value of topdown, the list of subdirectories is retrieved before the\n    tuples for the directory and its subdirectories are generated.\n\n    By default errors from the os.listdir() call are ignored.  If\n    optional arg 'onerror' is specified, it should be a function; it\n    will be called with one argument, an os.error instance.  It can\n    report the error to continue with the walk, or raise the exception\n    to abort the walk.  Note that the filename is available as the\n    filename attribute of the exception object.\n\n    By default, os.walk does not follow symbolic links to subdirectories on\n    systems that support them.  In order to get this functionality, set the\n    optional argument 'followlinks' to true.\n\n    Caution:  if you pass a relative pathname for top, don't change the\n    current working directory between resumptions of walk.  walk never\n    changes the current directory, and assumes that the client doesn't\n    either.\n\n    Example:\n\n    import os\n    from os.path import join, getsize\n    for root, dirs, files in os.walk('python/Lib/email'):\n        print root, \"consumes\",\n        print sum([getsize(join(root, name)) for name in files]),\n        print \"bytes in\", len(files), \"non-directory files\"\n        if 'CVS' in dirs:\n            dirs.remove('CVS')  # don't visit CVS directories\n\n    \"\"\"\n\n    islink, join, isdir = path.islink, path.join, path.isdir\n\n    # We may not have read permission for top, in which case we can't\n    # get a list of the files the directory contains.  os.path.walk\n    # always suppressed the exception then, rather than blow up for a\n    # minor reason when (say) a thousand readable directories are still\n    # left to visit.  That logic is copied here.\n    try:\n        # Note that listdir and error are globals in this module due\n        # to earlier import-*.\n        names = listdir(top)\n    except error, err:\n        if onerror is not None:\n            onerror(err)\n        return\n\n    dirs, nondirs = [], []\n    for name in names:\n        if isdir(join(top, name)):\n            dirs.append(name)\n        else:\n            nondirs.append(name)\n\n    if topdown:\n        yield top, dirs, nondirs\n    for name in dirs:\n        new_path = join(top, name)\n        if followlinks or not islink(new_path):\n            for x in walk(new_path, topdown, onerror, followlinks):\n                yield x\n    if not topdown:\n        yield top, dirs, nondirs\n\n__all__.append(\"walk\")\n\n# Make sure os.environ exists, at least\ntry:\n    environ\nexcept NameError:\n    environ = {}\n\ndef execl(file, *args):\n    \"\"\"execl(file, *args)\n\n    Execute the executable file with argument list args, replacing the\n    current process. \"\"\"\n    execv(file, args)\n\ndef execle(file, *args):\n    \"\"\"execle(file, *args, env)\n\n    Execute the executable file with argument list args and\n    environment env, replacing the current process. \"\"\"\n    env = args[-1]\n    execve(file, args[:-1], env)\n\ndef execlp(file, *args):\n    \"\"\"execlp(file, *args)\n\n    Execute the executable file (which is searched for along $PATH)\n    with argument list args, replacing the current process. \"\"\"\n    execvp(file, args)\n\ndef execlpe(file, *args):\n    \"\"\"execlpe(file, *args, env)\n\n    Execute the executable file (which is searched for along $PATH)\n    with argument list args and environment env, replacing the current\n    process. \"\"\"\n    env = args[-1]\n    execvpe(file, args[:-1], env)\n\ndef execvp(file, args):\n    \"\"\"execvp(file, args)\n\n    Execute the executable file (which is searched for along $PATH)\n    with argument list args, replacing the current process.\n    args may be a list or tuple of strings. \"\"\"\n    _execvpe(file, args)\n\ndef execvpe(file, args, env):\n    \"\"\"execvpe(file, args, env)\n\n    Execute the executable file (which is searched for along $PATH)\n    with argument list args and environment env , replacing the\n    current process.\n    args may be a list or tuple of strings. \"\"\"\n    _execvpe(file, args, env)\n\n__all__.extend([\"execl\",\"execle\",\"execlp\",\"execlpe\",\"execvp\",\"execvpe\"])\n\ndef _execvpe(file, args, env=None):\n    if env is not None:\n        func = execve\n        argrest = (args, env)\n    else:\n        func = execv\n        argrest = (args,)\n        env = environ\n\n    head, tail = path.split(file)\n    if head:\n        func(file, *argrest)\n        return\n    if 'PATH' in env:\n        envpath = env['PATH']\n    else:\n        envpath = defpath\n    PATH = envpath.split(pathsep)\n    saved_exc = None\n    saved_tb = None\n    for dir in PATH:\n        fullname = path.join(dir, file)\n        try:\n            func(fullname, *argrest)\n        except error, e:\n            tb = sys.exc_info()[2]\n            if (e.errno != errno.ENOENT and e.errno != errno.ENOTDIR\n                and saved_exc is None):\n                saved_exc = e\n                saved_tb = tb\n    if saved_exc:\n        raise error, saved_exc, saved_tb\n    raise error, e, tb\n\n# Change environ to automatically call putenv() if it exists\ntry:\n    # This will fail if there's no putenv\n    putenv\nexcept NameError:\n    pass\nelse:\n    import UserDict\n\n    # Fake unsetenv() for Windows\n    # not sure about os2 here but\n    # I'm guessing they are the same.\n\n    if name in ('os2', 'nt'):\n        def unsetenv(key):\n            putenv(key, \"\")\n\n    if name == \"riscos\":\n        # On RISC OS, all env access goes through getenv and putenv\n        from riscosenviron import _Environ\n    elif name in ('os2', 'nt'):  # Where Env Var Names Must Be UPPERCASE\n        # But we store them as upper case\n        class _Environ(UserDict.IterableUserDict):\n            def __init__(self, environ):\n                UserDict.UserDict.__init__(self)\n                data = self.data\n                for k, v in environ.items():\n                    data[k.upper()] = v\n            def __setitem__(self, key, item):\n                putenv(key, item)\n                self.data[key.upper()] = item\n            def __getitem__(self, key):\n                return self.data[key.upper()]\n            try:\n                unsetenv\n            except NameError:\n                def __delitem__(self, key):\n                    del self.data[key.upper()]\n            else:\n                def __delitem__(self, key):\n                    unsetenv(key)\n                    del self.data[key.upper()]\n                def clear(self):\n                    for key in self.data.keys():\n                        unsetenv(key)\n                        del self.data[key]\n                def pop(self, key, *args):\n                    unsetenv(key)\n                    return self.data.pop(key.upper(), *args)\n            def has_key(self, key):\n                return key.upper() in self.data\n            def __contains__(self, key):\n                return key.upper() in self.data\n            def get(self, key, failobj=None):\n                return self.data.get(key.upper(), failobj)\n            def update(self, dict=None, **kwargs):\n                if dict:\n                    try:\n                        keys = dict.keys()\n                    except AttributeError:\n                        # List of (key, value)\n                        for k, v in dict:\n                            self[k] = v\n                    else:\n                        # got keys\n                        # cannot use items(), since mappings\n                        # may not have them.\n                        for k in keys:\n                            self[k] = dict[k]\n                if kwargs:\n                    self.update(kwargs)\n            def copy(self):\n                return dict(self)\n\n    else:  # Where Env Var Names Can Be Mixed Case\n        class _Environ(UserDict.IterableUserDict):\n            def __init__(self, environ):\n                UserDict.UserDict.__init__(self)\n                self.data = environ\n            def __setitem__(self, key, item):\n                putenv(key, item)\n                self.data[key] = item\n            def update(self,  dict=None, **kwargs):\n                if dict:\n                    try:\n                        keys = dict.keys()\n                    except AttributeError:\n                        # List of (key, value)\n                        for k, v in dict:\n                            self[k] = v\n                    else:\n                        # got keys\n                        # cannot use items(), since mappings\n                        # may not have them.\n                        for k in keys:\n                            self[k] = dict[k]\n                if kwargs:\n                    self.update(kwargs)\n            try:\n                unsetenv\n            except NameError:\n                pass\n            else:\n                def __delitem__(self, key):\n                    unsetenv(key)\n                    del self.data[key]\n                def clear(self):\n                    for key in self.data.keys():\n                        unsetenv(key)\n                        del self.data[key]\n                def pop(self, key, *args):\n                    unsetenv(key)\n                    return self.data.pop(key, *args)\n            def copy(self):\n                return dict(self)\n\n\n    environ = _Environ(environ)\n\ndef getenv(key, default=None):\n    \"\"\"Get an environment variable, return None if it doesn't exist.\n    The optional second argument can specify an alternate default.\"\"\"\n    return environ.get(key, default)\n__all__.append(\"getenv\")\n\ndef _exists(name):\n    return name in globals()\n\n# Supply spawn*() (probably only for Unix)\nif _exists(\"fork\") and not _exists(\"spawnv\") and _exists(\"execv\"):\n\n    P_WAIT = 0\n    P_NOWAIT = P_NOWAITO = 1\n\n    # XXX Should we support P_DETACH?  I suppose it could fork()**2\n    # and close the std I/O streams.  Also, P_OVERLAY is the same\n    # as execv*()?\n\n    def _spawnvef(mode, file, args, env, func):\n        # Internal helper; func is the exec*() function to use\n        pid = fork()\n        if not pid:\n            # Child\n            try:\n                if env is None:\n                    func(file, args)\n                else:\n                    func(file, args, env)\n            except:\n                _exit(127)\n        else:\n            # Parent\n            if mode == P_NOWAIT:\n                return pid # Caller is responsible for waiting!\n            while 1:\n                wpid, sts = waitpid(pid, 0)\n                if WIFSTOPPED(sts):\n                    continue\n                elif WIFSIGNALED(sts):\n                    return -WTERMSIG(sts)\n                elif WIFEXITED(sts):\n                    return WEXITSTATUS(sts)\n                else:\n                    raise error, \"Not stopped, signaled or exited???\"\n\n    def spawnv(mode, file, args):\n        \"\"\"spawnv(mode, file, args) -> integer\n\nExecute file with arguments from args in a subprocess.\nIf mode == P_NOWAIT return the pid of the process.\nIf mode == P_WAIT return the process's exit code if it exits normally;\notherwise return -SIG, where SIG is the signal that killed it. \"\"\"\n        return _spawnvef(mode, file, args, None, execv)\n\n    def spawnve(mode, file, args, env):\n        \"\"\"spawnve(mode, file, args, env) -> integer\n\nExecute file with arguments from args in a subprocess with the\nspecified environment.\nIf mode == P_NOWAIT return the pid of the process.\nIf mode == P_WAIT return the process's exit code if it exits normally;\notherwise return -SIG, where SIG is the signal that killed it. \"\"\"\n        return _spawnvef(mode, file, args, env, execve)\n\n    # Note: spawnvp[e] is't currently supported on Windows\n\n    def spawnvp(mode, file, args):\n        \"\"\"spawnvp(mode, file, args) -> integer\n\nExecute file (which is looked for along $PATH) with arguments from\nargs in a subprocess.\nIf mode == P_NOWAIT return the pid of the process.\nIf mode == P_WAIT return the process's exit code if it exits normally;\notherwise return -SIG, where SIG is the signal that killed it. \"\"\"\n        return _spawnvef(mode, file, args, None, execvp)\n\n    def spawnvpe(mode, file, args, env):\n        \"\"\"spawnvpe(mode, file, args, env) -> integer\n\nExecute file (which is looked for along $PATH) with arguments from\nargs in a subprocess with the supplied environment.\nIf mode == P_NOWAIT return the pid of the process.\nIf mode == P_WAIT return the process's exit code if it exits normally;\notherwise return -SIG, where SIG is the signal that killed it. \"\"\"\n        return _spawnvef(mode, file, args, env, execvpe)\n\nif _exists(\"spawnv\"):\n    # These aren't supplied by the basic Windows code\n    # but can be easily implemented in Python\n\n    def spawnl(mode, file, *args):\n        \"\"\"spawnl(mode, file, *args) -> integer\n\nExecute file with arguments from args in a subprocess.\nIf mode == P_NOWAIT return the pid of the process.\nIf mode == P_WAIT return the process's exit code if it exits normally;\notherwise return -SIG, where SIG is the signal that killed it. \"\"\"\n        return spawnv(mode, file, args)\n\n    def spawnle(mode, file, *args):\n        \"\"\"spawnle(mode, file, *args, env) -> integer\n\nExecute file with arguments from args in a subprocess with the\nsupplied environment.\nIf mode == P_NOWAIT return the pid of the process.\nIf mode == P_WAIT return the process's exit code if it exits normally;\notherwise return -SIG, where SIG is the signal that killed it. \"\"\"\n        env = args[-1]\n        return spawnve(mode, file, args[:-1], env)\n\n\n    __all__.extend([\"spawnv\", \"spawnve\", \"spawnl\", \"spawnle\",])\n\n\nif _exists(\"spawnvp\"):\n    # At the moment, Windows doesn't implement spawnvp[e],\n    # so it won't have spawnlp[e] either.\n    def spawnlp(mode, file, *args):\n        \"\"\"spawnlp(mode, file, *args) -> integer\n\nExecute file (which is looked for along $PATH) with arguments from\nargs in a subprocess with the supplied environment.\nIf mode == P_NOWAIT return the pid of the process.\nIf mode == P_WAIT return the process's exit code if it exits normally;\notherwise return -SIG, where SIG is the signal that killed it. \"\"\"\n        return spawnvp(mode, file, args)\n\n    def spawnlpe(mode, file, *args):\n        \"\"\"spawnlpe(mode, file, *args, env) -> integer\n\nExecute file (which is looked for along $PATH) with arguments from\nargs in a subprocess with the supplied environment.\nIf mode == P_NOWAIT return the pid of the process.\nIf mode == P_WAIT return the process's exit code if it exits normally;\notherwise return -SIG, where SIG is the signal that killed it. \"\"\"\n        env = args[-1]\n        return spawnvpe(mode, file, args[:-1], env)\n\n\n    __all__.extend([\"spawnvp\", \"spawnvpe\", \"spawnlp\", \"spawnlpe\",])\n\n\n# Supply popen2 etc. (for Unix)\nif _exists(\"fork\"):\n    if not _exists(\"popen2\"):\n        def popen2(cmd, mode=\"t\", bufsize=-1):\n            \"\"\"Execute the shell command 'cmd' in a sub-process.  On UNIX, 'cmd'\n            may be a sequence, in which case arguments will be passed directly to\n            the program without shell intervention (as with os.spawnv()).  If 'cmd'\n            is a string it will be passed to the shell (as with os.system()). If\n            'bufsize' is specified, it sets the buffer size for the I/O pipes.  The\n            file objects (child_stdin, child_stdout) are returned.\"\"\"\n            import warnings\n            msg = \"os.popen2 is deprecated.  Use the subprocess module.\"\n            warnings.warn(msg, DeprecationWarning, stacklevel=2)\n\n            import subprocess\n            PIPE = subprocess.PIPE\n            p = subprocess.Popen(cmd, shell=isinstance(cmd, basestring),\n                                 bufsize=bufsize, stdin=PIPE, stdout=PIPE,\n                                 close_fds=True)\n            return p.stdin, p.stdout\n        __all__.append(\"popen2\")\n\n    if not _exists(\"popen3\"):\n        def popen3(cmd, mode=\"t\", bufsize=-1):\n            \"\"\"Execute the shell command 'cmd' in a sub-process.  On UNIX, 'cmd'\n            may be a sequence, in which case arguments will be passed directly to\n            the program without shell intervention (as with os.spawnv()).  If 'cmd'\n            is a string it will be passed to the shell (as with os.system()). If\n            'bufsize' is specified, it sets the buffer size for the I/O pipes.  The\n            file objects (child_stdin, child_stdout, child_stderr) are returned.\"\"\"\n            import warnings\n            msg = \"os.popen3 is deprecated.  Use the subprocess module.\"\n            warnings.warn(msg, DeprecationWarning, stacklevel=2)\n\n            import subprocess\n            PIPE = subprocess.PIPE\n            p = subprocess.Popen(cmd, shell=isinstance(cmd, basestring),\n                                 bufsize=bufsize, stdin=PIPE, stdout=PIPE,\n                                 stderr=PIPE, close_fds=True)\n            return p.stdin, p.stdout, p.stderr\n        __all__.append(\"popen3\")\n\n    if not _exists(\"popen4\"):\n        def popen4(cmd, mode=\"t\", bufsize=-1):\n            \"\"\"Execute the shell command 'cmd' in a sub-process.  On UNIX, 'cmd'\n            may be a sequence, in which case arguments will be passed directly to\n            the program without shell intervention (as with os.spawnv()).  If 'cmd'\n            is a string it will be passed to the shell (as with os.system()). If\n            'bufsize' is specified, it sets the buffer size for the I/O pipes.  The\n            file objects (child_stdin, child_stdout_stderr) are returned.\"\"\"\n            import warnings\n            msg = \"os.popen4 is deprecated.  Use the subprocess module.\"\n            warnings.warn(msg, DeprecationWarning, stacklevel=2)\n\n            import subprocess\n            PIPE = subprocess.PIPE\n            p = subprocess.Popen(cmd, shell=isinstance(cmd, basestring),\n                                 bufsize=bufsize, stdin=PIPE, stdout=PIPE,\n                                 stderr=subprocess.STDOUT, close_fds=True)\n            return p.stdin, p.stdout\n        __all__.append(\"popen4\")\n\nimport copy_reg as _copy_reg\n\ndef _make_stat_result(tup, dict):\n    return stat_result(tup, dict)\n\ndef _pickle_stat_result(sr):\n    (type, args) = sr.__reduce__()\n    return (_make_stat_result, args)\n\ntry:\n    _copy_reg.pickle(stat_result, _pickle_stat_result, _make_stat_result)\nexcept NameError: # stat_result may not exist\n    pass\n\ndef _make_statvfs_result(tup, dict):\n    return statvfs_result(tup, dict)\n\ndef _pickle_statvfs_result(sr):\n    (type, args) = sr.__reduce__()\n    return (_make_statvfs_result, args)\n\ntry:\n    _copy_reg.pickle(statvfs_result, _pickle_statvfs_result,\n                     _make_statvfs_result)\nexcept NameError: # statvfs_result may not exist\n    pass\n", 
    "pdb": "#! /usr/bin/env python\n\n\"\"\"A Python debugger.\"\"\"\n\n# (See pdb.doc for documentation.)\n\nimport sys\nimport linecache\nimport cmd\nimport bdb\nfrom repr import Repr\nimport os\nimport re\nimport pprint\nimport traceback\n\n\nclass Restart(Exception):\n    \"\"\"Causes a debugger to be restarted for the debugged python program.\"\"\"\n    pass\n\n# Create a custom safe Repr instance and increase its maxstring.\n# The default of 30 truncates error messages too easily.\n_repr = Repr()\n_repr.maxstring = 200\n_saferepr = _repr.repr\n\n__all__ = [\"run\", \"pm\", \"Pdb\", \"runeval\", \"runctx\", \"runcall\", \"set_trace\",\n           \"post_mortem\", \"help\"]\n\ndef find_function(funcname, filename):\n    cre = re.compile(r'def\\s+%s\\s*[(]' % re.escape(funcname))\n    try:\n        fp = open(filename)\n    except IOError:\n        return None\n    # consumer of this info expects the first line to be 1\n    lineno = 1\n    answer = None\n    while 1:\n        line = fp.readline()\n        if line == '':\n            break\n        if cre.match(line):\n            answer = funcname, filename, lineno\n            break\n        lineno = lineno + 1\n    fp.close()\n    return answer\n\n\n# Interaction prompt line will separate file and call info from code\n# text using value of line_prefix string.  A newline and arrow may\n# be to your liking.  You can set it once pdb is imported using the\n# command \"pdb.line_prefix = '\\n% '\".\n# line_prefix = ': '    # Use this to get the old situation back\nline_prefix = '\\n-> '   # Probably a better default\n\nclass Pdb(bdb.Bdb, cmd.Cmd):\n\n    def __init__(self, completekey='tab', stdin=None, stdout=None, skip=None):\n        bdb.Bdb.__init__(self, skip=skip)\n        cmd.Cmd.__init__(self, completekey, stdin, stdout)\n        if stdout:\n            self.use_rawinput = 0\n        self.prompt = '(Pdb) '\n        self.aliases = {}\n        self.mainpyfile = ''\n        self._wait_for_mainpyfile = 0\n        # Try to load readline if it exists\n        try:\n            import readline\n        except ImportError:\n            pass\n\n        # Read $HOME/.pdbrc and ./.pdbrc\n        self.rcLines = []\n        if 'HOME' in os.environ:\n            envHome = os.environ['HOME']\n            try:\n                rcFile = open(os.path.join(envHome, \".pdbrc\"))\n            except IOError:\n                pass\n            else:\n                for line in rcFile.readlines():\n                    self.rcLines.append(line)\n                rcFile.close()\n        try:\n            rcFile = open(\".pdbrc\")\n        except IOError:\n            pass\n        else:\n            for line in rcFile.readlines():\n                self.rcLines.append(line)\n            rcFile.close()\n\n        self.commands = {} # associates a command list to breakpoint numbers\n        self.commands_doprompt = {} # for each bp num, tells if the prompt\n                                    # must be disp. after execing the cmd list\n        self.commands_silent = {} # for each bp num, tells if the stack trace\n                                  # must be disp. after execing the cmd list\n        self.commands_defining = False # True while in the process of defining\n                                       # a command list\n        self.commands_bnum = None # The breakpoint number for which we are\n                                  # defining a list\n\n    def reset(self):\n        bdb.Bdb.reset(self)\n        self.forget()\n\n    def forget(self):\n        self.lineno = None\n        self.stack = []\n        self.curindex = 0\n        self.curframe = None\n\n    def setup(self, f, t):\n        self.forget()\n        self.stack, self.curindex = self.get_stack(f, t)\n        self.curframe = self.stack[self.curindex][0]\n        # The f_locals dictionary is updated from the actual frame\n        # locals whenever the .f_locals accessor is called, so we\n        # cache it here to ensure that modifications are not overwritten.\n        self.curframe_locals = self.curframe.f_locals\n        self.execRcLines()\n\n    # Can be executed earlier than 'setup' if desired\n    def execRcLines(self):\n        if self.rcLines:\n            # Make local copy because of recursion\n            rcLines = self.rcLines\n            # executed only once\n            self.rcLines = []\n            for line in rcLines:\n                line = line[:-1]\n                if len(line) > 0 and line[0] != '#':\n                    self.onecmd(line)\n\n    # Override Bdb methods\n\n    def user_call(self, frame, argument_list):\n        \"\"\"This method is called when there is the remote possibility\n        that we ever need to stop in this function.\"\"\"\n        if self._wait_for_mainpyfile:\n            return\n        if self.stop_here(frame):\n            print >>self.stdout, '--Call--'\n            self.interaction(frame, None)\n\n    def user_line(self, frame):\n        \"\"\"This function is called when we stop or break at this line.\"\"\"\n        if self._wait_for_mainpyfile:\n            if (self.mainpyfile != self.canonic(frame.f_code.co_filename)\n                or frame.f_lineno<= 0):\n                return\n            self._wait_for_mainpyfile = 0\n        if self.bp_commands(frame):\n            self.interaction(frame, None)\n\n    def bp_commands(self,frame):\n        \"\"\"Call every command that was set for the current active breakpoint\n        (if there is one).\n\n        Returns True if the normal interaction function must be called,\n        False otherwise.\"\"\"\n        # self.currentbp is set in bdb in Bdb.break_here if a breakpoint was hit\n        if getattr(self, \"currentbp\", False) and \\\n               self.currentbp in self.commands:\n            currentbp = self.currentbp\n            self.currentbp = 0\n            lastcmd_back = self.lastcmd\n            self.setup(frame, None)\n            for line in self.commands[currentbp]:\n                self.onecmd(line)\n            self.lastcmd = lastcmd_back\n            if not self.commands_silent[currentbp]:\n                self.print_stack_entry(self.stack[self.curindex])\n            if self.commands_doprompt[currentbp]:\n                self.cmdloop()\n            self.forget()\n            return\n        return 1\n\n    def user_return(self, frame, return_value):\n        \"\"\"This function is called when a return trap is set here.\"\"\"\n        if self._wait_for_mainpyfile:\n            return\n        frame.f_locals['__return__'] = return_value\n        print >>self.stdout, '--Return--'\n        self.interaction(frame, None)\n\n    def user_exception(self, frame, exc_info):\n        \"\"\"This function is called if an exception occurs,\n        but only if we are to stop at or just below this level.\"\"\"\n        if self._wait_for_mainpyfile:\n            return\n        exc_type, exc_value, exc_traceback = exc_info\n        frame.f_locals['__exception__'] = exc_type, exc_value\n        if type(exc_type) == type(''):\n            exc_type_name = exc_type\n        else: exc_type_name = exc_type.__name__\n        print >>self.stdout, exc_type_name + ':', _saferepr(exc_value)\n        self.interaction(frame, exc_traceback)\n\n    # General interaction function\n\n    def interaction(self, frame, traceback):\n        self.setup(frame, traceback)\n        self.print_stack_entry(self.stack[self.curindex])\n        self.cmdloop()\n        self.forget()\n\n    def displayhook(self, obj):\n        \"\"\"Custom displayhook for the exec in default(), which prevents\n        assignment of the _ variable in the builtins.\n        \"\"\"\n        # reproduce the behavior of the standard displayhook, not printing None\n        if obj is not None:\n            print repr(obj)\n\n    def default(self, line):\n        if line[:1] == '!': line = line[1:]\n        locals = self.curframe_locals\n        globals = self.curframe.f_globals\n        try:\n            code = compile(line + '\\n', '<stdin>', 'single')\n            save_stdout = sys.stdout\n            save_stdin = sys.stdin\n            save_displayhook = sys.displayhook\n            try:\n                sys.stdin = self.stdin\n                sys.stdout = self.stdout\n                sys.displayhook = self.displayhook\n                exec code in globals, locals\n            finally:\n                sys.stdout = save_stdout\n                sys.stdin = save_stdin\n                sys.displayhook = save_displayhook\n        except:\n            t, v = sys.exc_info()[:2]\n            if type(t) == type(''):\n                exc_type_name = t\n            else: exc_type_name = t.__name__\n            print >>self.stdout, '***', exc_type_name + ':', v\n\n    def precmd(self, line):\n        \"\"\"Handle alias expansion and ';;' separator.\"\"\"\n        if not line.strip():\n            return line\n        args = line.split()\n        while args[0] in self.aliases:\n            line = self.aliases[args[0]]\n            ii = 1\n            for tmpArg in args[1:]:\n                line = line.replace(\"%\" + str(ii),\n                                      tmpArg)\n                ii = ii + 1\n            line = line.replace(\"%*\", ' '.join(args[1:]))\n            args = line.split()\n        # split into ';;' separated commands\n        # unless it's an alias command\n        if args[0] != 'alias':\n            marker = line.find(';;')\n            if marker >= 0:\n                # queue up everything after marker\n                next = line[marker+2:].lstrip()\n                self.cmdqueue.append(next)\n                line = line[:marker].rstrip()\n        return line\n\n    def onecmd(self, line):\n        \"\"\"Interpret the argument as though it had been typed in response\n        to the prompt.\n\n        Checks whether this line is typed at the normal prompt or in\n        a breakpoint command list definition.\n        \"\"\"\n        if not self.commands_defining:\n            return cmd.Cmd.onecmd(self, line)\n        else:\n            return self.handle_command_def(line)\n\n    def handle_command_def(self,line):\n        \"\"\"Handles one command line during command list definition.\"\"\"\n        cmd, arg, line = self.parseline(line)\n        if not cmd:\n            return\n        if cmd == 'silent':\n            self.commands_silent[self.commands_bnum] = True\n            return # continue to handle other cmd def in the cmd list\n        elif cmd == 'end':\n            self.cmdqueue = []\n            return 1 # end of cmd list\n        cmdlist = self.commands[self.commands_bnum]\n        if arg:\n            cmdlist.append(cmd+' '+arg)\n        else:\n            cmdlist.append(cmd)\n        # Determine if we must stop\n        try:\n            func = getattr(self, 'do_' + cmd)\n        except AttributeError:\n            func = self.default\n        # one of the resuming commands\n        if func.func_name in self.commands_resuming:\n            self.commands_doprompt[self.commands_bnum] = False\n            self.cmdqueue = []\n            return 1\n        return\n\n    # Command definitions, called by cmdloop()\n    # The argument is the remaining string on the command line\n    # Return true to exit from the command loop\n\n    do_h = cmd.Cmd.do_help\n\n    def do_commands(self, arg):\n        \"\"\"Defines a list of commands associated to a breakpoint.\n\n        Those commands will be executed whenever the breakpoint causes\n        the program to stop execution.\"\"\"\n        if not arg:\n            bnum = len(bdb.Breakpoint.bpbynumber)-1\n        else:\n            try:\n                bnum = int(arg)\n            except:\n                print >>self.stdout, \"Usage : commands [bnum]\\n        ...\" \\\n                                     \"\\n        end\"\n                return\n        self.commands_bnum = bnum\n        self.commands[bnum] = []\n        self.commands_doprompt[bnum] = True\n        self.commands_silent[bnum] = False\n        prompt_back = self.prompt\n        self.prompt = '(com) '\n        self.commands_defining = True\n        try:\n            self.cmdloop()\n        finally:\n            self.commands_defining = False\n            self.prompt = prompt_back\n\n    def do_break(self, arg, temporary = 0):\n        # break [ ([filename:]lineno | function) [, \"condition\"] ]\n        if not arg:\n            if self.breaks:  # There's at least one\n                print >>self.stdout, \"Num Type         Disp Enb   Where\"\n                for bp in bdb.Breakpoint.bpbynumber:\n                    if bp:\n                        bp.bpprint(self.stdout)\n            return\n        # parse arguments; comma has lowest precedence\n        # and cannot occur in filename\n        filename = None\n        lineno = None\n        cond = None\n        comma = arg.find(',')\n        if comma > 0:\n            # parse stuff after comma: \"condition\"\n            cond = arg[comma+1:].lstrip()\n            arg = arg[:comma].rstrip()\n        # parse stuff before comma: [filename:]lineno | function\n        colon = arg.rfind(':')\n        funcname = None\n        if colon >= 0:\n            filename = arg[:colon].rstrip()\n            f = self.lookupmodule(filename)\n            if not f:\n                print >>self.stdout, '*** ', repr(filename),\n                print >>self.stdout, 'not found from sys.path'\n                return\n            else:\n                filename = f\n            arg = arg[colon+1:].lstrip()\n            try:\n                lineno = int(arg)\n            except ValueError, msg:\n                print >>self.stdout, '*** Bad lineno:', arg\n                return\n        else:\n            # no colon; can be lineno or function\n            try:\n                lineno = int(arg)\n            except ValueError:\n                try:\n                    func = eval(arg,\n                                self.curframe.f_globals,\n                                self.curframe_locals)\n                except:\n                    func = arg\n                try:\n                    if hasattr(func, 'im_func'):\n                        func = func.im_func\n                    code = func.func_code\n                    #use co_name to identify the bkpt (function names\n                    #could be aliased, but co_name is invariant)\n                    funcname = code.co_name\n                    lineno = code.co_firstlineno\n                    filename = code.co_filename\n                except:\n                    # last thing to try\n                    (ok, filename, ln) = self.lineinfo(arg)\n                    if not ok:\n                        print >>self.stdout, '*** The specified object',\n                        print >>self.stdout, repr(arg),\n                        print >>self.stdout, 'is not a function'\n                        print >>self.stdout, 'or was not found along sys.path.'\n                        return\n                    funcname = ok # ok contains a function name\n                    lineno = int(ln)\n        if not filename:\n            filename = self.defaultFile()\n        # Check for reasonable breakpoint\n        line = self.checkline(filename, lineno)\n        if line:\n            # now set the break point\n            err = self.set_break(filename, line, temporary, cond, funcname)\n            if err: print >>self.stdout, '***', err\n            else:\n                bp = self.get_breaks(filename, line)[-1]\n                print >>self.stdout, \"Breakpoint %d at %s:%d\" % (bp.number,\n                                                                 bp.file,\n                                                                 bp.line)\n\n    # To be overridden in derived debuggers\n    def defaultFile(self):\n        \"\"\"Produce a reasonable default.\"\"\"\n        filename = self.curframe.f_code.co_filename\n        if filename == '<string>' and self.mainpyfile:\n            filename = self.mainpyfile\n        return filename\n\n    do_b = do_break\n\n    def do_tbreak(self, arg):\n        self.do_break(arg, 1)\n\n    def lineinfo(self, identifier):\n        failed = (None, None, None)\n        # Input is identifier, may be in single quotes\n        idstring = identifier.split(\"'\")\n        if len(idstring) == 1:\n            # not in single quotes\n            id = idstring[0].strip()\n        elif len(idstring) == 3:\n            # quoted\n            id = idstring[1].strip()\n        else:\n            return failed\n        if id == '': return failed\n        parts = id.split('.')\n        # Protection for derived debuggers\n        if parts[0] == 'self':\n            del parts[0]\n            if len(parts) == 0:\n                return failed\n        # Best first guess at file to look at\n        fname = self.defaultFile()\n        if len(parts) == 1:\n            item = parts[0]\n        else:\n            # More than one part.\n            # First is module, second is method/class\n            f = self.lookupmodule(parts[0])\n            if f:\n                fname = f\n            item = parts[1]\n        answer = find_function(item, fname)\n        return answer or failed\n\n    def checkline(self, filename, lineno):\n        \"\"\"Check whether specified line seems to be executable.\n\n        Return `lineno` if it is, 0 if not (e.g. a docstring, comment, blank\n        line or EOF). Warning: testing is not comprehensive.\n        \"\"\"\n        # this method should be callable before starting debugging, so default\n        # to \"no globals\" if there is no current frame\n        globs = self.curframe.f_globals if hasattr(self, 'curframe') else None\n        line = linecache.getline(filename, lineno, globs)\n        if not line:\n            print >>self.stdout, 'End of file'\n            return 0\n        line = line.strip()\n        # Don't allow setting breakpoint at a blank line\n        if (not line or (line[0] == '#') or\n             (line[:3] == '\"\"\"') or line[:3] == \"'''\"):\n            print >>self.stdout, '*** Blank or comment'\n            return 0\n        return lineno\n\n    def do_enable(self, arg):\n        args = arg.split()\n        for i in args:\n            try:\n                i = int(i)\n            except ValueError:\n                print >>self.stdout, 'Breakpoint index %r is not a number' % i\n                continue\n\n            if not (0 <= i < len(bdb.Breakpoint.bpbynumber)):\n                print >>self.stdout, 'No breakpoint numbered', i\n                continue\n\n            bp = bdb.Breakpoint.bpbynumber[i]\n            if bp:\n                bp.enable()\n\n    def do_disable(self, arg):\n        args = arg.split()\n        for i in args:\n            try:\n                i = int(i)\n            except ValueError:\n                print >>self.stdout, 'Breakpoint index %r is not a number' % i\n                continue\n\n            if not (0 <= i < len(bdb.Breakpoint.bpbynumber)):\n                print >>self.stdout, 'No breakpoint numbered', i\n                continue\n\n            bp = bdb.Breakpoint.bpbynumber[i]\n            if bp:\n                bp.disable()\n\n    def do_condition(self, arg):\n        # arg is breakpoint number and condition\n        args = arg.split(' ', 1)\n        try:\n            bpnum = int(args[0].strip())\n        except ValueError:\n            # something went wrong\n            print >>self.stdout, \\\n                'Breakpoint index %r is not a number' % args[0]\n            return\n        try:\n            cond = args[1]\n        except:\n            cond = None\n        try:\n            bp = bdb.Breakpoint.bpbynumber[bpnum]\n        except IndexError:\n            print >>self.stdout, 'Breakpoint index %r is not valid' % args[0]\n            return\n        if bp:\n            bp.cond = cond\n            if not cond:\n                print >>self.stdout, 'Breakpoint', bpnum,\n                print >>self.stdout, 'is now unconditional.'\n\n    def do_ignore(self,arg):\n        \"\"\"arg is bp number followed by ignore count.\"\"\"\n        args = arg.split()\n        try:\n            bpnum = int(args[0].strip())\n        except ValueError:\n            # something went wrong\n            print >>self.stdout, \\\n                'Breakpoint index %r is not a number' % args[0]\n            return\n        try:\n            count = int(args[1].strip())\n        except:\n            count = 0\n        try:\n            bp = bdb.Breakpoint.bpbynumber[bpnum]\n        except IndexError:\n            print >>self.stdout, 'Breakpoint index %r is not valid' % args[0]\n            return\n        if bp:\n            bp.ignore = count\n            if count > 0:\n                reply = 'Will ignore next '\n                if count > 1:\n                    reply = reply + '%d crossings' % count\n                else:\n                    reply = reply + '1 crossing'\n                print >>self.stdout, reply + ' of breakpoint %d.' % bpnum\n            else:\n                print >>self.stdout, 'Will stop next time breakpoint',\n                print >>self.stdout, bpnum, 'is reached.'\n\n    def do_clear(self, arg):\n        \"\"\"Three possibilities, tried in this order:\n        clear -> clear all breaks, ask for confirmation\n        clear file:lineno -> clear all breaks at file:lineno\n        clear bpno bpno ... -> clear breakpoints by number\"\"\"\n        if not arg:\n            try:\n                reply = raw_input('Clear all breaks? ')\n            except EOFError:\n                reply = 'no'\n            reply = reply.strip().lower()\n            if reply in ('y', 'yes'):\n                self.clear_all_breaks()\n            return\n        if ':' in arg:\n            # Make sure it works for \"clear C:\\foo\\bar.py:12\"\n            i = arg.rfind(':')\n            filename = arg[:i]\n            arg = arg[i+1:]\n            try:\n                lineno = int(arg)\n            except ValueError:\n                err = \"Invalid line number (%s)\" % arg\n            else:\n                err = self.clear_break(filename, lineno)\n            if err: print >>self.stdout, '***', err\n            return\n        numberlist = arg.split()\n        for i in numberlist:\n            try:\n                i = int(i)\n            except ValueError:\n                print >>self.stdout, 'Breakpoint index %r is not a number' % i\n                continue\n\n            if not (0 <= i < len(bdb.Breakpoint.bpbynumber)):\n                print >>self.stdout, 'No breakpoint numbered', i\n                continue\n            err = self.clear_bpbynumber(i)\n            if err:\n                print >>self.stdout, '***', err\n            else:\n                print >>self.stdout, 'Deleted breakpoint', i\n    do_cl = do_clear # 'c' is already an abbreviation for 'continue'\n\n    def do_where(self, arg):\n        self.print_stack_trace()\n    do_w = do_where\n    do_bt = do_where\n\n    def do_up(self, arg):\n        if self.curindex == 0:\n            print >>self.stdout, '*** Oldest frame'\n        else:\n            self.curindex = self.curindex - 1\n            self.curframe = self.stack[self.curindex][0]\n            self.curframe_locals = self.curframe.f_locals\n            self.print_stack_entry(self.stack[self.curindex])\n            self.lineno = None\n    do_u = do_up\n\n    def do_down(self, arg):\n        if self.curindex + 1 == len(self.stack):\n            print >>self.stdout, '*** Newest frame'\n        else:\n            self.curindex = self.curindex + 1\n            self.curframe = self.stack[self.curindex][0]\n            self.curframe_locals = self.curframe.f_locals\n            self.print_stack_entry(self.stack[self.curindex])\n            self.lineno = None\n    do_d = do_down\n\n    def do_until(self, arg):\n        self.set_until(self.curframe)\n        return 1\n    do_unt = do_until\n\n    def do_step(self, arg):\n        self.set_step()\n        return 1\n    do_s = do_step\n\n    def do_next(self, arg):\n        self.set_next(self.curframe)\n        return 1\n    do_n = do_next\n\n    def do_run(self, arg):\n        \"\"\"Restart program by raising an exception to be caught in the main\n        debugger loop.  If arguments were given, set them in sys.argv.\"\"\"\n        if arg:\n            import shlex\n            argv0 = sys.argv[0:1]\n            sys.argv = shlex.split(arg)\n            sys.argv[:0] = argv0\n        raise Restart\n\n    do_restart = do_run\n\n    def do_return(self, arg):\n        self.set_return(self.curframe)\n        return 1\n    do_r = do_return\n\n    def do_continue(self, arg):\n        self.set_continue()\n        return 1\n    do_c = do_cont = do_continue\n\n    def do_jump(self, arg):\n        if self.curindex + 1 != len(self.stack):\n            print >>self.stdout, \"*** You can only jump within the bottom frame\"\n            return\n        try:\n            arg = int(arg)\n        except ValueError:\n            print >>self.stdout, \"*** The 'jump' command requires a line number.\"\n        else:\n            try:\n                # Do the jump, fix up our copy of the stack, and display the\n                # new position\n                self.curframe.f_lineno = arg\n                self.stack[self.curindex] = self.stack[self.curindex][0], arg\n                self.print_stack_entry(self.stack[self.curindex])\n            except ValueError, e:\n                print >>self.stdout, '*** Jump failed:', e\n    do_j = do_jump\n\n    def do_debug(self, arg):\n        sys.settrace(None)\n        globals = self.curframe.f_globals\n        locals = self.curframe_locals\n        p = Pdb(self.completekey, self.stdin, self.stdout)\n        p.prompt = \"(%s) \" % self.prompt.strip()\n        print >>self.stdout, \"ENTERING RECURSIVE DEBUGGER\"\n        sys.call_tracing(p.run, (arg, globals, locals))\n        print >>self.stdout, \"LEAVING RECURSIVE DEBUGGER\"\n        sys.settrace(self.trace_dispatch)\n        self.lastcmd = p.lastcmd\n\n    def do_quit(self, arg):\n        self._user_requested_quit = 1\n        self.set_quit()\n        return 1\n\n    do_q = do_quit\n    do_exit = do_quit\n\n    def do_EOF(self, arg):\n        print >>self.stdout\n        self._user_requested_quit = 1\n        self.set_quit()\n        return 1\n\n    def do_args(self, arg):\n        co = self.curframe.f_code\n        dict = self.curframe_locals\n        n = co.co_argcount\n        if co.co_flags & 4: n = n+1\n        if co.co_flags & 8: n = n+1\n        for i in range(n):\n            name = co.co_varnames[i]\n            print >>self.stdout, name, '=',\n            if name in dict: print >>self.stdout, dict[name]\n            else: print >>self.stdout, \"*** undefined ***\"\n    do_a = do_args\n\n    def do_retval(self, arg):\n        if '__return__' in self.curframe_locals:\n            print >>self.stdout, self.curframe_locals['__return__']\n        else:\n            print >>self.stdout, '*** Not yet returned!'\n    do_rv = do_retval\n\n    def _getval(self, arg):\n        try:\n            return eval(arg, self.curframe.f_globals,\n                        self.curframe_locals)\n        except:\n            t, v = sys.exc_info()[:2]\n            if isinstance(t, str):\n                exc_type_name = t\n            else: exc_type_name = t.__name__\n            print >>self.stdout, '***', exc_type_name + ':', repr(v)\n            raise\n\n    def do_p(self, arg):\n        try:\n            print >>self.stdout, repr(self._getval(arg))\n        except:\n            pass\n\n    def do_pp(self, arg):\n        try:\n            pprint.pprint(self._getval(arg), self.stdout)\n        except:\n            pass\n\n    def do_list(self, arg):\n        self.lastcmd = 'list'\n        last = None\n        if arg:\n            try:\n                x = eval(arg, {}, {})\n                if type(x) == type(()):\n                    first, last = x\n                    first = int(first)\n                    last = int(last)\n                    if last < first:\n                        # Assume it's a count\n                        last = first + last\n                else:\n                    first = max(1, int(x) - 5)\n            except:\n                print >>self.stdout, '*** Error in argument:', repr(arg)\n                return\n        elif self.lineno is None:\n            first = max(1, self.curframe.f_lineno - 5)\n        else:\n            first = self.lineno + 1\n        if last is None:\n            last = first + 10\n        filename = self.curframe.f_code.co_filename\n        breaklist = self.get_file_breaks(filename)\n        try:\n            for lineno in range(first, last+1):\n                line = linecache.getline(filename, lineno,\n                                         self.curframe.f_globals)\n                if not line:\n                    print >>self.stdout, '[EOF]'\n                    break\n                else:\n                    s = repr(lineno).rjust(3)\n                    if len(s) < 4: s = s + ' '\n                    if lineno in breaklist: s = s + 'B'\n                    else: s = s + ' '\n                    if lineno == self.curframe.f_lineno:\n                        s = s + '->'\n                    print >>self.stdout, s + '\\t' + line,\n                    self.lineno = lineno\n        except KeyboardInterrupt:\n            pass\n    do_l = do_list\n\n    def do_whatis(self, arg):\n        try:\n            value = eval(arg, self.curframe.f_globals,\n                            self.curframe_locals)\n        except:\n            t, v = sys.exc_info()[:2]\n            if type(t) == type(''):\n                exc_type_name = t\n            else: exc_type_name = t.__name__\n            print >>self.stdout, '***', exc_type_name + ':', repr(v)\n            return\n        code = None\n        # Is it a function?\n        try: code = value.func_code\n        except: pass\n        if code:\n            print >>self.stdout, 'Function', code.co_name\n            return\n        # Is it an instance method?\n        try: code = value.im_func.func_code\n        except: pass\n        if code:\n            print >>self.stdout, 'Method', code.co_name\n            return\n        # None of the above...\n        print >>self.stdout, type(value)\n\n    def do_alias(self, arg):\n        args = arg.split()\n        if len(args) == 0:\n            keys = self.aliases.keys()\n            keys.sort()\n            for alias in keys:\n                print >>self.stdout, \"%s = %s\" % (alias, self.aliases[alias])\n            return\n        if args[0] in self.aliases and len(args) == 1:\n            print >>self.stdout, \"%s = %s\" % (args[0], self.aliases[args[0]])\n        else:\n            self.aliases[args[0]] = ' '.join(args[1:])\n\n    def do_unalias(self, arg):\n        args = arg.split()\n        if len(args) == 0: return\n        if args[0] in self.aliases:\n            del self.aliases[args[0]]\n\n    #list of all the commands making the program resume execution.\n    commands_resuming = ['do_continue', 'do_step', 'do_next', 'do_return',\n                         'do_quit', 'do_jump']\n\n    # Print a traceback starting at the top stack frame.\n    # The most recently entered frame is printed last;\n    # this is different from dbx and gdb, but consistent with\n    # the Python interpreter's stack trace.\n    # It is also consistent with the up/down commands (which are\n    # compatible with dbx and gdb: up moves towards 'main()'\n    # and down moves towards the most recent stack frame).\n\n    def print_stack_trace(self):\n        try:\n            for frame_lineno in self.stack:\n                self.print_stack_entry(frame_lineno)\n        except KeyboardInterrupt:\n            pass\n\n    def print_stack_entry(self, frame_lineno, prompt_prefix=line_prefix):\n        frame, lineno = frame_lineno\n        if frame is self.curframe:\n            print >>self.stdout, '>',\n        else:\n            print >>self.stdout, ' ',\n        print >>self.stdout, self.format_stack_entry(frame_lineno,\n                                                     prompt_prefix)\n\n\n    # Help methods (derived from pdb.doc)\n\n    def help_help(self):\n        self.help_h()\n\n    def help_h(self):\n        print >>self.stdout, \"\"\"h(elp)\nWithout argument, print the list of available commands.\nWith a command name as argument, print help about that command\n\"help pdb\" pipes the full documentation file to the $PAGER\n\"help exec\" gives help on the ! command\"\"\"\n\n    def help_where(self):\n        self.help_w()\n\n    def help_w(self):\n        print >>self.stdout, \"\"\"w(here)\nPrint a stack trace, with the most recent frame at the bottom.\nAn arrow indicates the \"current frame\", which determines the\ncontext of most commands.  'bt' is an alias for this command.\"\"\"\n\n    help_bt = help_w\n\n    def help_down(self):\n        self.help_d()\n\n    def help_d(self):\n        print >>self.stdout, \"\"\"d(own)\nMove the current frame one level down in the stack trace\n(to a newer frame).\"\"\"\n\n    def help_up(self):\n        self.help_u()\n\n    def help_u(self):\n        print >>self.stdout, \"\"\"u(p)\nMove the current frame one level up in the stack trace\n(to an older frame).\"\"\"\n\n    def help_break(self):\n        self.help_b()\n\n    def help_b(self):\n        print >>self.stdout, \"\"\"b(reak) ([file:]lineno | function) [, condition]\nWith a line number argument, set a break there in the current\nfile.  With a function name, set a break at first executable line\nof that function.  Without argument, list all breaks.  If a second\nargument is present, it is a string specifying an expression\nwhich must evaluate to true before the breakpoint is honored.\n\nThe line number may be prefixed with a filename and a colon,\nto specify a breakpoint in another file (probably one that\nhasn't been loaded yet).  The file is searched for on sys.path;\nthe .py suffix may be omitted.\"\"\"\n\n    def help_clear(self):\n        self.help_cl()\n\n    def help_cl(self):\n        print >>self.stdout, \"cl(ear) filename:lineno\"\n        print >>self.stdout, \"\"\"cl(ear) [bpnumber [bpnumber...]]\nWith a space separated list of breakpoint numbers, clear\nthose breakpoints.  Without argument, clear all breaks (but\nfirst ask confirmation).  With a filename:lineno argument,\nclear all breaks at that line in that file.\n\nNote that the argument is different from previous versions of\nthe debugger (in python distributions 1.5.1 and before) where\na linenumber was used instead of either filename:lineno or\nbreakpoint numbers.\"\"\"\n\n    def help_tbreak(self):\n        print >>self.stdout, \"\"\"tbreak  same arguments as break, but breakpoint\nis removed when first hit.\"\"\"\n\n    def help_enable(self):\n        print >>self.stdout, \"\"\"enable bpnumber [bpnumber ...]\nEnables the breakpoints given as a space separated list of\nbp numbers.\"\"\"\n\n    def help_disable(self):\n        print >>self.stdout, \"\"\"disable bpnumber [bpnumber ...]\nDisables the breakpoints given as a space separated list of\nbp numbers.\"\"\"\n\n    def help_ignore(self):\n        print >>self.stdout, \"\"\"ignore bpnumber count\nSets the ignore count for the given breakpoint number.  A breakpoint\nbecomes active when the ignore count is zero.  When non-zero, the\ncount is decremented each time the breakpoint is reached and the\nbreakpoint is not disabled and any associated condition evaluates\nto true.\"\"\"\n\n    def help_condition(self):\n        print >>self.stdout, \"\"\"condition bpnumber str_condition\nstr_condition is a string specifying an expression which\nmust evaluate to true before the breakpoint is honored.\nIf str_condition is absent, any existing condition is removed;\ni.e., the breakpoint is made unconditional.\"\"\"\n\n    def help_step(self):\n        self.help_s()\n\n    def help_s(self):\n        print >>self.stdout, \"\"\"s(tep)\nExecute the current line, stop at the first possible occasion\n(either in a function that is called or in the current function).\"\"\"\n\n    def help_until(self):\n        self.help_unt()\n\n    def help_unt(self):\n        print \"\"\"unt(il)\nContinue execution until the line with a number greater than the current\none is reached or until the current frame returns\"\"\"\n\n    def help_next(self):\n        self.help_n()\n\n    def help_n(self):\n        print >>self.stdout, \"\"\"n(ext)\nContinue execution until the next line in the current function\nis reached or it returns.\"\"\"\n\n    def help_return(self):\n        self.help_r()\n\n    def help_r(self):\n        print >>self.stdout, \"\"\"r(eturn)\nContinue execution until the current function returns.\"\"\"\n\n    def help_continue(self):\n        self.help_c()\n\n    def help_cont(self):\n        self.help_c()\n\n    def help_c(self):\n        print >>self.stdout, \"\"\"c(ont(inue))\nContinue execution, only stop when a breakpoint is encountered.\"\"\"\n\n    def help_jump(self):\n        self.help_j()\n\n    def help_j(self):\n        print >>self.stdout, \"\"\"j(ump) lineno\nSet the next line that will be executed.\"\"\"\n\n    def help_debug(self):\n        print >>self.stdout, \"\"\"debug code\nEnter a recursive debugger that steps through the code argument\n(which is an arbitrary expression or statement to be executed\nin the current environment).\"\"\"\n\n    def help_list(self):\n        self.help_l()\n\n    def help_l(self):\n        print >>self.stdout, \"\"\"l(ist) [first [,last]]\nList source code for the current file.\nWithout arguments, list 11 lines around the current line\nor continue the previous listing.\nWith one argument, list 11 lines starting at that line.\nWith two arguments, list the given range;\nif the second argument is less than the first, it is a count.\"\"\"\n\n    def help_args(self):\n        self.help_a()\n\n    def help_a(self):\n        print >>self.stdout, \"\"\"a(rgs)\nPrint the arguments of the current function.\"\"\"\n\n    def help_p(self):\n        print >>self.stdout, \"\"\"p expression\nPrint the value of the expression.\"\"\"\n\n    def help_pp(self):\n        print >>self.stdout, \"\"\"pp expression\nPretty-print the value of the expression.\"\"\"\n\n    def help_exec(self):\n        print >>self.stdout, \"\"\"(!) statement\nExecute the (one-line) statement in the context of\nthe current stack frame.\nThe exclamation point can be omitted unless the first word\nof the statement resembles a debugger command.\nTo assign to a global variable you must always prefix the\ncommand with a 'global' command, e.g.:\n(Pdb) global list_options; list_options = ['-l']\n(Pdb)\"\"\"\n\n    def help_run(self):\n        print \"\"\"run [args...]\nRestart the debugged python program. If a string is supplied, it is\nsplit with \"shlex\" and the result is used as the new sys.argv.\nHistory, breakpoints, actions and debugger options are preserved.\n\"restart\" is an alias for \"run\".\"\"\"\n\n    help_restart = help_run\n\n    def help_quit(self):\n        self.help_q()\n\n    def help_q(self):\n        print >>self.stdout, \"\"\"q(uit) or exit - Quit from the debugger.\nThe program being executed is aborted.\"\"\"\n\n    help_exit = help_q\n\n    def help_whatis(self):\n        print >>self.stdout, \"\"\"whatis arg\nPrints the type of the argument.\"\"\"\n\n    def help_EOF(self):\n        print >>self.stdout, \"\"\"EOF\nHandles the receipt of EOF as a command.\"\"\"\n\n    def help_alias(self):\n        print >>self.stdout, \"\"\"alias [name [command [parameter parameter ...]]]\nCreates an alias called 'name' the executes 'command'.  The command\nmust *not* be enclosed in quotes.  Replaceable parameters are\nindicated by %1, %2, and so on, while %* is replaced by all the\nparameters.  If no command is given, the current alias for name\nis shown. If no name is given, all aliases are listed.\n\nAliases may be nested and can contain anything that can be\nlegally typed at the pdb prompt.  Note!  You *can* override\ninternal pdb commands with aliases!  Those internal commands\nare then hidden until the alias is removed.  Aliasing is recursively\napplied to the first word of the command line; all other words\nin the line are left alone.\n\nSome useful aliases (especially when placed in the .pdbrc file) are:\n\n#Print instance variables (usage \"pi classInst\")\nalias pi for k in %1.__dict__.keys(): print \"%1.\",k,\"=\",%1.__dict__[k]\n\n#Print instance variables in self\nalias ps pi self\n\"\"\"\n\n    def help_unalias(self):\n        print >>self.stdout, \"\"\"unalias name\nDeletes the specified alias.\"\"\"\n\n    def help_commands(self):\n        print >>self.stdout, \"\"\"commands [bpnumber]\n(com) ...\n(com) end\n(Pdb)\n\nSpecify a list of commands for breakpoint number bpnumber.  The\ncommands themselves appear on the following lines.  Type a line\ncontaining just 'end' to terminate the commands.\n\nTo remove all commands from a breakpoint, type commands and\nfollow it immediately with  end; that is, give no commands.\n\nWith no bpnumber argument, commands refers to the last\nbreakpoint set.\n\nYou can use breakpoint commands to start your program up again.\nSimply use the continue command, or step, or any other\ncommand that resumes execution.\n\nSpecifying any command resuming execution (currently continue,\nstep, next, return, jump, quit and their abbreviations) terminates\nthe command list (as if that command was immediately followed by end).\nThis is because any time you resume execution\n(even with a simple next or step), you may encounter\nanother breakpoint--which could have its own command list, leading to\nambiguities about which list to execute.\n\n   If you use the 'silent' command in the command list, the\nusual message about stopping at a breakpoint is not printed.  This may\nbe desirable for breakpoints that are to print a specific message and\nthen continue.  If none of the other commands print anything, you\nsee no sign that the breakpoint was reached.\n\"\"\"\n\n    def help_pdb(self):\n        help()\n\n    def lookupmodule(self, filename):\n        \"\"\"Helper function for break/clear parsing -- may be overridden.\n\n        lookupmodule() translates (possibly incomplete) file or module name\n        into an absolute file name.\n        \"\"\"\n        if os.path.isabs(filename) and  os.path.exists(filename):\n            return filename\n        f = os.path.join(sys.path[0], filename)\n        if  os.path.exists(f) and self.canonic(f) == self.mainpyfile:\n            return f\n        root, ext = os.path.splitext(filename)\n        if ext == '':\n            filename = filename + '.py'\n        if os.path.isabs(filename):\n            return filename\n        for dirname in sys.path:\n            while os.path.islink(dirname):\n                dirname = os.readlink(dirname)\n            fullname = os.path.join(dirname, filename)\n            if os.path.exists(fullname):\n                return fullname\n        return None\n\n    def _runscript(self, filename):\n        # The script has to run in __main__ namespace (or imports from\n        # __main__ will break).\n        #\n        # So we clear up the __main__ and set several special variables\n        # (this gets rid of pdb's globals and cleans old variables on restarts).\n        import __main__\n        __main__.__dict__.clear()\n        __main__.__dict__.update({\"__name__\"    : \"__main__\",\n                                  \"__file__\"    : filename,\n                                  \"__builtins__\": __builtins__,\n                                 })\n\n        # When bdb sets tracing, a number of call and line events happens\n        # BEFORE debugger even reaches user's code (and the exact sequence of\n        # events depends on python version). So we take special measures to\n        # avoid stopping before we reach the main script (see user_line and\n        # user_call for details).\n        self._wait_for_mainpyfile = 1\n        self.mainpyfile = self.canonic(filename)\n        self._user_requested_quit = 0\n        statement = 'execfile(%r)' % filename\n        self.run(statement)\n\n# Simplified interface\n\ndef run(statement, globals=None, locals=None):\n    Pdb().run(statement, globals, locals)\n\ndef runeval(expression, globals=None, locals=None):\n    return Pdb().runeval(expression, globals, locals)\n\ndef runctx(statement, globals, locals):\n    # B/W compatibility\n    run(statement, globals, locals)\n\ndef runcall(*args, **kwds):\n    return Pdb().runcall(*args, **kwds)\n\ndef set_trace():\n    Pdb().set_trace(sys._getframe().f_back)\n\n# Post-Mortem interface\n\ndef post_mortem(t=None):\n    # handling the default\n    if t is None:\n        # sys.exc_info() returns (type, value, traceback) if an exception is\n        # being handled, otherwise it returns None\n        t = sys.exc_info()[2]\n        if t is None:\n            raise ValueError(\"A valid traceback must be passed if no \"\n                                               \"exception is being handled\")\n\n    p = Pdb()\n    p.reset()\n    p.interaction(None, t)\n\ndef pm():\n    post_mortem(sys.last_traceback)\n\n\n# Main program for testing\n\nTESTCMD = 'import x; x.main()'\n\ndef test():\n    run(TESTCMD)\n\n# print help\ndef help():\n    for dirname in sys.path:\n        fullname = os.path.join(dirname, 'pdb.doc')\n        if os.path.exists(fullname):\n            sts = os.system('${PAGER-more} '+fullname)\n            if sts: print '*** Pager exit status:', sts\n            break\n    else:\n        print 'Sorry, can\\'t find the help file \"pdb.doc\"',\n        print 'along the Python search path'\n\ndef main():\n    if not sys.argv[1:] or sys.argv[1] in (\"--help\", \"-h\"):\n        print \"usage: pdb.py scriptfile [arg] ...\"\n        sys.exit(2)\n\n    mainpyfile =  sys.argv[1]     # Get script filename\n    if not os.path.exists(mainpyfile):\n        print 'Error:', mainpyfile, 'does not exist'\n        sys.exit(1)\n\n    del sys.argv[0]         # Hide \"pdb.py\" from argument list\n\n    # Replace pdb's dir with script's dir in front of module search path.\n    sys.path[0] = os.path.dirname(mainpyfile)\n\n    # Note on saving/restoring sys.argv: it's a good idea when sys.argv was\n    # modified by the script being debugged. It's a bad idea when it was\n    # changed by the user from the command line. There is a \"restart\" command\n    # which allows explicit specification of command line arguments.\n    pdb = Pdb()\n    while True:\n        try:\n            pdb._runscript(mainpyfile)\n            if pdb._user_requested_quit:\n                break\n            print \"The program finished and will be restarted\"\n        except Restart:\n            print \"Restarting\", mainpyfile, \"with arguments:\"\n            print \"\\t\" + \" \".join(sys.argv[1:])\n        except SystemExit:\n            # In most cases SystemExit does not warrant a post-mortem session.\n            print \"The program exited via sys.exit(). Exit status: \",\n            print sys.exc_info()[1]\n        except:\n            traceback.print_exc()\n            print \"Uncaught exception. Entering post mortem debugging\"\n            print \"Running 'cont' or 'step' will restart the program\"\n            t = sys.exc_info()[2]\n            pdb.interaction(None, t)\n            print \"Post mortem debugger finished. The \" + mainpyfile + \\\n                  \" will be restarted\"\n\n\n# When invoked as main program, invoke the debugger on a script\nif __name__ == '__main__':\n    import pdb\n    pdb.main()\n", 
    "pickle": "\"\"\"Create portable serialized representations of Python objects.\n\nSee module cPickle for a (much) faster implementation.\nSee module copy_reg for a mechanism for registering custom picklers.\nSee module pickletools source for extensive comments.\n\nClasses:\n\n    Pickler\n    Unpickler\n\nFunctions:\n\n    dump(object, file)\n    dumps(object) -> string\n    load(file) -> object\n    loads(string) -> object\n\nMisc variables:\n\n    __version__\n    format_version\n    compatible_formats\n\n\"\"\"\n\n__version__ = \"$Revision: 72223 $\"       # Code version\n\nfrom types import *\nfrom copy_reg import dispatch_table\nfrom copy_reg import _extension_registry, _inverted_registry, _extension_cache\nimport marshal\nimport sys\nimport struct\nimport re\n\n__all__ = [\"PickleError\", \"PicklingError\", \"UnpicklingError\", \"Pickler\",\n           \"Unpickler\", \"dump\", \"dumps\", \"load\", \"loads\"]\n\n# These are purely informational; no code uses these.\nformat_version = \"2.0\"                  # File format version we write\ncompatible_formats = [\"1.0\",            # Original protocol 0\n                      \"1.1\",            # Protocol 0 with INST added\n                      \"1.2\",            # Original protocol 1\n                      \"1.3\",            # Protocol 1 with BINFLOAT added\n                      \"2.0\",            # Protocol 2\n                      ]                 # Old format versions we can read\n\n# Keep in synch with cPickle.  This is the highest protocol number we\n# know how to read.\nHIGHEST_PROTOCOL = 2\n\n# Why use struct.pack() for pickling but marshal.loads() for\n# unpickling?  struct.pack() is 40% faster than marshal.dumps(), but\n# marshal.loads() is twice as fast as struct.unpack()!\nmloads = marshal.loads\n\nclass PickleError(Exception):\n    \"\"\"A common base class for the other pickling exceptions.\"\"\"\n    pass\n\nclass PicklingError(PickleError):\n    \"\"\"This exception is raised when an unpicklable object is passed to the\n    dump() method.\n\n    \"\"\"\n    pass\n\nclass UnpicklingError(PickleError):\n    \"\"\"This exception is raised when there is a problem unpickling an object,\n    such as a security violation.\n\n    Note that other exceptions may also be raised during unpickling, including\n    (but not necessarily limited to) AttributeError, EOFError, ImportError,\n    and IndexError.\n\n    \"\"\"\n    pass\n\n# An instance of _Stop is raised by Unpickler.load_stop() in response to\n# the STOP opcode, passing the object that is the result of unpickling.\nclass _Stop(Exception):\n    def __init__(self, value):\n        self.value = value\n\n# Jython has PyStringMap; it's a dict subclass with string keys\ntry:\n    from org.python.core import PyStringMap\nexcept ImportError:\n    PyStringMap = None\n\n# UnicodeType may or may not be exported (normally imported from types)\ntry:\n    UnicodeType\nexcept NameError:\n    UnicodeType = None\n\n# Pickle opcodes.  See pickletools.py for extensive docs.  The listing\n# here is in kind-of alphabetical order of 1-character pickle code.\n# pickletools groups them by purpose.\n\nMARK            = '('   # push special markobject on stack\nSTOP            = '.'   # every pickle ends with STOP\nPOP             = '0'   # discard topmost stack item\nPOP_MARK        = '1'   # discard stack top through topmost markobject\nDUP             = '2'   # duplicate top stack item\nFLOAT           = 'F'   # push float object; decimal string argument\nINT             = 'I'   # push integer or bool; decimal string argument\nBININT          = 'J'   # push four-byte signed int\nBININT1         = 'K'   # push 1-byte unsigned int\nLONG            = 'L'   # push long; decimal string argument\nBININT2         = 'M'   # push 2-byte unsigned int\nNONE            = 'N'   # push None\nPERSID          = 'P'   # push persistent object; id is taken from string arg\nBINPERSID       = 'Q'   #  \"       \"         \"  ;  \"  \"   \"     \"  stack\nREDUCE          = 'R'   # apply callable to argtuple, both on stack\nSTRING          = 'S'   # push string; NL-terminated string argument\nBINSTRING       = 'T'   # push string; counted binary string argument\nSHORT_BINSTRING = 'U'   #  \"     \"   ;    \"      \"       \"      \" < 256 bytes\nUNICODE         = 'V'   # push Unicode string; raw-unicode-escaped'd argument\nBINUNICODE      = 'X'   #   \"     \"       \"  ; counted UTF-8 string argument\nAPPEND          = 'a'   # append stack top to list below it\nBUILD           = 'b'   # call __setstate__ or __dict__.update()\nGLOBAL          = 'c'   # push self.find_class(modname, name); 2 string args\nDICT            = 'd'   # build a dict from stack items\nEMPTY_DICT      = '}'   # push empty dict\nAPPENDS         = 'e'   # extend list on stack by topmost stack slice\nGET             = 'g'   # push item from memo on stack; index is string arg\nBINGET          = 'h'   #   \"    \"    \"    \"   \"   \"  ;   \"    \" 1-byte arg\nINST            = 'i'   # build & push class instance\nLONG_BINGET     = 'j'   # push item from memo on stack; index is 4-byte arg\nLIST            = 'l'   # build list from topmost stack items\nEMPTY_LIST      = ']'   # push empty list\nOBJ             = 'o'   # build & push class instance\nPUT             = 'p'   # store stack top in memo; index is string arg\nBINPUT          = 'q'   #   \"     \"    \"   \"   \" ;   \"    \" 1-byte arg\nLONG_BINPUT     = 'r'   #   \"     \"    \"   \"   \" ;   \"    \" 4-byte arg\nSETITEM         = 's'   # add key+value pair to dict\nTUPLE           = 't'   # build tuple from topmost stack items\nEMPTY_TUPLE     = ')'   # push empty tuple\nSETITEMS        = 'u'   # modify dict by adding topmost key+value pairs\nBINFLOAT        = 'G'   # push float; arg is 8-byte float encoding\n\nTRUE            = 'I01\\n'  # not an opcode; see INT docs in pickletools.py\nFALSE           = 'I00\\n'  # not an opcode; see INT docs in pickletools.py\n\n# Protocol 2\n\nPROTO           = '\\x80'  # identify pickle protocol\nNEWOBJ          = '\\x81'  # build object by applying cls.__new__ to argtuple\nEXT1            = '\\x82'  # push object from extension registry; 1-byte index\nEXT2            = '\\x83'  # ditto, but 2-byte index\nEXT4            = '\\x84'  # ditto, but 4-byte index\nTUPLE1          = '\\x85'  # build 1-tuple from stack top\nTUPLE2          = '\\x86'  # build 2-tuple from two topmost stack items\nTUPLE3          = '\\x87'  # build 3-tuple from three topmost stack items\nNEWTRUE         = '\\x88'  # push True\nNEWFALSE        = '\\x89'  # push False\nLONG1           = '\\x8a'  # push long from < 256 bytes\nLONG4           = '\\x8b'  # push really big long\n\n_tuplesize2code = [EMPTY_TUPLE, TUPLE1, TUPLE2, TUPLE3]\n\n\n__all__.extend([x for x in dir() if re.match(\"[A-Z][A-Z0-9_]+$\",x)])\ndel x\n\n\n# Pickling machinery\n\nclass Pickler(object):\n\n    def __init__(self, file, protocol=None):\n        \"\"\"This takes a file-like object for writing a pickle data stream.\n\n        The optional protocol argument tells the pickler to use the\n        given protocol; supported protocols are 0, 1, 2.  The default\n        protocol is 0, to be backwards compatible.  (Protocol 0 is the\n        only protocol that can be written to a file opened in text\n        mode and read back successfully.  When using a protocol higher\n        than 0, make sure the file is opened in binary mode, both when\n        pickling and unpickling.)\n\n        Protocol 1 is more efficient than protocol 0; protocol 2 is\n        more efficient than protocol 1.\n\n        Specifying a negative protocol version selects the highest\n        protocol version supported.  The higher the protocol used, the\n        more recent the version of Python needed to read the pickle\n        produced.\n\n        The file parameter must have a write() method that accepts a single\n        string argument.  It can thus be an open file object, a StringIO\n        object, or any other custom object that meets this interface.\n\n        \"\"\"\n        if protocol is None:\n            protocol = 0\n        if protocol < 0:\n            protocol = HIGHEST_PROTOCOL\n        elif not 0 <= protocol <= HIGHEST_PROTOCOL:\n            raise ValueError(\"pickle protocol must be <= %d\" % HIGHEST_PROTOCOL)\n        self.write = file.write\n        self.memo = {}\n        self.proto = int(protocol)\n        self.bin = protocol >= 1\n        self.fast = 0\n\n    def clear_memo(self):\n        \"\"\"Clears the pickler's \"memo\".\n\n        The memo is the data structure that remembers which objects the\n        pickler has already seen, so that shared or recursive objects are\n        pickled by reference and not by value.  This method is useful when\n        re-using picklers.\n\n        \"\"\"\n        self.memo.clear()\n\n    def dump(self, obj):\n        \"\"\"Write a pickled representation of obj to the open file.\"\"\"\n        if self.proto >= 2:\n            self.write(PROTO + chr(self.proto))\n        self.save(obj)\n        self.write(STOP)\n\n    def memoize(self, obj):\n        \"\"\"Store an object in the memo.\"\"\"\n\n        # The Pickler memo is a dictionary mapping object ids to 2-tuples\n        # that contain the Unpickler memo key and the object being memoized.\n        # The memo key is written to the pickle and will become\n        # the key in the Unpickler's memo.  The object is stored in the\n        # Pickler memo so that transient objects are kept alive during\n        # pickling.\n\n        # The use of the Unpickler memo length as the memo key is just a\n        # convention.  The only requirement is that the memo values be unique.\n        # But there appears no advantage to any other scheme, and this\n        # scheme allows the Unpickler memo to be implemented as a plain (but\n        # growable) array, indexed by memo key.\n        if self.fast:\n            return\n        assert id(obj) not in self.memo\n        memo_len = len(self.memo)\n        self.write(self.put(memo_len))\n        self.memo[id(obj)] = memo_len, obj\n\n    # Return a PUT (BINPUT, LONG_BINPUT) opcode string, with argument i.\n    def put(self, i, pack=struct.pack):\n        if self.bin:\n            if i < 256:\n                return BINPUT + chr(i)\n            else:\n                return LONG_BINPUT + pack(\"<i\", i)\n\n        return PUT + repr(i) + '\\n'\n\n    # Return a GET (BINGET, LONG_BINGET) opcode string, with argument i.\n    def get(self, i, pack=struct.pack):\n        if self.bin:\n            if i < 256:\n                return BINGET + chr(i)\n            else:\n                return LONG_BINGET + pack(\"<i\", i)\n\n        return GET + repr(i) + '\\n'\n\n    def save(self, obj):\n        # Check for persistent id (defined by a subclass)\n        pid = self.persistent_id(obj)\n        if pid is not None:\n            self.save_pers(pid)\n            return\n\n        # Check the memo\n        x = self.memo.get(id(obj))\n        if x:\n            self.write(self.get(x[0]))\n            return\n\n        # Check the type dispatch table\n        t = type(obj)\n        f = self.dispatch.get(t)\n        if f:\n            f(self, obj) # Call unbound method with explicit self\n            return\n\n        # Check copy_reg.dispatch_table\n        reduce = dispatch_table.get(t)\n        if reduce:\n            rv = reduce(obj)\n        else:\n            # Check for a class with a custom metaclass; treat as regular class\n            try:\n                issc = issubclass(t, TypeType)\n            except TypeError: # t is not a class (old Boost; see SF #502085)\n                issc = 0\n            if issc:\n                self.save_global(obj)\n                return\n\n            # Check for a __reduce_ex__ method, fall back to __reduce__\n            reduce = getattr(obj, \"__reduce_ex__\", None)\n            if reduce:\n                rv = reduce(self.proto)\n            else:\n                reduce = getattr(obj, \"__reduce__\", None)\n                if reduce:\n                    rv = reduce()\n                else:\n                    raise PicklingError(\"Can't pickle %r object: %r\" %\n                                        (t.__name__, obj))\n\n        # Check for string returned by reduce(), meaning \"save as global\"\n        if type(rv) is StringType:\n            self.save_global(obj, rv)\n            return\n\n        # Assert that reduce() returned a tuple\n        if type(rv) is not TupleType:\n            raise PicklingError(\"%s must return string or tuple\" % reduce)\n\n        # Assert that it returned an appropriately sized tuple\n        l = len(rv)\n        if not (2 <= l <= 5):\n            raise PicklingError(\"Tuple returned by %s must have \"\n                                \"two to five elements\" % reduce)\n\n        # Save the reduce() output and finally memoize the object\n        self.save_reduce(obj=obj, *rv)\n\n    def persistent_id(self, obj):\n        # This exists so a subclass can override it\n        return None\n\n    def save_pers(self, pid):\n        # Save a persistent id reference\n        if self.bin:\n            self.save(pid)\n            self.write(BINPERSID)\n        else:\n            self.write(PERSID + str(pid) + '\\n')\n\n    def save_reduce(self, func, args, state=None,\n                    listitems=None, dictitems=None, obj=None):\n        # This API is called by some subclasses\n\n        # Assert that args is a tuple or None\n        if not isinstance(args, TupleType):\n            raise PicklingError(\"args from reduce() should be a tuple\")\n\n        # Assert that func is callable\n        if not hasattr(func, '__call__'):\n            raise PicklingError(\"func from reduce should be callable\")\n\n        save = self.save\n        write = self.write\n\n        # Protocol 2 special case: if func's name is __newobj__, use NEWOBJ\n        if self.proto >= 2 and getattr(func, \"__name__\", \"\") == \"__newobj__\":\n            # A __reduce__ implementation can direct protocol 2 to\n            # use the more efficient NEWOBJ opcode, while still\n            # allowing protocol 0 and 1 to work normally.  For this to\n            # work, the function returned by __reduce__ should be\n            # called __newobj__, and its first argument should be a\n            # new-style class.  The implementation for __newobj__\n            # should be as follows, although pickle has no way to\n            # verify this:\n            #\n            # def __newobj__(cls, *args):\n            #     return cls.__new__(cls, *args)\n            #\n            # Protocols 0 and 1 will pickle a reference to __newobj__,\n            # while protocol 2 (and above) will pickle a reference to\n            # cls, the remaining args tuple, and the NEWOBJ code,\n            # which calls cls.__new__(cls, *args) at unpickling time\n            # (see load_newobj below).  If __reduce__ returns a\n            # three-tuple, the state from the third tuple item will be\n            # pickled regardless of the protocol, calling __setstate__\n            # at unpickling time (see load_build below).\n            #\n            # Note that no standard __newobj__ implementation exists;\n            # you have to provide your own.  This is to enforce\n            # compatibility with Python 2.2 (pickles written using\n            # protocol 0 or 1 in Python 2.3 should be unpicklable by\n            # Python 2.2).\n            cls = args[0]\n            if not hasattr(cls, \"__new__\"):\n                raise PicklingError(\n                    \"args[0] from __newobj__ args has no __new__\")\n            if obj is not None and cls is not obj.__class__:\n                raise PicklingError(\n                    \"args[0] from __newobj__ args has the wrong class\")\n            args = args[1:]\n            save(cls)\n            save(args)\n            write(NEWOBJ)\n        else:\n            save(func)\n            save(args)\n            write(REDUCE)\n\n        if obj is not None:\n            self.memoize(obj)\n\n        # More new special cases (that work with older protocols as\n        # well): when __reduce__ returns a tuple with 4 or 5 items,\n        # the 4th and 5th item should be iterators that provide list\n        # items and dict items (as (key, value) tuples), or None.\n\n        if listitems is not None:\n            self._batch_appends(listitems)\n\n        if dictitems is not None:\n            self._batch_setitems(dictitems)\n\n        if state is not None:\n            save(state)\n            write(BUILD)\n\n    # Methods below this point are dispatched through the dispatch table\n\n    dispatch = {}\n\n    def save_none(self, obj):\n        self.write(NONE)\n    dispatch[NoneType] = save_none\n\n    def save_bool(self, obj):\n        if self.proto >= 2:\n            self.write(obj and NEWTRUE or NEWFALSE)\n        else:\n            self.write(obj and TRUE or FALSE)\n    dispatch[bool] = save_bool\n\n    def save_int(self, obj, pack=struct.pack):\n        if self.bin:\n            # If the int is small enough to fit in a signed 4-byte 2's-comp\n            # format, we can store it more efficiently than the general\n            # case.\n            # First one- and two-byte unsigned ints:\n            if obj >= 0:\n                if obj <= 0xff:\n                    self.write(BININT1 + chr(obj))\n                    return\n                if obj <= 0xffff:\n                    self.write(\"%c%c%c\" % (BININT2, obj&0xff, obj>>8))\n                    return\n            # Next check for 4-byte signed ints:\n            high_bits = obj >> 31  # note that Python shift sign-extends\n            if high_bits == 0 or high_bits == -1:\n                # All high bits are copies of bit 2**31, so the value\n                # fits in a 4-byte signed int.\n                self.write(BININT + pack(\"<i\", obj))\n                return\n        # Text pickle, or int too big to fit in signed 4-byte format.\n        self.write(INT + repr(obj) + '\\n')\n    dispatch[IntType] = save_int\n\n    def save_long(self, obj, pack=struct.pack):\n        if self.proto >= 2:\n            bytes = encode_long(obj)\n            n = len(bytes)\n            if n < 256:\n                self.write(LONG1 + chr(n) + bytes)\n            else:\n                self.write(LONG4 + pack(\"<i\", n) + bytes)\n            return\n        self.write(LONG + repr(obj) + '\\n')\n    dispatch[LongType] = save_long\n\n    def save_float(self, obj, pack=struct.pack):\n        if self.bin:\n            self.write(BINFLOAT + pack('>d', obj))\n        else:\n            self.write(FLOAT + repr(obj) + '\\n')\n    dispatch[FloatType] = save_float\n\n    def save_string(self, obj, pack=struct.pack):\n        if self.bin:\n            n = len(obj)\n            if n < 256:\n                self.write(SHORT_BINSTRING + chr(n) + obj)\n            else:\n                self.write(BINSTRING + pack(\"<i\", n) + obj)\n        else:\n            self.write(STRING + repr(obj) + '\\n')\n        self.memoize(obj)\n    dispatch[StringType] = save_string\n\n    def save_unicode(self, obj, pack=struct.pack):\n        if self.bin:\n            encoding = obj.encode('utf-8')\n            n = len(encoding)\n            self.write(BINUNICODE + pack(\"<i\", n) + encoding)\n        else:\n            obj = obj.replace(\"\\\\\", \"\\\\u005c\")\n            obj = obj.replace(\"\\n\", \"\\\\u000a\")\n            self.write(UNICODE + obj.encode('raw-unicode-escape') + '\\n')\n        self.memoize(obj)\n    dispatch[UnicodeType] = save_unicode\n\n    if StringType is UnicodeType:\n        # This is true for Jython\n        def save_string(self, obj, pack=struct.pack):\n            unicode = obj.isunicode()\n\n            if self.bin:\n                if unicode:\n                    obj = obj.encode(\"utf-8\")\n                l = len(obj)\n                if l < 256 and not unicode:\n                    self.write(SHORT_BINSTRING + chr(l) + obj)\n                else:\n                    s = pack(\"<i\", l)\n                    if unicode:\n                        self.write(BINUNICODE + s + obj)\n                    else:\n                        self.write(BINSTRING + s + obj)\n            else:\n                if unicode:\n                    obj = obj.replace(\"\\\\\", \"\\\\u005c\")\n                    obj = obj.replace(\"\\n\", \"\\\\u000a\")\n                    obj = obj.encode('raw-unicode-escape')\n                    self.write(UNICODE + obj + '\\n')\n                else:\n                    self.write(STRING + repr(obj) + '\\n')\n            self.memoize(obj)\n        dispatch[StringType] = save_string\n\n    def save_tuple(self, obj):\n        write = self.write\n        proto = self.proto\n\n        n = len(obj)\n        if n == 0:\n            if proto:\n                write(EMPTY_TUPLE)\n            else:\n                write(MARK + TUPLE)\n            return\n\n        save = self.save\n        memo = self.memo\n        if n <= 3 and proto >= 2:\n            for element in obj:\n                save(element)\n            # Subtle.  Same as in the big comment below.\n            if id(obj) in memo:\n                get = self.get(memo[id(obj)][0])\n                write(POP * n + get)\n            else:\n                write(_tuplesize2code[n])\n                self.memoize(obj)\n            return\n\n        # proto 0 or proto 1 and tuple isn't empty, or proto > 1 and tuple\n        # has more than 3 elements.\n        write(MARK)\n        for element in obj:\n            save(element)\n\n        if id(obj) in memo:\n            # Subtle.  d was not in memo when we entered save_tuple(), so\n            # the process of saving the tuple's elements must have saved\n            # the tuple itself:  the tuple is recursive.  The proper action\n            # now is to throw away everything we put on the stack, and\n            # simply GET the tuple (it's already constructed).  This check\n            # could have been done in the \"for element\" loop instead, but\n            # recursive tuples are a rare thing.\n            get = self.get(memo[id(obj)][0])\n            if proto:\n                write(POP_MARK + get)\n            else:   # proto 0 -- POP_MARK not available\n                write(POP * (n+1) + get)\n            return\n\n        # No recursion.\n        self.write(TUPLE)\n        self.memoize(obj)\n\n    dispatch[TupleType] = save_tuple\n\n    # save_empty_tuple() isn't used by anything in Python 2.3.  However, I\n    # found a Pickler subclass in Zope3 that calls it, so it's not harmless\n    # to remove it.\n    def save_empty_tuple(self, obj):\n        self.write(EMPTY_TUPLE)\n\n    def save_list(self, obj):\n        write = self.write\n\n        if self.bin:\n            write(EMPTY_LIST)\n        else:   # proto 0 -- can't use EMPTY_LIST\n            write(MARK + LIST)\n\n        self.memoize(obj)\n        self._batch_appends(iter(obj))\n\n    dispatch[ListType] = save_list\n\n    # Keep in synch with cPickle's BATCHSIZE.  Nothing will break if it gets\n    # out of synch, though.\n    _BATCHSIZE = 1000\n\n    def _batch_appends(self, items):\n        # Helper to batch up APPENDS sequences\n        save = self.save\n        write = self.write\n\n        if not self.bin:\n            for x in items:\n                save(x)\n                write(APPEND)\n            return\n\n        r = xrange(self._BATCHSIZE)\n        while items is not None:\n            tmp = []\n            for i in r:\n                try:\n                    x = items.next()\n                    tmp.append(x)\n                except StopIteration:\n                    items = None\n                    break\n            n = len(tmp)\n            if n > 1:\n                write(MARK)\n                for x in tmp:\n                    save(x)\n                write(APPENDS)\n            elif n:\n                save(tmp[0])\n                write(APPEND)\n            # else tmp is empty, and we're done\n\n    def save_dict(self, obj):\n        modict_saver = self._pickle_maybe_moduledict(obj)\n        if modict_saver is not None:\n            return self.save_reduce(*modict_saver)\n\n        write = self.write\n\n        if self.bin:\n            write(EMPTY_DICT)\n        else:   # proto 0 -- can't use EMPTY_DICT\n            write(MARK + DICT)\n\n        self.memoize(obj)\n        self._batch_setitems(obj.iteritems())\n\n    dispatch[DictionaryType] = save_dict\n    if not PyStringMap is None:\n        dispatch[PyStringMap] = save_dict\n\n    def _batch_setitems(self, items):\n        # Helper to batch up SETITEMS sequences; proto >= 1 only\n        save = self.save\n        write = self.write\n\n        if not self.bin:\n            for k, v in items:\n                save(k)\n                save(v)\n                write(SETITEM)\n            return\n\n        r = xrange(self._BATCHSIZE)\n        while items is not None:\n            tmp = []\n            for i in r:\n                try:\n                    tmp.append(items.next())\n                except StopIteration:\n                    items = None\n                    break\n            n = len(tmp)\n            if n > 1:\n                write(MARK)\n                for k, v in tmp:\n                    save(k)\n                    save(v)\n                write(SETITEMS)\n            elif n:\n                k, v = tmp[0]\n                save(k)\n                save(v)\n                write(SETITEM)\n            # else tmp is empty, and we're done\n\n    def _pickle_maybe_moduledict(self, obj):\n        # save module dictionary as \"getattr(module, '__dict__')\"\n        try:\n            name = obj['__name__']\n            if type(name) is not str:\n                return None\n            themodule = sys.modules[name]\n            if type(themodule) is not ModuleType:\n                return None\n            if themodule.__dict__ is not obj:\n                return None\n        except (AttributeError, KeyError, TypeError):\n            return None\n\n        return getattr, (themodule, '__dict__')\n\n\n    def save_inst(self, obj):\n        cls = obj.__class__\n\n        memo  = self.memo\n        write = self.write\n        save  = self.save\n\n        if hasattr(obj, '__getinitargs__'):\n            args = obj.__getinitargs__()\n            len(args) # XXX Assert it's a sequence\n            _keep_alive(args, memo)\n        else:\n            args = ()\n\n        write(MARK)\n\n        if self.bin:\n            save(cls)\n            for arg in args:\n                save(arg)\n            write(OBJ)\n        else:\n            for arg in args:\n                save(arg)\n            write(INST + cls.__module__ + '\\n' + cls.__name__ + '\\n')\n\n        self.memoize(obj)\n\n        try:\n            getstate = obj.__getstate__\n        except AttributeError:\n            stuff = obj.__dict__\n        else:\n            stuff = getstate()\n            _keep_alive(stuff, memo)\n        save(stuff)\n        write(BUILD)\n\n    dispatch[InstanceType] = save_inst\n\n    def save_function(self, obj):\n        try:\n            return self.save_global(obj)\n        except PicklingError, e:\n            pass\n        # Check copy_reg.dispatch_table\n        reduce = dispatch_table.get(type(obj))\n        if reduce:\n            rv = reduce(obj)\n        else:\n            # Check for a __reduce_ex__ method, fall back to __reduce__\n            reduce = getattr(obj, \"__reduce_ex__\", None)\n            if reduce:\n                rv = reduce(self.proto)\n            else:\n                reduce = getattr(obj, \"__reduce__\", None)\n                if reduce:\n                    rv = reduce()\n                else:\n                    raise e\n        return self.save_reduce(obj=obj, *rv)\n    dispatch[FunctionType] = save_function\n\n    def save_global(self, obj, name=None, pack=struct.pack):\n        write = self.write\n        memo = self.memo\n\n        if name is None:\n            name = obj.__name__\n\n        module = getattr(obj, \"__module__\", None)\n        if module is None:\n            module = whichmodule(obj, name)\n\n        try:\n            __import__(module)\n            mod = sys.modules[module]\n            klass = getattr(mod, name)\n        except (ImportError, KeyError, AttributeError):\n            raise PicklingError(\n                \"Can't pickle %r: it's not found as %s.%s\" %\n                (obj, module, name))\n        else:\n            if klass is not obj:\n                raise PicklingError(\n                    \"Can't pickle %r: it's not the same object as %s.%s\" %\n                    (obj, module, name))\n\n        if self.proto >= 2:\n            code = _extension_registry.get((module, name))\n            if code:\n                assert code > 0\n                if code <= 0xff:\n                    write(EXT1 + chr(code))\n                elif code <= 0xffff:\n                    write(\"%c%c%c\" % (EXT2, code&0xff, code>>8))\n                else:\n                    write(EXT4 + pack(\"<i\", code))\n                return\n\n        write(GLOBAL + module + '\\n' + name + '\\n')\n        self.memoize(obj)\n\n    dispatch[ClassType] = save_global\n    dispatch[BuiltinFunctionType] = save_global\n    dispatch[TypeType] = save_global\n\n# Pickling helpers\n\ndef _keep_alive(x, memo):\n    \"\"\"Keeps a reference to the object x in the memo.\n\n    Because we remember objects by their id, we have\n    to assure that possibly temporary objects are kept\n    alive by referencing them.\n    We store a reference at the id of the memo, which should\n    normally not be used unless someone tries to deepcopy\n    the memo itself...\n    \"\"\"\n    try:\n        memo[id(memo)].append(x)\n    except KeyError:\n        # aha, this is the first one :-)\n        memo[id(memo)]=[x]\n\n\n# A cache for whichmodule(), mapping a function object to the name of\n# the module in which the function was found.\n\nclassmap = {} # called classmap for backwards compatibility\n\ndef whichmodule(func, funcname):\n    \"\"\"Figure out the module in which a function occurs.\n\n    Search sys.modules for the module.\n    Cache in classmap.\n    Return a module name.\n    If the function cannot be found, return \"__main__\".\n    \"\"\"\n    # Python functions should always get an __module__ from their globals.\n    mod = getattr(func, \"__module__\", None)\n    if mod is not None:\n        return mod\n    if func in classmap:\n        return classmap[func]\n\n    for name, module in sys.modules.items():\n        if module is None:\n            continue # skip dummy package entries\n        if name != '__main__' and getattr(module, funcname, None) is func:\n            break\n    else:\n        name = '__main__'\n    classmap[func] = name\n    return name\n\n\n# Unpickling machinery\n\nclass Unpickler(object):\n\n    def __init__(self, file):\n        \"\"\"This takes a file-like object for reading a pickle data stream.\n\n        The protocol version of the pickle is detected automatically, so no\n        proto argument is needed.\n\n        The file-like object must have two methods, a read() method that\n        takes an integer argument, and a readline() method that requires no\n        arguments.  Both methods should return a string.  Thus file-like\n        object can be a file object opened for reading, a StringIO object,\n        or any other custom object that meets this interface.\n        \"\"\"\n        self.readline = file.readline\n        self.read = file.read\n        self.memo = {}\n\n    def load(self):\n        \"\"\"Read a pickled object representation from the open file.\n\n        Return the reconstituted object hierarchy specified in the file.\n        \"\"\"\n        self.mark = object() # any new unique object\n        self.stack = []\n        self.append = self.stack.append\n        read = self.read\n        dispatch = self.dispatch\n        try:\n            while 1:\n                key = read(1)\n                dispatch[key](self)\n        except _Stop, stopinst:\n            return stopinst.value\n\n    # Return largest index k such that self.stack[k] is self.mark.\n    # If the stack doesn't contain a mark, eventually raises IndexError.\n    # This could be sped by maintaining another stack, of indices at which\n    # the mark appears.  For that matter, the latter stack would suffice,\n    # and we wouldn't need to push mark objects on self.stack at all.\n    # Doing so is probably a good thing, though, since if the pickle is\n    # corrupt (or hostile) we may get a clue from finding self.mark embedded\n    # in unpickled objects.\n    def marker(self):\n        stack = self.stack\n        mark = self.mark\n        k = len(stack)-1\n        while stack[k] is not mark: k = k-1\n        return k\n\n    dispatch = {}\n\n    def load_eof(self):\n        raise EOFError\n    dispatch[''] = load_eof\n\n    def load_proto(self):\n        proto = ord(self.read(1))\n        if not 0 <= proto <= 2:\n            raise ValueError, \"unsupported pickle protocol: %d\" % proto\n    dispatch[PROTO] = load_proto\n\n    def load_persid(self):\n        pid = self.readline()[:-1]\n        self.append(self.persistent_load(pid))\n    dispatch[PERSID] = load_persid\n\n    def load_binpersid(self):\n        pid = self.stack.pop()\n        self.append(self.persistent_load(pid))\n    dispatch[BINPERSID] = load_binpersid\n\n    def load_none(self):\n        self.append(None)\n    dispatch[NONE] = load_none\n\n    def load_false(self):\n        self.append(False)\n    dispatch[NEWFALSE] = load_false\n\n    def load_true(self):\n        self.append(True)\n    dispatch[NEWTRUE] = load_true\n\n    def load_int(self):\n        data = self.readline()\n        if data == FALSE[1:]:\n            val = False\n        elif data == TRUE[1:]:\n            val = True\n        else:\n            try:\n                val = int(data)\n            except ValueError:\n                val = long(data)\n        self.append(val)\n    dispatch[INT] = load_int\n\n    def load_binint(self):\n        self.append(mloads('i' + self.read(4)))\n    dispatch[BININT] = load_binint\n\n    def load_binint1(self):\n        self.append(ord(self.read(1)))\n    dispatch[BININT1] = load_binint1\n\n    def load_binint2(self):\n        self.append(mloads('i' + self.read(2) + '\\000\\000'))\n    dispatch[BININT2] = load_binint2\n\n    def load_long(self):\n        self.append(long(self.readline()[:-1], 0))\n    dispatch[LONG] = load_long\n\n    def load_long1(self):\n        n = ord(self.read(1))\n        bytes = self.read(n)\n        self.append(decode_long(bytes))\n    dispatch[LONG1] = load_long1\n\n    def load_long4(self):\n        n = mloads('i' + self.read(4))\n        bytes = self.read(n)\n        self.append(decode_long(bytes))\n    dispatch[LONG4] = load_long4\n\n    def load_float(self):\n        self.append(float(self.readline()[:-1]))\n    dispatch[FLOAT] = load_float\n\n    def load_binfloat(self, unpack=struct.unpack):\n        self.append(unpack('>d', self.read(8))[0])\n    dispatch[BINFLOAT] = load_binfloat\n\n    def load_string(self):\n        rep = self.readline()[:-1]\n        for q in \"\\\"'\": # double or single quote\n            if rep.startswith(q):\n                if len(rep) < 2 or not rep.endswith(q):\n                    raise ValueError, \"insecure string pickle\"\n                rep = rep[len(q):-len(q)]\n                break\n        else:\n            raise ValueError, \"insecure string pickle\"\n        self.append(rep.decode(\"string-escape\"))\n    dispatch[STRING] = load_string\n\n    def load_binstring(self):\n        len = mloads('i' + self.read(4))\n        self.append(self.read(len))\n    dispatch[BINSTRING] = load_binstring\n\n    def load_unicode(self):\n        self.append(unicode(self.readline()[:-1],'raw-unicode-escape'))\n    dispatch[UNICODE] = load_unicode\n\n    def load_binunicode(self):\n        len = mloads('i' + self.read(4))\n        self.append(unicode(self.read(len),'utf-8'))\n    dispatch[BINUNICODE] = load_binunicode\n\n    def load_short_binstring(self):\n        len = ord(self.read(1))\n        self.append(self.read(len))\n    dispatch[SHORT_BINSTRING] = load_short_binstring\n\n    def load_tuple(self):\n        k = self.marker()\n        self.stack[k:] = [tuple(self.stack[k+1:])]\n    dispatch[TUPLE] = load_tuple\n\n    def load_empty_tuple(self):\n        self.stack.append(())\n    dispatch[EMPTY_TUPLE] = load_empty_tuple\n\n    def load_tuple1(self):\n        self.stack[-1] = (self.stack[-1],)\n    dispatch[TUPLE1] = load_tuple1\n\n    def load_tuple2(self):\n        self.stack[-2:] = [(self.stack[-2], self.stack[-1])]\n    dispatch[TUPLE2] = load_tuple2\n\n    def load_tuple3(self):\n        self.stack[-3:] = [(self.stack[-3], self.stack[-2], self.stack[-1])]\n    dispatch[TUPLE3] = load_tuple3\n\n    def load_empty_list(self):\n        self.stack.append([])\n    dispatch[EMPTY_LIST] = load_empty_list\n\n    def load_empty_dictionary(self):\n        self.stack.append({})\n    dispatch[EMPTY_DICT] = load_empty_dictionary\n\n    def load_list(self):\n        k = self.marker()\n        self.stack[k:] = [self.stack[k+1:]]\n    dispatch[LIST] = load_list\n\n    def load_dict(self):\n        k = self.marker()\n        d = {}\n        items = self.stack[k+1:]\n        for i in range(0, len(items), 2):\n            key = items[i]\n            value = items[i+1]\n            d[key] = value\n        self.stack[k:] = [d]\n    dispatch[DICT] = load_dict\n\n    # INST and OBJ differ only in how they get a class object.  It's not\n    # only sensible to do the rest in a common routine, the two routines\n    # previously diverged and grew different bugs.\n    # klass is the class to instantiate, and k points to the topmost mark\n    # object, following which are the arguments for klass.__init__.\n    def _instantiate(self, klass, k):\n        args = tuple(self.stack[k+1:])\n        del self.stack[k:]\n        instantiated = 0\n        if (not args and\n                type(klass) is ClassType and\n                not hasattr(klass, \"__getinitargs__\")):\n            try:\n                value = _EmptyClass()\n                value.__class__ = klass\n                instantiated = 1\n            except RuntimeError:\n                # In restricted execution, assignment to inst.__class__ is\n                # prohibited\n                pass\n        if not instantiated:\n            try:\n                value = klass(*args)\n            except TypeError, err:\n                raise TypeError, \"in constructor for %s: %s\" % (\n                    klass.__name__, str(err)), sys.exc_info()[2]\n        self.append(value)\n\n    def load_inst(self):\n        module = self.readline()[:-1]\n        name = self.readline()[:-1]\n        klass = self.find_class(module, name)\n        self._instantiate(klass, self.marker())\n    dispatch[INST] = load_inst\n\n    def load_obj(self):\n        # Stack is ... markobject classobject arg1 arg2 ...\n        k = self.marker()\n        klass = self.stack.pop(k+1)\n        self._instantiate(klass, k)\n    dispatch[OBJ] = load_obj\n\n    def load_newobj(self):\n        args = self.stack.pop()\n        cls = self.stack[-1]\n        obj = cls.__new__(cls, *args)\n        self.stack[-1] = obj\n    dispatch[NEWOBJ] = load_newobj\n\n    def load_global(self):\n        module = self.readline()[:-1]\n        name = self.readline()[:-1]\n        klass = self.find_class(module, name)\n        self.append(klass)\n    dispatch[GLOBAL] = load_global\n\n    def load_ext1(self):\n        code = ord(self.read(1))\n        self.get_extension(code)\n    dispatch[EXT1] = load_ext1\n\n    def load_ext2(self):\n        code = mloads('i' + self.read(2) + '\\000\\000')\n        self.get_extension(code)\n    dispatch[EXT2] = load_ext2\n\n    def load_ext4(self):\n        code = mloads('i' + self.read(4))\n        self.get_extension(code)\n    dispatch[EXT4] = load_ext4\n\n    def get_extension(self, code):\n        nil = []\n        obj = _extension_cache.get(code, nil)\n        if obj is not nil:\n            self.append(obj)\n            return\n        key = _inverted_registry.get(code)\n        if not key:\n            raise ValueError(\"unregistered extension code %d\" % code)\n        obj = self.find_class(*key)\n        _extension_cache[code] = obj\n        self.append(obj)\n\n    def find_class(self, module, name):\n        # Subclasses may override this\n        __import__(module)\n        mod = sys.modules[module]\n        klass = getattr(mod, name)\n        return klass\n\n    def load_reduce(self):\n        stack = self.stack\n        args = stack.pop()\n        func = stack[-1]\n        value = func(*args)\n        stack[-1] = value\n    dispatch[REDUCE] = load_reduce\n\n    def load_pop(self):\n        del self.stack[-1]\n    dispatch[POP] = load_pop\n\n    def load_pop_mark(self):\n        k = self.marker()\n        del self.stack[k:]\n    dispatch[POP_MARK] = load_pop_mark\n\n    def load_dup(self):\n        self.append(self.stack[-1])\n    dispatch[DUP] = load_dup\n\n    def load_get(self):\n        self.append(self.memo[self.readline()[:-1]])\n    dispatch[GET] = load_get\n\n    def load_binget(self):\n        i = ord(self.read(1))\n        self.append(self.memo[repr(i)])\n    dispatch[BINGET] = load_binget\n\n    def load_long_binget(self):\n        i = mloads('i' + self.read(4))\n        self.append(self.memo[repr(i)])\n    dispatch[LONG_BINGET] = load_long_binget\n\n    def load_put(self):\n        self.memo[self.readline()[:-1]] = self.stack[-1]\n    dispatch[PUT] = load_put\n\n    def load_binput(self):\n        i = ord(self.read(1))\n        self.memo[repr(i)] = self.stack[-1]\n    dispatch[BINPUT] = load_binput\n\n    def load_long_binput(self):\n        i = mloads('i' + self.read(4))\n        self.memo[repr(i)] = self.stack[-1]\n    dispatch[LONG_BINPUT] = load_long_binput\n\n    def load_append(self):\n        stack = self.stack\n        value = stack.pop()\n        list = stack[-1]\n        list.append(value)\n    dispatch[APPEND] = load_append\n\n    def load_appends(self):\n        stack = self.stack\n        mark = self.marker()\n        list = stack[mark - 1]\n        list.extend(stack[mark + 1:])\n        del stack[mark:]\n    dispatch[APPENDS] = load_appends\n\n    def load_setitem(self):\n        stack = self.stack\n        value = stack.pop()\n        key = stack.pop()\n        dict = stack[-1]\n        dict[key] = value\n    dispatch[SETITEM] = load_setitem\n\n    def load_setitems(self):\n        stack = self.stack\n        mark = self.marker()\n        dict = stack[mark - 1]\n        for i in range(mark + 1, len(stack), 2):\n            dict[stack[i]] = stack[i + 1]\n\n        del stack[mark:]\n    dispatch[SETITEMS] = load_setitems\n\n    def load_build(self):\n        stack = self.stack\n        state = stack.pop()\n        inst = stack[-1]\n        setstate = getattr(inst, \"__setstate__\", None)\n        if setstate:\n            setstate(state)\n            return\n        slotstate = None\n        if isinstance(state, tuple) and len(state) == 2:\n            state, slotstate = state\n        if state:\n            try:\n                d = inst.__dict__\n                try:\n                    for k, v in state.iteritems():\n                        d[intern(k)] = v\n                # keys in state don't have to be strings\n                # don't blow up, but don't go out of our way\n                except TypeError:\n                    d.update(state)\n\n            except RuntimeError:\n                # XXX In restricted execution, the instance's __dict__\n                # is not accessible.  Use the old way of unpickling\n                # the instance variables.  This is a semantic\n                # difference when unpickling in restricted\n                # vs. unrestricted modes.\n                # Note, however, that cPickle has never tried to do the\n                # .update() business, and always uses\n                #     PyObject_SetItem(inst.__dict__, key, value) in a\n                # loop over state.items().\n                for k, v in state.items():\n                    setattr(inst, k, v)\n        if slotstate:\n            for k, v in slotstate.items():\n                setattr(inst, k, v)\n    dispatch[BUILD] = load_build\n\n    def load_mark(self):\n        self.append(self.mark)\n    dispatch[MARK] = load_mark\n\n    def load_stop(self):\n        value = self.stack.pop()\n        raise _Stop(value)\n    dispatch[STOP] = load_stop\n\n# Helper class for load_inst/load_obj\n\nclass _EmptyClass:\n    pass\n\n# Encode/decode longs in linear time.\n\nimport binascii as _binascii\n\ndef encode_long(x):\n    r\"\"\"Encode a long to a two's complement little-endian binary string.\n    Note that 0L is a special case, returning an empty string, to save a\n    byte in the LONG1 pickling context.\n\n    >>> encode_long(0L)\n    ''\n    >>> encode_long(255L)\n    '\\xff\\x00'\n    >>> encode_long(32767L)\n    '\\xff\\x7f'\n    >>> encode_long(-256L)\n    '\\x00\\xff'\n    >>> encode_long(-32768L)\n    '\\x00\\x80'\n    >>> encode_long(-128L)\n    '\\x80'\n    >>> encode_long(127L)\n    '\\x7f'\n    >>>\n    \"\"\"\n\n    if x == 0:\n        return ''\n    if x > 0:\n        ashex = hex(x)\n        assert ashex.startswith(\"0x\")\n        njunkchars = 2 + ashex.endswith('L')\n        nibbles = len(ashex) - njunkchars\n        if nibbles & 1:\n            # need an even # of nibbles for unhexlify\n            ashex = \"0x0\" + ashex[2:]\n        elif int(ashex[2], 16) >= 8:\n            # \"looks negative\", so need a byte of sign bits\n            ashex = \"0x00\" + ashex[2:]\n    else:\n        # Build the 256's-complement:  (1L << nbytes) + x.  The trick is\n        # to find the number of bytes in linear time (although that should\n        # really be a constant-time task).\n        ashex = hex(-x)\n        assert ashex.startswith(\"0x\")\n        njunkchars = 2 + ashex.endswith('L')\n        nibbles = len(ashex) - njunkchars\n        if nibbles & 1:\n            # Extend to a full byte.\n            nibbles += 1\n        nbits = nibbles * 4\n        x += 1L << nbits\n        assert x > 0\n        ashex = hex(x)\n        njunkchars = 2 + ashex.endswith('L')\n        newnibbles = len(ashex) - njunkchars\n        if newnibbles < nibbles:\n            ashex = \"0x\" + \"0\" * (nibbles - newnibbles) + ashex[2:]\n        if int(ashex[2], 16) < 8:\n            # \"looks positive\", so need a byte of sign bits\n            ashex = \"0xff\" + ashex[2:]\n\n    if ashex.endswith('L'):\n        ashex = ashex[2:-1]\n    else:\n        ashex = ashex[2:]\n    assert len(ashex) & 1 == 0, (x, ashex)\n    binary = _binascii.unhexlify(ashex)\n    return binary[::-1]\n\ndef decode_long(data):\n    r\"\"\"Decode a long from a two's complement little-endian binary string.\n\n    >>> decode_long('')\n    0L\n    >>> decode_long(\"\\xff\\x00\")\n    255L\n    >>> decode_long(\"\\xff\\x7f\")\n    32767L\n    >>> decode_long(\"\\x00\\xff\")\n    -256L\n    >>> decode_long(\"\\x00\\x80\")\n    -32768L\n    >>> decode_long(\"\\x80\")\n    -128L\n    >>> decode_long(\"\\x7f\")\n    127L\n    \"\"\"\n\n    nbytes = len(data)\n    if nbytes == 0:\n        return 0L\n    ashex = _binascii.hexlify(data[::-1])\n    n = long(ashex, 16) # quadratic time before Python 2.3; linear now\n    if data[-1] >= '\\x80':\n        n -= 1L << (nbytes * 8)\n    return n\n\n# Shorthands\n\ntry:\n    from cStringIO import StringIO\nexcept ImportError:\n    from StringIO import StringIO\n\ndef dump(obj, file, protocol=None):\n    Pickler(file, protocol).dump(obj)\n\ndef dumps(obj, protocol=None):\n    file = StringIO()\n    Pickler(file, protocol).dump(obj)\n    return file.getvalue()\n\ndef load(file):\n    return Unpickler(file).load()\n\ndef loads(str):\n    file = StringIO(str)\n    return Unpickler(file).load()\n\n# Doctest\n\ndef _test():\n    import doctest\n    return doctest.testmod()\n\nif __name__ == \"__main__\":\n    _test()\n", 
    "posixpath": "\"\"\"Common operations on Posix pathnames.\n\nInstead of importing this module directly, import os and refer to\nthis module as os.path.  The \"os.path\" name is an alias for this\nmodule on Posix systems; on other systems (e.g. Mac, Windows),\nos.path provides the same operations in a manner specific to that\nplatform, and is an alias to another module (e.g. macpath, ntpath).\n\nSome of this can actually be useful on non-Posix systems too, e.g.\nfor manipulation of the pathname component of URLs.\n\"\"\"\n\nimport os\nimport sys\nimport stat\nimport genericpath\nimport warnings\nfrom genericpath import *\n\ntry:\n    _unicode = unicode\nexcept NameError:\n    # If Python is built without Unicode support, the unicode type\n    # will not exist. Fake one.\n    class _unicode(object):\n        pass\n\n__all__ = [\"normcase\",\"isabs\",\"join\",\"splitdrive\",\"split\",\"splitext\",\n           \"basename\",\"dirname\",\"commonprefix\",\"getsize\",\"getmtime\",\n           \"getatime\",\"getctime\",\"islink\",\"exists\",\"lexists\",\"isdir\",\"isfile\",\n           \"ismount\",\"walk\",\"expanduser\",\"expandvars\",\"normpath\",\"abspath\",\n           \"samefile\",\"sameopenfile\",\"samestat\",\n           \"curdir\",\"pardir\",\"sep\",\"pathsep\",\"defpath\",\"altsep\",\"extsep\",\n           \"devnull\",\"realpath\",\"supports_unicode_filenames\",\"relpath\"]\n\n# strings representing various path-related bits and pieces\ncurdir = '.'\npardir = '..'\nextsep = '.'\nsep = '/'\npathsep = ':'\ndefpath = ':/bin:/usr/bin'\naltsep = None\ndevnull = '/dev/null'\n\n# Normalize the case of a pathname.  Trivial in Posix, string.lower on Mac.\n# On MS-DOS this may also turn slashes into backslashes; however, other\n# normalizations (such as optimizing '../' away) are not allowed\n# (another function should be defined to do that).\n\ndef normcase(s):\n    \"\"\"Normalize case of pathname.  Has no effect under Posix\"\"\"\n    return s\n\n\n# Return whether a path is absolute.\n# Trivial in Posix, harder on the Mac or MS-DOS.\n\ndef isabs(s):\n    \"\"\"Test whether a path is absolute\"\"\"\n    return s.startswith('/')\n\n\n# Join pathnames.\n# Ignore the previous parts if a part is absolute.\n# Insert a '/' unless the first part is empty or already ends in '/'.\n\ndef join(a, *p):\n    \"\"\"Join two or more pathname components, inserting '/' as needed.\n    If any component is an absolute path, all previous path components\n    will be discarded.  An empty last part will result in a path that\n    ends with a separator.\"\"\"\n    path = a\n    for b in p:\n        if b.startswith('/'):\n            path = b\n        elif path == '' or path.endswith('/'):\n            path +=  b\n        else:\n            path += '/' + b\n    return path\n\n\n# Split a path in head (everything up to the last '/') and tail (the\n# rest).  If the path ends in '/', tail will be empty.  If there is no\n# '/' in the path, head  will be empty.\n# Trailing '/'es are stripped from head unless it is the root.\n\ndef split(p):\n    \"\"\"Split a pathname.  Returns tuple \"(head, tail)\" where \"tail\" is\n    everything after the final slash.  Either part may be empty.\"\"\"\n    i = p.rfind('/') + 1\n    head, tail = p[:i], p[i:]\n    if head and head != '/'*len(head):\n        head = head.rstrip('/')\n    return head, tail\n\n\n# Split a path in root and extension.\n# The extension is everything starting at the last dot in the last\n# pathname component; the root is everything before that.\n# It is always true that root + ext == p.\n\ndef splitext(p):\n    return genericpath._splitext(p, sep, altsep, extsep)\nsplitext.__doc__ = genericpath._splitext.__doc__\n\n# Split a pathname into a drive specification and the rest of the\n# path.  Useful on DOS/Windows/NT; on Unix, the drive is always empty.\n\ndef splitdrive(p):\n    \"\"\"Split a pathname into drive and path. On Posix, drive is always\n    empty.\"\"\"\n    return '', p\n\n\n# Return the tail (basename) part of a path, same as split(path)[1].\n\ndef basename(p):\n    \"\"\"Returns the final component of a pathname\"\"\"\n    i = p.rfind('/') + 1\n    return p[i:]\n\n\n# Return the head (dirname) part of a path, same as split(path)[0].\n\ndef dirname(p):\n    \"\"\"Returns the directory component of a pathname\"\"\"\n    i = p.rfind('/') + 1\n    head = p[:i]\n    if head and head != '/'*len(head):\n        head = head.rstrip('/')\n    return head\n\n\n# Is a path a symbolic link?\n# This will always return false on systems where os.lstat doesn't exist.\n\ndef islink(path):\n    \"\"\"Test whether a path is a symbolic link\"\"\"\n    try:\n        st = os.lstat(path)\n    except (os.error, AttributeError):\n        return False\n    return stat.S_ISLNK(st.st_mode)\n\n# Being true for dangling symbolic links is also useful.\n\ndef lexists(path):\n    \"\"\"Test whether a path exists.  Returns True for broken symbolic links\"\"\"\n    try:\n        os.lstat(path)\n    except os.error:\n        return False\n    return True\n\n\n# Are two filenames really pointing to the same file?\n\ndef samefile(f1, f2):\n    \"\"\"Test whether two pathnames reference the same actual file\"\"\"\n    s1 = os.stat(f1)\n    s2 = os.stat(f2)\n    return samestat(s1, s2)\n\n\n# Are two open files really referencing the same file?\n# (Not necessarily the same file descriptor!)\n\ndef sameopenfile(fp1, fp2):\n    \"\"\"Test whether two open file objects reference the same file\"\"\"\n    s1 = os.fstat(fp1)\n    s2 = os.fstat(fp2)\n    return samestat(s1, s2)\n\n\n# Are two stat buffers (obtained from stat, fstat or lstat)\n# describing the same file?\n\ndef samestat(s1, s2):\n    \"\"\"Test whether two stat buffers reference the same file\"\"\"\n    return s1.st_ino == s2.st_ino and \\\n           s1.st_dev == s2.st_dev\n\n\n# Is a path a mount point?\n# (Does this work for all UNIXes?  Is it even guaranteed to work by Posix?)\n\ndef ismount(path):\n    \"\"\"Test whether a path is a mount point\"\"\"\n    if islink(path):\n        # A symlink can never be a mount point\n        return False\n    try:\n        s1 = os.lstat(path)\n        s2 = os.lstat(join(path, '..'))\n    except os.error:\n        return False # It doesn't exist -- so not a mount point :-)\n    dev1 = s1.st_dev\n    dev2 = s2.st_dev\n    if dev1 != dev2:\n        return True     # path/.. on a different device as path\n    ino1 = s1.st_ino\n    ino2 = s2.st_ino\n    if ino1 == ino2:\n        return True     # path/.. is the same i-node as path\n    return False\n\n\n# Directory tree walk.\n# For each directory under top (including top itself, but excluding\n# '.' and '..'), func(arg, dirname, filenames) is called, where\n# dirname is the name of the directory and filenames is the list\n# of files (and subdirectories etc.) in the directory.\n# The func may modify the filenames list, to implement a filter,\n# or to impose a different order of visiting.\n\ndef walk(top, func, arg):\n    \"\"\"Directory tree walk with callback function.\n\n    For each directory in the directory tree rooted at top (including top\n    itself, but excluding '.' and '..'), call func(arg, dirname, fnames).\n    dirname is the name of the directory, and fnames a list of the names of\n    the files and subdirectories in dirname (excluding '.' and '..').  func\n    may modify the fnames list in-place (e.g. via del or slice assignment),\n    and walk will only recurse into the subdirectories whose names remain in\n    fnames; this can be used to implement a filter, or to impose a specific\n    order of visiting.  No semantics are defined for, or required of, arg,\n    beyond that arg is always passed to func.  It can be used, e.g., to pass\n    a filename pattern, or a mutable object designed to accumulate\n    statistics.  Passing None for arg is common.\"\"\"\n    warnings.warnpy3k(\"In 3.x, os.path.walk is removed in favor of os.walk.\",\n                      stacklevel=2)\n    try:\n        names = os.listdir(top)\n    except os.error:\n        return\n    func(arg, top, names)\n    for name in names:\n        name = join(top, name)\n        try:\n            st = os.lstat(name)\n        except os.error:\n            continue\n        if stat.S_ISDIR(st.st_mode):\n            walk(name, func, arg)\n\n\n# Expand paths beginning with '~' or '~user'.\n# '~' means $HOME; '~user' means that user's home directory.\n# If the path doesn't begin with '~', or if the user or $HOME is unknown,\n# the path is returned unchanged (leaving error reporting to whatever\n# function is called with the expanded path as argument).\n# See also module 'glob' for expansion of *, ? and [...] in pathnames.\n# (A function should also be defined to do full *sh-style environment\n# variable expansion.)\n\ndef expanduser(path):\n    \"\"\"Expand ~ and ~user constructions.  If user or $HOME is unknown,\n    do nothing.\"\"\"\n    if not path.startswith('~'):\n        return path\n    i = path.find('/', 1)\n    if i < 0:\n        i = len(path)\n    if i == 1:\n        if 'HOME' not in os.environ:\n            import pwd\n            userhome = pwd.getpwuid(os.getuid()).pw_dir\n        else:\n            userhome = os.environ['HOME']\n    else:\n        import pwd\n        try:\n            pwent = pwd.getpwnam(path[1:i])\n        except KeyError:\n            return path\n        userhome = pwent.pw_dir\n    userhome = userhome.rstrip('/')\n    return (userhome + path[i:]) or '/'\n\n\n# Expand paths containing shell variable substitutions.\n# This expands the forms $variable and ${variable} only.\n# Non-existent variables are left unchanged.\n\n_varprog = None\n_uvarprog = None\n\ndef expandvars(path):\n    \"\"\"Expand shell variables of form $var and ${var}.  Unknown variables\n    are left unchanged.\"\"\"\n    global _varprog, _uvarprog\n    if '$' not in path:\n        return path\n    if isinstance(path, _unicode):\n        if not _varprog:\n            import re\n            _varprog = re.compile(r'\\$(\\w+|\\{[^}]*\\})')\n        varprog = _varprog\n        encoding = sys.getfilesystemencoding()\n    else:\n        if not _uvarprog:\n            import re\n            _uvarprog = re.compile(_unicode(r'\\$(\\w+|\\{[^}]*\\})'), re.UNICODE)\n        varprog = _uvarprog\n        encoding = None\n    i = 0\n    while True:\n        m = varprog.search(path, i)\n        if not m:\n            break\n        i, j = m.span(0)\n        name = m.group(1)\n        if name.startswith('{') and name.endswith('}'):\n            name = name[1:-1]\n        if encoding:\n            name = name.encode(encoding)\n        if name in os.environ:\n            tail = path[j:]\n            value = os.environ[name]\n            if encoding:\n                value = value.decode(encoding)\n            path = path[:i] + value\n            i = len(path)\n            path += tail\n        else:\n            i = j\n    return path\n\n\n# Normalize a path, e.g. A//B, A/./B and A/foo/../B all become A/B.\n# It should be understood that this may change the meaning of the path\n# if it contains symbolic links!\n\ndef normpath(path):\n    \"\"\"Normalize path, eliminating double slashes, etc.\"\"\"\n    # Preserve unicode (if path is unicode)\n    slash, dot = (u'/', u'.') if isinstance(path, _unicode) else ('/', '.')\n    if path == '':\n        return dot\n    initial_slashes = path.startswith('/')\n    # POSIX allows one or two initial slashes, but treats three or more\n    # as single slash.\n    if (initial_slashes and\n        path.startswith('//') and not path.startswith('///')):\n        initial_slashes = 2\n    comps = path.split('/')\n    new_comps = []\n    for comp in comps:\n        if comp in ('', '.'):\n            continue\n        if (comp != '..' or (not initial_slashes and not new_comps) or\n             (new_comps and new_comps[-1] == '..')):\n            new_comps.append(comp)\n        elif new_comps:\n            new_comps.pop()\n    comps = new_comps\n    path = slash.join(comps)\n    if initial_slashes:\n        path = slash*initial_slashes + path\n    return path or dot\n\n\ndef abspath(path):\n    \"\"\"Return an absolute path.\"\"\"\n    if not isabs(path):\n        if isinstance(path, _unicode):\n            cwd = os.getcwdu()\n        else:\n            cwd = os.getcwd()\n        path = join(cwd, path)\n    return normpath(path)\n\n\n# Return a canonical path (i.e. the absolute location of a file on the\n# filesystem).\n\ndef realpath(filename):\n    \"\"\"Return the canonical path of the specified filename, eliminating any\nsymbolic links encountered in the path.\"\"\"\n    path, ok = _joinrealpath('', filename, {})\n    return abspath(path)\n\n# Join two paths, normalizing ang eliminating any symbolic links\n# encountered in the second path.\ndef _joinrealpath(path, rest, seen):\n    if isabs(rest):\n        rest = rest[1:]\n        path = sep\n\n    while rest:\n        name, _, rest = rest.partition(sep)\n        if not name or name == curdir:\n            # current dir\n            continue\n        if name == pardir:\n            # parent dir\n            if path:\n                path, name = split(path)\n                if name == pardir:\n                    path = join(path, pardir, pardir)\n            else:\n                path = pardir\n            continue\n        newpath = join(path, name)\n        if not islink(newpath):\n            path = newpath\n            continue\n        # Resolve the symbolic link\n        if newpath in seen:\n            # Already seen this path\n            path = seen[newpath]\n            if path is not None:\n                # use cached value\n                continue\n            # The symlink is not resolved, so we must have a symlink loop.\n            # Return already resolved part + rest of the path unchanged.\n            return join(newpath, rest), False\n        seen[newpath] = None # not resolved symlink\n        path, ok = _joinrealpath(path, os.readlink(newpath), seen)\n        if not ok:\n            return join(path, rest), False\n        seen[newpath] = path # resolved symlink\n\n    return path, True\n\n\nsupports_unicode_filenames = (sys.platform == 'darwin')\n\ndef relpath(path, start=curdir):\n    \"\"\"Return a relative version of a path\"\"\"\n\n    if not path:\n        raise ValueError(\"no path specified\")\n\n    start_list = [x for x in abspath(start).split(sep) if x]\n    path_list = [x for x in abspath(path).split(sep) if x]\n\n    # Work out how much of the filepath is shared by start and path.\n    i = len(commonprefix([start_list, path_list]))\n\n    rel_list = [pardir] * (len(start_list)-i) + path_list[i:]\n    if not rel_list:\n        return curdir\n    return join(*rel_list)\n", 
    "pprint": "#  Author:      Fred L. Drake, Jr.\n#               fdrake@acm.org\n#\n#  This is a simple little module I wrote to make life easier.  I didn't\n#  see anything quite like it in the library, though I may have overlooked\n#  something.  I wrote this when I was trying to read some heavily nested\n#  tuples with fairly non-descriptive content.  This is modeled very much\n#  after Lisp/Scheme - style pretty-printing of lists.  If you find it\n#  useful, thank small children who sleep at night.\n\n\"\"\"Support to pretty-print lists, tuples, & dictionaries recursively.\n\nVery simple, but useful, especially in debugging data structures.\n\nClasses\n-------\n\nPrettyPrinter()\n    Handle pretty-printing operations onto a stream using a configured\n    set of formatting parameters.\n\nFunctions\n---------\n\npformat()\n    Format a Python object into a pretty-printed representation.\n\npprint()\n    Pretty-print a Python object to a stream [default is sys.stdout].\n\nsaferepr()\n    Generate a 'standard' repr()-like value, but protect against recursive\n    data structures.\n\n\"\"\"\n\nimport sys as _sys\nimport warnings\n\ntry:\n    from cStringIO import StringIO as _StringIO\nexcept ImportError:\n    from StringIO import StringIO as _StringIO\n\n__all__ = [\"pprint\",\"pformat\",\"isreadable\",\"isrecursive\",\"saferepr\",\n           \"PrettyPrinter\"]\n\n# cache these for faster access:\n_commajoin = \", \".join\n_id = id\n_len = len\n_type = type\n\n\ndef pprint(object, stream=None, indent=1, width=80, depth=None):\n    \"\"\"Pretty-print a Python object to a stream [default is sys.stdout].\"\"\"\n    printer = PrettyPrinter(\n        stream=stream, indent=indent, width=width, depth=depth)\n    printer.pprint(object)\n\ndef pformat(object, indent=1, width=80, depth=None):\n    \"\"\"Format a Python object into a pretty-printed representation.\"\"\"\n    return PrettyPrinter(indent=indent, width=width, depth=depth).pformat(object)\n\ndef saferepr(object):\n    \"\"\"Version of repr() which can handle recursive data structures.\"\"\"\n    return _safe_repr(object, {}, None, 0)[0]\n\ndef isreadable(object):\n    \"\"\"Determine if saferepr(object) is readable by eval().\"\"\"\n    return _safe_repr(object, {}, None, 0)[1]\n\ndef isrecursive(object):\n    \"\"\"Determine if object requires a recursive representation.\"\"\"\n    return _safe_repr(object, {}, None, 0)[2]\n\ndef _sorted(iterable):\n    with warnings.catch_warnings():\n        if _sys.py3kwarning:\n            warnings.filterwarnings(\"ignore\", \"comparing unequal types \"\n                                    \"not supported\", DeprecationWarning)\n        return sorted(iterable)\n\nclass PrettyPrinter:\n    def __init__(self, indent=1, width=80, depth=None, stream=None):\n        \"\"\"Handle pretty printing operations onto a stream using a set of\n        configured parameters.\n\n        indent\n            Number of spaces to indent for each level of nesting.\n\n        width\n            Attempted maximum number of columns in the output.\n\n        depth\n            The maximum depth to print out nested structures.\n\n        stream\n            The desired output stream.  If omitted (or false), the standard\n            output stream available at construction will be used.\n\n        \"\"\"\n        indent = int(indent)\n        width = int(width)\n        assert indent >= 0, \"indent must be >= 0\"\n        assert depth is None or depth > 0, \"depth must be > 0\"\n        assert width, \"width must be != 0\"\n        self._depth = depth\n        self._indent_per_level = indent\n        self._width = width\n        if stream is not None:\n            self._stream = stream\n        else:\n            self._stream = _sys.stdout\n\n    def pprint(self, object):\n        self._format(object, self._stream, 0, 0, {}, 0)\n        self._stream.write(\"\\n\")\n\n    def pformat(self, object):\n        sio = _StringIO()\n        self._format(object, sio, 0, 0, {}, 0)\n        return sio.getvalue()\n\n    def isrecursive(self, object):\n        return self.format(object, {}, 0, 0)[2]\n\n    def isreadable(self, object):\n        s, readable, recursive = self.format(object, {}, 0, 0)\n        return readable and not recursive\n\n    def _format(self, object, stream, indent, allowance, context, level):\n        level = level + 1\n        objid = _id(object)\n        if objid in context:\n            stream.write(_recursion(object))\n            self._recursive = True\n            self._readable = False\n            return\n        rep = self._repr(object, context, level - 1)\n        typ = _type(object)\n        sepLines = _len(rep) > (self._width - 1 - indent - allowance)\n        write = stream.write\n\n        if self._depth and level > self._depth:\n            write(rep)\n            return\n\n        r = getattr(typ, \"__repr__\", None)\n        if issubclass(typ, dict) and r == dict.__repr__:\n            write('{')\n            if self._indent_per_level > 1:\n                write((self._indent_per_level - 1) * ' ')\n            length = _len(object)\n            if length:\n                context[objid] = 1\n                indent = indent + self._indent_per_level\n                items = _sorted(object.items())\n                key, ent = items[0]\n                rep = self._repr(key, context, level)\n                write(rep)\n                write(': ')\n                self._format(ent, stream, indent + _len(rep) + 2,\n                              allowance + 1, context, level)\n                if length > 1:\n                    for key, ent in items[1:]:\n                        rep = self._repr(key, context, level)\n                        if sepLines:\n                            write(',\\n%s%s: ' % (' '*indent, rep))\n                        else:\n                            write(', %s: ' % rep)\n                        self._format(ent, stream, indent + _len(rep) + 2,\n                                      allowance + 1, context, level)\n                indent = indent - self._indent_per_level\n                del context[objid]\n            write('}')\n            return\n\n        if ((issubclass(typ, list) and r == list.__repr__) or\n            (issubclass(typ, tuple) and r == tuple.__repr__) or\n            (issubclass(typ, set) and r == set.__repr__) or\n            (issubclass(typ, frozenset) and r == frozenset.__repr__)\n           ):\n            length = _len(object)\n            if issubclass(typ, list):\n                write('[')\n                endchar = ']'\n            elif issubclass(typ, tuple):\n                write('(')\n                endchar = ')'\n            else:\n                if not length:\n                    write(rep)\n                    return\n                write(typ.__name__)\n                write('([')\n                endchar = '])'\n                indent += len(typ.__name__) + 1\n                object = _sorted(object)\n            if self._indent_per_level > 1 and sepLines:\n                write((self._indent_per_level - 1) * ' ')\n            if length:\n                context[objid] = 1\n                indent = indent + self._indent_per_level\n                self._format(object[0], stream, indent, allowance + 1,\n                             context, level)\n                if length > 1:\n                    for ent in object[1:]:\n                        if sepLines:\n                            write(',\\n' + ' '*indent)\n                        else:\n                            write(', ')\n                        self._format(ent, stream, indent,\n                                      allowance + 1, context, level)\n                indent = indent - self._indent_per_level\n                del context[objid]\n            if issubclass(typ, tuple) and length == 1:\n                write(',')\n            write(endchar)\n            return\n\n        write(rep)\n\n    def _repr(self, object, context, level):\n        repr, readable, recursive = self.format(object, context.copy(),\n                                                self._depth, level)\n        if not readable:\n            self._readable = False\n        if recursive:\n            self._recursive = True\n        return repr\n\n    def format(self, object, context, maxlevels, level):\n        \"\"\"Format object for a specific context, returning a string\n        and flags indicating whether the representation is 'readable'\n        and whether the object represents a recursive construct.\n        \"\"\"\n        return _safe_repr(object, context, maxlevels, level)\n\n\n# Return triple (repr_string, isreadable, isrecursive).\n\ndef _safe_repr(object, context, maxlevels, level):\n    typ = _type(object)\n    if typ is str:\n        if 'locale' not in _sys.modules:\n            return repr(object), True, False\n        if \"'\" in object and '\"' not in object:\n            closure = '\"'\n            quotes = {'\"': '\\\\\"'}\n        else:\n            closure = \"'\"\n            quotes = {\"'\": \"\\\\'\"}\n        qget = quotes.get\n        sio = _StringIO()\n        write = sio.write\n        for char in object:\n            if char.isalpha():\n                write(char)\n            else:\n                write(qget(char, repr(char)[1:-1]))\n        return (\"%s%s%s\" % (closure, sio.getvalue(), closure)), True, False\n\n    r = getattr(typ, \"__repr__\", None)\n    if issubclass(typ, dict) and r == dict.__repr__:\n        if not object:\n            return \"{}\", True, False\n        objid = _id(object)\n        if maxlevels and level >= maxlevels:\n            return \"{...}\", False, objid in context\n        if objid in context:\n            return _recursion(object), False, True\n        context[objid] = 1\n        readable = True\n        recursive = False\n        components = []\n        append = components.append\n        level += 1\n        saferepr = _safe_repr\n        for k, v in _sorted(object.items()):\n            krepr, kreadable, krecur = saferepr(k, context, maxlevels, level)\n            vrepr, vreadable, vrecur = saferepr(v, context, maxlevels, level)\n            append(\"%s: %s\" % (krepr, vrepr))\n            readable = readable and kreadable and vreadable\n            if krecur or vrecur:\n                recursive = True\n        del context[objid]\n        return \"{%s}\" % _commajoin(components), readable, recursive\n\n    if (issubclass(typ, list) and r == list.__repr__) or \\\n       (issubclass(typ, tuple) and r == tuple.__repr__):\n        if issubclass(typ, list):\n            if not object:\n                return \"[]\", True, False\n            format = \"[%s]\"\n        elif _len(object) == 1:\n            format = \"(%s,)\"\n        else:\n            if not object:\n                return \"()\", True, False\n            format = \"(%s)\"\n        objid = _id(object)\n        if maxlevels and level >= maxlevels:\n            return format % \"...\", False, objid in context\n        if objid in context:\n            return _recursion(object), False, True\n        context[objid] = 1\n        readable = True\n        recursive = False\n        components = []\n        append = components.append\n        level += 1\n        for o in object:\n            orepr, oreadable, orecur = _safe_repr(o, context, maxlevels, level)\n            append(orepr)\n            if not oreadable:\n                readable = False\n            if orecur:\n                recursive = True\n        del context[objid]\n        return format % _commajoin(components), readable, recursive\n\n    rep = repr(object)\n    return rep, (rep and not rep.startswith('<')), False\n\n\ndef _recursion(object):\n    return (\"<Recursion on %s with id=%s>\"\n            % (_type(object).__name__, _id(object)))\n\n\ndef _perfcheck(object=None):\n    import time\n    if object is None:\n        object = [(\"string\", (1, 2), [3, 4], {5: 6, 7: 8})] * 100000\n    p = PrettyPrinter()\n    t1 = time.time()\n    _safe_repr(object, {}, None, 0)\n    t2 = time.time()\n    p.pformat(object)\n    t3 = time.time()\n    print \"_safe_repr:\", t2 - t1\n    print \"pformat:\", t3 - t2\n\nif __name__ == \"__main__\":\n    _perfcheck()\n", 
    "promise.__init__": "from .promise import Promise, promise_for_dict, promisify, is_thenable\n\n__all__ = ['Promise', 'promise_for_dict', 'promisify', 'is_thenable']\n", 
    "promise.compat": "try:\n    from asyncio import Future, iscoroutine, ensure_future\nexcept ImportError:\n    class Future:\n        def __init__(self):\n            raise Exception(\"You need asyncio for using Futures\")\n\n    def ensure_future():\n        raise Exception(\"ensure_future needs asyncio for executing\")\n\n    def iscoroutine(obj):\n        return False\n\ntry:\n    from .iterate_promise import iterate_promise\nexcept SyntaxError:\n    def iterate_promise(promise):\n        raise Exception('You need \"yield from\" syntax for iterate in a Promise.')\n", 
    "promise.iterate_promise": "# flake8: noqa\n\n\ndef iterate_promise(promise):\n    if not promise.is_fulfilled:\n        yield from promise.future\n    assert promise.is_fulfilled\n    return promise.get()\n", 
    "promise.promise": "from threading import Event, RLock\nfrom .compat import Future, iscoroutine, ensure_future, iterate_promise\n\n\nclass CountdownLatch(object):\n\n    def __init__(self, count):\n        assert count >= 0\n        self._lock = RLock()\n        self._count = count\n\n    def dec(self):\n        with self._lock:\n            assert self._count > 0\n            self._count -= 1\n            # Return inside lock to return the correct value,\n            # otherwise an other thread could already have\n            # decremented again.\n            return self._count\n\n    @property\n    def count(self):\n        return self._count\n\n\nclass Promise(object):\n    \"\"\"\n    This is the Promise class that complies\n    Promises/A+ specification and test suite:\n    http://promises-aplus.github.io/promises-spec/\n    \"\"\"\n\n    # These are the potential states of a promise\n    PENDING = -1\n    REJECTED = 0\n    FULFILLED = 1\n\n    def __init__(self, fn=None):\n        \"\"\"\n        Initialize the Promise into a pending state.\n        \"\"\"\n        self._state = self.PENDING\n        self._value = None\n        self._reason = None\n        self._cb_lock = RLock()\n        self._callbacks = []\n        self._errbacks = []\n        self._event = Event()\n        self._future = None\n        if fn:\n            self.do_resolve(fn)\n\n    def __iter__(self):\n        return iterate_promise(self)\n\n    __await__ = __iter__\n\n    @property\n    def future(self):\n        if not self._future:\n            self._future = Future()\n            self.add_callback(self._future.set_result)\n            self.add_errback(self._future.set_exception)\n        return self._future\n\n    def do_resolve(self, fn):\n        self._done = False\n\n        def resolve_fn(x):\n            if self._done:\n                return\n            self._done = True\n            self.fulfill(x)\n\n        def reject_fn(x):\n            if self._done:\n                return\n            self._done = True\n            self.reject(x)\n        try:\n            fn(resolve_fn, reject_fn)\n        except Exception as e:\n            self.reject(e)\n\n    @classmethod\n    def fulfilled(cls, x):\n        p = cls()\n        p.fulfill(x)\n        return p\n\n    @classmethod\n    def rejected(cls, reason):\n        p = cls()\n        p.reject(reason)\n        return p\n\n    def fulfill(self, x):\n        \"\"\"\n        Fulfill the promise with a given value.\n        \"\"\"\n\n        if self is x:\n            raise TypeError(\"Cannot resolve promise with itself.\")\n        elif is_thenable(x):\n            try:\n                self.promisify(x).done(self.fulfill, self.reject)\n            except Exception as e:\n                self.reject(e)\n        else:\n            self._fulfill(x)\n\n    resolve = fulfilled\n\n    def _fulfill(self, value):\n        with self._cb_lock:\n            if self._state != self.PENDING:\n                return\n\n            self._value = value\n            self._state = self.FULFILLED\n\n            callbacks = self._callbacks\n            # We will never call these callbacks again, so allow\n            # them to be garbage collected.  This is important since\n            # they probably include closures which are binding variables\n            # that might otherwise be garbage collected.\n            #\n            # Prevent future appending\n            self._callbacks = None\n\n            # Notify all waiting\n            self._event.set()\n\n        for callback in callbacks:\n            try:\n                callback(value)\n            except Exception:\n                # Ignore errors in callbacks\n                pass\n\n    def reject(self, reason):\n        \"\"\"\n        Reject this promise for a given reason.\n        \"\"\"\n        assert isinstance(reason, Exception)\n\n        with self._cb_lock:\n            if self._state != self.PENDING:\n                return\n\n            self._reason = reason\n            self._state = self.REJECTED\n\n            errbacks = self._errbacks\n            # We will never call these errbacks again, so allow\n            # them to be garbage collected.  This is important since\n            # they probably include closures which are binding variables\n            # that might otherwise be garbage collected.\n            #\n            # Prevent future appending\n            self._errbacks = None\n\n            # Notify all waiting\n            self._event.set()\n\n        for errback in errbacks:\n            try:\n                errback(reason)\n            except Exception:\n                # Ignore errors in errback\n                pass\n\n    @property\n    def is_pending(self):\n        \"\"\"Indicate whether the Promise is still pending. Could be wrong the moment the function returns.\"\"\"\n        return self._state == self.PENDING\n\n    @property\n    def is_fulfilled(self):\n        \"\"\"Indicate whether the Promise has been fulfilled. Could be wrong the moment the function returns.\"\"\"\n        return self._state == self.FULFILLED\n\n    @property\n    def is_rejected(self):\n        \"\"\"Indicate whether the Promise has been rejected. Could be wrong the moment the function returns.\"\"\"\n        return self._state == self.REJECTED\n\n    @property\n    def value(self):\n        return self._value\n\n    @property\n    def reason(self):\n        return self._reason\n\n    def get(self, timeout=None):\n        \"\"\"Get the value of the promise, waiting if necessary.\"\"\"\n        self.wait(timeout)\n\n        if self._state == self.PENDING:\n            raise ValueError(\"Value not available, promise is still pending\")\n        elif self._state == self.FULFILLED:\n            return self._value\n        else:\n            raise self._reason\n\n    def wait(self, timeout=None):\n        \"\"\"\n        An implementation of the wait method which doesn't involve\n        polling but instead utilizes a \"real\" synchronization\n        scheme.\n        \"\"\"\n        self._event.wait(timeout)\n\n    def add_callback(self, f):\n        \"\"\"\n        Add a callback for when this promis is fulfilled.  Note that\n        if you intend to use the value of the promise somehow in\n        the callback, it is more convenient to use the 'then' method.\n        \"\"\"\n        assert callable(f)\n\n        with self._cb_lock:\n            if self._state == self.PENDING:\n                self._callbacks.append(f)\n                return\n\n        # This is a correct performance optimization in case of concurrency.\n        # State can never change once it is not PENDING anymore and is thus safe to read\n        # without acquiring the lock.\n        if self._state == self.FULFILLED:\n            f(self._value)\n        else:\n            pass\n\n    def add_errback(self, f):\n        \"\"\"\n        Add a callback for when this promis is rejected.  Note that\n        if you intend to use the rejection reason of the promise\n        somehow in the callback, it is more convenient to use\n        the 'then' method.\n        \"\"\"\n        assert callable(f)\n\n        with self._cb_lock:\n            if self._state == self.PENDING:\n                self._errbacks.append(f)\n                return\n\n        # This is a correct performance optimization in case of concurrency.\n        # State can never change once it is not PENDING anymore and is thus safe to read\n        # without acquiring the lock.\n        if self._state == self.REJECTED:\n            f(self._reason)\n        else:\n            pass\n\n    def catch(self, on_rejection):\n        \"\"\"\n        This method returns a Promise and deals with rejected cases only.\n        It behaves the same as calling Promise.then(None, on_rejection).\n        \"\"\"\n        return self.then(None, on_rejection)\n\n    def done(self, success=None, failure=None):\n        \"\"\"\n        This method takes two optional arguments.  The first argument\n        is used if the \"self promise\" is fulfilled and the other is\n        used if the \"self promise\" is rejected. In contrast to then,\n        the return value of these callback is ignored and nothing is\n        returned.\n        \"\"\"\n        with self._cb_lock:\n            if success is not None:\n                self.add_callback(success)\n            if failure is not None:\n                self.add_errback(failure)\n\n    def done_all(self, *handlers):\n        \"\"\"\n        :type handlers: list[(object) -> object] | list[((object) -> object, (object) -> object)]\n        \"\"\"\n        if len(handlers) == 0:\n            return\n        elif len(handlers) == 1 and isinstance(handlers[0], list):\n            handlers = handlers[0]\n\n        for handler in handlers:\n            if isinstance(handler, tuple):\n                s, f = handler\n\n                self.done(s, f)\n            elif isinstance(handler, dict):\n                s = handler.get('success')\n                f = handler.get('failure')\n\n                self.done(s, f)\n            else:\n                self.done(success=handler)\n\n    def then(self, success=None, failure=None):\n        \"\"\"\n        This method takes two optional arguments.  The first argument\n        is used if the \"self promise\" is fulfilled and the other is\n        used if the \"self promise\" is rejected.  In either case, this\n        method returns another promise that effectively represents\n        the result of either the first of the second argument (in the\n        case that the \"self promise\" is fulfilled or rejected,\n        respectively).\n        Each argument can be either:\n          * None - Meaning no action is taken\n          * A function - which will be called with either the value\n            of the \"self promise\" or the reason for rejection of\n            the \"self promise\".  The function may return:\n            * A value - which will be used to fulfill the promise\n              returned by this method.\n            * A promise - which, when fulfilled or rejected, will\n              cascade its value or reason to the promise returned\n              by this method.\n          * A value - which will be assigned as either the value\n            or the reason for the promise returned by this method\n            when the \"self promise\" is either fulfilled or rejected,\n            respectively.\n        :type success: (object) -> object\n        :type failure: (object) -> object\n        :rtype : Promise\n        \"\"\"\n        ret = self.__class__()\n\n        def call_and_fulfill(v):\n            \"\"\"\n            A callback to be invoked if the \"self promise\"\n            is fulfilled.\n            \"\"\"\n            try:\n                if callable(success):\n                    ret.fulfill(success(v))\n                else:\n                    ret.fulfill(v)\n            except Exception as e:\n                ret.reject(e)\n\n        def call_and_reject(r):\n            \"\"\"\n            A callback to be invoked if the \"self promise\"\n            is rejected.\n            \"\"\"\n            try:\n                if callable(failure):\n                    ret.fulfill(failure(r))\n                else:\n                    ret.reject(r)\n            except Exception as e:\n                ret.reject(e)\n\n        self.done(call_and_fulfill, call_and_reject)\n\n        return ret\n\n    def then_all(self, *handlers):\n        \"\"\"\n        Utility function which calls 'then' for each handler provided. Handler can either\n        be a function in which case it is used as success handler, or a tuple containing\n        the success and the failure handler, where each of them could be None.\n        :type handlers: list[(object) -> object] | list[((object) -> object, (object) -> object)]\n        :param handlers\n        :rtype : list[Promise]\n        \"\"\"\n        if len(handlers) == 0:\n            return []\n        elif len(handlers) == 1 and isinstance(handlers[0], list):\n            handlers = handlers[0]\n\n        promises = []\n\n        for handler in handlers:\n            if isinstance(handler, tuple):\n                s, f = handler\n\n                promises.append(self.then(s, f))\n            elif isinstance(handler, dict):\n                s = handler.get('success')\n                f = handler.get('failure')\n\n                promises.append(self.then(s, f))\n            else:\n                promises.append(self.then(success=handler))\n\n        return promises\n\n    @classmethod\n    def all(cls, values_or_promises):\n        \"\"\"\n        A special function that takes a bunch of promises\n        and turns them into a promise for a vector of values.\n        In other words, this turns an list of promises for values\n        into a promise for a list of values.\n        \"\"\"\n        promises = list(filter(is_thenable, values_or_promises))\n        if len(promises) == 0:\n            # All the values or promises are resolved\n            return cls.fulfilled(values_or_promises)\n\n        all_promise = cls()\n        counter = CountdownLatch(len(promises))\n\n        def handle_success(_):\n            if counter.dec() == 0:\n                values = list(map(lambda p: p.value if p in promises else p, values_or_promises))\n                all_promise.fulfill(values)\n\n        for p in promises:\n            cls.promisify(p).done(handle_success, all_promise.reject)\n\n        return all_promise\n\n    @classmethod\n    def promisify(cls, obj):\n        if isinstance(obj, cls):\n            return obj\n        elif is_future(obj):\n            promise = cls()\n            obj.add_done_callback(_process_future_result(promise))\n            return promise\n        elif hasattr(obj, \"done\") and callable(getattr(obj, \"done\")):\n            p = cls()\n            obj.done(p.fulfill, p.reject)\n            return p\n        elif hasattr(obj, \"then\") and callable(getattr(obj, \"then\")):\n            p = cls()\n            obj.then(p.fulfill, p.reject)\n            return p\n        elif iscoroutine(obj):\n            return cls.promisify(ensure_future(obj))\n        else:\n            raise TypeError(\"Object is not a Promise like object.\")\n\n    @classmethod\n    def for_dict(cls, m):\n        \"\"\"\n        A special function that takes a dictionary of promises\n        and turns them into a promise for a dictionary of values.\n        In other words, this turns an dictionary of promises for values\n        into a promise for a dictionary of values.\n        \"\"\"\n        if not m:\n            return cls.fulfilled({})\n\n        keys, values = zip(*m.items())\n        dict_type = type(m)\n\n        def handle_success(resolved_values):\n            return dict_type(zip(keys, resolved_values))\n\n        return cls.all(values).then(handle_success)\n\n\npromisify = Promise.promisify\npromise_for_dict = Promise.for_dict\n\n\ndef _process_future_result(promise):\n    def handle_future_result(future):\n        exception = future.exception()\n        if exception:\n            promise.reject(exception)\n        else:\n            promise.fulfill(future.result())\n\n    return handle_future_result\n\n\ndef is_future(obj):\n    return hasattr(obj, \"add_done_callback\") and callable(getattr(obj, \"add_done_callback\"))\n\n\ndef is_thenable(obj):\n    \"\"\"\n    A utility function to determine if the specified\n    object is a promise using \"duck typing\".\n    \"\"\"\n    return isinstance(obj, Promise) or is_future(obj) or (\n        hasattr(obj, \"done\") and callable(getattr(obj, \"done\"))) or (\n        hasattr(obj, \"then\") and callable(getattr(obj, \"then\")))\n", 
    "pwd": "# ctypes implementation: Victor Stinner, 2008-05-08\n\"\"\"\nThis module provides access to the Unix password database.\nIt is available on all Unix versions.\n\nPassword database entries are reported as 7-tuples containing the following\nitems from the password database (see `<pwd.h>'), in order:\npw_name, pw_passwd, pw_uid, pw_gid, pw_gecos, pw_dir, pw_shell.\nThe uid and gid items are integers, all others are strings. An\nexception is raised if the entry asked for cannot be found.\n\"\"\"\n\nfrom _pwdgrp_cffi import ffi, lib\nimport _structseq\n\ntry: from __pypy__ import builtinify\nexcept ImportError: builtinify = lambda f: f\n\n\nclass struct_passwd:\n    \"\"\"\n    pwd.struct_passwd: Results from getpw*() routines.\n\n    This object may be accessed either as a tuple of\n      (pw_name,pw_passwd,pw_uid,pw_gid,pw_gecos,pw_dir,pw_shell)\n    or via the object attributes as named in the above tuple.\n    \"\"\"\n    __metaclass__ = _structseq.structseqtype\n    name = \"pwd.struct_passwd\"\n\n    pw_name = _structseq.structseqfield(0)\n    pw_passwd = _structseq.structseqfield(1)\n    pw_uid = _structseq.structseqfield(2)\n    pw_gid = _structseq.structseqfield(3)\n    pw_gecos = _structseq.structseqfield(4)\n    pw_dir = _structseq.structseqfield(5)\n    pw_shell = _structseq.structseqfield(6)\n\n\ndef _mkpwent(pw):\n    return struct_passwd([\n        ffi.string(pw.pw_name),\n        ffi.string(pw.pw_passwd),\n        pw.pw_uid,\n        pw.pw_gid,\n        ffi.string(pw.pw_gecos),\n        ffi.string(pw.pw_dir),\n        ffi.string(pw.pw_shell)])\n\n@builtinify\ndef getpwuid(uid):\n    \"\"\"\n    getpwuid(uid) -> (pw_name,pw_passwd,pw_uid,\n                      pw_gid,pw_gecos,pw_dir,pw_shell)\n    Return the password database entry for the given numeric user ID.\n    See pwd.__doc__ for more on password database entries.\n    \"\"\"\n    pw = lib.getpwuid(uid)\n    if not pw:\n        raise KeyError(\"getpwuid(): uid not found: %s\" % uid)\n    return _mkpwent(pw)\n\n@builtinify\ndef getpwnam(name):\n    \"\"\"\n    getpwnam(name) -> (pw_name,pw_passwd,pw_uid,\n                        pw_gid,pw_gecos,pw_dir,pw_shell)\n    Return the password database entry for the given user name.\n    See pwd.__doc__ for more on password database entries.\n    \"\"\"\n    if not isinstance(name, basestring):\n        raise TypeError(\"expected string\")\n    name = str(name)\n    pw = lib.getpwnam(name)\n    if not pw:\n        raise KeyError(\"getpwname(): name not found: %s\" % name)\n    return _mkpwent(pw)\n\n@builtinify\ndef getpwall():\n    \"\"\"\n    getpwall() -> list_of_entries\n    Return a list of all available password database entries, in arbitrary order.\n    See pwd.__doc__ for more on password database entries.\n    \"\"\"\n    users = []\n    lib.setpwent()\n    while True:\n        pw = lib.getpwent()\n        if not pw:\n            break\n        users.append(_mkpwent(pw))\n    lib.endpwent()\n    return users\n\n__all__ = ('struct_passwd', 'getpwuid', 'getpwnam', 'getpwall')\n\nif __name__ == \"__main__\":\n# Uncomment next line to test CPython implementation\n#    from pwd import getpwuid, getpwnam, getpwall\n    from os import getuid\n    uid = getuid()\n    pw = getpwuid(uid)\n    print(\"uid %s: %s\" % (pw.pw_uid, pw))\n    name = pw.pw_name\n    print(\"name %r: %s\" % (name, getpwnam(name)))\n    print(\"All:\")\n    for pw in getpwall():\n        print(pw)\n", 
    "random": "\"\"\"Random variable generators.\n\n    integers\n    --------\n           uniform within range\n\n    sequences\n    ---------\n           pick random element\n           pick random sample\n           generate random permutation\n\n    distributions on the real line:\n    ------------------------------\n           uniform\n           triangular\n           normal (Gaussian)\n           lognormal\n           negative exponential\n           gamma\n           beta\n           pareto\n           Weibull\n\n    distributions on the circle (angles 0 to 2pi)\n    ---------------------------------------------\n           circular uniform\n           von Mises\n\nGeneral notes on the underlying Mersenne Twister core generator:\n\n* The period is 2**19937-1.\n* It is one of the most extensively tested generators in existence.\n* Without a direct way to compute N steps forward, the semantics of\n  jumpahead(n) are weakened to simply jump to another distant state and rely\n  on the large period to avoid overlapping sequences.\n* The random() method is implemented in C, executes in a single Python step,\n  and is, therefore, threadsafe.\n\n\"\"\"\n\nfrom __future__ import division\nfrom warnings import warn as _warn\nfrom math import log as _log, exp as _exp, pi as _pi, e as _e, ceil as _ceil\nfrom math import sqrt as _sqrt, acos as _acos, cos as _cos, sin as _sin\nfrom os import urandom as _urandom\nfrom binascii import hexlify as _hexlify\nimport hashlib as _hashlib\n\n__all__ = [\"Random\",\"seed\",\"random\",\"uniform\",\"randint\",\"choice\",\"sample\",\n           \"randrange\",\"shuffle\",\"normalvariate\",\"lognormvariate\",\n           \"expovariate\",\"vonmisesvariate\",\"gammavariate\",\"triangular\",\n           \"gauss\",\"betavariate\",\"paretovariate\",\"weibullvariate\",\n           \"getstate\",\"setstate\",\"jumpahead\", \"WichmannHill\", \"getrandbits\",\n           \"SystemRandom\"]\n\nNV_MAGICCONST = 4 * _exp(-0.5)/_sqrt(2.0)\nTWOPI = 2.0*_pi\nLOG4 = _log(4.0)\nSG_MAGICCONST = 1.0 + _log(4.5)\nBPF = 53        # Number of bits in a float\nRECIP_BPF = 2**-BPF\n\n\n# Translated by Guido van Rossum from C source provided by\n# Adrian Baddeley.  Adapted by Raymond Hettinger for use with\n# the Mersenne Twister  and os.urandom() core generators.\n\nimport _random\n\nclass Random(_random.Random):\n    \"\"\"Random number generator base class used by bound module functions.\n\n    Used to instantiate instances of Random to get generators that don't\n    share state.  Especially useful for multi-threaded programs, creating\n    a different instance of Random for each thread, and using the jumpahead()\n    method to ensure that the generated sequences seen by each thread don't\n    overlap.\n\n    Class Random can also be subclassed if you want to use a different basic\n    generator of your own devising: in that case, override the following\n    methods: random(), seed(), getstate(), setstate() and jumpahead().\n    Optionally, implement a getrandbits() method so that randrange() can cover\n    arbitrarily large ranges.\n\n    \"\"\"\n\n    VERSION = 3     # used by getstate/setstate\n\n    def __init__(self, x=None):\n        \"\"\"Initialize an instance.\n\n        Optional argument x controls seeding, as for Random.seed().\n        \"\"\"\n\n        self.seed(x)\n        self.gauss_next = None\n\n    def seed(self, a=None):\n        \"\"\"Initialize internal state from hashable object.\n\n        None or no argument seeds from current time or from an operating\n        system specific randomness source if available.\n\n        If a is not None or an int or long, hash(a) is used instead.\n        \"\"\"\n\n        if a is None:\n            try:\n                # Seed with enough bytes to span the 19937 bit\n                # state space for the Mersenne Twister\n                a = long(_hexlify(_urandom(2500)), 16)\n            except NotImplementedError:\n                import time\n                a = long(time.time() * 256) # use fractional seconds\n\n        super(Random, self).seed(a)\n        self.gauss_next = None\n\n    def getstate(self):\n        \"\"\"Return internal state; can be passed to setstate() later.\"\"\"\n        return self.VERSION, super(Random, self).getstate(), self.gauss_next\n\n    def setstate(self, state):\n        \"\"\"Restore internal state from object returned by getstate().\"\"\"\n        version = state[0]\n        if version == 3:\n            version, internalstate, self.gauss_next = state\n            super(Random, self).setstate(internalstate)\n        elif version == 2:\n            version, internalstate, self.gauss_next = state\n            # In version 2, the state was saved as signed ints, which causes\n            #   inconsistencies between 32/64-bit systems. The state is\n            #   really unsigned 32-bit ints, so we convert negative ints from\n            #   version 2 to positive longs for version 3.\n            try:\n                internalstate = tuple( long(x) % (2**32) for x in internalstate )\n            except ValueError, e:\n                raise TypeError, e\n            super(Random, self).setstate(internalstate)\n        else:\n            raise ValueError(\"state with version %s passed to \"\n                             \"Random.setstate() of version %s\" %\n                             (version, self.VERSION))\n\n    def jumpahead(self, n):\n        \"\"\"Change the internal state to one that is likely far away\n        from the current state.  This method will not be in Py3.x,\n        so it is better to simply reseed.\n        \"\"\"\n        # The super.jumpahead() method uses shuffling to change state,\n        # so it needs a large and \"interesting\" n to work with.  Here,\n        # we use hashing to create a large n for the shuffle.\n        s = repr(n) + repr(self.getstate())\n        n = int(_hashlib.new('sha512', s).hexdigest(), 16)\n        super(Random, self).jumpahead(n)\n\n## ---- Methods below this point do not need to be overridden when\n## ---- subclassing for the purpose of using a different core generator.\n\n## -------------------- pickle support  -------------------\n\n    def __getstate__(self): # for pickle\n        return self.getstate()\n\n    def __setstate__(self, state):  # for pickle\n        self.setstate(state)\n\n    def __reduce__(self):\n        return self.__class__, (), self.getstate()\n\n## -------------------- integer methods  -------------------\n\n    def randrange(self, start, stop=None, step=1, _int=int, _maxwidth=1L<<BPF):\n        \"\"\"Choose a random item from range(start, stop[, step]).\n\n        This fixes the problem with randint() which includes the\n        endpoint; in Python this is usually not what you want.\n\n        \"\"\"\n\n        # This code is a bit messy to make it fast for the\n        # common case while still doing adequate error checking.\n        istart = _int(start)\n        if istart != start:\n            raise ValueError, \"non-integer arg 1 for randrange()\"\n        if stop is None:\n            if istart > 0:\n                if istart >= _maxwidth:\n                    return self._randbelow(istart)\n                return _int(self.random() * istart)\n            raise ValueError, \"empty range for randrange()\"\n\n        # stop argument supplied.\n        istop = _int(stop)\n        if istop != stop:\n            raise ValueError, \"non-integer stop for randrange()\"\n        width = istop - istart\n        if step == 1 and width > 0:\n            # Note that\n            #     int(istart + self.random()*width)\n            # instead would be incorrect.  For example, consider istart\n            # = -2 and istop = 0.  Then the guts would be in\n            # -2.0 to 0.0 exclusive on both ends (ignoring that random()\n            # might return 0.0), and because int() truncates toward 0, the\n            # final result would be -1 or 0 (instead of -2 or -1).\n            #     istart + int(self.random()*width)\n            # would also be incorrect, for a subtler reason:  the RHS\n            # can return a long, and then randrange() would also return\n            # a long, but we're supposed to return an int (for backward\n            # compatibility).\n\n            if width >= _maxwidth:\n                return _int(istart + self._randbelow(width))\n            return _int(istart + _int(self.random()*width))\n        if step == 1:\n            raise ValueError, \"empty range for randrange() (%d,%d, %d)\" % (istart, istop, width)\n\n        # Non-unit step argument supplied.\n        istep = _int(step)\n        if istep != step:\n            raise ValueError, \"non-integer step for randrange()\"\n        if istep > 0:\n            n = (width + istep - 1) // istep\n        elif istep < 0:\n            n = (width + istep + 1) // istep\n        else:\n            raise ValueError, \"zero step for randrange()\"\n\n        if n <= 0:\n            raise ValueError, \"empty range for randrange()\"\n\n        if n >= _maxwidth:\n            return istart + istep*self._randbelow(n)\n        return istart + istep*_int(self.random() * n)\n\n    def randint(self, a, b):\n        \"\"\"Return random integer in range [a, b], including both end points.\n        \"\"\"\n\n        return self.randrange(a, b+1)\n\n    def _randbelow(self, n, _log=_log, _int=int, _maxwidth=1L<<BPF):\n        \"\"\"Return a random int in the range [0,n)\n\n        Handles the case where n has more bits than returned\n        by a single call to the underlying generator.\n        \"\"\"\n\n        try:\n            getrandbits = self.getrandbits\n        except AttributeError:\n            pass\n        else:\n            # Only call self.getrandbits if the original random() builtin method\n            # has not been overridden or if a new getrandbits() was supplied.\n            # This assures that the two methods correspond.\n            if (self.random == super(Random, self).random or\n                    getrandbits != super(Random, self).getrandbits):\n                k = _int(1.00001 + _log(n-1, 2.0))   # 2**k > n-1 > 2**(k-2)\n                r = getrandbits(k)\n                while r >= n:\n                    r = getrandbits(k)\n                return r\n        if n >= _maxwidth:\n            _warn(\"Underlying random() generator does not supply \\n\"\n                \"enough bits to choose from a population range this large\")\n        return _int(self.random() * n)\n\n## -------------------- sequence methods  -------------------\n\n    def choice(self, seq):\n        \"\"\"Choose a random element from a non-empty sequence.\"\"\"\n        return seq[int(self.random() * len(seq))]  # raises IndexError if seq is empty\n\n    def shuffle(self, x, random=None):\n        \"\"\"x, random=random.random -> shuffle list x in place; return None.\n\n        Optional arg random is a 0-argument function returning a random\n        float in [0.0, 1.0); by default, the standard random.random.\n\n        \"\"\"\n\n        if random is None:\n            random = self.random\n        _int = int\n        for i in reversed(xrange(1, len(x))):\n            # pick an element in x[:i+1] with which to exchange x[i]\n            j = _int(random() * (i+1))\n            x[i], x[j] = x[j], x[i]\n\n    def sample(self, population, k):\n        \"\"\"Chooses k unique random elements from a population sequence.\n\n        Returns a new list containing elements from the population while\n        leaving the original population unchanged.  The resulting list is\n        in selection order so that all sub-slices will also be valid random\n        samples.  This allows raffle winners (the sample) to be partitioned\n        into grand prize and second place winners (the subslices).\n\n        Members of the population need not be hashable or unique.  If the\n        population contains repeats, then each occurrence is a possible\n        selection in the sample.\n\n        To choose a sample in a range of integers, use xrange as an argument.\n        This is especially fast and space efficient for sampling from a\n        large population:   sample(xrange(10000000), 60)\n        \"\"\"\n\n        # Sampling without replacement entails tracking either potential\n        # selections (the pool) in a list or previous selections in a set.\n\n        # When the number of selections is small compared to the\n        # population, then tracking selections is efficient, requiring\n        # only a small set and an occasional reselection.  For\n        # a larger number of selections, the pool tracking method is\n        # preferred since the list takes less space than the\n        # set and it doesn't suffer from frequent reselections.\n\n        n = len(population)\n        if not 0 <= k <= n:\n            raise ValueError(\"sample larger than population\")\n        random = self.random\n        _int = int\n        result = [None] * k\n        setsize = 21        # size of a small set minus size of an empty list\n        if k > 5:\n            setsize += 4 ** _ceil(_log(k * 3, 4)) # table size for big sets\n        if n <= setsize or hasattr(population, \"keys\"):\n            # An n-length list is smaller than a k-length set, or this is a\n            # mapping type so the other algorithm wouldn't work.\n            pool = list(population)\n            for i in xrange(k):         # invariant:  non-selected at [0,n-i)\n                j = _int(random() * (n-i))\n                result[i] = pool[j]\n                pool[j] = pool[n-i-1]   # move non-selected item into vacancy\n        else:\n            try:\n                selected = set()\n                selected_add = selected.add\n                for i in xrange(k):\n                    j = _int(random() * n)\n                    while j in selected:\n                        j = _int(random() * n)\n                    selected_add(j)\n                    result[i] = population[j]\n            except (TypeError, KeyError):   # handle (at least) sets\n                if isinstance(population, list):\n                    raise\n                return self.sample(tuple(population), k)\n        return result\n\n## -------------------- real-valued distributions  -------------------\n\n## -------------------- uniform distribution -------------------\n\n    def uniform(self, a, b):\n        \"Get a random number in the range [a, b) or [a, b] depending on rounding.\"\n        return a + (b-a) * self.random()\n\n## -------------------- triangular --------------------\n\n    def triangular(self, low=0.0, high=1.0, mode=None):\n        \"\"\"Triangular distribution.\n\n        Continuous distribution bounded by given lower and upper limits,\n        and having a given mode value in-between.\n\n        http://en.wikipedia.org/wiki/Triangular_distribution\n\n        \"\"\"\n        u = self.random()\n        try:\n            c = 0.5 if mode is None else (mode - low) / (high - low)\n        except ZeroDivisionError:\n            return low\n        if u > c:\n            u = 1.0 - u\n            c = 1.0 - c\n            low, high = high, low\n        return low + (high - low) * (u * c) ** 0.5\n\n## -------------------- normal distribution --------------------\n\n    def normalvariate(self, mu, sigma):\n        \"\"\"Normal distribution.\n\n        mu is the mean, and sigma is the standard deviation.\n\n        \"\"\"\n        # mu = mean, sigma = standard deviation\n\n        # Uses Kinderman and Monahan method. Reference: Kinderman,\n        # A.J. and Monahan, J.F., \"Computer generation of random\n        # variables using the ratio of uniform deviates\", ACM Trans\n        # Math Software, 3, (1977), pp257-260.\n\n        random = self.random\n        while 1:\n            u1 = random()\n            u2 = 1.0 - random()\n            z = NV_MAGICCONST*(u1-0.5)/u2\n            zz = z*z/4.0\n            if zz <= -_log(u2):\n                break\n        return mu + z*sigma\n\n## -------------------- lognormal distribution --------------------\n\n    def lognormvariate(self, mu, sigma):\n        \"\"\"Log normal distribution.\n\n        If you take the natural logarithm of this distribution, you'll get a\n        normal distribution with mean mu and standard deviation sigma.\n        mu can have any value, and sigma must be greater than zero.\n\n        \"\"\"\n        return _exp(self.normalvariate(mu, sigma))\n\n## -------------------- exponential distribution --------------------\n\n    def expovariate(self, lambd):\n        \"\"\"Exponential distribution.\n\n        lambd is 1.0 divided by the desired mean.  It should be\n        nonzero.  (The parameter would be called \"lambda\", but that is\n        a reserved word in Python.)  Returned values range from 0 to\n        positive infinity if lambd is positive, and from negative\n        infinity to 0 if lambd is negative.\n\n        \"\"\"\n        # lambd: rate lambd = 1/mean\n        # ('lambda' is a Python reserved word)\n\n        # we use 1-random() instead of random() to preclude the\n        # possibility of taking the log of zero.\n        return -_log(1.0 - self.random())/lambd\n\n## -------------------- von Mises distribution --------------------\n\n    def vonmisesvariate(self, mu, kappa):\n        \"\"\"Circular data distribution.\n\n        mu is the mean angle, expressed in radians between 0 and 2*pi, and\n        kappa is the concentration parameter, which must be greater than or\n        equal to zero.  If kappa is equal to zero, this distribution reduces\n        to a uniform random angle over the range 0 to 2*pi.\n\n        \"\"\"\n        # mu:    mean angle (in radians between 0 and 2*pi)\n        # kappa: concentration parameter kappa (>= 0)\n        # if kappa = 0 generate uniform random angle\n\n        # Based upon an algorithm published in: Fisher, N.I.,\n        # \"Statistical Analysis of Circular Data\", Cambridge\n        # University Press, 1993.\n\n        # Thanks to Magnus Kessler for a correction to the\n        # implementation of step 4.\n\n        random = self.random\n        if kappa <= 1e-6:\n            return TWOPI * random()\n\n        s = 0.5 / kappa\n        r = s + _sqrt(1.0 + s * s)\n\n        while 1:\n            u1 = random()\n            z = _cos(_pi * u1)\n\n            d = z / (r + z)\n            u2 = random()\n            if u2 < 1.0 - d * d or u2 <= (1.0 - d) * _exp(d):\n                break\n\n        q = 1.0 / r\n        f = (q + z) / (1.0 + q * z)\n        u3 = random()\n        if u3 > 0.5:\n            theta = (mu + _acos(f)) % TWOPI\n        else:\n            theta = (mu - _acos(f)) % TWOPI\n\n        return theta\n\n## -------------------- gamma distribution --------------------\n\n    def gammavariate(self, alpha, beta):\n        \"\"\"Gamma distribution.  Not the gamma function!\n\n        Conditions on the parameters are alpha > 0 and beta > 0.\n\n        The probability distribution function is:\n\n                    x ** (alpha - 1) * math.exp(-x / beta)\n          pdf(x) =  --------------------------------------\n                      math.gamma(alpha) * beta ** alpha\n\n        \"\"\"\n\n        # alpha > 0, beta > 0, mean is alpha*beta, variance is alpha*beta**2\n\n        # Warning: a few older sources define the gamma distribution in terms\n        # of alpha > -1.0\n        if alpha <= 0.0 or beta <= 0.0:\n            raise ValueError, 'gammavariate: alpha and beta must be > 0.0'\n\n        random = self.random\n        if alpha > 1.0:\n\n            # Uses R.C.H. Cheng, \"The generation of Gamma\n            # variables with non-integral shape parameters\",\n            # Applied Statistics, (1977), 26, No. 1, p71-74\n\n            ainv = _sqrt(2.0 * alpha - 1.0)\n            bbb = alpha - LOG4\n            ccc = alpha + ainv\n\n            while 1:\n                u1 = random()\n                if not 1e-7 < u1 < .9999999:\n                    continue\n                u2 = 1.0 - random()\n                v = _log(u1/(1.0-u1))/ainv\n                x = alpha*_exp(v)\n                z = u1*u1*u2\n                r = bbb+ccc*v-x\n                if r + SG_MAGICCONST - 4.5*z >= 0.0 or r >= _log(z):\n                    return x * beta\n\n        elif alpha == 1.0:\n            # expovariate(1)\n            u = random()\n            while u <= 1e-7:\n                u = random()\n            return -_log(u) * beta\n\n        else:   # alpha is between 0 and 1 (exclusive)\n\n            # Uses ALGORITHM GS of Statistical Computing - Kennedy & Gentle\n\n            while 1:\n                u = random()\n                b = (_e + alpha)/_e\n                p = b*u\n                if p <= 1.0:\n                    x = p ** (1.0/alpha)\n                else:\n                    x = -_log((b-p)/alpha)\n                u1 = random()\n                if p > 1.0:\n                    if u1 <= x ** (alpha - 1.0):\n                        break\n                elif u1 <= _exp(-x):\n                    break\n            return x * beta\n\n## -------------------- Gauss (faster alternative) --------------------\n\n    def gauss(self, mu, sigma):\n        \"\"\"Gaussian distribution.\n\n        mu is the mean, and sigma is the standard deviation.  This is\n        slightly faster than the normalvariate() function.\n\n        Not thread-safe without a lock around calls.\n\n        \"\"\"\n\n        # When x and y are two variables from [0, 1), uniformly\n        # distributed, then\n        #\n        #    cos(2*pi*x)*sqrt(-2*log(1-y))\n        #    sin(2*pi*x)*sqrt(-2*log(1-y))\n        #\n        # are two *independent* variables with normal distribution\n        # (mu = 0, sigma = 1).\n        # (Lambert Meertens)\n        # (corrected version; bug discovered by Mike Miller, fixed by LM)\n\n        # Multithreading note: When two threads call this function\n        # simultaneously, it is possible that they will receive the\n        # same return value.  The window is very small though.  To\n        # avoid this, you have to use a lock around all calls.  (I\n        # didn't want to slow this down in the serial case by using a\n        # lock here.)\n\n        random = self.random\n        z = self.gauss_next\n        self.gauss_next = None\n        if z is None:\n            x2pi = random() * TWOPI\n            g2rad = _sqrt(-2.0 * _log(1.0 - random()))\n            z = _cos(x2pi) * g2rad\n            self.gauss_next = _sin(x2pi) * g2rad\n\n        return mu + z*sigma\n\n## -------------------- beta --------------------\n## See\n## http://mail.python.org/pipermail/python-bugs-list/2001-January/003752.html\n## for Ivan Frohne's insightful analysis of why the original implementation:\n##\n##    def betavariate(self, alpha, beta):\n##        # Discrete Event Simulation in C, pp 87-88.\n##\n##        y = self.expovariate(alpha)\n##        z = self.expovariate(1.0/beta)\n##        return z/(y+z)\n##\n## was dead wrong, and how it probably got that way.\n\n    def betavariate(self, alpha, beta):\n        \"\"\"Beta distribution.\n\n        Conditions on the parameters are alpha > 0 and beta > 0.\n        Returned values range between 0 and 1.\n\n        \"\"\"\n\n        # This version due to Janne Sinkkonen, and matches all the std\n        # texts (e.g., Knuth Vol 2 Ed 3 pg 134 \"the beta distribution\").\n        y = self.gammavariate(alpha, 1.)\n        if y == 0:\n            return 0.0\n        else:\n            return y / (y + self.gammavariate(beta, 1.))\n\n## -------------------- Pareto --------------------\n\n    def paretovariate(self, alpha):\n        \"\"\"Pareto distribution.  alpha is the shape parameter.\"\"\"\n        # Jain, pg. 495\n\n        u = 1.0 - self.random()\n        return 1.0 / pow(u, 1.0/alpha)\n\n## -------------------- Weibull --------------------\n\n    def weibullvariate(self, alpha, beta):\n        \"\"\"Weibull distribution.\n\n        alpha is the scale parameter and beta is the shape parameter.\n\n        \"\"\"\n        # Jain, pg. 499; bug fix courtesy Bill Arms\n\n        u = 1.0 - self.random()\n        return alpha * pow(-_log(u), 1.0/beta)\n\n## -------------------- Wichmann-Hill -------------------\n\nclass WichmannHill(Random):\n\n    VERSION = 1     # used by getstate/setstate\n\n    def seed(self, a=None):\n        \"\"\"Initialize internal state from hashable object.\n\n        None or no argument seeds from current time or from an operating\n        system specific randomness source if available.\n\n        If a is not None or an int or long, hash(a) is used instead.\n\n        If a is an int or long, a is used directly.  Distinct values between\n        0 and 27814431486575L inclusive are guaranteed to yield distinct\n        internal states (this guarantee is specific to the default\n        Wichmann-Hill generator).\n        \"\"\"\n\n        if a is None:\n            try:\n                a = long(_hexlify(_urandom(16)), 16)\n            except NotImplementedError:\n                import time\n                a = long(time.time() * 256) # use fractional seconds\n\n        if not isinstance(a, (int, long)):\n            a = hash(a)\n\n        a, x = divmod(a, 30268)\n        a, y = divmod(a, 30306)\n        a, z = divmod(a, 30322)\n        self._seed = int(x)+1, int(y)+1, int(z)+1\n\n        self.gauss_next = None\n\n    def random(self):\n        \"\"\"Get the next random number in the range [0.0, 1.0).\"\"\"\n\n        # Wichman-Hill random number generator.\n        #\n        # Wichmann, B. A. & Hill, I. D. (1982)\n        # Algorithm AS 183:\n        # An efficient and portable pseudo-random number generator\n        # Applied Statistics 31 (1982) 188-190\n        #\n        # see also:\n        #        Correction to Algorithm AS 183\n        #        Applied Statistics 33 (1984) 123\n        #\n        #        McLeod, A. I. (1985)\n        #        A remark on Algorithm AS 183\n        #        Applied Statistics 34 (1985),198-200\n\n        # This part is thread-unsafe:\n        # BEGIN CRITICAL SECTION\n        x, y, z = self._seed\n        x = (171 * x) % 30269\n        y = (172 * y) % 30307\n        z = (170 * z) % 30323\n        self._seed = x, y, z\n        # END CRITICAL SECTION\n\n        # Note:  on a platform using IEEE-754 double arithmetic, this can\n        # never return 0.0 (asserted by Tim; proof too long for a comment).\n        return (x/30269.0 + y/30307.0 + z/30323.0) % 1.0\n\n    def getstate(self):\n        \"\"\"Return internal state; can be passed to setstate() later.\"\"\"\n        return self.VERSION, self._seed, self.gauss_next\n\n    def setstate(self, state):\n        \"\"\"Restore internal state from object returned by getstate().\"\"\"\n        version = state[0]\n        if version == 1:\n            version, self._seed, self.gauss_next = state\n        else:\n            raise ValueError(\"state with version %s passed to \"\n                             \"Random.setstate() of version %s\" %\n                             (version, self.VERSION))\n\n    def jumpahead(self, n):\n        \"\"\"Act as if n calls to random() were made, but quickly.\n\n        n is an int, greater than or equal to 0.\n\n        Example use:  If you have 2 threads and know that each will\n        consume no more than a million random numbers, create two Random\n        objects r1 and r2, then do\n            r2.setstate(r1.getstate())\n            r2.jumpahead(1000000)\n        Then r1 and r2 will use guaranteed-disjoint segments of the full\n        period.\n        \"\"\"\n\n        if not n >= 0:\n            raise ValueError(\"n must be >= 0\")\n        x, y, z = self._seed\n        x = int(x * pow(171, n, 30269)) % 30269\n        y = int(y * pow(172, n, 30307)) % 30307\n        z = int(z * pow(170, n, 30323)) % 30323\n        self._seed = x, y, z\n\n    def __whseed(self, x=0, y=0, z=0):\n        \"\"\"Set the Wichmann-Hill seed from (x, y, z).\n\n        These must be integers in the range [0, 256).\n        \"\"\"\n\n        if not type(x) == type(y) == type(z) == int:\n            raise TypeError('seeds must be integers')\n        if not (0 <= x < 256 and 0 <= y < 256 and 0 <= z < 256):\n            raise ValueError('seeds must be in range(0, 256)')\n        if 0 == x == y == z:\n            # Initialize from current time\n            import time\n            t = long(time.time() * 256)\n            t = int((t&0xffffff) ^ (t>>24))\n            t, x = divmod(t, 256)\n            t, y = divmod(t, 256)\n            t, z = divmod(t, 256)\n        # Zero is a poor seed, so substitute 1\n        self._seed = (x or 1, y or 1, z or 1)\n\n        self.gauss_next = None\n\n    def whseed(self, a=None):\n        \"\"\"Seed from hashable object's hash code.\n\n        None or no argument seeds from current time.  It is not guaranteed\n        that objects with distinct hash codes lead to distinct internal\n        states.\n\n        This is obsolete, provided for compatibility with the seed routine\n        used prior to Python 2.1.  Use the .seed() method instead.\n        \"\"\"\n\n        if a is None:\n            self.__whseed()\n            return\n        a = hash(a)\n        a, x = divmod(a, 256)\n        a, y = divmod(a, 256)\n        a, z = divmod(a, 256)\n        x = (x + a) % 256 or 1\n        y = (y + a) % 256 or 1\n        z = (z + a) % 256 or 1\n        self.__whseed(x, y, z)\n\n## --------------- Operating System Random Source  ------------------\n\nclass SystemRandom(Random):\n    \"\"\"Alternate random number generator using sources provided\n    by the operating system (such as /dev/urandom on Unix or\n    CryptGenRandom on Windows).\n\n     Not available on all systems (see os.urandom() for details).\n    \"\"\"\n\n    def random(self):\n        \"\"\"Get the next random number in the range [0.0, 1.0).\"\"\"\n        return (long(_hexlify(_urandom(7)), 16) >> 3) * RECIP_BPF\n\n    def getrandbits(self, k):\n        \"\"\"getrandbits(k) -> x.  Generates a long int with k random bits.\"\"\"\n        if k <= 0:\n            raise ValueError('number of bits must be greater than zero')\n        if k != int(k):\n            raise TypeError('number of bits should be an integer')\n        bytes = (k + 7) // 8                    # bits / 8 and rounded up\n        x = long(_hexlify(_urandom(bytes)), 16)\n        return x >> (bytes * 8 - k)             # trim excess bits\n\n    def _stub(self, *args, **kwds):\n        \"Stub method.  Not used for a system random number generator.\"\n        return None\n    seed = jumpahead = _stub\n\n    def _notimplemented(self, *args, **kwds):\n        \"Method should not be called for a system random number generator.\"\n        raise NotImplementedError('System entropy source does not have state.')\n    getstate = setstate = _notimplemented\n\n## -------------------- test program --------------------\n\ndef _test_generator(n, func, args):\n    import time\n    print n, 'times', func.__name__\n    total = 0.0\n    sqsum = 0.0\n    smallest = 1e10\n    largest = -1e10\n    t0 = time.time()\n    for i in range(n):\n        x = func(*args)\n        total += x\n        sqsum = sqsum + x*x\n        smallest = min(x, smallest)\n        largest = max(x, largest)\n    t1 = time.time()\n    print round(t1-t0, 3), 'sec,',\n    avg = total/n\n    stddev = _sqrt(sqsum/n - avg*avg)\n    print 'avg %g, stddev %g, min %g, max %g' % \\\n              (avg, stddev, smallest, largest)\n\n\ndef _test(N=2000):\n    _test_generator(N, random, ())\n    _test_generator(N, normalvariate, (0.0, 1.0))\n    _test_generator(N, lognormvariate, (0.0, 1.0))\n    _test_generator(N, vonmisesvariate, (0.0, 1.0))\n    _test_generator(N, gammavariate, (0.01, 1.0))\n    _test_generator(N, gammavariate, (0.1, 1.0))\n    _test_generator(N, gammavariate, (0.1, 2.0))\n    _test_generator(N, gammavariate, (0.5, 1.0))\n    _test_generator(N, gammavariate, (0.9, 1.0))\n    _test_generator(N, gammavariate, (1.0, 1.0))\n    _test_generator(N, gammavariate, (2.0, 1.0))\n    _test_generator(N, gammavariate, (20.0, 1.0))\n    _test_generator(N, gammavariate, (200.0, 1.0))\n    _test_generator(N, gauss, (0.0, 1.0))\n    _test_generator(N, betavariate, (3.0, 3.0))\n    _test_generator(N, triangular, (0.0, 1.0, 1.0/3.0))\n\n# Create one instance, seeded from current time, and export its methods\n# as module-level functions.  The functions share state across all uses\n#(both in the user's code and in the Python libraries), but that's fine\n# for most programs and is easier for the casual user than making them\n# instantiate their own Random() instance.\n\n_inst = Random()\nseed = _inst.seed\nrandom = _inst.random\nuniform = _inst.uniform\ntriangular = _inst.triangular\nrandint = _inst.randint\nchoice = _inst.choice\nrandrange = _inst.randrange\nsample = _inst.sample\nshuffle = _inst.shuffle\nnormalvariate = _inst.normalvariate\nlognormvariate = _inst.lognormvariate\nexpovariate = _inst.expovariate\nvonmisesvariate = _inst.vonmisesvariate\ngammavariate = _inst.gammavariate\ngauss = _inst.gauss\nbetavariate = _inst.betavariate\nparetovariate = _inst.paretovariate\nweibullvariate = _inst.weibullvariate\ngetstate = _inst.getstate\nsetstate = _inst.setstate\njumpahead = _inst.jumpahead\ngetrandbits = _inst.getrandbits\n\nif __name__ == '__main__':\n    _test()\n", 
    "re": "#\n# Secret Labs' Regular Expression Engine\n#\n# re-compatible interface for the sre matching engine\n#\n# Copyright (c) 1998-2001 by Secret Labs AB.  All rights reserved.\n#\n# This version of the SRE library can be redistributed under CNRI's\n# Python 1.6 license.  For any other use, please contact Secret Labs\n# AB (info@pythonware.com).\n#\n# Portions of this engine have been developed in cooperation with\n# CNRI.  Hewlett-Packard provided funding for 1.6 integration and\n# other compatibility work.\n#\n\nr\"\"\"Support for regular expressions (RE).\n\nThis module provides regular expression matching operations similar to\nthose found in Perl.  It supports both 8-bit and Unicode strings; both\nthe pattern and the strings being processed can contain null bytes and\ncharacters outside the US ASCII range.\n\nRegular expressions can contain both special and ordinary characters.\nMost ordinary characters, like \"A\", \"a\", or \"0\", are the simplest\nregular expressions; they simply match themselves.  You can\nconcatenate ordinary characters, so last matches the string 'last'.\n\nThe special characters are:\n    \".\"      Matches any character except a newline.\n    \"^\"      Matches the start of the string.\n    \"$\"      Matches the end of the string or just before the newline at\n             the end of the string.\n    \"*\"      Matches 0 or more (greedy) repetitions of the preceding RE.\n             Greedy means that it will match as many repetitions as possible.\n    \"+\"      Matches 1 or more (greedy) repetitions of the preceding RE.\n    \"?\"      Matches 0 or 1 (greedy) of the preceding RE.\n    *?,+?,?? Non-greedy versions of the previous three special characters.\n    {m,n}    Matches from m to n repetitions of the preceding RE.\n    {m,n}?   Non-greedy version of the above.\n    \"\\\\\"     Either escapes special characters or signals a special sequence.\n    []       Indicates a set of characters.\n             A \"^\" as the first character indicates a complementing set.\n    \"|\"      A|B, creates an RE that will match either A or B.\n    (...)    Matches the RE inside the parentheses.\n             The contents can be retrieved or matched later in the string.\n    (?iLmsux) Set the I, L, M, S, U, or X flag for the RE (see below).\n    (?:...)  Non-grouping version of regular parentheses.\n    (?P<name>...) The substring matched by the group is accessible by name.\n    (?P=name)     Matches the text matched earlier by the group named name.\n    (?#...)  A comment; ignored.\n    (?=...)  Matches if ... matches next, but doesn't consume the string.\n    (?!...)  Matches if ... doesn't match next.\n    (?<=...) Matches if preceded by ... (must be fixed length).\n    (?<!...) Matches if not preceded by ... (must be fixed length).\n    (?(id/name)yes|no) Matches yes pattern if the group with id/name matched,\n                       the (optional) no pattern otherwise.\n\nThe special sequences consist of \"\\\\\" and a character from the list\nbelow.  If the ordinary character is not on the list, then the\nresulting RE will match the second character.\n    \\number  Matches the contents of the group of the same number.\n    \\A       Matches only at the start of the string.\n    \\Z       Matches only at the end of the string.\n    \\b       Matches the empty string, but only at the start or end of a word.\n    \\B       Matches the empty string, but not at the start or end of a word.\n    \\d       Matches any decimal digit; equivalent to the set [0-9].\n    \\D       Matches any non-digit character; equivalent to the set [^0-9].\n    \\s       Matches any whitespace character; equivalent to [ \\t\\n\\r\\f\\v].\n    \\S       Matches any non-whitespace character; equiv. to [^ \\t\\n\\r\\f\\v].\n    \\w       Matches any alphanumeric character; equivalent to [a-zA-Z0-9_].\n             With LOCALE, it will match the set [0-9_] plus characters defined\n             as letters for the current locale.\n    \\W       Matches the complement of \\w.\n    \\\\       Matches a literal backslash.\n\nThis module exports the following functions:\n    match    Match a regular expression pattern to the beginning of a string.\n    search   Search a string for the presence of a pattern.\n    sub      Substitute occurrences of a pattern found in a string.\n    subn     Same as sub, but also return the number of substitutions made.\n    split    Split a string by the occurrences of a pattern.\n    findall  Find all occurrences of a pattern in a string.\n    finditer Return an iterator yielding a match object for each match.\n    compile  Compile a pattern into a RegexObject.\n    purge    Clear the regular expression cache.\n    escape   Backslash all non-alphanumerics in a string.\n\nSome of the functions in this module takes flags as optional parameters:\n    I  IGNORECASE  Perform case-insensitive matching.\n    L  LOCALE      Make \\w, \\W, \\b, \\B, dependent on the current locale.\n    M  MULTILINE   \"^\" matches the beginning of lines (after a newline)\n                   as well as the string.\n                   \"$\" matches the end of lines (before a newline) as well\n                   as the end of the string.\n    S  DOTALL      \".\" matches any character at all, including the newline.\n    X  VERBOSE     Ignore whitespace and comments for nicer looking RE's.\n    U  UNICODE     Make \\w, \\W, \\b, \\B, dependent on the Unicode locale.\n\nThis module also defines an exception 'error'.\n\n\"\"\"\n\nimport sys\nimport sre_compile\nimport sre_parse\ntry:\n    import _locale\nexcept ImportError:\n    _locale = None\n\n# public symbols\n__all__ = [ \"match\", \"search\", \"sub\", \"subn\", \"split\", \"findall\",\n    \"compile\", \"purge\", \"template\", \"escape\", \"I\", \"L\", \"M\", \"S\", \"X\",\n    \"U\", \"IGNORECASE\", \"LOCALE\", \"MULTILINE\", \"DOTALL\", \"VERBOSE\",\n    \"UNICODE\", \"error\" ]\n\n__version__ = \"2.2.1\"\n\n# flags\nI = IGNORECASE = sre_compile.SRE_FLAG_IGNORECASE # ignore case\nL = LOCALE = sre_compile.SRE_FLAG_LOCALE # assume current 8-bit locale\nU = UNICODE = sre_compile.SRE_FLAG_UNICODE # assume unicode locale\nM = MULTILINE = sre_compile.SRE_FLAG_MULTILINE # make anchors look for newline\nS = DOTALL = sre_compile.SRE_FLAG_DOTALL # make dot match newline\nX = VERBOSE = sre_compile.SRE_FLAG_VERBOSE # ignore whitespace and comments\n\n# sre extensions (experimental, don't rely on these)\nT = TEMPLATE = sre_compile.SRE_FLAG_TEMPLATE # disable backtracking\nDEBUG = sre_compile.SRE_FLAG_DEBUG # dump pattern after compilation\n\n# sre exception\nerror = sre_compile.error\n\n# --------------------------------------------------------------------\n# public interface\n\ndef match(pattern, string, flags=0):\n    \"\"\"Try to apply the pattern at the start of the string, returning\n    a match object, or None if no match was found.\"\"\"\n    return _compile(pattern, flags).match(string)\n\ndef search(pattern, string, flags=0):\n    \"\"\"Scan through string looking for a match to the pattern, returning\n    a match object, or None if no match was found.\"\"\"\n    return _compile(pattern, flags).search(string)\n\ndef sub(pattern, repl, string, count=0, flags=0):\n    \"\"\"Return the string obtained by replacing the leftmost\n    non-overlapping occurrences of the pattern in string by the\n    replacement repl.  repl can be either a string or a callable;\n    if a string, backslash escapes in it are processed.  If it is\n    a callable, it's passed the match object and must return\n    a replacement string to be used.\"\"\"\n    return _compile(pattern, flags).sub(repl, string, count)\n\ndef subn(pattern, repl, string, count=0, flags=0):\n    \"\"\"Return a 2-tuple containing (new_string, number).\n    new_string is the string obtained by replacing the leftmost\n    non-overlapping occurrences of the pattern in the source\n    string by the replacement repl.  number is the number of\n    substitutions that were made. repl can be either a string or a\n    callable; if a string, backslash escapes in it are processed.\n    If it is a callable, it's passed the match object and must\n    return a replacement string to be used.\"\"\"\n    return _compile(pattern, flags).subn(repl, string, count)\n\ndef split(pattern, string, maxsplit=0, flags=0):\n    \"\"\"Split the source string by the occurrences of the pattern,\n    returning a list containing the resulting substrings.\"\"\"\n    return _compile(pattern, flags).split(string, maxsplit)\n\ndef findall(pattern, string, flags=0):\n    \"\"\"Return a list of all non-overlapping matches in the string.\n\n    If one or more groups are present in the pattern, return a\n    list of groups; this will be a list of tuples if the pattern\n    has more than one group.\n\n    Empty matches are included in the result.\"\"\"\n    return _compile(pattern, flags).findall(string)\n\nif sys.hexversion >= 0x02020000:\n    __all__.append(\"finditer\")\n    def finditer(pattern, string, flags=0):\n        \"\"\"Return an iterator over all non-overlapping matches in the\n        string.  For each match, the iterator returns a match object.\n\n        Empty matches are included in the result.\"\"\"\n        return _compile(pattern, flags).finditer(string)\n\ndef compile(pattern, flags=0):\n    \"Compile a regular expression pattern, returning a pattern object.\"\n    return _compile(pattern, flags)\n\ndef purge():\n    \"Clear the regular expression cache\"\n    _cache.clear()\n    _cache_repl.clear()\n\ndef template(pattern, flags=0):\n    \"Compile a template pattern, returning a pattern object\"\n    return _compile(pattern, flags|T)\n\n_alphanum = frozenset(\n    \"abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789\")\n\ndef escape(pattern):\n    \"Escape all non-alphanumeric characters in pattern.\"\n    s = list(pattern)\n    alphanum = _alphanum\n    for i, c in enumerate(pattern):\n        if c not in alphanum:\n            if c == \"\\000\":\n                s[i] = \"\\\\000\"\n            else:\n                s[i] = \"\\\\\" + c\n    return pattern[:0].join(s)\n\n# --------------------------------------------------------------------\n# internals\n\n_cache = {}\n_cache_repl = {}\n\n_pattern_type = type(sre_compile.compile(\"\", 0))\n\n_MAXCACHE = 100\n\ndef _compile(*key):\n    # internal: compile pattern\n    pattern, flags = key\n    bypass_cache = flags & DEBUG\n    if not bypass_cache:\n        cachekey = (type(key[0]),) + key\n        try:\n            p, loc = _cache[cachekey]\n            if loc is None or loc == _locale.setlocale(_locale.LC_CTYPE):\n                return p\n        except KeyError:\n            pass\n    if isinstance(pattern, _pattern_type):\n        if flags:\n            raise ValueError('Cannot process flags argument with a compiled pattern')\n        return pattern\n    if not sre_compile.isstring(pattern):\n        raise TypeError, \"first argument must be string or compiled pattern\"\n    try:\n        p = sre_compile.compile(pattern, flags)\n    except error, v:\n        raise error, v # invalid expression\n    if not bypass_cache:\n        if len(_cache) >= _MAXCACHE:\n            _cache.clear()\n        if p.flags & LOCALE:\n            if not _locale:\n                return p\n            loc = _locale.setlocale(_locale.LC_CTYPE)\n        else:\n            loc = None\n        _cache[cachekey] = p, loc\n    return p\n\ndef _compile_repl(*key):\n    # internal: compile replacement pattern\n    p = _cache_repl.get(key)\n    if p is not None:\n        return p\n    repl, pattern = key\n    try:\n        p = sre_parse.parse_template(repl, pattern)\n    except error, v:\n        raise error, v # invalid expression\n    if len(_cache_repl) >= _MAXCACHE:\n        _cache_repl.clear()\n    _cache_repl[key] = p\n    return p\n\ndef _expand(pattern, match, template):\n    # internal: match.expand implementation hook\n    template = sre_parse.parse_template(template, pattern)\n    return sre_parse.expand_template(template, match)\n\ndef _subx(pattern, template):\n    # internal: pattern.sub/subn implementation helper\n    template = _compile_repl(template, pattern)\n    if not template[0] and len(template[1]) == 1:\n        # literal replacement\n        return template[1][0]\n    def filter(match, template=template):\n        return sre_parse.expand_template(template, match)\n    return filter\n\n# register myself for pickling\n\nimport copy_reg\n\ndef _pickle(p):\n    return _compile, (p.pattern, p.flags)\n\ncopy_reg.pickle(_pattern_type, _pickle, _compile)\n\n# --------------------------------------------------------------------\n# experimental stuff (see python-dev discussions for details)\n\nclass Scanner:\n    def __init__(self, lexicon, flags=0):\n        from sre_constants import BRANCH, SUBPATTERN\n        self.lexicon = lexicon\n        # combine phrases into a compound pattern\n        p = []\n        s = sre_parse.Pattern()\n        s.flags = flags\n        for phrase, action in lexicon:\n            p.append(sre_parse.SubPattern(s, [\n                (SUBPATTERN, (len(p)+1, sre_parse.parse(phrase, flags))),\n                ]))\n        s.groups = len(p)+1\n        p = sre_parse.SubPattern(s, [(BRANCH, (None, p))])\n        self.scanner = sre_compile.compile(p)\n    def scan(self, string):\n        result = []\n        append = result.append\n        match = self.scanner.scanner(string).match\n        i = 0\n        while 1:\n            m = match()\n            if not m:\n                break\n            j = m.end()\n            if i == j:\n                break\n            action = self.lexicon[m.lastindex-1][1]\n            if hasattr(action, '__call__'):\n                self.match = m\n                action = action(self, m.group())\n            if action is not None:\n                append(action)\n            i = j\n        return result, string[i:]\n", 
    "repr": "\"\"\"Redo the builtin repr() (representation) but with limits on most sizes.\"\"\"\n\n__all__ = [\"Repr\",\"repr\"]\n\nimport __builtin__\nfrom itertools import islice\n\nclass Repr:\n\n    def __init__(self):\n        self.maxlevel = 6\n        self.maxtuple = 6\n        self.maxlist = 6\n        self.maxarray = 5\n        self.maxdict = 4\n        self.maxset = 6\n        self.maxfrozenset = 6\n        self.maxdeque = 6\n        self.maxstring = 30\n        self.maxlong = 40\n        self.maxother = 20\n\n    def repr(self, x):\n        return self.repr1(x, self.maxlevel)\n\n    def repr1(self, x, level):\n        typename = type(x).__name__\n        if ' ' in typename:\n            parts = typename.split()\n            typename = '_'.join(parts)\n        if hasattr(self, 'repr_' + typename):\n            return getattr(self, 'repr_' + typename)(x, level)\n        else:\n            s = __builtin__.repr(x)\n            if len(s) > self.maxother:\n                i = max(0, (self.maxother-3)//2)\n                j = max(0, self.maxother-3-i)\n                s = s[:i] + '...' + s[len(s)-j:]\n            return s\n\n    def _repr_iterable(self, x, level, left, right, maxiter, trail=''):\n        n = len(x)\n        if level <= 0 and n:\n            s = '...'\n        else:\n            newlevel = level - 1\n            repr1 = self.repr1\n            pieces = [repr1(elem, newlevel) for elem in islice(x, maxiter)]\n            if n > maxiter:  pieces.append('...')\n            s = ', '.join(pieces)\n            if n == 1 and trail:  right = trail + right\n        return '%s%s%s' % (left, s, right)\n\n    def repr_tuple(self, x, level):\n        return self._repr_iterable(x, level, '(', ')', self.maxtuple, ',')\n\n    def repr_list(self, x, level):\n        return self._repr_iterable(x, level, '[', ']', self.maxlist)\n\n    def repr_array(self, x, level):\n        header = \"array('%s', [\" % x.typecode\n        return self._repr_iterable(x, level, header, '])', self.maxarray)\n\n    def repr_set(self, x, level):\n        x = _possibly_sorted(x)\n        return self._repr_iterable(x, level, 'set([', '])', self.maxset)\n\n    def repr_frozenset(self, x, level):\n        x = _possibly_sorted(x)\n        return self._repr_iterable(x, level, 'frozenset([', '])',\n                                   self.maxfrozenset)\n\n    def repr_deque(self, x, level):\n        return self._repr_iterable(x, level, 'deque([', '])', self.maxdeque)\n\n    def repr_dict(self, x, level):\n        n = len(x)\n        if n == 0: return '{}'\n        if level <= 0: return '{...}'\n        newlevel = level - 1\n        repr1 = self.repr1\n        pieces = []\n        for key in islice(_possibly_sorted(x), self.maxdict):\n            keyrepr = repr1(key, newlevel)\n            valrepr = repr1(x[key], newlevel)\n            pieces.append('%s: %s' % (keyrepr, valrepr))\n        if n > self.maxdict: pieces.append('...')\n        s = ', '.join(pieces)\n        return '{%s}' % (s,)\n\n    def repr_str(self, x, level):\n        s = __builtin__.repr(x[:self.maxstring])\n        if len(s) > self.maxstring:\n            i = max(0, (self.maxstring-3)//2)\n            j = max(0, self.maxstring-3-i)\n            s = __builtin__.repr(x[:i] + x[len(x)-j:])\n            s = s[:i] + '...' + s[len(s)-j:]\n        return s\n\n    def repr_long(self, x, level):\n        s = __builtin__.repr(x) # XXX Hope this isn't too slow...\n        if len(s) > self.maxlong:\n            i = max(0, (self.maxlong-3)//2)\n            j = max(0, self.maxlong-3-i)\n            s = s[:i] + '...' + s[len(s)-j:]\n        return s\n\n    def repr_instance(self, x, level):\n        try:\n            s = __builtin__.repr(x)\n            # Bugs in x.__repr__() can cause arbitrary\n            # exceptions -- then make up something\n        except Exception:\n            return '<%s instance at %x>' % (x.__class__.__name__, id(x))\n        if len(s) > self.maxstring:\n            i = max(0, (self.maxstring-3)//2)\n            j = max(0, self.maxstring-3-i)\n            s = s[:i] + '...' + s[len(s)-j:]\n        return s\n\n\ndef _possibly_sorted(x):\n    # Since not all sequences of items can be sorted and comparison\n    # functions may raise arbitrary exceptions, return an unsorted\n    # sequence in that case.\n    try:\n        return sorted(x)\n    except Exception:\n        return list(x)\n\naRepr = Repr()\nrepr = aRepr.repr\n", 
    "shlex": "# -*- coding: utf-8 -*-\n\"\"\"A lexical analyzer class for simple shell-like syntaxes.\"\"\"\n\n# Module and documentation by Eric S. Raymond, 21 Dec 1998\n# Input stacking and error message cleanup added by ESR, March 2000\n# push_source() and pop_source() made explicit by ESR, January 2001.\n# Posix compliance, split(), string arguments, and\n# iterator interface by Gustavo Niemeyer, April 2003.\n\nimport os.path\nimport sys\nfrom collections import deque\n\ntry:\n    from cStringIO import StringIO\nexcept ImportError:\n    from StringIO import StringIO\n\n__all__ = [\"shlex\", \"split\"]\n\nclass shlex:\n    \"A lexical analyzer class for simple shell-like syntaxes.\"\n    def __init__(self, instream=None, infile=None, posix=False):\n        if isinstance(instream, basestring):\n            instream = StringIO(instream)\n        if instream is not None:\n            self.instream = instream\n            self.infile = infile\n        else:\n            self.instream = sys.stdin\n            self.infile = None\n        self.posix = posix\n        if posix:\n            self.eof = None\n        else:\n            self.eof = ''\n        self.commenters = '#'\n        self.wordchars = ('abcdfeghijklmnopqrstuvwxyz'\n                          'ABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789_')\n        if self.posix:\n            self.wordchars += ('\u00df\u00e0\u00e1\u00e2\u00e3\u00e4\u00e5\u00e6\u00e7\u00e8\u00e9\u00ea\u00eb\u00ec\u00ed\u00ee\u00ef\u00f0\u00f1\u00f2\u00f3\u00f4\u00f5\u00f6\u00f8\u00f9\u00fa\u00fb\u00fc\u00fd\u00fe\u00ff'\n                               '\u00c0\u00c1\u00c2\u00c3\u00c4\u00c5\u00c6\u00c7\u00c8\u00c9\u00ca\u00cb\u00cc\u00cd\u00ce\u00cf\u00d0\u00d1\u00d2\u00d3\u00d4\u00d5\u00d6\u00d8\u00d9\u00da\u00db\u00dc\u00dd\u00de')\n        self.whitespace = ' \\t\\r\\n'\n        self.whitespace_split = False\n        self.quotes = '\\'\"'\n        self.escape = '\\\\'\n        self.escapedquotes = '\"'\n        self.state = ' '\n        self.pushback = deque()\n        self.lineno = 1\n        self.debug = 0\n        self.token = ''\n        self.filestack = deque()\n        self.source = None\n        if self.debug:\n            print 'shlex: reading from %s, line %d' \\\n                  % (self.instream, self.lineno)\n\n    def push_token(self, tok):\n        \"Push a token onto the stack popped by the get_token method\"\n        if self.debug >= 1:\n            print \"shlex: pushing token \" + repr(tok)\n        self.pushback.appendleft(tok)\n\n    def push_source(self, newstream, newfile=None):\n        \"Push an input source onto the lexer's input source stack.\"\n        if isinstance(newstream, basestring):\n            newstream = StringIO(newstream)\n        self.filestack.appendleft((self.infile, self.instream, self.lineno))\n        self.infile = newfile\n        self.instream = newstream\n        self.lineno = 1\n        if self.debug:\n            if newfile is not None:\n                print 'shlex: pushing to file %s' % (self.infile,)\n            else:\n                print 'shlex: pushing to stream %s' % (self.instream,)\n\n    def pop_source(self):\n        \"Pop the input source stack.\"\n        self.instream.close()\n        (self.infile, self.instream, self.lineno) = self.filestack.popleft()\n        if self.debug:\n            print 'shlex: popping to %s, line %d' \\\n                  % (self.instream, self.lineno)\n        self.state = ' '\n\n    def get_token(self):\n        \"Get a token from the input stream (or from stack if it's nonempty)\"\n        if self.pushback:\n            tok = self.pushback.popleft()\n            if self.debug >= 1:\n                print \"shlex: popping token \" + repr(tok)\n            return tok\n        # No pushback.  Get a token.\n        raw = self.read_token()\n        # Handle inclusions\n        if self.source is not None:\n            while raw == self.source:\n                spec = self.sourcehook(self.read_token())\n                if spec:\n                    (newfile, newstream) = spec\n                    self.push_source(newstream, newfile)\n                raw = self.get_token()\n        # Maybe we got EOF instead?\n        while raw == self.eof:\n            if not self.filestack:\n                return self.eof\n            else:\n                self.pop_source()\n                raw = self.get_token()\n        # Neither inclusion nor EOF\n        if self.debug >= 1:\n            if raw != self.eof:\n                print \"shlex: token=\" + repr(raw)\n            else:\n                print \"shlex: token=EOF\"\n        return raw\n\n    def read_token(self):\n        quoted = False\n        escapedstate = ' '\n        while True:\n            nextchar = self.instream.read(1)\n            if nextchar == '\\n':\n                self.lineno = self.lineno + 1\n            if self.debug >= 3:\n                print \"shlex: in state\", repr(self.state), \\\n                      \"I see character:\", repr(nextchar)\n            if self.state is None:\n                self.token = ''        # past end of file\n                break\n            elif self.state == ' ':\n                if not nextchar:\n                    self.state = None  # end of file\n                    break\n                elif nextchar in self.whitespace:\n                    if self.debug >= 2:\n                        print \"shlex: I see whitespace in whitespace state\"\n                    if self.token or (self.posix and quoted):\n                        break   # emit current token\n                    else:\n                        continue\n                elif nextchar in self.commenters:\n                    self.instream.readline()\n                    self.lineno = self.lineno + 1\n                elif self.posix and nextchar in self.escape:\n                    escapedstate = 'a'\n                    self.state = nextchar\n                elif nextchar in self.wordchars:\n                    self.token = nextchar\n                    self.state = 'a'\n                elif nextchar in self.quotes:\n                    if not self.posix:\n                        self.token = nextchar\n                    self.state = nextchar\n                elif self.whitespace_split:\n                    self.token = nextchar\n                    self.state = 'a'\n                else:\n                    self.token = nextchar\n                    if self.token or (self.posix and quoted):\n                        break   # emit current token\n                    else:\n                        continue\n            elif self.state in self.quotes:\n                quoted = True\n                if not nextchar:      # end of file\n                    if self.debug >= 2:\n                        print \"shlex: I see EOF in quotes state\"\n                    # XXX what error should be raised here?\n                    raise ValueError, \"No closing quotation\"\n                if nextchar == self.state:\n                    if not self.posix:\n                        self.token = self.token + nextchar\n                        self.state = ' '\n                        break\n                    else:\n                        self.state = 'a'\n                elif self.posix and nextchar in self.escape and \\\n                     self.state in self.escapedquotes:\n                    escapedstate = self.state\n                    self.state = nextchar\n                else:\n                    self.token = self.token + nextchar\n            elif self.state in self.escape:\n                if not nextchar:      # end of file\n                    if self.debug >= 2:\n                        print \"shlex: I see EOF in escape state\"\n                    # XXX what error should be raised here?\n                    raise ValueError, \"No escaped character\"\n                # In posix shells, only the quote itself or the escape\n                # character may be escaped within quotes.\n                if escapedstate in self.quotes and \\\n                   nextchar != self.state and nextchar != escapedstate:\n                    self.token = self.token + self.state\n                self.token = self.token + nextchar\n                self.state = escapedstate\n            elif self.state == 'a':\n                if not nextchar:\n                    self.state = None   # end of file\n                    break\n                elif nextchar in self.whitespace:\n                    if self.debug >= 2:\n                        print \"shlex: I see whitespace in word state\"\n                    self.state = ' '\n                    if self.token or (self.posix and quoted):\n                        break   # emit current token\n                    else:\n                        continue\n                elif nextchar in self.commenters:\n                    self.instream.readline()\n                    self.lineno = self.lineno + 1\n                    if self.posix:\n                        self.state = ' '\n                        if self.token or (self.posix and quoted):\n                            break   # emit current token\n                        else:\n                            continue\n                elif self.posix and nextchar in self.quotes:\n                    self.state = nextchar\n                elif self.posix and nextchar in self.escape:\n                    escapedstate = 'a'\n                    self.state = nextchar\n                elif nextchar in self.wordchars or nextchar in self.quotes \\\n                    or self.whitespace_split:\n                    self.token = self.token + nextchar\n                else:\n                    self.pushback.appendleft(nextchar)\n                    if self.debug >= 2:\n                        print \"shlex: I see punctuation in word state\"\n                    self.state = ' '\n                    if self.token:\n                        break   # emit current token\n                    else:\n                        continue\n        result = self.token\n        self.token = ''\n        if self.posix and not quoted and result == '':\n            result = None\n        if self.debug > 1:\n            if result:\n                print \"shlex: raw token=\" + repr(result)\n            else:\n                print \"shlex: raw token=EOF\"\n        return result\n\n    def sourcehook(self, newfile):\n        \"Hook called on a filename to be sourced.\"\n        if newfile[0] == '\"':\n            newfile = newfile[1:-1]\n        # This implements cpp-like semantics for relative-path inclusion.\n        if isinstance(self.infile, basestring) and not os.path.isabs(newfile):\n            newfile = os.path.join(os.path.dirname(self.infile), newfile)\n        return (newfile, open(newfile, \"r\"))\n\n    def error_leader(self, infile=None, lineno=None):\n        \"Emit a C-compiler-like, Emacs-friendly error-message leader.\"\n        if infile is None:\n            infile = self.infile\n        if lineno is None:\n            lineno = self.lineno\n        return \"\\\"%s\\\", line %d: \" % (infile, lineno)\n\n    def __iter__(self):\n        return self\n\n    def next(self):\n        token = self.get_token()\n        if token == self.eof:\n            raise StopIteration\n        return token\n\ndef split(s, comments=False, posix=True):\n    lex = shlex(s, posix=posix)\n    lex.whitespace_split = True\n    if not comments:\n        lex.commenters = ''\n    return list(lex)\n\nif __name__ == '__main__':\n    if len(sys.argv) == 1:\n        lexer = shlex()\n    else:\n        file = sys.argv[1]\n        lexer = shlex(open(file), file)\n    while 1:\n        tt = lexer.get_token()\n        if tt:\n            print \"Token: \" + repr(tt)\n        else:\n            break\n", 
    "six": "\"\"\"Utilities for writing code that runs on Python 2 and 3\"\"\"\n\n# Copyright (c) 2010-2015 Benjamin Peterson\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy\n# of this software and associated documentation files (the \"Software\"), to deal\n# in the Software without restriction, including without limitation the rights\n# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n# copies of the Software, and to permit persons to whom the Software is\n# furnished to do so, subject to the following conditions:\n#\n# The above copyright notice and this permission notice shall be included in all\n# copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n# SOFTWARE.\n\nfrom __future__ import absolute_import\n\nimport functools\nimport itertools\nimport operator\nimport sys\nimport types\n\n__author__ = \"Benjamin Peterson <benjamin@python.org>\"\n__version__ = \"1.10.0\"\n\n\n# Useful for very coarse version differentiation.\nPY2 = sys.version_info[0] == 2\nPY3 = sys.version_info[0] == 3\nPY34 = sys.version_info[0:2] >= (3, 4)\n\nif PY3:\n    string_types = str,\n    integer_types = int,\n    class_types = type,\n    text_type = str\n    binary_type = bytes\n\n    MAXSIZE = sys.maxsize\nelse:\n    string_types = basestring,\n    integer_types = (int, long)\n    class_types = (type, types.ClassType)\n    text_type = unicode\n    binary_type = str\n\n    if sys.platform.startswith(\"java\"):\n        # Jython always uses 32 bits.\n        MAXSIZE = int((1 << 31) - 1)\n    else:\n        # It's possible to have sizeof(long) != sizeof(Py_ssize_t).\n        class X(object):\n\n            def __len__(self):\n                return 1 << 31\n        try:\n            len(X())\n        except OverflowError:\n            # 32-bit\n            MAXSIZE = int((1 << 31) - 1)\n        else:\n            # 64-bit\n            MAXSIZE = int((1 << 63) - 1)\n        del X\n\n\ndef _add_doc(func, doc):\n    \"\"\"Add documentation to a function.\"\"\"\n    func.__doc__ = doc\n\n\ndef _import_module(name):\n    \"\"\"Import module, returning the module after the last dot.\"\"\"\n    __import__(name)\n    return sys.modules[name]\n\n\nclass _LazyDescr(object):\n\n    def __init__(self, name):\n        self.name = name\n\n    def __get__(self, obj, tp):\n        result = self._resolve()\n        setattr(obj, self.name, result)  # Invokes __set__.\n        try:\n            # This is a bit ugly, but it avoids running this again by\n            # removing this descriptor.\n            delattr(obj.__class__, self.name)\n        except AttributeError:\n            pass\n        return result\n\n\nclass MovedModule(_LazyDescr):\n\n    def __init__(self, name, old, new=None):\n        super(MovedModule, self).__init__(name)\n        if PY3:\n            if new is None:\n                new = name\n            self.mod = new\n        else:\n            self.mod = old\n\n    def _resolve(self):\n        return _import_module(self.mod)\n\n    def __getattr__(self, attr):\n        _module = self._resolve()\n        value = getattr(_module, attr)\n        setattr(self, attr, value)\n        return value\n\n\nclass _LazyModule(types.ModuleType):\n\n    def __init__(self, name):\n        super(_LazyModule, self).__init__(name)\n        self.__doc__ = self.__class__.__doc__\n\n    def __dir__(self):\n        attrs = [\"__doc__\", \"__name__\"]\n        attrs += [attr.name for attr in self._moved_attributes]\n        return attrs\n\n    # Subclasses should override this\n    _moved_attributes = []\n\n\nclass MovedAttribute(_LazyDescr):\n\n    def __init__(self, name, old_mod, new_mod, old_attr=None, new_attr=None):\n        super(MovedAttribute, self).__init__(name)\n        if PY3:\n            if new_mod is None:\n                new_mod = name\n            self.mod = new_mod\n            if new_attr is None:\n                if old_attr is None:\n                    new_attr = name\n                else:\n                    new_attr = old_attr\n            self.attr = new_attr\n        else:\n            self.mod = old_mod\n            if old_attr is None:\n                old_attr = name\n            self.attr = old_attr\n\n    def _resolve(self):\n        module = _import_module(self.mod)\n        return getattr(module, self.attr)\n\n\nclass _SixMetaPathImporter(object):\n\n    \"\"\"\n    A meta path importer to import six.moves and its submodules.\n\n    This class implements a PEP302 finder and loader. It should be compatible\n    with Python 2.5 and all existing versions of Python3\n    \"\"\"\n\n    def __init__(self, six_module_name):\n        self.name = six_module_name\n        self.known_modules = {}\n\n    def _add_module(self, mod, *fullnames):\n        for fullname in fullnames:\n            self.known_modules[self.name + \".\" + fullname] = mod\n\n    def _get_module(self, fullname):\n        return self.known_modules[self.name + \".\" + fullname]\n\n    def find_module(self, fullname, path=None):\n        if fullname in self.known_modules:\n            return self\n        return None\n\n    def __get_module(self, fullname):\n        try:\n            return self.known_modules[fullname]\n        except KeyError:\n            raise ImportError(\"This loader does not know module \" + fullname)\n\n    def load_module(self, fullname):\n        try:\n            # in case of a reload\n            return sys.modules[fullname]\n        except KeyError:\n            pass\n        mod = self.__get_module(fullname)\n        if isinstance(mod, MovedModule):\n            mod = mod._resolve()\n        else:\n            mod.__loader__ = self\n        sys.modules[fullname] = mod\n        return mod\n\n    def is_package(self, fullname):\n        \"\"\"\n        Return true, if the named module is a package.\n\n        We need this method to get correct spec objects with\n        Python 3.4 (see PEP451)\n        \"\"\"\n        return hasattr(self.__get_module(fullname), \"__path__\")\n\n    def get_code(self, fullname):\n        \"\"\"Return None\n\n        Required, if is_package is implemented\"\"\"\n        self.__get_module(fullname)  # eventually raises ImportError\n        return None\n    get_source = get_code  # same as get_code\n\n_importer = _SixMetaPathImporter(__name__)\n\n\nclass _MovedItems(_LazyModule):\n\n    \"\"\"Lazy loading of moved objects\"\"\"\n    __path__ = []  # mark as package\n\n\n_moved_attributes = [\n    MovedAttribute(\"cStringIO\", \"cStringIO\", \"io\", \"StringIO\"),\n    MovedAttribute(\"filter\", \"itertools\", \"builtins\", \"ifilter\", \"filter\"),\n    MovedAttribute(\"filterfalse\", \"itertools\", \"itertools\", \"ifilterfalse\", \"filterfalse\"),\n    MovedAttribute(\"input\", \"__builtin__\", \"builtins\", \"raw_input\", \"input\"),\n    MovedAttribute(\"intern\", \"__builtin__\", \"sys\"),\n    MovedAttribute(\"map\", \"itertools\", \"builtins\", \"imap\", \"map\"),\n    MovedAttribute(\"getcwd\", \"os\", \"os\", \"getcwdu\", \"getcwd\"),\n    MovedAttribute(\"getcwdb\", \"os\", \"os\", \"getcwd\", \"getcwdb\"),\n    MovedAttribute(\"range\", \"__builtin__\", \"builtins\", \"xrange\", \"range\"),\n    MovedAttribute(\"reload_module\", \"__builtin__\", \"importlib\" if PY34 else \"imp\", \"reload\"),\n    MovedAttribute(\"reduce\", \"__builtin__\", \"functools\"),\n    MovedAttribute(\"shlex_quote\", \"pipes\", \"shlex\", \"quote\"),\n    MovedAttribute(\"StringIO\", \"StringIO\", \"io\"),\n    MovedAttribute(\"UserDict\", \"UserDict\", \"collections\"),\n    MovedAttribute(\"UserList\", \"UserList\", \"collections\"),\n    MovedAttribute(\"UserString\", \"UserString\", \"collections\"),\n    MovedAttribute(\"xrange\", \"__builtin__\", \"builtins\", \"xrange\", \"range\"),\n    MovedAttribute(\"zip\", \"itertools\", \"builtins\", \"izip\", \"zip\"),\n    MovedAttribute(\"zip_longest\", \"itertools\", \"itertools\", \"izip_longest\", \"zip_longest\"),\n    MovedModule(\"builtins\", \"__builtin__\"),\n    MovedModule(\"configparser\", \"ConfigParser\"),\n    MovedModule(\"copyreg\", \"copy_reg\"),\n    MovedModule(\"dbm_gnu\", \"gdbm\", \"dbm.gnu\"),\n    MovedModule(\"_dummy_thread\", \"dummy_thread\", \"_dummy_thread\"),\n    MovedModule(\"http_cookiejar\", \"cookielib\", \"http.cookiejar\"),\n    MovedModule(\"http_cookies\", \"Cookie\", \"http.cookies\"),\n    MovedModule(\"html_entities\", \"htmlentitydefs\", \"html.entities\"),\n    MovedModule(\"html_parser\", \"HTMLParser\", \"html.parser\"),\n    MovedModule(\"http_client\", \"httplib\", \"http.client\"),\n    MovedModule(\"email_mime_multipart\", \"email.MIMEMultipart\", \"email.mime.multipart\"),\n    MovedModule(\"email_mime_nonmultipart\", \"email.MIMENonMultipart\", \"email.mime.nonmultipart\"),\n    MovedModule(\"email_mime_text\", \"email.MIMEText\", \"email.mime.text\"),\n    MovedModule(\"email_mime_base\", \"email.MIMEBase\", \"email.mime.base\"),\n    MovedModule(\"BaseHTTPServer\", \"BaseHTTPServer\", \"http.server\"),\n    MovedModule(\"CGIHTTPServer\", \"CGIHTTPServer\", \"http.server\"),\n    MovedModule(\"SimpleHTTPServer\", \"SimpleHTTPServer\", \"http.server\"),\n    MovedModule(\"cPickle\", \"cPickle\", \"pickle\"),\n    MovedModule(\"queue\", \"Queue\"),\n    MovedModule(\"reprlib\", \"repr\"),\n    MovedModule(\"socketserver\", \"SocketServer\"),\n    MovedModule(\"_thread\", \"thread\", \"_thread\"),\n    MovedModule(\"tkinter\", \"Tkinter\"),\n    MovedModule(\"tkinter_dialog\", \"Dialog\", \"tkinter.dialog\"),\n    MovedModule(\"tkinter_filedialog\", \"FileDialog\", \"tkinter.filedialog\"),\n    MovedModule(\"tkinter_scrolledtext\", \"ScrolledText\", \"tkinter.scrolledtext\"),\n    MovedModule(\"tkinter_simpledialog\", \"SimpleDialog\", \"tkinter.simpledialog\"),\n    MovedModule(\"tkinter_tix\", \"Tix\", \"tkinter.tix\"),\n    MovedModule(\"tkinter_ttk\", \"ttk\", \"tkinter.ttk\"),\n    MovedModule(\"tkinter_constants\", \"Tkconstants\", \"tkinter.constants\"),\n    MovedModule(\"tkinter_dnd\", \"Tkdnd\", \"tkinter.dnd\"),\n    MovedModule(\"tkinter_colorchooser\", \"tkColorChooser\",\n                \"tkinter.colorchooser\"),\n    MovedModule(\"tkinter_commondialog\", \"tkCommonDialog\",\n                \"tkinter.commondialog\"),\n    MovedModule(\"tkinter_tkfiledialog\", \"tkFileDialog\", \"tkinter.filedialog\"),\n    MovedModule(\"tkinter_font\", \"tkFont\", \"tkinter.font\"),\n    MovedModule(\"tkinter_messagebox\", \"tkMessageBox\", \"tkinter.messagebox\"),\n    MovedModule(\"tkinter_tksimpledialog\", \"tkSimpleDialog\",\n                \"tkinter.simpledialog\"),\n    MovedModule(\"urllib_parse\", __name__ + \".moves.urllib_parse\", \"urllib.parse\"),\n    MovedModule(\"urllib_error\", __name__ + \".moves.urllib_error\", \"urllib.error\"),\n    MovedModule(\"urllib\", __name__ + \".moves.urllib\", __name__ + \".moves.urllib\"),\n    MovedModule(\"urllib_robotparser\", \"robotparser\", \"urllib.robotparser\"),\n    MovedModule(\"xmlrpc_client\", \"xmlrpclib\", \"xmlrpc.client\"),\n    MovedModule(\"xmlrpc_server\", \"SimpleXMLRPCServer\", \"xmlrpc.server\"),\n]\n# Add windows specific modules.\nif sys.platform == \"win32\":\n    _moved_attributes += [\n        MovedModule(\"winreg\", \"_winreg\"),\n    ]\n\nfor attr in _moved_attributes:\n    setattr(_MovedItems, attr.name, attr)\n    if isinstance(attr, MovedModule):\n        _importer._add_module(attr, \"moves.\" + attr.name)\ndel attr\n\n_MovedItems._moved_attributes = _moved_attributes\n\nmoves = _MovedItems(__name__ + \".moves\")\n_importer._add_module(moves, \"moves\")\n\n\nclass Module_six_moves_urllib_parse(_LazyModule):\n\n    \"\"\"Lazy loading of moved objects in six.moves.urllib_parse\"\"\"\n\n\n_urllib_parse_moved_attributes = [\n    MovedAttribute(\"ParseResult\", \"urlparse\", \"urllib.parse\"),\n    MovedAttribute(\"SplitResult\", \"urlparse\", \"urllib.parse\"),\n    MovedAttribute(\"parse_qs\", \"urlparse\", \"urllib.parse\"),\n    MovedAttribute(\"parse_qsl\", \"urlparse\", \"urllib.parse\"),\n    MovedAttribute(\"urldefrag\", \"urlparse\", \"urllib.parse\"),\n    MovedAttribute(\"urljoin\", \"urlparse\", \"urllib.parse\"),\n    MovedAttribute(\"urlparse\", \"urlparse\", \"urllib.parse\"),\n    MovedAttribute(\"urlsplit\", \"urlparse\", \"urllib.parse\"),\n    MovedAttribute(\"urlunparse\", \"urlparse\", \"urllib.parse\"),\n    MovedAttribute(\"urlunsplit\", \"urlparse\", \"urllib.parse\"),\n    MovedAttribute(\"quote\", \"urllib\", \"urllib.parse\"),\n    MovedAttribute(\"quote_plus\", \"urllib\", \"urllib.parse\"),\n    MovedAttribute(\"unquote\", \"urllib\", \"urllib.parse\"),\n    MovedAttribute(\"unquote_plus\", \"urllib\", \"urllib.parse\"),\n    MovedAttribute(\"urlencode\", \"urllib\", \"urllib.parse\"),\n    MovedAttribute(\"splitquery\", \"urllib\", \"urllib.parse\"),\n    MovedAttribute(\"splittag\", \"urllib\", \"urllib.parse\"),\n    MovedAttribute(\"splituser\", \"urllib\", \"urllib.parse\"),\n    MovedAttribute(\"uses_fragment\", \"urlparse\", \"urllib.parse\"),\n    MovedAttribute(\"uses_netloc\", \"urlparse\", \"urllib.parse\"),\n    MovedAttribute(\"uses_params\", \"urlparse\", \"urllib.parse\"),\n    MovedAttribute(\"uses_query\", \"urlparse\", \"urllib.parse\"),\n    MovedAttribute(\"uses_relative\", \"urlparse\", \"urllib.parse\"),\n]\nfor attr in _urllib_parse_moved_attributes:\n    setattr(Module_six_moves_urllib_parse, attr.name, attr)\ndel attr\n\nModule_six_moves_urllib_parse._moved_attributes = _urllib_parse_moved_attributes\n\n_importer._add_module(Module_six_moves_urllib_parse(__name__ + \".moves.urllib_parse\"),\n                      \"moves.urllib_parse\", \"moves.urllib.parse\")\n\n\nclass Module_six_moves_urllib_error(_LazyModule):\n\n    \"\"\"Lazy loading of moved objects in six.moves.urllib_error\"\"\"\n\n\n_urllib_error_moved_attributes = [\n    MovedAttribute(\"URLError\", \"urllib2\", \"urllib.error\"),\n    MovedAttribute(\"HTTPError\", \"urllib2\", \"urllib.error\"),\n    MovedAttribute(\"ContentTooShortError\", \"urllib\", \"urllib.error\"),\n]\nfor attr in _urllib_error_moved_attributes:\n    setattr(Module_six_moves_urllib_error, attr.name, attr)\ndel attr\n\nModule_six_moves_urllib_error._moved_attributes = _urllib_error_moved_attributes\n\n_importer._add_module(Module_six_moves_urllib_error(__name__ + \".moves.urllib.error\"),\n                      \"moves.urllib_error\", \"moves.urllib.error\")\n\n\nclass Module_six_moves_urllib_request(_LazyModule):\n\n    \"\"\"Lazy loading of moved objects in six.moves.urllib_request\"\"\"\n\n\n_urllib_request_moved_attributes = [\n    MovedAttribute(\"urlopen\", \"urllib2\", \"urllib.request\"),\n    MovedAttribute(\"install_opener\", \"urllib2\", \"urllib.request\"),\n    MovedAttribute(\"build_opener\", \"urllib2\", \"urllib.request\"),\n    MovedAttribute(\"pathname2url\", \"urllib\", \"urllib.request\"),\n    MovedAttribute(\"url2pathname\", \"urllib\", \"urllib.request\"),\n    MovedAttribute(\"getproxies\", \"urllib\", \"urllib.request\"),\n    MovedAttribute(\"Request\", \"urllib2\", \"urllib.request\"),\n    MovedAttribute(\"OpenerDirector\", \"urllib2\", \"urllib.request\"),\n    MovedAttribute(\"HTTPDefaultErrorHandler\", \"urllib2\", \"urllib.request\"),\n    MovedAttribute(\"HTTPRedirectHandler\", \"urllib2\", \"urllib.request\"),\n    MovedAttribute(\"HTTPCookieProcessor\", \"urllib2\", \"urllib.request\"),\n    MovedAttribute(\"ProxyHandler\", \"urllib2\", \"urllib.request\"),\n    MovedAttribute(\"BaseHandler\", \"urllib2\", \"urllib.request\"),\n    MovedAttribute(\"HTTPPasswordMgr\", \"urllib2\", \"urllib.request\"),\n    MovedAttribute(\"HTTPPasswordMgrWithDefaultRealm\", \"urllib2\", \"urllib.request\"),\n    MovedAttribute(\"AbstractBasicAuthHandler\", \"urllib2\", \"urllib.request\"),\n    MovedAttribute(\"HTTPBasicAuthHandler\", \"urllib2\", \"urllib.request\"),\n    MovedAttribute(\"ProxyBasicAuthHandler\", \"urllib2\", \"urllib.request\"),\n    MovedAttribute(\"AbstractDigestAuthHandler\", \"urllib2\", \"urllib.request\"),\n    MovedAttribute(\"HTTPDigestAuthHandler\", \"urllib2\", \"urllib.request\"),\n    MovedAttribute(\"ProxyDigestAuthHandler\", \"urllib2\", \"urllib.request\"),\n    MovedAttribute(\"HTTPHandler\", \"urllib2\", \"urllib.request\"),\n    MovedAttribute(\"HTTPSHandler\", \"urllib2\", \"urllib.request\"),\n    MovedAttribute(\"FileHandler\", \"urllib2\", \"urllib.request\"),\n    MovedAttribute(\"FTPHandler\", \"urllib2\", \"urllib.request\"),\n    MovedAttribute(\"CacheFTPHandler\", \"urllib2\", \"urllib.request\"),\n    MovedAttribute(\"UnknownHandler\", \"urllib2\", \"urllib.request\"),\n    MovedAttribute(\"HTTPErrorProcessor\", \"urllib2\", \"urllib.request\"),\n    MovedAttribute(\"urlretrieve\", \"urllib\", \"urllib.request\"),\n    MovedAttribute(\"urlcleanup\", \"urllib\", \"urllib.request\"),\n    MovedAttribute(\"URLopener\", \"urllib\", \"urllib.request\"),\n    MovedAttribute(\"FancyURLopener\", \"urllib\", \"urllib.request\"),\n    MovedAttribute(\"proxy_bypass\", \"urllib\", \"urllib.request\"),\n]\nfor attr in _urllib_request_moved_attributes:\n    setattr(Module_six_moves_urllib_request, attr.name, attr)\ndel attr\n\nModule_six_moves_urllib_request._moved_attributes = _urllib_request_moved_attributes\n\n_importer._add_module(Module_six_moves_urllib_request(__name__ + \".moves.urllib.request\"),\n                      \"moves.urllib_request\", \"moves.urllib.request\")\n\n\nclass Module_six_moves_urllib_response(_LazyModule):\n\n    \"\"\"Lazy loading of moved objects in six.moves.urllib_response\"\"\"\n\n\n_urllib_response_moved_attributes = [\n    MovedAttribute(\"addbase\", \"urllib\", \"urllib.response\"),\n    MovedAttribute(\"addclosehook\", \"urllib\", \"urllib.response\"),\n    MovedAttribute(\"addinfo\", \"urllib\", \"urllib.response\"),\n    MovedAttribute(\"addinfourl\", \"urllib\", \"urllib.response\"),\n]\nfor attr in _urllib_response_moved_attributes:\n    setattr(Module_six_moves_urllib_response, attr.name, attr)\ndel attr\n\nModule_six_moves_urllib_response._moved_attributes = _urllib_response_moved_attributes\n\n_importer._add_module(Module_six_moves_urllib_response(__name__ + \".moves.urllib.response\"),\n                      \"moves.urllib_response\", \"moves.urllib.response\")\n\n\nclass Module_six_moves_urllib_robotparser(_LazyModule):\n\n    \"\"\"Lazy loading of moved objects in six.moves.urllib_robotparser\"\"\"\n\n\n_urllib_robotparser_moved_attributes = [\n    MovedAttribute(\"RobotFileParser\", \"robotparser\", \"urllib.robotparser\"),\n]\nfor attr in _urllib_robotparser_moved_attributes:\n    setattr(Module_six_moves_urllib_robotparser, attr.name, attr)\ndel attr\n\nModule_six_moves_urllib_robotparser._moved_attributes = _urllib_robotparser_moved_attributes\n\n_importer._add_module(Module_six_moves_urllib_robotparser(__name__ + \".moves.urllib.robotparser\"),\n                      \"moves.urllib_robotparser\", \"moves.urllib.robotparser\")\n\n\nclass Module_six_moves_urllib(types.ModuleType):\n\n    \"\"\"Create a six.moves.urllib namespace that resembles the Python 3 namespace\"\"\"\n    __path__ = []  # mark as package\n    parse = _importer._get_module(\"moves.urllib_parse\")\n    error = _importer._get_module(\"moves.urllib_error\")\n    request = _importer._get_module(\"moves.urllib_request\")\n    response = _importer._get_module(\"moves.urllib_response\")\n    robotparser = _importer._get_module(\"moves.urllib_robotparser\")\n\n    def __dir__(self):\n        return ['parse', 'error', 'request', 'response', 'robotparser']\n\n_importer._add_module(Module_six_moves_urllib(__name__ + \".moves.urllib\"),\n                      \"moves.urllib\")\n\n\ndef add_move(move):\n    \"\"\"Add an item to six.moves.\"\"\"\n    setattr(_MovedItems, move.name, move)\n\n\ndef remove_move(name):\n    \"\"\"Remove item from six.moves.\"\"\"\n    try:\n        delattr(_MovedItems, name)\n    except AttributeError:\n        try:\n            del moves.__dict__[name]\n        except KeyError:\n            raise AttributeError(\"no such move, %r\" % (name,))\n\n\nif PY3:\n    _meth_func = \"__func__\"\n    _meth_self = \"__self__\"\n\n    _func_closure = \"__closure__\"\n    _func_code = \"__code__\"\n    _func_defaults = \"__defaults__\"\n    _func_globals = \"__globals__\"\nelse:\n    _meth_func = \"im_func\"\n    _meth_self = \"im_self\"\n\n    _func_closure = \"func_closure\"\n    _func_code = \"func_code\"\n    _func_defaults = \"func_defaults\"\n    _func_globals = \"func_globals\"\n\n\ntry:\n    advance_iterator = next\nexcept NameError:\n    def advance_iterator(it):\n        return it.next()\nnext = advance_iterator\n\n\ntry:\n    callable = callable\nexcept NameError:\n    def callable(obj):\n        return any(\"__call__\" in klass.__dict__ for klass in type(obj).__mro__)\n\n\nif PY3:\n    def get_unbound_function(unbound):\n        return unbound\n\n    create_bound_method = types.MethodType\n\n    def create_unbound_method(func, cls):\n        return func\n\n    Iterator = object\nelse:\n    def get_unbound_function(unbound):\n        return unbound.im_func\n\n    def create_bound_method(func, obj):\n        return types.MethodType(func, obj, obj.__class__)\n\n    def create_unbound_method(func, cls):\n        return types.MethodType(func, None, cls)\n\n    class Iterator(object):\n\n        def next(self):\n            return type(self).__next__(self)\n\n    callable = callable\n_add_doc(get_unbound_function,\n         \"\"\"Get the function out of a possibly unbound function\"\"\")\n\n\nget_method_function = operator.attrgetter(_meth_func)\nget_method_self = operator.attrgetter(_meth_self)\nget_function_closure = operator.attrgetter(_func_closure)\nget_function_code = operator.attrgetter(_func_code)\nget_function_defaults = operator.attrgetter(_func_defaults)\nget_function_globals = operator.attrgetter(_func_globals)\n\n\nif PY3:\n    def iterkeys(d, **kw):\n        return iter(d.keys(**kw))\n\n    def itervalues(d, **kw):\n        return iter(d.values(**kw))\n\n    def iteritems(d, **kw):\n        return iter(d.items(**kw))\n\n    def iterlists(d, **kw):\n        return iter(d.lists(**kw))\n\n    viewkeys = operator.methodcaller(\"keys\")\n\n    viewvalues = operator.methodcaller(\"values\")\n\n    viewitems = operator.methodcaller(\"items\")\nelse:\n    def iterkeys(d, **kw):\n        return d.iterkeys(**kw)\n\n    def itervalues(d, **kw):\n        return d.itervalues(**kw)\n\n    def iteritems(d, **kw):\n        return d.iteritems(**kw)\n\n    def iterlists(d, **kw):\n        return d.iterlists(**kw)\n\n    viewkeys = operator.methodcaller(\"viewkeys\")\n\n    viewvalues = operator.methodcaller(\"viewvalues\")\n\n    viewitems = operator.methodcaller(\"viewitems\")\n\n_add_doc(iterkeys, \"Return an iterator over the keys of a dictionary.\")\n_add_doc(itervalues, \"Return an iterator over the values of a dictionary.\")\n_add_doc(iteritems,\n         \"Return an iterator over the (key, value) pairs of a dictionary.\")\n_add_doc(iterlists,\n         \"Return an iterator over the (key, [values]) pairs of a dictionary.\")\n\n\nif PY3:\n    def b(s):\n        return s.encode(\"latin-1\")\n\n    def u(s):\n        return s\n    unichr = chr\n    import struct\n    int2byte = struct.Struct(\">B\").pack\n    del struct\n    byte2int = operator.itemgetter(0)\n    indexbytes = operator.getitem\n    iterbytes = iter\n    import io\n    StringIO = io.StringIO\n    BytesIO = io.BytesIO\n    _assertCountEqual = \"assertCountEqual\"\n    if sys.version_info[1] <= 1:\n        _assertRaisesRegex = \"assertRaisesRegexp\"\n        _assertRegex = \"assertRegexpMatches\"\n    else:\n        _assertRaisesRegex = \"assertRaisesRegex\"\n        _assertRegex = \"assertRegex\"\nelse:\n    def b(s):\n        return s\n    # Workaround for standalone backslash\n\n    def u(s):\n        return unicode(s.replace(r'\\\\', r'\\\\\\\\'), \"unicode_escape\")\n    unichr = unichr\n    int2byte = chr\n\n    def byte2int(bs):\n        return ord(bs[0])\n\n    def indexbytes(buf, i):\n        return ord(buf[i])\n    iterbytes = functools.partial(itertools.imap, ord)\n    import StringIO\n    StringIO = BytesIO = StringIO.StringIO\n    _assertCountEqual = \"assertItemsEqual\"\n    _assertRaisesRegex = \"assertRaisesRegexp\"\n    _assertRegex = \"assertRegexpMatches\"\n_add_doc(b, \"\"\"Byte literal\"\"\")\n_add_doc(u, \"\"\"Text literal\"\"\")\n\n\ndef assertCountEqual(self, *args, **kwargs):\n    return getattr(self, _assertCountEqual)(*args, **kwargs)\n\n\ndef assertRaisesRegex(self, *args, **kwargs):\n    return getattr(self, _assertRaisesRegex)(*args, **kwargs)\n\n\ndef assertRegex(self, *args, **kwargs):\n    return getattr(self, _assertRegex)(*args, **kwargs)\n\n\nif PY3:\n    exec_ = getattr(moves.builtins, \"exec\")\n\n    def reraise(tp, value, tb=None):\n        if value is None:\n            value = tp()\n        if value.__traceback__ is not tb:\n            raise value.with_traceback(tb)\n        raise value\n\nelse:\n    def exec_(_code_, _globs_=None, _locs_=None):\n        \"\"\"Execute code in a namespace.\"\"\"\n        if _globs_ is None:\n            frame = sys._getframe(1)\n            _globs_ = frame.f_globals\n            if _locs_ is None:\n                _locs_ = frame.f_locals\n            del frame\n        elif _locs_ is None:\n            _locs_ = _globs_\n        exec(\"\"\"exec _code_ in _globs_, _locs_\"\"\")\n\n    exec_(\"\"\"def reraise(tp, value, tb=None):\n    raise tp, value, tb\n\"\"\")\n\n\nif sys.version_info[:2] == (3, 2):\n    exec_(\"\"\"def raise_from(value, from_value):\n    if from_value is None:\n        raise value\n    raise value from from_value\n\"\"\")\nelif sys.version_info[:2] > (3, 2):\n    exec_(\"\"\"def raise_from(value, from_value):\n    raise value from from_value\n\"\"\")\nelse:\n    def raise_from(value, from_value):\n        raise value\n\n\nprint_ = getattr(moves.builtins, \"print\", None)\nif print_ is None:\n    def print_(*args, **kwargs):\n        \"\"\"The new-style print function for Python 2.4 and 2.5.\"\"\"\n        fp = kwargs.pop(\"file\", sys.stdout)\n        if fp is None:\n            return\n\n        def write(data):\n            if not isinstance(data, basestring):\n                data = str(data)\n            # If the file has an encoding, encode unicode with it.\n            if (isinstance(fp, file) and\n                    isinstance(data, unicode) and\n                    fp.encoding is not None):\n                errors = getattr(fp, \"errors\", None)\n                if errors is None:\n                    errors = \"strict\"\n                data = data.encode(fp.encoding, errors)\n            fp.write(data)\n        want_unicode = False\n        sep = kwargs.pop(\"sep\", None)\n        if sep is not None:\n            if isinstance(sep, unicode):\n                want_unicode = True\n            elif not isinstance(sep, str):\n                raise TypeError(\"sep must be None or a string\")\n        end = kwargs.pop(\"end\", None)\n        if end is not None:\n            if isinstance(end, unicode):\n                want_unicode = True\n            elif not isinstance(end, str):\n                raise TypeError(\"end must be None or a string\")\n        if kwargs:\n            raise TypeError(\"invalid keyword arguments to print()\")\n        if not want_unicode:\n            for arg in args:\n                if isinstance(arg, unicode):\n                    want_unicode = True\n                    break\n        if want_unicode:\n            newline = unicode(\"\\n\")\n            space = unicode(\" \")\n        else:\n            newline = \"\\n\"\n            space = \" \"\n        if sep is None:\n            sep = space\n        if end is None:\n            end = newline\n        for i, arg in enumerate(args):\n            if i:\n                write(sep)\n            write(arg)\n        write(end)\nif sys.version_info[:2] < (3, 3):\n    _print = print_\n\n    def print_(*args, **kwargs):\n        fp = kwargs.get(\"file\", sys.stdout)\n        flush = kwargs.pop(\"flush\", False)\n        _print(*args, **kwargs)\n        if flush and fp is not None:\n            fp.flush()\n\n_add_doc(reraise, \"\"\"Reraise an exception.\"\"\")\n\nif sys.version_info[0:2] < (3, 4):\n    def wraps(wrapped, assigned=functools.WRAPPER_ASSIGNMENTS,\n              updated=functools.WRAPPER_UPDATES):\n        def wrapper(f):\n            f = functools.wraps(wrapped, assigned, updated)(f)\n            f.__wrapped__ = wrapped\n            return f\n        return wrapper\nelse:\n    wraps = functools.wraps\n\n\ndef with_metaclass(meta, *bases):\n    \"\"\"Create a base class with a metaclass.\"\"\"\n    # This requires a bit of explanation: the basic idea is to make a dummy\n    # metaclass for one level of class instantiation that replaces itself with\n    # the actual metaclass.\n    class metaclass(meta):\n\n        def __new__(cls, name, this_bases, d):\n            return meta(name, bases, d)\n    return type.__new__(metaclass, 'temporary_class', (), {})\n\n\ndef add_metaclass(metaclass):\n    \"\"\"Class decorator for creating a class with a metaclass.\"\"\"\n    def wrapper(cls):\n        orig_vars = cls.__dict__.copy()\n        slots = orig_vars.get('__slots__')\n        if slots is not None:\n            if isinstance(slots, str):\n                slots = [slots]\n            for slots_var in slots:\n                orig_vars.pop(slots_var)\n        orig_vars.pop('__dict__', None)\n        orig_vars.pop('__weakref__', None)\n        return metaclass(cls.__name__, cls.__bases__, orig_vars)\n    return wrapper\n\n\ndef python_2_unicode_compatible(klass):\n    \"\"\"\n    A decorator that defines __unicode__ and __str__ methods under Python 2.\n    Under Python 3 it does nothing.\n\n    To support Python 2 and 3 with a single code base, define a __str__ method\n    returning text and apply this decorator to the class.\n    \"\"\"\n    if PY2:\n        if '__str__' not in klass.__dict__:\n            raise ValueError(\"@python_2_unicode_compatible cannot be applied \"\n                             \"to %s because it doesn't define __str__().\" %\n                             klass.__name__)\n        klass.__unicode__ = klass.__str__\n        klass.__str__ = lambda self: self.__unicode__().encode('utf-8')\n    return klass\n\n\n# Complete the moves implementation.\n# This code is at the end of this module to speed up module loading.\n# Turn this module into a package.\n__path__ = []  # required for PEP 302 and PEP 451\n__package__ = __name__  # see PEP 366 @ReservedAssignment\nif globals().get(\"__spec__\") is not None:\n    __spec__.submodule_search_locations = []  # PEP 451 @UndefinedVariable\n# Remove other six meta path importers, since they cause problems. This can\n# happen if six is removed from sys.modules and then reloaded. (Setuptools does\n# this for some reason.)\nif sys.meta_path:\n    for i, importer in enumerate(sys.meta_path):\n        # Here's some real nastiness: Another \"instance\" of the six module might\n        # be floating around. Therefore, we can't use isinstance() to check for\n        # the six meta path importer, since the other six instance will have\n        # inserted an importer with different class.\n        if (type(importer).__name__ == \"_SixMetaPathImporter\" and\n                importer.name == __name__):\n            del sys.meta_path[i]\n            break\n    del i, importer\n# Finally, add the importer to the meta path import hook.\nsys.meta_path.append(_importer)\n", 
    "sre_compile": "# -*- coding: utf-8 -*-\n#\n# Secret Labs' Regular Expression Engine\n#\n# convert template to internal format\n#\n# Copyright (c) 1997-2001 by Secret Labs AB.  All rights reserved.\n#\n# See the sre.py file for information on usage and redistribution.\n#\n\n\"\"\"Internal support module for sre\"\"\"\n\nimport _sre, sys\nimport sre_parse\nfrom sre_constants import *\n\nassert _sre.MAGIC == MAGIC, \"SRE module mismatch\"\n\nif _sre.CODESIZE == 2:\n    MAXCODE = 65535\nelse:\n    MAXCODE = 0xFFFFFFFFL\n\n_LITERAL_CODES = set([LITERAL, NOT_LITERAL])\n_REPEATING_CODES = set([REPEAT, MIN_REPEAT, MAX_REPEAT])\n_SUCCESS_CODES = set([SUCCESS, FAILURE])\n_ASSERT_CODES = set([ASSERT, ASSERT_NOT])\n\n# Sets of lowercase characters which have the same uppercase.\n_equivalences = (\n    # LATIN SMALL LETTER I, LATIN SMALL LETTER DOTLESS I\n    (0x69, 0x131), # i\u0131\n    # LATIN SMALL LETTER S, LATIN SMALL LETTER LONG S\n    (0x73, 0x17f), # s\u017f\n    # MICRO SIGN, GREEK SMALL LETTER MU\n    (0xb5, 0x3bc), # \u00b5\u03bc\n    # COMBINING GREEK YPOGEGRAMMENI, GREEK SMALL LETTER IOTA, GREEK PROSGEGRAMMENI\n    (0x345, 0x3b9, 0x1fbe), # \\u0345\u03b9\u1fbe\n    # GREEK SMALL LETTER BETA, GREEK BETA SYMBOL\n    (0x3b2, 0x3d0), # \u03b2\u03d0\n    # GREEK SMALL LETTER EPSILON, GREEK LUNATE EPSILON SYMBOL\n    (0x3b5, 0x3f5), # \u03b5\u03f5\n    # GREEK SMALL LETTER THETA, GREEK THETA SYMBOL\n    (0x3b8, 0x3d1), # \u03b8\u03d1\n    # GREEK SMALL LETTER KAPPA, GREEK KAPPA SYMBOL\n    (0x3ba, 0x3f0), # \u03ba\u03f0\n    # GREEK SMALL LETTER PI, GREEK PI SYMBOL\n    (0x3c0, 0x3d6), # \u03c0\u03d6\n    # GREEK SMALL LETTER RHO, GREEK RHO SYMBOL\n    (0x3c1, 0x3f1), # \u03c1\u03f1\n    # GREEK SMALL LETTER FINAL SIGMA, GREEK SMALL LETTER SIGMA\n    (0x3c2, 0x3c3), # \u03c2\u03c3\n    # GREEK SMALL LETTER PHI, GREEK PHI SYMBOL\n    (0x3c6, 0x3d5), # \u03c6\u03d5\n    # LATIN SMALL LETTER S WITH DOT ABOVE, LATIN SMALL LETTER LONG S WITH DOT ABOVE\n    (0x1e61, 0x1e9b), # \u1e61\u1e9b\n)\n\n# Maps the lowercase code to lowercase codes which have the same uppercase.\n_ignorecase_fixes = {i: tuple(j for j in t if i != j)\n                     for t in _equivalences for i in t}\n\ndef _compile(code, pattern, flags):\n    # internal: compile a (sub)pattern\n    emit = code.append\n    _len = len\n    LITERAL_CODES = _LITERAL_CODES\n    REPEATING_CODES = _REPEATING_CODES\n    SUCCESS_CODES = _SUCCESS_CODES\n    ASSERT_CODES = _ASSERT_CODES\n    if (flags & SRE_FLAG_IGNORECASE and\n            not (flags & SRE_FLAG_LOCALE) and\n            flags & SRE_FLAG_UNICODE):\n        fixes = _ignorecase_fixes\n    else:\n        fixes = None\n    for op, av in pattern:\n        if op in LITERAL_CODES:\n            if flags & SRE_FLAG_IGNORECASE:\n                lo = _sre.getlower(av, flags)\n                if fixes and lo in fixes:\n                    emit(OPCODES[IN_IGNORE])\n                    skip = _len(code); emit(0)\n                    if op is NOT_LITERAL:\n                        emit(OPCODES[NEGATE])\n                    for k in (lo,) + fixes[lo]:\n                        emit(OPCODES[LITERAL])\n                        emit(k)\n                    emit(OPCODES[FAILURE])\n                    code[skip] = _len(code) - skip\n                else:\n                    emit(OPCODES[OP_IGNORE[op]])\n                    emit(lo)\n            else:\n                emit(OPCODES[op])\n                emit(av)\n        elif op is IN:\n            if flags & SRE_FLAG_IGNORECASE:\n                emit(OPCODES[OP_IGNORE[op]])\n                def fixup(literal, flags=flags):\n                    return _sre.getlower(literal, flags)\n            else:\n                emit(OPCODES[op])\n                fixup = None\n            skip = _len(code); emit(0)\n            _compile_charset(av, flags, code, fixup, fixes)\n            code[skip] = _len(code) - skip\n        elif op is ANY:\n            if flags & SRE_FLAG_DOTALL:\n                emit(OPCODES[ANY_ALL])\n            else:\n                emit(OPCODES[ANY])\n        elif op in REPEATING_CODES:\n            if flags & SRE_FLAG_TEMPLATE:\n                raise error, \"internal: unsupported template operator\"\n                emit(OPCODES[REPEAT])\n                skip = _len(code); emit(0)\n                emit(av[0])\n                emit(av[1])\n                _compile(code, av[2], flags)\n                emit(OPCODES[SUCCESS])\n                code[skip] = _len(code) - skip\n            elif _simple(av) and op is not REPEAT:\n                if op is MAX_REPEAT:\n                    emit(OPCODES[REPEAT_ONE])\n                else:\n                    emit(OPCODES[MIN_REPEAT_ONE])\n                skip = _len(code); emit(0)\n                emit(av[0])\n                emit(av[1])\n                _compile(code, av[2], flags)\n                emit(OPCODES[SUCCESS])\n                code[skip] = _len(code) - skip\n            else:\n                emit(OPCODES[REPEAT])\n                skip = _len(code); emit(0)\n                emit(av[0])\n                emit(av[1])\n                _compile(code, av[2], flags)\n                code[skip] = _len(code) - skip\n                if op is MAX_REPEAT:\n                    emit(OPCODES[MAX_UNTIL])\n                else:\n                    emit(OPCODES[MIN_UNTIL])\n        elif op is SUBPATTERN:\n            if av[0]:\n                emit(OPCODES[MARK])\n                emit((av[0]-1)*2)\n            # _compile_info(code, av[1], flags)\n            _compile(code, av[1], flags)\n            if av[0]:\n                emit(OPCODES[MARK])\n                emit((av[0]-1)*2+1)\n        elif op in SUCCESS_CODES:\n            emit(OPCODES[op])\n        elif op in ASSERT_CODES:\n            emit(OPCODES[op])\n            skip = _len(code); emit(0)\n            if av[0] >= 0:\n                emit(0) # look ahead\n            else:\n                lo, hi = av[1].getwidth()\n                if lo != hi:\n                    raise error, \"look-behind requires fixed-width pattern\"\n                emit(lo) # look behind\n            _compile(code, av[1], flags)\n            emit(OPCODES[SUCCESS])\n            code[skip] = _len(code) - skip\n        elif op is CALL:\n            emit(OPCODES[op])\n            skip = _len(code); emit(0)\n            _compile(code, av, flags)\n            emit(OPCODES[SUCCESS])\n            code[skip] = _len(code) - skip\n        elif op is AT:\n            emit(OPCODES[op])\n            if flags & SRE_FLAG_MULTILINE:\n                av = AT_MULTILINE.get(av, av)\n            if flags & SRE_FLAG_LOCALE:\n                av = AT_LOCALE.get(av, av)\n            elif flags & SRE_FLAG_UNICODE:\n                av = AT_UNICODE.get(av, av)\n            emit(ATCODES[av])\n        elif op is BRANCH:\n            emit(OPCODES[op])\n            tail = []\n            tailappend = tail.append\n            for av in av[1]:\n                skip = _len(code); emit(0)\n                # _compile_info(code, av, flags)\n                _compile(code, av, flags)\n                emit(OPCODES[JUMP])\n                tailappend(_len(code)); emit(0)\n                code[skip] = _len(code) - skip\n            emit(0) # end of branch\n            for tail in tail:\n                code[tail] = _len(code) - tail\n        elif op is CATEGORY:\n            emit(OPCODES[op])\n            if flags & SRE_FLAG_LOCALE:\n                av = CH_LOCALE[av]\n            elif flags & SRE_FLAG_UNICODE:\n                av = CH_UNICODE[av]\n            emit(CHCODES[av])\n        elif op is GROUPREF:\n            if flags & SRE_FLAG_IGNORECASE:\n                emit(OPCODES[OP_IGNORE[op]])\n            else:\n                emit(OPCODES[op])\n            emit(av-1)\n        elif op is GROUPREF_EXISTS:\n            emit(OPCODES[op])\n            emit(av[0]-1)\n            skipyes = _len(code); emit(0)\n            _compile(code, av[1], flags)\n            if av[2]:\n                emit(OPCODES[JUMP])\n                skipno = _len(code); emit(0)\n                code[skipyes] = _len(code) - skipyes + 1\n                _compile(code, av[2], flags)\n                code[skipno] = _len(code) - skipno\n            else:\n                code[skipyes] = _len(code) - skipyes + 1\n        else:\n            raise ValueError, (\"unsupported operand type\", op)\n\ndef _compile_charset(charset, flags, code, fixup=None, fixes=None):\n    # compile charset subprogram\n    emit = code.append\n    for op, av in _optimize_charset(charset, fixup, fixes,\n                                    flags & SRE_FLAG_UNICODE):\n        emit(OPCODES[op])\n        if op is NEGATE:\n            pass\n        elif op is LITERAL:\n            emit(av)\n        elif op is RANGE:\n            emit(av[0])\n            emit(av[1])\n        elif op is CHARSET:\n            code.extend(av)\n        elif op is BIGCHARSET:\n            code.extend(av)\n        elif op is CATEGORY:\n            if flags & SRE_FLAG_LOCALE:\n                emit(CHCODES[CH_LOCALE[av]])\n            elif flags & SRE_FLAG_UNICODE:\n                emit(CHCODES[CH_UNICODE[av]])\n            else:\n                emit(CHCODES[av])\n        else:\n            raise error, \"internal: unsupported set operator\"\n    emit(OPCODES[FAILURE])\n\ndef _optimize_charset(charset, fixup, fixes, isunicode):\n    # internal: optimize character set\n    out = []\n    tail = []\n    charmap = bytearray(256)\n    for op, av in charset:\n        while True:\n            try:\n                if op is LITERAL:\n                    if fixup:\n                        i = fixup(av)\n                        charmap[i] = 1\n                        if fixes and i in fixes:\n                            for k in fixes[i]:\n                                charmap[k] = 1\n                    else:\n                        charmap[av] = 1\n                elif op is RANGE:\n                    r = range(av[0], av[1]+1)\n                    if fixup:\n                        r = map(fixup, r)\n                    if fixup and fixes:\n                        for i in r:\n                            charmap[i] = 1\n                            if i in fixes:\n                                for k in fixes[i]:\n                                    charmap[k] = 1\n                    else:\n                        for i in r:\n                            charmap[i] = 1\n                elif op is NEGATE:\n                    out.append((op, av))\n                else:\n                    tail.append((op, av))\n            except IndexError:\n                if len(charmap) == 256:\n                    # character set contains non-UCS1 character codes\n                    charmap += b'\\0' * 0xff00\n                    continue\n                # character set contains non-BMP character codes\n                if fixup and isunicode and op is RANGE:\n                    lo, hi = av\n                    ranges = [av]\n                    # There are only two ranges of cased astral characters:\n                    # 10400-1044F (Deseret) and 118A0-118DF (Warang Citi).\n                    _fixup_range(max(0x10000, lo), min(0x11fff, hi),\n                                 ranges, fixup)\n                    for lo, hi in ranges:\n                        if lo == hi:\n                            tail.append((LITERAL, hi))\n                        else:\n                            tail.append((RANGE, (lo, hi)))\n                else:\n                    tail.append((op, av))\n            break\n\n    # compress character map\n    runs = []\n    q = 0\n    while True:\n        p = charmap.find(b'\\1', q)\n        if p < 0:\n            break\n        if len(runs) >= 2:\n            runs = None\n            break\n        q = charmap.find(b'\\0', p)\n        if q < 0:\n            runs.append((p, len(charmap)))\n            break\n        runs.append((p, q))\n    if runs is not None:\n        # use literal/range\n        for p, q in runs:\n            if q - p == 1:\n                out.append((LITERAL, p))\n            else:\n                out.append((RANGE, (p, q - 1)))\n        out += tail\n        # if the case was changed or new representation is more compact\n        if fixup or len(out) < len(charset):\n            return out\n        # else original character set is good enough\n        return charset\n\n    # use bitmap\n    if len(charmap) == 256:\n        data = _mk_bitmap(charmap)\n        out.append((CHARSET, data))\n        out += tail\n        return out\n\n    # To represent a big charset, first a bitmap of all characters in the\n    # set is constructed. Then, this bitmap is sliced into chunks of 256\n    # characters, duplicate chunks are eliminated, and each chunk is\n    # given a number. In the compiled expression, the charset is\n    # represented by a 32-bit word sequence, consisting of one word for\n    # the number of different chunks, a sequence of 256 bytes (64 words)\n    # of chunk numbers indexed by their original chunk position, and a\n    # sequence of 256-bit chunks (8 words each).\n\n    # Compression is normally good: in a typical charset, large ranges of\n    # Unicode will be either completely excluded (e.g. if only cyrillic\n    # letters are to be matched), or completely included (e.g. if large\n    # subranges of Kanji match). These ranges will be represented by\n    # chunks of all one-bits or all zero-bits.\n\n    # Matching can be also done efficiently: the more significant byte of\n    # the Unicode character is an index into the chunk number, and the\n    # less significant byte is a bit index in the chunk (just like the\n    # CHARSET matching).\n\n    # In UCS-4 mode, the BIGCHARSET opcode still supports only subsets\n    # of the basic multilingual plane; an efficient representation\n    # for all of Unicode has not yet been developed.\n\n    charmap = bytes(charmap) # should be hashable\n    comps = {}\n    mapping = bytearray(256)\n    block = 0\n    data = bytearray()\n    for i in range(0, 65536, 256):\n        chunk = charmap[i: i + 256]\n        if chunk in comps:\n            mapping[i // 256] = comps[chunk]\n        else:\n            mapping[i // 256] = comps[chunk] = block\n            block += 1\n            data += chunk\n    data = _mk_bitmap(data)\n    data[0:0] = [block] + _bytes_to_codes(mapping)\n    out.append((BIGCHARSET, data))\n    out += tail\n    return out\n\ndef _fixup_range(lo, hi, ranges, fixup):\n    for i in map(fixup, range(lo, hi+1)):\n        for k, (lo, hi) in enumerate(ranges):\n            if i < lo:\n                if l == lo - 1:\n                    ranges[k] = (i, hi)\n                else:\n                    ranges.insert(k, (i, i))\n                break\n            elif i > hi:\n                if i == hi + 1:\n                    ranges[k] = (lo, i)\n                    break\n            else:\n                break\n        else:\n            ranges.append((i, i))\n\n_CODEBITS = _sre.CODESIZE * 8\n_BITS_TRANS = b'0' + b'1' * 255\ndef _mk_bitmap(bits, _CODEBITS=_CODEBITS, _int=int):\n    s = bytes(bits).translate(_BITS_TRANS)[::-1]\n    return [_int(s[i - _CODEBITS: i], 2)\n            for i in range(len(s), 0, -_CODEBITS)]\n\ndef _bytes_to_codes(b):\n    # Convert block indices to word array\n    import array\n    if _sre.CODESIZE == 2:\n        code = 'H'\n    else:\n        code = 'I'\n    a = array.array(code, bytes(b))\n    assert a.itemsize == _sre.CODESIZE\n    assert len(a) * a.itemsize == len(b)\n    return a.tolist()\n\ndef _simple(av):\n    # check if av is a \"simple\" operator\n    lo, hi = av[2].getwidth()\n    return lo == hi == 1 and av[2][0][0] != SUBPATTERN\n\ndef _compile_info(code, pattern, flags):\n    # internal: compile an info block.  in the current version,\n    # this contains min/max pattern width, and an optional literal\n    # prefix or a character map\n    lo, hi = pattern.getwidth()\n    if lo == 0:\n        return # not worth it\n    # look for a literal prefix\n    prefix = []\n    prefixappend = prefix.append\n    prefix_skip = 0\n    charset = [] # not used\n    charsetappend = charset.append\n    if not (flags & SRE_FLAG_IGNORECASE):\n        # look for literal prefix\n        for op, av in pattern.data:\n            if op is LITERAL:\n                if len(prefix) == prefix_skip:\n                    prefix_skip = prefix_skip + 1\n                prefixappend(av)\n            elif op is SUBPATTERN and len(av[1]) == 1:\n                op, av = av[1][0]\n                if op is LITERAL:\n                    prefixappend(av)\n                else:\n                    break\n            else:\n                break\n        # if no prefix, look for charset prefix\n        if not prefix and pattern.data:\n            op, av = pattern.data[0]\n            if op is SUBPATTERN and av[1]:\n                op, av = av[1][0]\n                if op is LITERAL:\n                    charsetappend((op, av))\n                elif op is BRANCH:\n                    c = []\n                    cappend = c.append\n                    for p in av[1]:\n                        if not p:\n                            break\n                        op, av = p[0]\n                        if op is LITERAL:\n                            cappend((op, av))\n                        else:\n                            break\n                    else:\n                        charset = c\n            elif op is BRANCH:\n                c = []\n                cappend = c.append\n                for p in av[1]:\n                    if not p:\n                        break\n                    op, av = p[0]\n                    if op is LITERAL:\n                        cappend((op, av))\n                    else:\n                        break\n                else:\n                    charset = c\n            elif op is IN:\n                charset = av\n##     if prefix:\n##         print \"*** PREFIX\", prefix, prefix_skip\n##     if charset:\n##         print \"*** CHARSET\", charset\n    # add an info block\n    emit = code.append\n    emit(OPCODES[INFO])\n    skip = len(code); emit(0)\n    # literal flag\n    mask = 0\n    if prefix:\n        mask = SRE_INFO_PREFIX\n        if len(prefix) == prefix_skip == len(pattern.data):\n            mask = mask + SRE_INFO_LITERAL\n    elif charset:\n        mask = mask + SRE_INFO_CHARSET\n    emit(mask)\n    # pattern length\n    if lo < MAXCODE:\n        emit(lo)\n    else:\n        emit(MAXCODE)\n        prefix = prefix[:MAXCODE]\n    if hi < MAXCODE:\n        emit(hi)\n    else:\n        emit(0)\n    # add literal prefix\n    if prefix:\n        emit(len(prefix)) # length\n        emit(prefix_skip) # skip\n        code.extend(prefix)\n        # generate overlap table\n        table = [-1] + ([0]*len(prefix))\n        for i in xrange(len(prefix)):\n            table[i+1] = table[i]+1\n            while table[i+1] > 0 and prefix[i] != prefix[table[i+1]-1]:\n                table[i+1] = table[table[i+1]-1]+1\n        code.extend(table[1:]) # don't store first entry\n    elif charset:\n        _compile_charset(charset, flags, code)\n    code[skip] = len(code) - skip\n\ntry:\n    unicode\nexcept NameError:\n    STRING_TYPES = (type(\"\"),)\nelse:\n    STRING_TYPES = (type(\"\"), type(unicode(\"\")))\n\ndef isstring(obj):\n    for tp in STRING_TYPES:\n        if isinstance(obj, tp):\n            return 1\n    return 0\n\ndef _code(p, flags):\n\n    flags = p.pattern.flags | flags\n    code = []\n\n    # compile info block\n    _compile_info(code, p, flags)\n\n    # compile the pattern\n    _compile(code, p.data, flags)\n\n    code.append(OPCODES[SUCCESS])\n\n    return code\n\ndef compile(p, flags=0):\n    # internal: convert pattern list to internal format\n\n    if isstring(p):\n        pattern = p\n        p = sre_parse.parse(p, flags)\n    else:\n        pattern = None\n\n    code = _code(p, flags)\n\n    # print code\n\n    # XXX: <fl> get rid of this limitation!\n    if p.pattern.groups > 100:\n        raise AssertionError(\n            \"sorry, but this version only supports 100 named groups\"\n            )\n\n    # map in either direction\n    groupindex = p.pattern.groupdict\n    indexgroup = [None] * p.pattern.groups\n    for k, i in groupindex.items():\n        indexgroup[i] = k\n\n    return _sre.compile(\n        pattern, flags | p.pattern.flags, code,\n        p.pattern.groups-1,\n        groupindex, indexgroup\n        )\n", 
    "sre_constants": "#\n# Secret Labs' Regular Expression Engine\n#\n# various symbols used by the regular expression engine.\n# run this script to update the _sre include files!\n#\n# Copyright (c) 1998-2001 by Secret Labs AB.  All rights reserved.\n#\n# See the sre.py file for information on usage and redistribution.\n#\n\n\"\"\"Internal support module for sre\"\"\"\n\n# update when constants are added or removed\n\nMAGIC = 20031017\n\ntry:\n    from _sre import MAXREPEAT\nexcept ImportError:\n    import _sre\n    MAXREPEAT = _sre.MAXREPEAT = 65535\n\n# SRE standard exception (access as sre.error)\n# should this really be here?\n\nclass error(Exception):\n    pass\n\n# operators\n\nFAILURE = \"failure\"\nSUCCESS = \"success\"\n\nANY = \"any\"\nANY_ALL = \"any_all\"\nASSERT = \"assert\"\nASSERT_NOT = \"assert_not\"\nAT = \"at\"\nBIGCHARSET = \"bigcharset\"\nBRANCH = \"branch\"\nCALL = \"call\"\nCATEGORY = \"category\"\nCHARSET = \"charset\"\nGROUPREF = \"groupref\"\nGROUPREF_IGNORE = \"groupref_ignore\"\nGROUPREF_EXISTS = \"groupref_exists\"\nIN = \"in\"\nIN_IGNORE = \"in_ignore\"\nINFO = \"info\"\nJUMP = \"jump\"\nLITERAL = \"literal\"\nLITERAL_IGNORE = \"literal_ignore\"\nMARK = \"mark\"\nMAX_REPEAT = \"max_repeat\"\nMAX_UNTIL = \"max_until\"\nMIN_REPEAT = \"min_repeat\"\nMIN_UNTIL = \"min_until\"\nNEGATE = \"negate\"\nNOT_LITERAL = \"not_literal\"\nNOT_LITERAL_IGNORE = \"not_literal_ignore\"\nRANGE = \"range\"\nREPEAT = \"repeat\"\nREPEAT_ONE = \"repeat_one\"\nSUBPATTERN = \"subpattern\"\nMIN_REPEAT_ONE = \"min_repeat_one\"\n\n# positions\nAT_BEGINNING = \"at_beginning\"\nAT_BEGINNING_LINE = \"at_beginning_line\"\nAT_BEGINNING_STRING = \"at_beginning_string\"\nAT_BOUNDARY = \"at_boundary\"\nAT_NON_BOUNDARY = \"at_non_boundary\"\nAT_END = \"at_end\"\nAT_END_LINE = \"at_end_line\"\nAT_END_STRING = \"at_end_string\"\nAT_LOC_BOUNDARY = \"at_loc_boundary\"\nAT_LOC_NON_BOUNDARY = \"at_loc_non_boundary\"\nAT_UNI_BOUNDARY = \"at_uni_boundary\"\nAT_UNI_NON_BOUNDARY = \"at_uni_non_boundary\"\n\n# categories\nCATEGORY_DIGIT = \"category_digit\"\nCATEGORY_NOT_DIGIT = \"category_not_digit\"\nCATEGORY_SPACE = \"category_space\"\nCATEGORY_NOT_SPACE = \"category_not_space\"\nCATEGORY_WORD = \"category_word\"\nCATEGORY_NOT_WORD = \"category_not_word\"\nCATEGORY_LINEBREAK = \"category_linebreak\"\nCATEGORY_NOT_LINEBREAK = \"category_not_linebreak\"\nCATEGORY_LOC_WORD = \"category_loc_word\"\nCATEGORY_LOC_NOT_WORD = \"category_loc_not_word\"\nCATEGORY_UNI_DIGIT = \"category_uni_digit\"\nCATEGORY_UNI_NOT_DIGIT = \"category_uni_not_digit\"\nCATEGORY_UNI_SPACE = \"category_uni_space\"\nCATEGORY_UNI_NOT_SPACE = \"category_uni_not_space\"\nCATEGORY_UNI_WORD = \"category_uni_word\"\nCATEGORY_UNI_NOT_WORD = \"category_uni_not_word\"\nCATEGORY_UNI_LINEBREAK = \"category_uni_linebreak\"\nCATEGORY_UNI_NOT_LINEBREAK = \"category_uni_not_linebreak\"\n\nOPCODES = [\n\n    # failure=0 success=1 (just because it looks better that way :-)\n    FAILURE, SUCCESS,\n\n    ANY, ANY_ALL,\n    ASSERT, ASSERT_NOT,\n    AT,\n    BRANCH,\n    CALL,\n    CATEGORY,\n    CHARSET, BIGCHARSET,\n    GROUPREF, GROUPREF_EXISTS, GROUPREF_IGNORE,\n    IN, IN_IGNORE,\n    INFO,\n    JUMP,\n    LITERAL, LITERAL_IGNORE,\n    MARK,\n    MAX_UNTIL,\n    MIN_UNTIL,\n    NOT_LITERAL, NOT_LITERAL_IGNORE,\n    NEGATE,\n    RANGE,\n    REPEAT,\n    REPEAT_ONE,\n    SUBPATTERN,\n    MIN_REPEAT_ONE\n\n]\n\nATCODES = [\n    AT_BEGINNING, AT_BEGINNING_LINE, AT_BEGINNING_STRING, AT_BOUNDARY,\n    AT_NON_BOUNDARY, AT_END, AT_END_LINE, AT_END_STRING,\n    AT_LOC_BOUNDARY, AT_LOC_NON_BOUNDARY, AT_UNI_BOUNDARY,\n    AT_UNI_NON_BOUNDARY\n]\n\nCHCODES = [\n    CATEGORY_DIGIT, CATEGORY_NOT_DIGIT, CATEGORY_SPACE,\n    CATEGORY_NOT_SPACE, CATEGORY_WORD, CATEGORY_NOT_WORD,\n    CATEGORY_LINEBREAK, CATEGORY_NOT_LINEBREAK, CATEGORY_LOC_WORD,\n    CATEGORY_LOC_NOT_WORD, CATEGORY_UNI_DIGIT, CATEGORY_UNI_NOT_DIGIT,\n    CATEGORY_UNI_SPACE, CATEGORY_UNI_NOT_SPACE, CATEGORY_UNI_WORD,\n    CATEGORY_UNI_NOT_WORD, CATEGORY_UNI_LINEBREAK,\n    CATEGORY_UNI_NOT_LINEBREAK\n]\n\ndef makedict(list):\n    d = {}\n    i = 0\n    for item in list:\n        d[item] = i\n        i = i + 1\n    return d\n\nOPCODES = makedict(OPCODES)\nATCODES = makedict(ATCODES)\nCHCODES = makedict(CHCODES)\n\n# replacement operations for \"ignore case\" mode\nOP_IGNORE = {\n    GROUPREF: GROUPREF_IGNORE,\n    IN: IN_IGNORE,\n    LITERAL: LITERAL_IGNORE,\n    NOT_LITERAL: NOT_LITERAL_IGNORE\n}\n\nAT_MULTILINE = {\n    AT_BEGINNING: AT_BEGINNING_LINE,\n    AT_END: AT_END_LINE\n}\n\nAT_LOCALE = {\n    AT_BOUNDARY: AT_LOC_BOUNDARY,\n    AT_NON_BOUNDARY: AT_LOC_NON_BOUNDARY\n}\n\nAT_UNICODE = {\n    AT_BOUNDARY: AT_UNI_BOUNDARY,\n    AT_NON_BOUNDARY: AT_UNI_NON_BOUNDARY\n}\n\nCH_LOCALE = {\n    CATEGORY_DIGIT: CATEGORY_DIGIT,\n    CATEGORY_NOT_DIGIT: CATEGORY_NOT_DIGIT,\n    CATEGORY_SPACE: CATEGORY_SPACE,\n    CATEGORY_NOT_SPACE: CATEGORY_NOT_SPACE,\n    CATEGORY_WORD: CATEGORY_LOC_WORD,\n    CATEGORY_NOT_WORD: CATEGORY_LOC_NOT_WORD,\n    CATEGORY_LINEBREAK: CATEGORY_LINEBREAK,\n    CATEGORY_NOT_LINEBREAK: CATEGORY_NOT_LINEBREAK\n}\n\nCH_UNICODE = {\n    CATEGORY_DIGIT: CATEGORY_UNI_DIGIT,\n    CATEGORY_NOT_DIGIT: CATEGORY_UNI_NOT_DIGIT,\n    CATEGORY_SPACE: CATEGORY_UNI_SPACE,\n    CATEGORY_NOT_SPACE: CATEGORY_UNI_NOT_SPACE,\n    CATEGORY_WORD: CATEGORY_UNI_WORD,\n    CATEGORY_NOT_WORD: CATEGORY_UNI_NOT_WORD,\n    CATEGORY_LINEBREAK: CATEGORY_UNI_LINEBREAK,\n    CATEGORY_NOT_LINEBREAK: CATEGORY_UNI_NOT_LINEBREAK\n}\n\n# flags\nSRE_FLAG_TEMPLATE = 1 # template mode (disable backtracking)\nSRE_FLAG_IGNORECASE = 2 # case insensitive\nSRE_FLAG_LOCALE = 4 # honour system locale\nSRE_FLAG_MULTILINE = 8 # treat target as multiline string\nSRE_FLAG_DOTALL = 16 # treat target as a single string\nSRE_FLAG_UNICODE = 32 # use unicode locale\nSRE_FLAG_VERBOSE = 64 # ignore whitespace and comments\nSRE_FLAG_DEBUG = 128 # debugging\n\n# flags for INFO primitive\nSRE_INFO_PREFIX = 1 # has prefix\nSRE_INFO_LITERAL = 2 # entire pattern is literal (given by prefix)\nSRE_INFO_CHARSET = 4 # pattern starts with character from given set\n\nif __name__ == \"__main__\":\n    def dump(f, d, prefix):\n        items = d.items()\n        items.sort(key=lambda a: a[1])\n        for k, v in items:\n            f.write(\"#define %s_%s %s\\n\" % (prefix, k.upper(), v))\n    f = open(\"sre_constants.h\", \"w\")\n    f.write(\"\"\"\\\n/*\n * Secret Labs' Regular Expression Engine\n *\n * regular expression matching engine\n *\n * NOTE: This file is generated by sre_constants.py.  If you need\n * to change anything in here, edit sre_constants.py and run it.\n *\n * Copyright (c) 1997-2001 by Secret Labs AB.  All rights reserved.\n *\n * See the _sre.c file for information on usage and redistribution.\n */\n\n\"\"\")\n\n    f.write(\"#define SRE_MAGIC %d\\n\" % MAGIC)\n\n    dump(f, OPCODES, \"SRE_OP\")\n    dump(f, ATCODES, \"SRE\")\n    dump(f, CHCODES, \"SRE\")\n\n    f.write(\"#define SRE_FLAG_TEMPLATE %d\\n\" % SRE_FLAG_TEMPLATE)\n    f.write(\"#define SRE_FLAG_IGNORECASE %d\\n\" % SRE_FLAG_IGNORECASE)\n    f.write(\"#define SRE_FLAG_LOCALE %d\\n\" % SRE_FLAG_LOCALE)\n    f.write(\"#define SRE_FLAG_MULTILINE %d\\n\" % SRE_FLAG_MULTILINE)\n    f.write(\"#define SRE_FLAG_DOTALL %d\\n\" % SRE_FLAG_DOTALL)\n    f.write(\"#define SRE_FLAG_UNICODE %d\\n\" % SRE_FLAG_UNICODE)\n    f.write(\"#define SRE_FLAG_VERBOSE %d\\n\" % SRE_FLAG_VERBOSE)\n\n    f.write(\"#define SRE_INFO_PREFIX %d\\n\" % SRE_INFO_PREFIX)\n    f.write(\"#define SRE_INFO_LITERAL %d\\n\" % SRE_INFO_LITERAL)\n    f.write(\"#define SRE_INFO_CHARSET %d\\n\" % SRE_INFO_CHARSET)\n\n    f.close()\n    print \"done\"\n", 
    "sre_parse": "#\n# Secret Labs' Regular Expression Engine\n#\n# convert re-style regular expression to sre pattern\n#\n# Copyright (c) 1998-2001 by Secret Labs AB.  All rights reserved.\n#\n# See the sre.py file for information on usage and redistribution.\n#\n\n\"\"\"Internal support module for sre\"\"\"\n\n# XXX: show string offset and offending character for all errors\n\nimport sys\n\nfrom sre_constants import *\n\ntry:\n    from __pypy__ import newdict\nexcept ImportError:\n    assert '__pypy__' not in sys.builtin_module_names\n    newdict = lambda _ : {}\n\nSPECIAL_CHARS = \".\\\\[{()*+?^$|\"\nREPEAT_CHARS = \"*+?{\"\n\nDIGITS = set(\"0123456789\")\n\nOCTDIGITS = set(\"01234567\")\nHEXDIGITS = set(\"0123456789abcdefABCDEF\")\n\nWHITESPACE = set(\" \\t\\n\\r\\v\\f\")\n\nESCAPES = {\n    r\"\\a\": (LITERAL, ord(\"\\a\")),\n    r\"\\b\": (LITERAL, ord(\"\\b\")),\n    r\"\\f\": (LITERAL, ord(\"\\f\")),\n    r\"\\n\": (LITERAL, ord(\"\\n\")),\n    r\"\\r\": (LITERAL, ord(\"\\r\")),\n    r\"\\t\": (LITERAL, ord(\"\\t\")),\n    r\"\\v\": (LITERAL, ord(\"\\v\")),\n    r\"\\\\\": (LITERAL, ord(\"\\\\\"))\n}\n\nCATEGORIES = {\n    r\"\\A\": (AT, AT_BEGINNING_STRING), # start of string\n    r\"\\b\": (AT, AT_BOUNDARY),\n    r\"\\B\": (AT, AT_NON_BOUNDARY),\n    r\"\\d\": (IN, [(CATEGORY, CATEGORY_DIGIT)]),\n    r\"\\D\": (IN, [(CATEGORY, CATEGORY_NOT_DIGIT)]),\n    r\"\\s\": (IN, [(CATEGORY, CATEGORY_SPACE)]),\n    r\"\\S\": (IN, [(CATEGORY, CATEGORY_NOT_SPACE)]),\n    r\"\\w\": (IN, [(CATEGORY, CATEGORY_WORD)]),\n    r\"\\W\": (IN, [(CATEGORY, CATEGORY_NOT_WORD)]),\n    r\"\\Z\": (AT, AT_END_STRING), # end of string\n}\n\nFLAGS = {\n    # standard flags\n    \"i\": SRE_FLAG_IGNORECASE,\n    \"L\": SRE_FLAG_LOCALE,\n    \"m\": SRE_FLAG_MULTILINE,\n    \"s\": SRE_FLAG_DOTALL,\n    \"x\": SRE_FLAG_VERBOSE,\n    # extensions\n    \"t\": SRE_FLAG_TEMPLATE,\n    \"u\": SRE_FLAG_UNICODE,\n}\n\nclass Pattern:\n    # master pattern object.  keeps track of global attributes\n    def __init__(self):\n        self.flags = 0\n        self.open = []\n        self.groups = 1\n        self.groupdict = newdict(\"module\")\n    def opengroup(self, name=None):\n        gid = self.groups\n        self.groups = gid + 1\n        if name is not None:\n            ogid = self.groupdict.get(name, None)\n            if ogid is not None:\n                raise error, (\"redefinition of group name %s as group %d; \"\n                              \"was group %d\" % (repr(name), gid,  ogid))\n            self.groupdict[name] = gid\n        self.open.append(gid)\n        return gid\n    def closegroup(self, gid):\n        self.open.remove(gid)\n    def checkgroup(self, gid):\n        return gid < self.groups and gid not in self.open\n\nclass SubPattern:\n    # a subpattern, in intermediate form\n    def __init__(self, pattern, data=None):\n        self.pattern = pattern\n        if data is None:\n            data = []\n        self.data = data\n        self.width = None\n    def dump(self, level=0):\n        seqtypes = (tuple, list)\n        for op, av in self.data:\n            print level*\"  \" + op,\n            if op == IN:\n                # member sublanguage\n                print\n                for op, a in av:\n                    print (level+1)*\"  \" + op, a\n            elif op == BRANCH:\n                print\n                for i, a in enumerate(av[1]):\n                    if i:\n                        print level*\"  \" + \"or\"\n                    a.dump(level+1)\n            elif op == GROUPREF_EXISTS:\n                condgroup, item_yes, item_no = av\n                print condgroup\n                item_yes.dump(level+1)\n                if item_no:\n                    print level*\"  \" + \"else\"\n                    item_no.dump(level+1)\n            elif isinstance(av, seqtypes):\n                nl = 0\n                for a in av:\n                    if isinstance(a, SubPattern):\n                        if not nl:\n                            print\n                        a.dump(level+1)\n                        nl = 1\n                    else:\n                        print a,\n                        nl = 0\n                if not nl:\n                    print\n            else:\n                print av\n    def __repr__(self):\n        return repr(self.data)\n    def __len__(self):\n        return len(self.data)\n    def __delitem__(self, index):\n        del self.data[index]\n    def __getitem__(self, index):\n        if isinstance(index, slice):\n            return SubPattern(self.pattern, self.data[index])\n        return self.data[index]\n    def __setitem__(self, index, code):\n        self.data[index] = code\n    def insert(self, index, code):\n        self.data.insert(index, code)\n    def append(self, code):\n        self.data.append(code)\n    def getwidth(self):\n        # determine the width (min, max) for this subpattern\n        if self.width:\n            return self.width\n        lo = hi = 0\n        UNITCODES = (ANY, RANGE, IN, LITERAL, NOT_LITERAL, CATEGORY)\n        REPEATCODES = (MIN_REPEAT, MAX_REPEAT)\n        for op, av in self.data:\n            if op is BRANCH:\n                i = MAXREPEAT - 1\n                j = 0\n                for av in av[1]:\n                    l, h = av.getwidth()\n                    i = min(i, l)\n                    j = max(j, h)\n                lo = lo + i\n                hi = hi + j\n            elif op is CALL:\n                i, j = av.getwidth()\n                lo = lo + i\n                hi = hi + j\n            elif op is SUBPATTERN:\n                i, j = av[1].getwidth()\n                lo = lo + i\n                hi = hi + j\n            elif op in REPEATCODES:\n                i, j = av[2].getwidth()\n                lo = lo + i * av[0]\n                hi = hi + j * av[1]\n            elif op in UNITCODES:\n                lo = lo + 1\n                hi = hi + 1\n            elif op == SUCCESS:\n                break\n        self.width = min(lo, MAXREPEAT - 1), min(hi, MAXREPEAT)\n        return self.width\n\nclass Tokenizer:\n    def __init__(self, string):\n        self.string = string\n        self.index = 0\n        self.__next()\n    def __next(self):\n        if self.index >= len(self.string):\n            self.next = None\n            return\n        char = self.string[self.index]\n        if char[0] == \"\\\\\":\n            try:\n                c = self.string[self.index + 1]\n            except IndexError:\n                raise error, \"bogus escape (end of line)\"\n            char = char + c\n        self.index = self.index + len(char)\n        self.next = char\n    def match(self, char, skip=1):\n        if char == self.next:\n            if skip:\n                self.__next()\n            return 1\n        return 0\n    def get(self):\n        this = self.next\n        self.__next()\n        return this\n    def tell(self):\n        return self.index, self.next\n    def seek(self, index):\n        self.index, self.next = index\n\ndef isident(char):\n    return \"a\" <= char <= \"z\" or \"A\" <= char <= \"Z\" or char == \"_\"\n\ndef isdigit(char):\n    return \"0\" <= char <= \"9\"\n\ndef isname(name):\n    # check that group name is a valid string\n    if not isident(name[0]):\n        return False\n    for char in name[1:]:\n        if not isident(char) and not isdigit(char):\n            return False\n    return True\n\ndef _class_escape(source, escape):\n    # handle escape code inside character class\n    code = ESCAPES.get(escape)\n    if code:\n        return code\n    code = CATEGORIES.get(escape)\n    if code and code[0] == IN:\n        return code\n    try:\n        c = escape[1:2]\n        if c == \"x\":\n            # hexadecimal escape (exactly two digits)\n            while source.next in HEXDIGITS and len(escape) < 4:\n                escape = escape + source.get()\n            escape = escape[2:]\n            if len(escape) != 2:\n                raise error, \"bogus escape: %s\" % repr(\"\\\\\" + escape)\n            return LITERAL, int(escape, 16) & 0xff\n        elif c in OCTDIGITS:\n            # octal escape (up to three digits)\n            while source.next in OCTDIGITS and len(escape) < 4:\n                escape = escape + source.get()\n            escape = escape[1:]\n            return LITERAL, int(escape, 8) & 0xff\n        elif c in DIGITS:\n            raise error, \"bogus escape: %s\" % repr(escape)\n        if len(escape) == 2:\n            return LITERAL, ord(escape[1])\n    except ValueError:\n        pass\n    raise error, \"bogus escape: %s\" % repr(escape)\n\ndef _escape(source, escape, state):\n    # handle escape code in expression\n    code = CATEGORIES.get(escape)\n    if code:\n        return code\n    code = ESCAPES.get(escape)\n    if code:\n        return code\n    try:\n        c = escape[1:2]\n        if c == \"x\":\n            # hexadecimal escape\n            while source.next in HEXDIGITS and len(escape) < 4:\n                escape = escape + source.get()\n            if len(escape) != 4:\n                raise ValueError\n            return LITERAL, int(escape[2:], 16) & 0xff\n        elif c == \"0\":\n            # octal escape\n            while source.next in OCTDIGITS and len(escape) < 4:\n                escape = escape + source.get()\n            return LITERAL, int(escape[1:], 8) & 0xff\n        elif c in DIGITS:\n            # octal escape *or* decimal group reference (sigh)\n            if source.next in DIGITS:\n                escape = escape + source.get()\n                if (escape[1] in OCTDIGITS and escape[2] in OCTDIGITS and\n                    source.next in OCTDIGITS):\n                    # got three octal digits; this is an octal escape\n                    escape = escape + source.get()\n                    return LITERAL, int(escape[1:], 8) & 0xff\n            # not an octal escape, so this is a group reference\n            group = int(escape[1:])\n            if group < state.groups:\n                if not state.checkgroup(group):\n                    raise error, \"cannot refer to open group\"\n                return GROUPREF, group\n            raise ValueError\n        if len(escape) == 2:\n            return LITERAL, ord(escape[1])\n    except ValueError:\n        pass\n    raise error, \"bogus escape: %s\" % repr(escape)\n\ndef _parse_sub(source, state, nested=1):\n    # parse an alternation: a|b|c\n\n    items = []\n    itemsappend = items.append\n    sourcematch = source.match\n    while 1:\n        itemsappend(_parse(source, state))\n        if sourcematch(\"|\"):\n            continue\n        if not nested:\n            break\n        if not source.next or sourcematch(\")\", 0):\n            break\n        else:\n            raise error, \"pattern not properly closed\"\n\n    if len(items) == 1:\n        return items[0]\n\n    subpattern = SubPattern(state)\n    subpatternappend = subpattern.append\n\n    # check if all items share a common prefix\n    while 1:\n        prefix = None\n        for item in items:\n            if not item:\n                break\n            if prefix is None:\n                prefix = item[0]\n            elif item[0] != prefix:\n                break\n        else:\n            # all subitems start with a common \"prefix\".\n            # move it out of the branch\n            for item in items:\n                del item[0]\n            subpatternappend(prefix)\n            continue # check next one\n        break\n\n    # check if the branch can be replaced by a character set\n    for item in items:\n        if len(item) != 1 or item[0][0] != LITERAL:\n            break\n    else:\n        # we can store this as a character set instead of a\n        # branch (the compiler may optimize this even more)\n        set = []\n        setappend = set.append\n        for item in items:\n            setappend(item[0])\n        subpatternappend((IN, set))\n        return subpattern\n\n    subpattern.append((BRANCH, (None, items)))\n    return subpattern\n\ndef _parse_sub_cond(source, state, condgroup):\n    item_yes = _parse(source, state)\n    if source.match(\"|\"):\n        item_no = _parse(source, state)\n        if source.match(\"|\"):\n            raise error, \"conditional backref with more than two branches\"\n    else:\n        item_no = None\n    if source.next and not source.match(\")\", 0):\n        raise error, \"pattern not properly closed\"\n    subpattern = SubPattern(state)\n    subpattern.append((GROUPREF_EXISTS, (condgroup, item_yes, item_no)))\n    return subpattern\n\n_PATTERNENDERS = set(\"|)\")\n_ASSERTCHARS = set(\"=!<\")\n_LOOKBEHINDASSERTCHARS = set(\"=!\")\n_REPEATCODES = set([MIN_REPEAT, MAX_REPEAT])\n\ndef _parse(source, state):\n    # parse a simple pattern\n    subpattern = SubPattern(state)\n\n    # precompute constants into local variables\n    subpatternappend = subpattern.append\n    sourceget = source.get\n    sourcematch = source.match\n    _len = len\n    PATTERNENDERS = _PATTERNENDERS\n    ASSERTCHARS = _ASSERTCHARS\n    LOOKBEHINDASSERTCHARS = _LOOKBEHINDASSERTCHARS\n    REPEATCODES = _REPEATCODES\n\n    while 1:\n\n        if source.next in PATTERNENDERS:\n            break # end of subpattern\n        this = sourceget()\n        if this is None:\n            break # end of pattern\n\n        if state.flags & SRE_FLAG_VERBOSE:\n            # skip whitespace and comments\n            if this in WHITESPACE:\n                continue\n            if this == \"#\":\n                while 1:\n                    this = sourceget()\n                    if this in (None, \"\\n\"):\n                        break\n                continue\n\n        if this and this[0] not in SPECIAL_CHARS:\n            subpatternappend((LITERAL, ord(this)))\n\n        elif this == \"[\":\n            # character set\n            set = []\n            setappend = set.append\n##          if sourcematch(\":\"):\n##              pass # handle character classes\n            if sourcematch(\"^\"):\n                setappend((NEGATE, None))\n            # check remaining characters\n            start = set[:]\n            while 1:\n                this = sourceget()\n                if this == \"]\" and set != start:\n                    break\n                elif this and this[0] == \"\\\\\":\n                    code1 = _class_escape(source, this)\n                elif this:\n                    code1 = LITERAL, ord(this)\n                else:\n                    raise error, \"unexpected end of regular expression\"\n                if sourcematch(\"-\"):\n                    # potential range\n                    this = sourceget()\n                    if this == \"]\":\n                        if code1[0] is IN:\n                            code1 = code1[1][0]\n                        setappend(code1)\n                        setappend((LITERAL, ord(\"-\")))\n                        break\n                    elif this:\n                        if this[0] == \"\\\\\":\n                            code2 = _class_escape(source, this)\n                        else:\n                            code2 = LITERAL, ord(this)\n                        if code1[0] != LITERAL or code2[0] != LITERAL:\n                            raise error, \"bad character range\"\n                        lo = code1[1]\n                        hi = code2[1]\n                        if hi < lo:\n                            raise error, \"bad character range\"\n                        setappend((RANGE, (lo, hi)))\n                    else:\n                        raise error, \"unexpected end of regular expression\"\n                else:\n                    if code1[0] is IN:\n                        code1 = code1[1][0]\n                    setappend(code1)\n\n            # XXX: <fl> should move set optimization to compiler!\n            if _len(set)==1 and set[0][0] is LITERAL:\n                subpatternappend(set[0]) # optimization\n            elif _len(set)==2 and set[0][0] is NEGATE and set[1][0] is LITERAL:\n                subpatternappend((NOT_LITERAL, set[1][1])) # optimization\n            else:\n                # XXX: <fl> should add charmap optimization here\n                subpatternappend((IN, set))\n\n        elif this and this[0] in REPEAT_CHARS:\n            # repeat previous item\n            if this == \"?\":\n                min, max = 0, 1\n            elif this == \"*\":\n                min, max = 0, MAXREPEAT\n\n            elif this == \"+\":\n                min, max = 1, MAXREPEAT\n            elif this == \"{\":\n                if source.next == \"}\":\n                    subpatternappend((LITERAL, ord(this)))\n                    continue\n                here = source.tell()\n                min, max = 0, MAXREPEAT\n                lo = hi = \"\"\n                while source.next in DIGITS:\n                    lo = lo + source.get()\n                if sourcematch(\",\"):\n                    while source.next in DIGITS:\n                        hi = hi + sourceget()\n                else:\n                    hi = lo\n                if not sourcematch(\"}\"):\n                    subpatternappend((LITERAL, ord(this)))\n                    source.seek(here)\n                    continue\n                if lo:\n                    min = int(lo)\n                    if min >= MAXREPEAT:\n                        raise OverflowError(\"the repetition number is too large\")\n                if hi:\n                    max = int(hi)\n                    if max >= MAXREPEAT:\n                        raise OverflowError(\"the repetition number is too large\")\n                    if max < min:\n                        raise error(\"bad repeat interval\")\n            else:\n                raise error, \"not supported\"\n            # figure out which item to repeat\n            if subpattern:\n                item = subpattern[-1:]\n            else:\n                item = None\n            if not item or (_len(item) == 1 and item[0][0] == AT):\n                raise error, \"nothing to repeat\"\n            if item[0][0] in REPEATCODES:\n                raise error, \"multiple repeat\"\n            if sourcematch(\"?\"):\n                subpattern[-1] = (MIN_REPEAT, (min, max, item))\n            else:\n                subpattern[-1] = (MAX_REPEAT, (min, max, item))\n\n        elif this == \".\":\n            subpatternappend((ANY, None))\n\n        elif this == \"(\":\n            group = 1\n            name = None\n            condgroup = None\n            if sourcematch(\"?\"):\n                group = 0\n                # options\n                if sourcematch(\"P\"):\n                    # python extensions\n                    if sourcematch(\"<\"):\n                        # named group: skip forward to end of name\n                        name = \"\"\n                        while 1:\n                            char = sourceget()\n                            if char is None:\n                                raise error, \"unterminated name\"\n                            if char == \">\":\n                                break\n                            name = name + char\n                        group = 1\n                        if not name:\n                            raise error(\"missing group name\")\n                        if not isname(name):\n                            raise error(\"bad character in group name %r\" %\n                                        name)\n                    elif sourcematch(\"=\"):\n                        # named backreference\n                        name = \"\"\n                        while 1:\n                            char = sourceget()\n                            if char is None:\n                                raise error, \"unterminated name\"\n                            if char == \")\":\n                                break\n                            name = name + char\n                        if not name:\n                            raise error(\"missing group name\")\n                        if not isname(name):\n                            raise error(\"bad character in backref group name \"\n                                        \"%r\" % name)\n                        gid = state.groupdict.get(name)\n                        if gid is None:\n                            msg = \"unknown group name: {0!r}\".format(name)\n                            raise error(msg)\n                        subpatternappend((GROUPREF, gid))\n                        continue\n                    else:\n                        char = sourceget()\n                        if char is None:\n                            raise error, \"unexpected end of pattern\"\n                        raise error, \"unknown specifier: ?P%s\" % char\n                elif sourcematch(\":\"):\n                    # non-capturing group\n                    group = 2\n                elif sourcematch(\"#\"):\n                    # comment\n                    while 1:\n                        if source.next is None or source.next == \")\":\n                            break\n                        sourceget()\n                    if not sourcematch(\")\"):\n                        raise error, \"unbalanced parenthesis\"\n                    continue\n                elif source.next in ASSERTCHARS:\n                    # lookahead assertions\n                    char = sourceget()\n                    dir = 1\n                    if char == \"<\":\n                        if source.next not in LOOKBEHINDASSERTCHARS:\n                            raise error, \"syntax error\"\n                        dir = -1 # lookbehind\n                        char = sourceget()\n                    p = _parse_sub(source, state)\n                    if not sourcematch(\")\"):\n                        raise error, \"unbalanced parenthesis\"\n                    if char == \"=\":\n                        subpatternappend((ASSERT, (dir, p)))\n                    else:\n                        subpatternappend((ASSERT_NOT, (dir, p)))\n                    continue\n                elif sourcematch(\"(\"):\n                    # conditional backreference group\n                    condname = \"\"\n                    while 1:\n                        char = sourceget()\n                        if char is None:\n                            raise error, \"unterminated name\"\n                        if char == \")\":\n                            break\n                        condname = condname + char\n                    group = 2\n                    if not condname:\n                        raise error(\"missing group name\")\n                    if isname(condname):\n                        condgroup = state.groupdict.get(condname)\n                        if condgroup is None:\n                            msg = \"unknown group name: {0!r}\".format(condname)\n                            raise error(msg)\n                    else:\n                        try:\n                            condgroup = int(condname)\n                        except ValueError:\n                            raise error, \"bad character in group name\"\n                else:\n                    # flags\n                    if not source.next in FLAGS:\n                        raise error, \"unexpected end of pattern\"\n                    while source.next in FLAGS:\n                        state.flags = state.flags | FLAGS[sourceget()]\n            if group:\n                # parse group contents\n                if group == 2:\n                    # anonymous group\n                    group = None\n                else:\n                    group = state.opengroup(name)\n                if condgroup:\n                    p = _parse_sub_cond(source, state, condgroup)\n                else:\n                    p = _parse_sub(source, state)\n                if not sourcematch(\")\"):\n                    raise error, \"unbalanced parenthesis\"\n                if group is not None:\n                    state.closegroup(group)\n                subpatternappend((SUBPATTERN, (group, p)))\n            else:\n                while 1:\n                    char = sourceget()\n                    if char is None:\n                        raise error, \"unexpected end of pattern\"\n                    if char == \")\":\n                        break\n                    raise error, \"unknown extension\"\n\n        elif this == \"^\":\n            subpatternappend((AT, AT_BEGINNING))\n\n        elif this == \"$\":\n            subpattern.append((AT, AT_END))\n\n        elif this and this[0] == \"\\\\\":\n            code = _escape(source, this, state)\n            subpatternappend(code)\n\n        else:\n            raise error, \"parser error\"\n\n    return subpattern\n\ndef parse(str, flags=0, pattern=None):\n    # parse 're' pattern into list of (opcode, argument) tuples\n\n    source = Tokenizer(str)\n\n    if pattern is None:\n        pattern = Pattern()\n    pattern.flags = flags\n    pattern.str = str\n\n    p = _parse_sub(source, pattern, 0)\n\n    tail = source.get()\n    if tail == \")\":\n        raise error, \"unbalanced parenthesis\"\n    elif tail:\n        raise error, \"bogus characters at end of regular expression\"\n\n    if flags & SRE_FLAG_DEBUG:\n        p.dump()\n\n    if not (flags & SRE_FLAG_VERBOSE) and p.pattern.flags & SRE_FLAG_VERBOSE:\n        # the VERBOSE flag was switched on inside the pattern.  to be\n        # on the safe side, we'll parse the whole thing again...\n        return parse(str, p.pattern.flags)\n\n    return p\n\ndef parse_template(source, pattern):\n    # parse 're' replacement string into list of literals and\n    # group references\n    s = Tokenizer(source)\n    sget = s.get\n    p = []\n    a = p.append\n    def literal(literal, p=p, pappend=a):\n        if p and p[-1][0] is LITERAL:\n            p[-1] = LITERAL, p[-1][1] + literal\n        else:\n            pappend((LITERAL, literal))\n    sep = source[:0]\n    if type(sep) is type(\"\"):\n        makechar = chr\n    else:\n        makechar = unichr\n    while 1:\n        this = sget()\n        if this is None:\n            break # end of replacement string\n        if this and this[0] == \"\\\\\":\n            # group\n            c = this[1:2]\n            if c == \"g\":\n                name = \"\"\n                if s.match(\"<\"):\n                    while 1:\n                        char = sget()\n                        if char is None:\n                            raise error, \"unterminated group name\"\n                        if char == \">\":\n                            break\n                        name = name + char\n                if not name:\n                    raise error, \"missing group name\"\n                try:\n                    index = int(name)\n                    if index < 0:\n                        raise error, \"negative group number\"\n                except ValueError:\n                    if not isname(name):\n                        raise error, \"bad character in group name\"\n                    try:\n                        index = pattern.groupindex[name]\n                    except KeyError:\n                        msg = \"unknown group name: {0!r}\".format(name)\n                        raise IndexError(msg)\n                a((MARK, index))\n            elif c == \"0\":\n                if s.next in OCTDIGITS:\n                    this = this + sget()\n                    if s.next in OCTDIGITS:\n                        this = this + sget()\n                literal(makechar(int(this[1:], 8) & 0xff))\n            elif c in DIGITS:\n                isoctal = False\n                if s.next in DIGITS:\n                    this = this + sget()\n                    if (c in OCTDIGITS and this[2] in OCTDIGITS and\n                        s.next in OCTDIGITS):\n                        this = this + sget()\n                        isoctal = True\n                        literal(makechar(int(this[1:], 8) & 0xff))\n                if not isoctal:\n                    a((MARK, int(this[1:])))\n            else:\n                try:\n                    this = makechar(ESCAPES[this][1])\n                except KeyError:\n                    pass\n                literal(this)\n        else:\n            literal(this)\n    # convert template to groups and literals lists\n    i = 0\n    groups = []\n    groupsappend = groups.append\n    literals = [None] * len(p)\n    for c, s in p:\n        if c is MARK:\n            groupsappend((i, s))\n            # literal[i] is already None\n        else:\n            literals[i] = s\n        i = i + 1\n    return groups, literals\n\ndef expand_template(template, match):\n    g = match.group\n    sep = match.string[:0]\n    groups, literals = template\n    literals = literals[:]\n    try:\n        for index, group in groups:\n            literals[index] = s = g(group)\n            if s is None:\n                raise error, \"unmatched group\"\n    except IndexError:\n        raise error, \"invalid group reference\"\n    return sep.join(literals)\n", 
    "stat": "\"\"\"Constants/functions for interpreting results of os.stat() and os.lstat().\n\nSuggested usage: from stat import *\n\"\"\"\n\n# Indices for stat struct members in the tuple returned by os.stat()\n\nST_MODE  = 0\nST_INO   = 1\nST_DEV   = 2\nST_NLINK = 3\nST_UID   = 4\nST_GID   = 5\nST_SIZE  = 6\nST_ATIME = 7\nST_MTIME = 8\nST_CTIME = 9\n\n# Extract bits from the mode\n\ndef S_IMODE(mode):\n    return mode & 07777\n\ndef S_IFMT(mode):\n    return mode & 0170000\n\n# Constants used as S_IFMT() for various file types\n# (not all are implemented on all systems)\n\nS_IFDIR  = 0040000\nS_IFCHR  = 0020000\nS_IFBLK  = 0060000\nS_IFREG  = 0100000\nS_IFIFO  = 0010000\nS_IFLNK  = 0120000\nS_IFSOCK = 0140000\n\n# Functions to test for each file type\n\ndef S_ISDIR(mode):\n    return S_IFMT(mode) == S_IFDIR\n\ndef S_ISCHR(mode):\n    return S_IFMT(mode) == S_IFCHR\n\ndef S_ISBLK(mode):\n    return S_IFMT(mode) == S_IFBLK\n\ndef S_ISREG(mode):\n    return S_IFMT(mode) == S_IFREG\n\ndef S_ISFIFO(mode):\n    return S_IFMT(mode) == S_IFIFO\n\ndef S_ISLNK(mode):\n    return S_IFMT(mode) == S_IFLNK\n\ndef S_ISSOCK(mode):\n    return S_IFMT(mode) == S_IFSOCK\n\n# Names for permission bits\n\nS_ISUID = 04000\nS_ISGID = 02000\nS_ENFMT = S_ISGID\nS_ISVTX = 01000\nS_IREAD = 00400\nS_IWRITE = 00200\nS_IEXEC = 00100\nS_IRWXU = 00700\nS_IRUSR = 00400\nS_IWUSR = 00200\nS_IXUSR = 00100\nS_IRWXG = 00070\nS_IRGRP = 00040\nS_IWGRP = 00020\nS_IXGRP = 00010\nS_IRWXO = 00007\nS_IROTH = 00004\nS_IWOTH = 00002\nS_IXOTH = 00001\n\n# Names for file flags\n\nUF_NODUMP    = 0x00000001\nUF_IMMUTABLE = 0x00000002\nUF_APPEND    = 0x00000004\nUF_OPAQUE    = 0x00000008\nUF_NOUNLINK  = 0x00000010\nUF_COMPRESSED = 0x00000020  # OS X: file is hfs-compressed\nUF_HIDDEN    = 0x00008000   # OS X: file should not be displayed\nSF_ARCHIVED  = 0x00010000\nSF_IMMUTABLE = 0x00020000\nSF_APPEND    = 0x00040000\nSF_NOUNLINK  = 0x00100000\nSF_SNAPSHOT  = 0x00200000\n", 
    "string": "\"\"\"A collection of string operations (most are no longer used).\n\nWarning: most of the code you see here isn't normally used nowadays.\nBeginning with Python 1.6, many of these functions are implemented as\nmethods on the standard string object. They used to be implemented by\na built-in module called strop, but strop is now obsolete itself.\n\nPublic module variables:\n\nwhitespace -- a string containing all characters considered whitespace\nlowercase -- a string containing all characters considered lowercase letters\nuppercase -- a string containing all characters considered uppercase letters\nletters -- a string containing all characters considered letters\ndigits -- a string containing all characters considered decimal digits\nhexdigits -- a string containing all characters considered hexadecimal digits\noctdigits -- a string containing all characters considered octal digits\npunctuation -- a string containing all characters considered punctuation\nprintable -- a string containing all characters considered printable\n\n\"\"\"\n\n# Some strings for ctype-style character classification\nwhitespace = ' \\t\\n\\r\\v\\f'\nlowercase = 'abcdefghijklmnopqrstuvwxyz'\nuppercase = 'ABCDEFGHIJKLMNOPQRSTUVWXYZ'\nletters = lowercase + uppercase\nascii_lowercase = lowercase\nascii_uppercase = uppercase\nascii_letters = ascii_lowercase + ascii_uppercase\ndigits = '0123456789'\nhexdigits = digits + 'abcdef' + 'ABCDEF'\noctdigits = '01234567'\npunctuation = \"\"\"!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\"\"\"\nprintable = digits + letters + punctuation + whitespace\n\n# Case conversion helpers\n# Use str to convert Unicode literal in case of -U\nl = map(chr, xrange(256))\n_idmap = str('').join(l)\ndel l\n\n# Functions which aren't available as string methods.\n\n# Capitalize the words in a string, e.g. \" aBc  dEf \" -> \"Abc Def\".\ndef capwords(s, sep=None):\n    \"\"\"capwords(s [,sep]) -> string\n\n    Split the argument into words using split, capitalize each\n    word using capitalize, and join the capitalized words using\n    join.  If the optional second argument sep is absent or None,\n    runs of whitespace characters are replaced by a single space\n    and leading and trailing whitespace are removed, otherwise\n    sep is used to split and join the words.\n\n    \"\"\"\n    return (sep or ' ').join(x.capitalize() for x in s.split(sep))\n\n\n# Construct a translation string\n_idmapL = None\ndef maketrans(fromstr, tostr):\n    \"\"\"maketrans(frm, to) -> string\n\n    Return a translation table (a string of 256 bytes long)\n    suitable for use in string.translate.  The strings frm and to\n    must be of the same length.\n\n    \"\"\"\n    n = len(fromstr)\n    if n != len(tostr):\n        raise ValueError, \"maketrans arguments must have same length\"\n    # this function has been rewritten to suit PyPy better; it is\n    # almost 10x faster than the original.\n    buf = bytearray(256)\n    for i in range(256):\n        buf[i] = i\n    for i in range(n):\n        buf[ord(fromstr[i])] = tostr[i]\n    return str(buf)\n\n\n\n####################################################################\nimport re as _re\n\nclass _multimap:\n    \"\"\"Helper class for combining multiple mappings.\n\n    Used by .{safe_,}substitute() to combine the mapping and keyword\n    arguments.\n    \"\"\"\n    def __init__(self, primary, secondary):\n        self._primary = primary\n        self._secondary = secondary\n\n    def __getitem__(self, key):\n        try:\n            return self._primary[key]\n        except KeyError:\n            return self._secondary[key]\n\n\nclass _TemplateMetaclass(type):\n    pattern = r\"\"\"\n    %(delim)s(?:\n      (?P<escaped>%(delim)s) |   # Escape sequence of two delimiters\n      (?P<named>%(id)s)      |   # delimiter and a Python identifier\n      {(?P<braced>%(id)s)}   |   # delimiter and a braced identifier\n      (?P<invalid>)              # Other ill-formed delimiter exprs\n    )\n    \"\"\"\n\n    def __init__(cls, name, bases, dct):\n        super(_TemplateMetaclass, cls).__init__(name, bases, dct)\n        if 'pattern' in dct:\n            pattern = cls.pattern\n        else:\n            pattern = _TemplateMetaclass.pattern % {\n                'delim' : _re.escape(cls.delimiter),\n                'id'    : cls.idpattern,\n                }\n        cls.pattern = _re.compile(pattern, _re.IGNORECASE | _re.VERBOSE)\n\n\nclass Template:\n    \"\"\"A string class for supporting $-substitutions.\"\"\"\n    __metaclass__ = _TemplateMetaclass\n\n    delimiter = '$'\n    idpattern = r'[_a-z][_a-z0-9]*'\n\n    def __init__(self, template):\n        self.template = template\n\n    # Search for $$, $identifier, ${identifier}, and any bare $'s\n\n    def _invalid(self, mo):\n        i = mo.start('invalid')\n        lines = self.template[:i].splitlines(True)\n        if not lines:\n            colno = 1\n            lineno = 1\n        else:\n            colno = i - len(''.join(lines[:-1]))\n            lineno = len(lines)\n        raise ValueError('Invalid placeholder in string: line %d, col %d' %\n                         (lineno, colno))\n\n    def substitute(self, *args, **kws):\n        if len(args) > 1:\n            raise TypeError('Too many positional arguments')\n        if not args:\n            mapping = kws\n        elif kws:\n            mapping = _multimap(kws, args[0])\n        else:\n            mapping = args[0]\n        # Helper function for .sub()\n        def convert(mo):\n            # Check the most common path first.\n            named = mo.group('named') or mo.group('braced')\n            if named is not None:\n                val = mapping[named]\n                # We use this idiom instead of str() because the latter will\n                # fail if val is a Unicode containing non-ASCII characters.\n                return '%s' % (val,)\n            if mo.group('escaped') is not None:\n                return self.delimiter\n            if mo.group('invalid') is not None:\n                self._invalid(mo)\n            raise ValueError('Unrecognized named group in pattern',\n                             self.pattern)\n        return self.pattern.sub(convert, self.template)\n\n    def safe_substitute(self, *args, **kws):\n        if len(args) > 1:\n            raise TypeError('Too many positional arguments')\n        if not args:\n            mapping = kws\n        elif kws:\n            mapping = _multimap(kws, args[0])\n        else:\n            mapping = args[0]\n        # Helper function for .sub()\n        def convert(mo):\n            named = mo.group('named') or mo.group('braced')\n            if named is not None:\n                try:\n                    # We use this idiom instead of str() because the latter\n                    # will fail if val is a Unicode containing non-ASCII\n                    return '%s' % (mapping[named],)\n                except KeyError:\n                    return mo.group()\n            if mo.group('escaped') is not None:\n                return self.delimiter\n            if mo.group('invalid') is not None:\n                return mo.group()\n            raise ValueError('Unrecognized named group in pattern',\n                             self.pattern)\n        return self.pattern.sub(convert, self.template)\n\n\n\n####################################################################\n# NOTE: Everything below here is deprecated.  Use string methods instead.\n# This stuff will go away in Python 3.0.\n\n# Backward compatible names for exceptions\nindex_error = ValueError\natoi_error = ValueError\natof_error = ValueError\natol_error = ValueError\n\n# convert UPPER CASE letters to lower case\ndef lower(s):\n    \"\"\"lower(s) -> string\n\n    Return a copy of the string s converted to lowercase.\n\n    \"\"\"\n    return s.lower()\n\n# Convert lower case letters to UPPER CASE\ndef upper(s):\n    \"\"\"upper(s) -> string\n\n    Return a copy of the string s converted to uppercase.\n\n    \"\"\"\n    return s.upper()\n\n# Swap lower case letters and UPPER CASE\ndef swapcase(s):\n    \"\"\"swapcase(s) -> string\n\n    Return a copy of the string s with upper case characters\n    converted to lowercase and vice versa.\n\n    \"\"\"\n    return s.swapcase()\n\n# Strip leading and trailing tabs and spaces\ndef strip(s, chars=None):\n    \"\"\"strip(s [,chars]) -> string\n\n    Return a copy of the string s with leading and trailing\n    whitespace removed.\n    If chars is given and not None, remove characters in chars instead.\n    If chars is unicode, S will be converted to unicode before stripping.\n\n    \"\"\"\n    return s.strip(chars)\n\n# Strip leading tabs and spaces\ndef lstrip(s, chars=None):\n    \"\"\"lstrip(s [,chars]) -> string\n\n    Return a copy of the string s with leading whitespace removed.\n    If chars is given and not None, remove characters in chars instead.\n\n    \"\"\"\n    return s.lstrip(chars)\n\n# Strip trailing tabs and spaces\ndef rstrip(s, chars=None):\n    \"\"\"rstrip(s [,chars]) -> string\n\n    Return a copy of the string s with trailing whitespace removed.\n    If chars is given and not None, remove characters in chars instead.\n\n    \"\"\"\n    return s.rstrip(chars)\n\n\n# Split a string into a list of space/tab-separated words\ndef split(s, sep=None, maxsplit=-1):\n    \"\"\"split(s [,sep [,maxsplit]]) -> list of strings\n\n    Return a list of the words in the string s, using sep as the\n    delimiter string.  If maxsplit is given, splits at no more than\n    maxsplit places (resulting in at most maxsplit+1 words).  If sep\n    is not specified or is None, any whitespace string is a separator.\n\n    (split and splitfields are synonymous)\n\n    \"\"\"\n    return s.split(sep, maxsplit)\nsplitfields = split\n\n# Split a string into a list of space/tab-separated words\ndef rsplit(s, sep=None, maxsplit=-1):\n    \"\"\"rsplit(s [,sep [,maxsplit]]) -> list of strings\n\n    Return a list of the words in the string s, using sep as the\n    delimiter string, starting at the end of the string and working\n    to the front.  If maxsplit is given, at most maxsplit splits are\n    done. If sep is not specified or is None, any whitespace string\n    is a separator.\n    \"\"\"\n    return s.rsplit(sep, maxsplit)\n\n# Join fields with optional separator\ndef join(words, sep = ' '):\n    \"\"\"join(list [,sep]) -> string\n\n    Return a string composed of the words in list, with\n    intervening occurrences of sep.  The default separator is a\n    single space.\n\n    (joinfields and join are synonymous)\n\n    \"\"\"\n    return sep.join(words)\njoinfields = join\n\n# Find substring, raise exception if not found\ndef index(s, *args):\n    \"\"\"index(s, sub [,start [,end]]) -> int\n\n    Like find but raises ValueError when the substring is not found.\n\n    \"\"\"\n    return s.index(*args)\n\n# Find last substring, raise exception if not found\ndef rindex(s, *args):\n    \"\"\"rindex(s, sub [,start [,end]]) -> int\n\n    Like rfind but raises ValueError when the substring is not found.\n\n    \"\"\"\n    return s.rindex(*args)\n\n# Count non-overlapping occurrences of substring\ndef count(s, *args):\n    \"\"\"count(s, sub[, start[,end]]) -> int\n\n    Return the number of occurrences of substring sub in string\n    s[start:end].  Optional arguments start and end are\n    interpreted as in slice notation.\n\n    \"\"\"\n    return s.count(*args)\n\n# Find substring, return -1 if not found\ndef find(s, *args):\n    \"\"\"find(s, sub [,start [,end]]) -> in\n\n    Return the lowest index in s where substring sub is found,\n    such that sub is contained within s[start,end].  Optional\n    arguments start and end are interpreted as in slice notation.\n\n    Return -1 on failure.\n\n    \"\"\"\n    return s.find(*args)\n\n# Find last substring, return -1 if not found\ndef rfind(s, *args):\n    \"\"\"rfind(s, sub [,start [,end]]) -> int\n\n    Return the highest index in s where substring sub is found,\n    such that sub is contained within s[start,end].  Optional\n    arguments start and end are interpreted as in slice notation.\n\n    Return -1 on failure.\n\n    \"\"\"\n    return s.rfind(*args)\n\n# for a bit of speed\n_float = float\n_int = int\n_long = long\n\n# Convert string to float\ndef atof(s):\n    \"\"\"atof(s) -> float\n\n    Return the floating point number represented by the string s.\n\n    \"\"\"\n    return _float(s)\n\n\n# Convert string to integer\ndef atoi(s , base=10):\n    \"\"\"atoi(s [,base]) -> int\n\n    Return the integer represented by the string s in the given\n    base, which defaults to 10.  The string s must consist of one\n    or more digits, possibly preceded by a sign.  If base is 0, it\n    is chosen from the leading characters of s, 0 for octal, 0x or\n    0X for hexadecimal.  If base is 16, a preceding 0x or 0X is\n    accepted.\n\n    \"\"\"\n    return _int(s, base)\n\n\n# Convert string to long integer\ndef atol(s, base=10):\n    \"\"\"atol(s [,base]) -> long\n\n    Return the long integer represented by the string s in the\n    given base, which defaults to 10.  The string s must consist\n    of one or more digits, possibly preceded by a sign.  If base\n    is 0, it is chosen from the leading characters of s, 0 for\n    octal, 0x or 0X for hexadecimal.  If base is 16, a preceding\n    0x or 0X is accepted.  A trailing L or l is not accepted,\n    unless base is 0.\n\n    \"\"\"\n    return _long(s, base)\n\n\n# Left-justify a string\ndef ljust(s, width, *args):\n    \"\"\"ljust(s, width[, fillchar]) -> string\n\n    Return a left-justified version of s, in a field of the\n    specified width, padded with spaces as needed.  The string is\n    never truncated.  If specified the fillchar is used instead of spaces.\n\n    \"\"\"\n    return s.ljust(width, *args)\n\n# Right-justify a string\ndef rjust(s, width, *args):\n    \"\"\"rjust(s, width[, fillchar]) -> string\n\n    Return a right-justified version of s, in a field of the\n    specified width, padded with spaces as needed.  The string is\n    never truncated.  If specified the fillchar is used instead of spaces.\n\n    \"\"\"\n    return s.rjust(width, *args)\n\n# Center a string\ndef center(s, width, *args):\n    \"\"\"center(s, width[, fillchar]) -> string\n\n    Return a center version of s, in a field of the specified\n    width. padded with spaces as needed.  The string is never\n    truncated.  If specified the fillchar is used instead of spaces.\n\n    \"\"\"\n    return s.center(width, *args)\n\n# Zero-fill a number, e.g., (12, 3) --> '012' and (-3, 3) --> '-03'\n# Decadent feature: the argument may be a string or a number\n# (Use of this is deprecated; it should be a string as with ljust c.s.)\ndef zfill(x, width):\n    \"\"\"zfill(x, width) -> string\n\n    Pad a numeric string x with zeros on the left, to fill a field\n    of the specified width.  The string x is never truncated.\n\n    \"\"\"\n    if not isinstance(x, basestring):\n        x = repr(x)\n    return x.zfill(width)\n\n# Expand tabs in a string.\n# Doesn't take non-printing chars into account, but does understand \\n.\ndef expandtabs(s, tabsize=8):\n    \"\"\"expandtabs(s [,tabsize]) -> string\n\n    Return a copy of the string s with all tab characters replaced\n    by the appropriate number of spaces, depending on the current\n    column, and the tabsize (default 8).\n\n    \"\"\"\n    return s.expandtabs(tabsize)\n\n# Character translation through look-up table.\ndef translate(s, table, deletions=\"\"):\n    \"\"\"translate(s,table [,deletions]) -> string\n\n    Return a copy of the string s, where all characters occurring\n    in the optional argument deletions are removed, and the\n    remaining characters have been mapped through the given\n    translation table, which must be a string of length 256.  The\n    deletions argument is not allowed for Unicode strings.\n\n    \"\"\"\n    if deletions or table is None:\n        return s.translate(table, deletions)\n    else:\n        # Add s[:0] so that if s is Unicode and table is an 8-bit string,\n        # table is converted to Unicode.  This means that table *cannot*\n        # be a dictionary -- for that feature, use u.translate() directly.\n        return s.translate(table + s[:0])\n\n# Capitalize a string, e.g. \"aBc  dEf\" -> \"Abc  def\".\ndef capitalize(s):\n    \"\"\"capitalize(s) -> string\n\n    Return a copy of the string s with only its first character\n    capitalized.\n\n    \"\"\"\n    return s.capitalize()\n\n# Substring replacement (global)\ndef replace(s, old, new, maxreplace=-1):\n    \"\"\"replace (str, old, new[, maxreplace]) -> string\n\n    Return a copy of string str with all occurrences of substring\n    old replaced by new. If the optional argument maxreplace is\n    given, only the first maxreplace occurrences are replaced.\n\n    \"\"\"\n    return s.replace(old, new, maxreplace)\n\n\n# Try importing optional built-in module \"strop\" -- if it exists,\n# it redefines some string operations that are 100-1000 times faster.\n# It also defines values for whitespace, lowercase and uppercase\n# that match <ctype.h>'s definitions.\n\ntry:\n    from strop import maketrans, lowercase, uppercase, whitespace\n    letters = lowercase + uppercase\nexcept ImportError:\n    pass                                          # Use the original versions\n\n########################################################################\n# the Formatter class\n# see PEP 3101 for details and purpose of this class\n\n# The hard parts are reused from the C implementation.  They're exposed as \"_\"\n# prefixed methods of str and unicode.\n\n# The overall parser is implemented in str._formatter_parser.\n# The field name parser is implemented in str._formatter_field_name_split\n\nclass Formatter(object):\n    def format(self, format_string, *args, **kwargs):\n        return self.vformat(format_string, args, kwargs)\n\n    def vformat(self, format_string, args, kwargs):\n        used_args = set()\n        result = self._vformat(format_string, args, kwargs, used_args, 2)\n        self.check_unused_args(used_args, args, kwargs)\n        return result\n\n    def _vformat(self, format_string, args, kwargs, used_args, recursion_depth):\n        if recursion_depth < 0:\n            raise ValueError('Max string recursion exceeded')\n        result = []\n        for literal_text, field_name, format_spec, conversion in \\\n                self.parse(format_string):\n\n            # output the literal text\n            if literal_text:\n                result.append(literal_text)\n\n            # if there's a field, output it\n            if field_name is not None:\n                # this is some markup, find the object and do\n                #  the formatting\n\n                # given the field_name, find the object it references\n                #  and the argument it came from\n                obj, arg_used = self.get_field(field_name, args, kwargs)\n                used_args.add(arg_used)\n\n                # do any conversion on the resulting object\n                obj = self.convert_field(obj, conversion)\n\n                # expand the format spec, if needed\n                format_spec = self._vformat(format_spec, args, kwargs,\n                                            used_args, recursion_depth-1)\n\n                # format the object and append to the result\n                result.append(self.format_field(obj, format_spec))\n\n        return ''.join(result)\n\n\n    def get_value(self, key, args, kwargs):\n        if isinstance(key, (int, long)):\n            return args[key]\n        else:\n            return kwargs[key]\n\n\n    def check_unused_args(self, used_args, args, kwargs):\n        pass\n\n\n    def format_field(self, value, format_spec):\n        return format(value, format_spec)\n\n\n    def convert_field(self, value, conversion):\n        # do any conversion on the resulting object\n        if conversion is None:\n            return value\n        elif conversion == 's':\n            return str(value)\n        elif conversion == 'r':\n            return repr(value)\n        raise ValueError(\"Unknown conversion specifier {0!s}\".format(conversion))\n\n\n    # returns an iterable that contains tuples of the form:\n    # (literal_text, field_name, format_spec, conversion)\n    # literal_text can be zero length\n    # field_name can be None, in which case there's no\n    #  object to format and output\n    # if field_name is not None, it is looked up, formatted\n    #  with format_spec and conversion and then used\n    def parse(self, format_string):\n        return format_string._formatter_parser()\n\n\n    # given a field_name, find the object it references.\n    #  field_name:   the field being looked up, e.g. \"0.name\"\n    #                 or \"lookup[3]\"\n    #  used_args:    a set of which args have been used\n    #  args, kwargs: as passed in to vformat\n    def get_field(self, field_name, args, kwargs):\n        first, rest = field_name._formatter_field_name_split()\n\n        obj = self.get_value(first, args, kwargs)\n\n        # loop through the rest of the field_name, doing\n        #  getattr or getitem as needed\n        for is_attr, i in rest:\n            if is_attr:\n                obj = getattr(obj, i)\n            else:\n                obj = obj[i]\n\n        return obj, first\n", 
    "struct": "from _struct import *\nfrom _struct import _clearcache\nfrom _struct import __doc__\n", 
    "subprocess": "# Fake subprocess file\n", 
    "tempfile": "\"\"\"Temporary files.\n\nThis module provides generic, low- and high-level interfaces for\ncreating temporary files and directories.  All of the interfaces\nprovided by this module can be used without fear of race conditions\nexcept for 'mktemp'.  'mktemp' is subject to race conditions and\nshould not be used; it is provided for backward compatibility only.\n\nThis module also provides some data items to the user:\n\n  TMP_MAX  - maximum number of names that will be tried before\n             giving up.\n  template - the default prefix for all temporary names.\n             You may change this to control the default prefix.\n  tempdir  - If this is set to a string before the first use of\n             any routine from this module, it will be considered as\n             another candidate location to store temporary files.\n\"\"\"\n\n__all__ = [\n    \"NamedTemporaryFile\", \"TemporaryFile\", # high level safe interfaces\n    \"SpooledTemporaryFile\",\n    \"mkstemp\", \"mkdtemp\",                  # low level safe interfaces\n    \"mktemp\",                              # deprecated unsafe interface\n    \"TMP_MAX\", \"gettempprefix\",            # constants\n    \"tempdir\", \"gettempdir\"\n   ]\n\n\n# Imports.\n\nimport io as _io\nimport os as _os\nimport errno as _errno\nfrom random import Random as _Random\n\ntry:\n    from cStringIO import StringIO as _StringIO\nexcept ImportError:\n    from StringIO import StringIO as _StringIO\n\ntry:\n    import fcntl as _fcntl\nexcept ImportError:\n    def _set_cloexec(fd):\n        pass\nelse:\n    def _set_cloexec(fd):\n        try:\n            flags = _fcntl.fcntl(fd, _fcntl.F_GETFD, 0)\n        except IOError:\n            pass\n        else:\n            # flags read successfully, modify\n            flags |= _fcntl.FD_CLOEXEC\n            _fcntl.fcntl(fd, _fcntl.F_SETFD, flags)\n\n\ntry:\n    import thread as _thread\nexcept ImportError:\n    import dummy_thread as _thread\n_allocate_lock = _thread.allocate_lock\n\n_text_openflags = _os.O_RDWR | _os.O_CREAT | _os.O_EXCL\nif hasattr(_os, 'O_NOINHERIT'):\n    _text_openflags |= _os.O_NOINHERIT\nif hasattr(_os, 'O_NOFOLLOW'):\n    _text_openflags |= _os.O_NOFOLLOW\n\n_bin_openflags = _text_openflags\nif hasattr(_os, 'O_BINARY'):\n    _bin_openflags |= _os.O_BINARY\n\nif hasattr(_os, 'TMP_MAX'):\n    TMP_MAX = _os.TMP_MAX\nelse:\n    TMP_MAX = 10000\n\ntemplate = \"tmp\"\n\n# Internal routines.\n\n_once_lock = _allocate_lock()\n\nif hasattr(_os, \"lstat\"):\n    _stat = _os.lstat\nelif hasattr(_os, \"stat\"):\n    _stat = _os.stat\nelse:\n    # Fallback.  All we need is something that raises os.error if the\n    # file doesn't exist.\n    def _stat(fn):\n        try:\n            f = open(fn)\n        except IOError:\n            raise _os.error\n        f.close()\n\ndef _exists(fn):\n    try:\n        _stat(fn)\n    except _os.error:\n        return False\n    else:\n        return True\n\nclass _RandomNameSequence:\n    \"\"\"An instance of _RandomNameSequence generates an endless\n    sequence of unpredictable strings which can safely be incorporated\n    into file names.  Each string is six characters long.  Multiple\n    threads can safely use the same instance at the same time.\n\n    _RandomNameSequence is an iterator.\"\"\"\n\n    characters = (\"abcdefghijklmnopqrstuvwxyz\" +\n                  \"ABCDEFGHIJKLMNOPQRSTUVWXYZ\" +\n                  \"0123456789_\")\n\n    def __init__(self):\n        self.mutex = _allocate_lock()\n        self.normcase = _os.path.normcase\n\n    @property\n    def rng(self):\n        cur_pid = _os.getpid()\n        if cur_pid != getattr(self, '_rng_pid', None):\n            self._rng = _Random()\n            self._rng_pid = cur_pid\n        return self._rng\n\n    def __iter__(self):\n        return self\n\n    def next(self):\n        m = self.mutex\n        c = self.characters\n        choose = self.rng.choice\n\n        m.acquire()\n        try:\n            letters = [choose(c) for dummy in \"123456\"]\n        finally:\n            m.release()\n\n        return self.normcase(''.join(letters))\n\ndef _candidate_tempdir_list():\n    \"\"\"Generate a list of candidate temporary directories which\n    _get_default_tempdir will try.\"\"\"\n\n    dirlist = []\n\n    # First, try the environment.\n    for envname in 'TMPDIR', 'TEMP', 'TMP':\n        dirname = _os.getenv(envname)\n        if dirname: dirlist.append(dirname)\n\n    # Failing that, try OS-specific locations.\n    if _os.name == 'riscos':\n        dirname = _os.getenv('Wimp$ScrapDir')\n        if dirname: dirlist.append(dirname)\n    elif _os.name == 'nt':\n        dirlist.extend([ r'c:\\temp', r'c:\\tmp', r'\\temp', r'\\tmp' ])\n    else:\n        dirlist.extend([ '/tmp', '/var/tmp', '/usr/tmp' ])\n\n    # As a last resort, the current directory.\n    try:\n        dirlist.append(_os.getcwd())\n    except (AttributeError, _os.error):\n        dirlist.append(_os.curdir)\n\n    return dirlist\n\ndef _get_default_tempdir():\n    \"\"\"Calculate the default directory to use for temporary files.\n    This routine should be called exactly once.\n\n    We determine whether or not a candidate temp dir is usable by\n    trying to create and write to a file in that directory.  If this\n    is successful, the test file is deleted.  To prevent denial of\n    service, the name of the test file must be randomized.\"\"\"\n\n    namer = _RandomNameSequence()\n    dirlist = _candidate_tempdir_list()\n    flags = _text_openflags\n\n    for dir in dirlist:\n        if dir != _os.curdir:\n            dir = _os.path.normcase(_os.path.abspath(dir))\n        # Try only a few names per directory.\n        for seq in xrange(100):\n            name = namer.next()\n            filename = _os.path.join(dir, name)\n            try:\n                fd = _os.open(filename, flags, 0o600)\n                try:\n                    try:\n                        with _io.open(fd, 'wb', closefd=False) as fp:\n                            fp.write(b'blat')\n                    finally:\n                        _os.close(fd)\n                finally:\n                    _os.unlink(filename)\n                return dir\n            except (OSError, IOError) as e:\n                if e.args[0] != _errno.EEXIST:\n                    break # no point trying more names in this directory\n                pass\n    raise IOError, (_errno.ENOENT,\n                    (\"No usable temporary directory found in %s\" % dirlist))\n\n_name_sequence = None\n\ndef _get_candidate_names():\n    \"\"\"Common setup sequence for all user-callable interfaces.\"\"\"\n\n    global _name_sequence\n    if _name_sequence is None:\n        _once_lock.acquire()\n        try:\n            if _name_sequence is None:\n                _name_sequence = _RandomNameSequence()\n        finally:\n            _once_lock.release()\n    return _name_sequence\n\n\ndef _mkstemp_inner(dir, pre, suf, flags):\n    \"\"\"Code common to mkstemp, TemporaryFile, and NamedTemporaryFile.\"\"\"\n\n    names = _get_candidate_names()\n\n    for seq in xrange(TMP_MAX):\n        name = names.next()\n        file = _os.path.join(dir, pre + name + suf)\n        try:\n            fd = _os.open(file, flags, 0600)\n            _set_cloexec(fd)\n            return (fd, _os.path.abspath(file))\n        except OSError, e:\n            if e.errno == _errno.EEXIST:\n                continue # try again\n            if _os.name == 'nt' and e.errno == _errno.EACCES:\n                # On windows, when a directory with the chosen name already\n                # exists, EACCES error code is returned instead of EEXIST.\n                continue\n            raise\n\n    raise IOError, (_errno.EEXIST, \"No usable temporary file name found\")\n\n\n# User visible interfaces.\n\ndef gettempprefix():\n    \"\"\"Accessor for tempdir.template.\"\"\"\n    return template\n\ntempdir = None\n\ndef gettempdir():\n    \"\"\"Accessor for tempfile.tempdir.\"\"\"\n    global tempdir\n    if tempdir is None:\n        _once_lock.acquire()\n        try:\n            if tempdir is None:\n                tempdir = _get_default_tempdir()\n        finally:\n            _once_lock.release()\n    return tempdir\n\ndef mkstemp(suffix=\"\", prefix=template, dir=None, text=False):\n    \"\"\"User-callable function to create and return a unique temporary\n    file.  The return value is a pair (fd, name) where fd is the\n    file descriptor returned by os.open, and name is the filename.\n\n    If 'suffix' is specified, the file name will end with that suffix,\n    otherwise there will be no suffix.\n\n    If 'prefix' is specified, the file name will begin with that prefix,\n    otherwise a default prefix is used.\n\n    If 'dir' is specified, the file will be created in that directory,\n    otherwise a default directory is used.\n\n    If 'text' is specified and true, the file is opened in text\n    mode.  Else (the default) the file is opened in binary mode.  On\n    some operating systems, this makes no difference.\n\n    The file is readable and writable only by the creating user ID.\n    If the operating system uses permission bits to indicate whether a\n    file is executable, the file is executable by no one. The file\n    descriptor is not inherited by children of this process.\n\n    Caller is responsible for deleting the file when done with it.\n    \"\"\"\n\n    if dir is None:\n        dir = gettempdir()\n\n    if text:\n        flags = _text_openflags\n    else:\n        flags = _bin_openflags\n\n    return _mkstemp_inner(dir, prefix, suffix, flags)\n\n\ndef mkdtemp(suffix=\"\", prefix=template, dir=None):\n    \"\"\"User-callable function to create and return a unique temporary\n    directory.  The return value is the pathname of the directory.\n\n    Arguments are as for mkstemp, except that the 'text' argument is\n    not accepted.\n\n    The directory is readable, writable, and searchable only by the\n    creating user.\n\n    Caller is responsible for deleting the directory when done with it.\n    \"\"\"\n\n    if dir is None:\n        dir = gettempdir()\n\n    names = _get_candidate_names()\n\n    for seq in xrange(TMP_MAX):\n        name = names.next()\n        file = _os.path.join(dir, prefix + name + suffix)\n        try:\n            _os.mkdir(file, 0700)\n            return file\n        except OSError, e:\n            if e.errno == _errno.EEXIST:\n                continue # try again\n            raise\n\n    raise IOError, (_errno.EEXIST, \"No usable temporary directory name found\")\n\ndef mktemp(suffix=\"\", prefix=template, dir=None):\n    \"\"\"User-callable function to return a unique temporary file name.  The\n    file is not created.\n\n    Arguments are as for mkstemp, except that the 'text' argument is\n    not accepted.\n\n    This function is unsafe and should not be used.  The file name\n    refers to a file that did not exist at some point, but by the time\n    you get around to creating it, someone else may have beaten you to\n    the punch.\n    \"\"\"\n\n##    from warnings import warn as _warn\n##    _warn(\"mktemp is a potential security risk to your program\",\n##          RuntimeWarning, stacklevel=2)\n\n    if dir is None:\n        dir = gettempdir()\n\n    names = _get_candidate_names()\n    for seq in xrange(TMP_MAX):\n        name = names.next()\n        file = _os.path.join(dir, prefix + name + suffix)\n        if not _exists(file):\n            return file\n\n    raise IOError, (_errno.EEXIST, \"No usable temporary filename found\")\n\n\nclass _TemporaryFileWrapper:\n    \"\"\"Temporary file wrapper\n\n    This class provides a wrapper around files opened for\n    temporary use.  In particular, it seeks to automatically\n    remove the file when it is no longer needed.\n    \"\"\"\n\n    def __init__(self, file, name, delete=True):\n        self.file = file\n        self.name = name\n        self.close_called = False\n        self.delete = delete\n\n    def __getattr__(self, name):\n        # Attribute lookups are delegated to the underlying file\n        # and cached for non-numeric results\n        # (i.e. methods are cached, closed and friends are not)\n        file = self.__dict__['file']\n        a = getattr(file, name)\n        if not issubclass(type(a), type(0)):\n            setattr(self, name, a)\n        return a\n\n    # The underlying __enter__ method returns the wrong object\n    # (self.file) so override it to return the wrapper\n    def __enter__(self):\n        self.file.__enter__()\n        return self\n\n    # NT provides delete-on-close as a primitive, so we don't need\n    # the wrapper to do anything special.  We still use it so that\n    # file.name is useful (i.e. not \"(fdopen)\") with NamedTemporaryFile.\n    if _os.name != 'nt':\n        # Cache the unlinker so we don't get spurious errors at\n        # shutdown when the module-level \"os\" is None'd out.  Note\n        # that this must be referenced as self.unlink, because the\n        # name TemporaryFileWrapper may also get None'd out before\n        # __del__ is called.\n        unlink = _os.unlink\n\n        def close(self):\n            if not self.close_called:\n                self.close_called = True\n                self.file.close()\n                if self.delete:\n                    self.unlink(self.name)\n\n        def __del__(self):\n            self.close()\n\n        # Need to trap __exit__ as well to ensure the file gets\n        # deleted when used in a with statement\n        def __exit__(self, exc, value, tb):\n            result = self.file.__exit__(exc, value, tb)\n            self.close()\n            return result\n    else:\n        def __exit__(self, exc, value, tb):\n            self.file.__exit__(exc, value, tb)\n\n\ndef NamedTemporaryFile(mode='w+b', bufsize=-1, suffix=\"\",\n                       prefix=template, dir=None, delete=True):\n    \"\"\"Create and return a temporary file.\n    Arguments:\n    'prefix', 'suffix', 'dir' -- as for mkstemp.\n    'mode' -- the mode argument to os.fdopen (default \"w+b\").\n    'bufsize' -- the buffer size argument to os.fdopen (default -1).\n    'delete' -- whether the file is deleted on close (default True).\n    The file is created as mkstemp() would do it.\n\n    Returns an object with a file-like interface; the name of the file\n    is accessible as file.name.  The file will be automatically deleted\n    when it is closed unless the 'delete' argument is set to False.\n    \"\"\"\n\n    if dir is None:\n        dir = gettempdir()\n\n    if 'b' in mode:\n        flags = _bin_openflags\n    else:\n        flags = _text_openflags\n\n    # Setting O_TEMPORARY in the flags causes the OS to delete\n    # the file when it is closed.  This is only supported by Windows.\n    if _os.name == 'nt' and delete:\n        flags |= _os.O_TEMPORARY\n\n    (fd, name) = _mkstemp_inner(dir, prefix, suffix, flags)\n    try:\n        file = _os.fdopen(fd, mode, bufsize)\n        return _TemporaryFileWrapper(file, name, delete)\n    except:\n        _os.close(fd)\n        raise\n\nif _os.name != 'posix' or _os.sys.platform == 'cygwin':\n    # On non-POSIX and Cygwin systems, assume that we cannot unlink a file\n    # while it is open.\n    TemporaryFile = NamedTemporaryFile\n\nelse:\n    def TemporaryFile(mode='w+b', bufsize=-1, suffix=\"\",\n                      prefix=template, dir=None):\n        \"\"\"Create and return a temporary file.\n        Arguments:\n        'prefix', 'suffix', 'dir' -- as for mkstemp.\n        'mode' -- the mode argument to os.fdopen (default \"w+b\").\n        'bufsize' -- the buffer size argument to os.fdopen (default -1).\n        The file is created as mkstemp() would do it.\n\n        Returns an object with a file-like interface.  The file has no\n        name, and will cease to exist when it is closed.\n        \"\"\"\n\n        if dir is None:\n            dir = gettempdir()\n\n        if 'b' in mode:\n            flags = _bin_openflags\n        else:\n            flags = _text_openflags\n\n        (fd, name) = _mkstemp_inner(dir, prefix, suffix, flags)\n        try:\n            _os.unlink(name)\n            return _os.fdopen(fd, mode, bufsize)\n        except:\n            _os.close(fd)\n            raise\n\nclass SpooledTemporaryFile:\n    \"\"\"Temporary file wrapper, specialized to switch from\n    StringIO to a real file when it exceeds a certain size or\n    when a fileno is needed.\n    \"\"\"\n    _rolled = False\n\n    def __init__(self, max_size=0, mode='w+b', bufsize=-1,\n                 suffix=\"\", prefix=template, dir=None):\n        self._file = _StringIO()\n        self._max_size = max_size\n        self._rolled = False\n        self._TemporaryFileArgs = (mode, bufsize, suffix, prefix, dir)\n\n    def _check(self, file):\n        if self._rolled: return\n        max_size = self._max_size\n        if max_size and file.tell() > max_size:\n            self.rollover()\n\n    def rollover(self):\n        if self._rolled: return\n        file = self._file\n        newfile = self._file = TemporaryFile(*self._TemporaryFileArgs)\n        del self._TemporaryFileArgs\n\n        newfile.write(file.getvalue())\n        newfile.seek(file.tell(), 0)\n\n        self._rolled = True\n\n    # The method caching trick from NamedTemporaryFile\n    # won't work here, because _file may change from a\n    # _StringIO instance to a real file. So we list\n    # all the methods directly.\n\n    # Context management protocol\n    def __enter__(self):\n        if self._file.closed:\n            raise ValueError(\"Cannot enter context with closed file\")\n        return self\n\n    def __exit__(self, exc, value, tb):\n        self._file.close()\n\n    # file protocol\n    def __iter__(self):\n        return self._file.__iter__()\n\n    def close(self):\n        self._file.close()\n\n    @property\n    def closed(self):\n        return self._file.closed\n\n    def fileno(self):\n        self.rollover()\n        return self._file.fileno()\n\n    def flush(self):\n        self._file.flush()\n\n    def isatty(self):\n        return self._file.isatty()\n\n    @property\n    def mode(self):\n        try:\n            return self._file.mode\n        except AttributeError:\n            return self._TemporaryFileArgs[0]\n\n    @property\n    def name(self):\n        try:\n            return self._file.name\n        except AttributeError:\n            return None\n\n    def next(self):\n        return self._file.next\n\n    def read(self, *args):\n        return self._file.read(*args)\n\n    def readline(self, *args):\n        return self._file.readline(*args)\n\n    def readlines(self, *args):\n        return self._file.readlines(*args)\n\n    def seek(self, *args):\n        self._file.seek(*args)\n\n    @property\n    def softspace(self):\n        return self._file.softspace\n\n    def tell(self):\n        return self._file.tell()\n\n    def truncate(self):\n        self._file.truncate()\n\n    def write(self, s):\n        file = self._file\n        rv = file.write(s)\n        self._check(file)\n        return rv\n\n    def writelines(self, iterable):\n        file = self._file\n        rv = file.writelines(iterable)\n        self._check(file)\n        return rv\n\n    def xreadlines(self, *args):\n        if hasattr(self._file, 'xreadlines'):  # real file\n            return iter(self._file)\n        else:  # StringIO()\n            return iter(self._file.readlines(*args))\n", 
    "textwrap": "\"\"\"Text wrapping and filling.\n\"\"\"\n\n# Copyright (C) 1999-2001 Gregory P. Ward.\n# Copyright (C) 2002, 2003 Python Software Foundation.\n# Written by Greg Ward <gward@python.net>\n\n__revision__ = \"$Id$\"\n\nimport string, re\n\ntry:\n    _unicode = unicode\nexcept NameError:\n    # If Python is built without Unicode support, the unicode type\n    # will not exist. Fake one.\n    class _unicode(object):\n        pass\n\n# Do the right thing with boolean values for all known Python versions\n# (so this module can be copied to projects that don't depend on Python\n# 2.3, e.g. Optik and Docutils) by uncommenting the block of code below.\n#try:\n#    True, False\n#except NameError:\n#    (True, False) = (1, 0)\n\n__all__ = ['TextWrapper', 'wrap', 'fill', 'dedent']\n\n# Hardcode the recognized whitespace characters to the US-ASCII\n# whitespace characters.  The main reason for doing this is that in\n# ISO-8859-1, 0xa0 is non-breaking whitespace, so in certain locales\n# that character winds up in string.whitespace.  Respecting\n# string.whitespace in those cases would 1) make textwrap treat 0xa0 the\n# same as any other whitespace char, which is clearly wrong (it's a\n# *non-breaking* space), 2) possibly cause problems with Unicode,\n# since 0xa0 is not in range(128).\n_whitespace = '\\t\\n\\x0b\\x0c\\r '\n\nclass TextWrapper:\n    \"\"\"\n    Object for wrapping/filling text.  The public interface consists of\n    the wrap() and fill() methods; the other methods are just there for\n    subclasses to override in order to tweak the default behaviour.\n    If you want to completely replace the main wrapping algorithm,\n    you'll probably have to override _wrap_chunks().\n\n    Several instance attributes control various aspects of wrapping:\n      width (default: 70)\n        the maximum width of wrapped lines (unless break_long_words\n        is false)\n      initial_indent (default: \"\")\n        string that will be prepended to the first line of wrapped\n        output.  Counts towards the line's width.\n      subsequent_indent (default: \"\")\n        string that will be prepended to all lines save the first\n        of wrapped output; also counts towards each line's width.\n      expand_tabs (default: true)\n        Expand tabs in input text to spaces before further processing.\n        Each tab will become 1 .. 8 spaces, depending on its position in\n        its line.  If false, each tab is treated as a single character.\n      replace_whitespace (default: true)\n        Replace all whitespace characters in the input text by spaces\n        after tab expansion.  Note that if expand_tabs is false and\n        replace_whitespace is true, every tab will be converted to a\n        single space!\n      fix_sentence_endings (default: false)\n        Ensure that sentence-ending punctuation is always followed\n        by two spaces.  Off by default because the algorithm is\n        (unavoidably) imperfect.\n      break_long_words (default: true)\n        Break words longer than 'width'.  If false, those words will not\n        be broken, and some lines might be longer than 'width'.\n      break_on_hyphens (default: true)\n        Allow breaking hyphenated words. If true, wrapping will occur\n        preferably on whitespaces and right after hyphens part of\n        compound words.\n      drop_whitespace (default: true)\n        Drop leading and trailing whitespace from lines.\n    \"\"\"\n\n    whitespace_trans = string.maketrans(_whitespace, ' ' * len(_whitespace))\n\n    unicode_whitespace_trans = {}\n    uspace = ord(u' ')\n    for x in map(ord, _whitespace):\n        unicode_whitespace_trans[x] = uspace\n\n    # This funky little regex is just the trick for splitting\n    # text up into word-wrappable chunks.  E.g.\n    #   \"Hello there -- you goof-ball, use the -b option!\"\n    # splits into\n    #   Hello/ /there/ /--/ /you/ /goof-/ball,/ /use/ /the/ /-b/ /option!\n    # (after stripping out empty strings).\n    wordsep_re = re.compile(\n        r'(\\s+|'                                  # any whitespace\n        r'[^\\s\\w]*\\w+[^0-9\\W]-(?=\\w+[^0-9\\W])|'   # hyphenated words\n        r'(?<=[\\w\\!\\\"\\'\\&\\.\\,\\?])-{2,}(?=\\w))')   # em-dash\n\n    # This less funky little regex just split on recognized spaces. E.g.\n    #   \"Hello there -- you goof-ball, use the -b option!\"\n    # splits into\n    #   Hello/ /there/ /--/ /you/ /goof-ball,/ /use/ /the/ /-b/ /option!/\n    wordsep_simple_re = re.compile(r'(\\s+)')\n\n    # XXX this is not locale- or charset-aware -- string.lowercase\n    # is US-ASCII only (and therefore English-only)\n    sentence_end_re = re.compile(r'[%s]'              # lowercase letter\n                                 r'[\\.\\!\\?]'          # sentence-ending punct.\n                                 r'[\\\"\\']?'           # optional end-of-quote\n                                 r'\\Z'                # end of chunk\n                                 % string.lowercase)\n\n\n    def __init__(self,\n                 width=70,\n                 initial_indent=\"\",\n                 subsequent_indent=\"\",\n                 expand_tabs=True,\n                 replace_whitespace=True,\n                 fix_sentence_endings=False,\n                 break_long_words=True,\n                 drop_whitespace=True,\n                 break_on_hyphens=True):\n        self.width = width\n        self.initial_indent = initial_indent\n        self.subsequent_indent = subsequent_indent\n        self.expand_tabs = expand_tabs\n        self.replace_whitespace = replace_whitespace\n        self.fix_sentence_endings = fix_sentence_endings\n        self.break_long_words = break_long_words\n        self.drop_whitespace = drop_whitespace\n        self.break_on_hyphens = break_on_hyphens\n\n        # recompile the regexes for Unicode mode -- done in this clumsy way for\n        # backwards compatibility because it's rather common to monkey-patch\n        # the TextWrapper class' wordsep_re attribute.\n        self.wordsep_re_uni = re.compile(self.wordsep_re.pattern, re.U)\n        self.wordsep_simple_re_uni = re.compile(\n            self.wordsep_simple_re.pattern, re.U)\n\n\n    # -- Private methods -----------------------------------------------\n    # (possibly useful for subclasses to override)\n\n    def _munge_whitespace(self, text):\n        \"\"\"_munge_whitespace(text : string) -> string\n\n        Munge whitespace in text: expand tabs and convert all other\n        whitespace characters to spaces.  Eg. \" foo\\tbar\\n\\nbaz\"\n        becomes \" foo    bar  baz\".\n        \"\"\"\n        if self.expand_tabs:\n            text = text.expandtabs()\n        if self.replace_whitespace:\n            if isinstance(text, str):\n                text = text.translate(self.whitespace_trans)\n            elif isinstance(text, _unicode):\n                text = text.translate(self.unicode_whitespace_trans)\n        return text\n\n\n    def _split(self, text):\n        \"\"\"_split(text : string) -> [string]\n\n        Split the text to wrap into indivisible chunks.  Chunks are\n        not quite the same as words; see _wrap_chunks() for full\n        details.  As an example, the text\n          Look, goof-ball -- use the -b option!\n        breaks into the following chunks:\n          'Look,', ' ', 'goof-', 'ball', ' ', '--', ' ',\n          'use', ' ', 'the', ' ', '-b', ' ', 'option!'\n        if break_on_hyphens is True, or in:\n          'Look,', ' ', 'goof-ball', ' ', '--', ' ',\n          'use', ' ', 'the', ' ', '-b', ' ', option!'\n        otherwise.\n        \"\"\"\n        if isinstance(text, _unicode):\n            if self.break_on_hyphens:\n                pat = self.wordsep_re_uni\n            else:\n                pat = self.wordsep_simple_re_uni\n        else:\n            if self.break_on_hyphens:\n                pat = self.wordsep_re\n            else:\n                pat = self.wordsep_simple_re\n        chunks = pat.split(text)\n        chunks = filter(None, chunks)  # remove empty chunks\n        return chunks\n\n    def _fix_sentence_endings(self, chunks):\n        \"\"\"_fix_sentence_endings(chunks : [string])\n\n        Correct for sentence endings buried in 'chunks'.  Eg. when the\n        original text contains \"... foo.\\nBar ...\", munge_whitespace()\n        and split() will convert that to [..., \"foo.\", \" \", \"Bar\", ...]\n        which has one too few spaces; this method simply changes the one\n        space to two.\n        \"\"\"\n        i = 0\n        patsearch = self.sentence_end_re.search\n        while i < len(chunks)-1:\n            if chunks[i+1] == \" \" and patsearch(chunks[i]):\n                chunks[i+1] = \"  \"\n                i += 2\n            else:\n                i += 1\n\n    def _handle_long_word(self, reversed_chunks, cur_line, cur_len, width):\n        \"\"\"_handle_long_word(chunks : [string],\n                             cur_line : [string],\n                             cur_len : int, width : int)\n\n        Handle a chunk of text (most likely a word, not whitespace) that\n        is too long to fit in any line.\n        \"\"\"\n        # Figure out when indent is larger than the specified width, and make\n        # sure at least one character is stripped off on every pass\n        if width < 1:\n            space_left = 1\n        else:\n            space_left = width - cur_len\n\n        # If we're allowed to break long words, then do so: put as much\n        # of the next chunk onto the current line as will fit.\n        if self.break_long_words:\n            cur_line.append(reversed_chunks[-1][:space_left])\n            reversed_chunks[-1] = reversed_chunks[-1][space_left:]\n\n        # Otherwise, we have to preserve the long word intact.  Only add\n        # it to the current line if there's nothing already there --\n        # that minimizes how much we violate the width constraint.\n        elif not cur_line:\n            cur_line.append(reversed_chunks.pop())\n\n        # If we're not allowed to break long words, and there's already\n        # text on the current line, do nothing.  Next time through the\n        # main loop of _wrap_chunks(), we'll wind up here again, but\n        # cur_len will be zero, so the next line will be entirely\n        # devoted to the long word that we can't handle right now.\n\n    def _wrap_chunks(self, chunks):\n        \"\"\"_wrap_chunks(chunks : [string]) -> [string]\n\n        Wrap a sequence of text chunks and return a list of lines of\n        length 'self.width' or less.  (If 'break_long_words' is false,\n        some lines may be longer than this.)  Chunks correspond roughly\n        to words and the whitespace between them: each chunk is\n        indivisible (modulo 'break_long_words'), but a line break can\n        come between any two chunks.  Chunks should not have internal\n        whitespace; ie. a chunk is either all whitespace or a \"word\".\n        Whitespace chunks will be removed from the beginning and end of\n        lines, but apart from that whitespace is preserved.\n        \"\"\"\n        lines = []\n        if self.width <= 0:\n            raise ValueError(\"invalid width %r (must be > 0)\" % self.width)\n\n        # Arrange in reverse order so items can be efficiently popped\n        # from a stack of chucks.\n        chunks.reverse()\n\n        while chunks:\n\n            # Start the list of chunks that will make up the current line.\n            # cur_len is just the length of all the chunks in cur_line.\n            cur_line = []\n            cur_len = 0\n\n            # Figure out which static string will prefix this line.\n            if lines:\n                indent = self.subsequent_indent\n            else:\n                indent = self.initial_indent\n\n            # Maximum width for this line.\n            width = self.width - len(indent)\n\n            # First chunk on line is whitespace -- drop it, unless this\n            # is the very beginning of the text (ie. no lines started yet).\n            if self.drop_whitespace and chunks[-1].strip() == '' and lines:\n                del chunks[-1]\n\n            while chunks:\n                l = len(chunks[-1])\n\n                # Can at least squeeze this chunk onto the current line.\n                if cur_len + l <= width:\n                    cur_line.append(chunks.pop())\n                    cur_len += l\n\n                # Nope, this line is full.\n                else:\n                    break\n\n            # The current line is full, and the next chunk is too big to\n            # fit on *any* line (not just this one).\n            if chunks and len(chunks[-1]) > width:\n                self._handle_long_word(chunks, cur_line, cur_len, width)\n\n            # If the last chunk on this line is all whitespace, drop it.\n            if self.drop_whitespace and cur_line and cur_line[-1].strip() == '':\n                del cur_line[-1]\n\n            # Convert current line back to a string and store it in list\n            # of all lines (return value).\n            if cur_line:\n                lines.append(indent + ''.join(cur_line))\n\n        return lines\n\n\n    # -- Public interface ----------------------------------------------\n\n    def wrap(self, text):\n        \"\"\"wrap(text : string) -> [string]\n\n        Reformat the single paragraph in 'text' so it fits in lines of\n        no more than 'self.width' columns, and return a list of wrapped\n        lines.  Tabs in 'text' are expanded with string.expandtabs(),\n        and all other whitespace characters (including newline) are\n        converted to space.\n        \"\"\"\n        text = self._munge_whitespace(text)\n        chunks = self._split(text)\n        if self.fix_sentence_endings:\n            self._fix_sentence_endings(chunks)\n        return self._wrap_chunks(chunks)\n\n    def fill(self, text):\n        \"\"\"fill(text : string) -> string\n\n        Reformat the single paragraph in 'text' to fit in lines of no\n        more than 'self.width' columns, and return a new string\n        containing the entire wrapped paragraph.\n        \"\"\"\n        return \"\\n\".join(self.wrap(text))\n\n\n# -- Convenience interface ---------------------------------------------\n\ndef wrap(text, width=70, **kwargs):\n    \"\"\"Wrap a single paragraph of text, returning a list of wrapped lines.\n\n    Reformat the single paragraph in 'text' so it fits in lines of no\n    more than 'width' columns, and return a list of wrapped lines.  By\n    default, tabs in 'text' are expanded with string.expandtabs(), and\n    all other whitespace characters (including newline) are converted to\n    space.  See TextWrapper class for available keyword args to customize\n    wrapping behaviour.\n    \"\"\"\n    w = TextWrapper(width=width, **kwargs)\n    return w.wrap(text)\n\ndef fill(text, width=70, **kwargs):\n    \"\"\"Fill a single paragraph of text, returning a new string.\n\n    Reformat the single paragraph in 'text' to fit in lines of no more\n    than 'width' columns, and return a new string containing the entire\n    wrapped paragraph.  As with wrap(), tabs are expanded and other\n    whitespace characters converted to space.  See TextWrapper class for\n    available keyword args to customize wrapping behaviour.\n    \"\"\"\n    w = TextWrapper(width=width, **kwargs)\n    return w.fill(text)\n\n\n# -- Loosely related functionality -------------------------------------\n\n_whitespace_only_re = re.compile('^[ \\t]+$', re.MULTILINE)\n_leading_whitespace_re = re.compile('(^[ \\t]*)(?:[^ \\t\\n])', re.MULTILINE)\n\ndef dedent(text):\n    \"\"\"Remove any common leading whitespace from every line in `text`.\n\n    This can be used to make triple-quoted strings line up with the left\n    edge of the display, while still presenting them in the source code\n    in indented form.\n\n    Note that tabs and spaces are both treated as whitespace, but they\n    are not equal: the lines \"  hello\" and \"\\thello\" are\n    considered to have no common leading whitespace.  (This behaviour is\n    new in Python 2.5; older versions of this module incorrectly\n    expanded tabs before searching for common leading whitespace.)\n    \"\"\"\n    # Look for the longest leading string of spaces and tabs common to\n    # all lines.\n    margin = None\n    text = _whitespace_only_re.sub('', text)\n    indents = _leading_whitespace_re.findall(text)\n    for indent in indents:\n        if margin is None:\n            margin = indent\n\n        # Current line more deeply indented than previous winner:\n        # no change (previous winner is still on top).\n        elif indent.startswith(margin):\n            pass\n\n        # Current line consistent with and no deeper than previous winner:\n        # it's the new winner.\n        elif margin.startswith(indent):\n            margin = indent\n\n        # Current line and previous winner have no common whitespace:\n        # there is no margin.\n        else:\n            margin = \"\"\n            break\n\n    # sanity check (testing/debugging only)\n    if 0 and margin:\n        for line in text.split(\"\\n\"):\n            assert not line or line.startswith(margin), \\\n                   \"line = %r, margin = %r\" % (line, margin)\n\n    if margin:\n        text = re.sub(r'(?m)^' + margin, '', text)\n    return text\n\nif __name__ == \"__main__\":\n    #print dedent(\"\\tfoo\\n\\tbar\")\n    #print dedent(\"  \\thello there\\n  \\t  how are you?\")\n    print dedent(\"Hello there.\\n  This is indented.\")\n", 
    "threading": "# Hack for making promise work in pypyjs\n\ndef do_nothing(*args, **kwargs):\n\tpass\n\nclass RLock(object):\n    __init__ = do_nothing\n    __enter__ = do_nothing\n    __exit__ = do_nothing\n    acquire = do_nothing\n    release = do_nothing\n\nclass Event(object):\n    __init__ = do_nothing\n    set = do_nothing\n    wait = do_nothing\n\n_shutdown = do_nothing\n", 
    "token": "\"\"\"Token constants (from \"token.h\").\"\"\"\n\n#  This file is automatically generated; please don't muck it up!\n#\n#  To update the symbols in this file, 'cd' to the top directory of\n#  the python source tree after building the interpreter and run:\n#\n#    ./python Lib/token.py\n\n#--start constants--\nENDMARKER = 0\nNAME = 1\nNUMBER = 2\nSTRING = 3\nNEWLINE = 4\nINDENT = 5\nDEDENT = 6\nLPAR = 7\nRPAR = 8\nLSQB = 9\nRSQB = 10\nCOLON = 11\nCOMMA = 12\nSEMI = 13\nPLUS = 14\nMINUS = 15\nSTAR = 16\nSLASH = 17\nVBAR = 18\nAMPER = 19\nLESS = 20\nGREATER = 21\nEQUAL = 22\nDOT = 23\nPERCENT = 24\nBACKQUOTE = 25\nLBRACE = 26\nRBRACE = 27\nEQEQUAL = 28\nNOTEQUAL = 29\nLESSEQUAL = 30\nGREATEREQUAL = 31\nTILDE = 32\nCIRCUMFLEX = 33\nLEFTSHIFT = 34\nRIGHTSHIFT = 35\nDOUBLESTAR = 36\nPLUSEQUAL = 37\nMINEQUAL = 38\nSTAREQUAL = 39\nSLASHEQUAL = 40\nPERCENTEQUAL = 41\nAMPEREQUAL = 42\nVBAREQUAL = 43\nCIRCUMFLEXEQUAL = 44\nLEFTSHIFTEQUAL = 45\nRIGHTSHIFTEQUAL = 46\nDOUBLESTAREQUAL = 47\nDOUBLESLASH = 48\nDOUBLESLASHEQUAL = 49\nAT = 50\nOP = 51\nERRORTOKEN = 52\nN_TOKENS = 53\nNT_OFFSET = 256\n#--end constants--\n\ntok_name = {}\nfor _name, _value in globals().items():\n    if type(_value) is type(0):\n        tok_name[_value] = _name\ndel _name, _value\n\n\ndef ISTERMINAL(x):\n    return x < NT_OFFSET\n\ndef ISNONTERMINAL(x):\n    return x >= NT_OFFSET\n\ndef ISEOF(x):\n    return x == ENDMARKER\n\n\ndef main():\n    import re\n    import sys\n    args = sys.argv[1:]\n    inFileName = args and args[0] or \"Include/token.h\"\n    outFileName = \"Lib/token.py\"\n    if len(args) > 1:\n        outFileName = args[1]\n    try:\n        fp = open(inFileName)\n    except IOError, err:\n        sys.stdout.write(\"I/O error: %s\\n\" % str(err))\n        sys.exit(1)\n    lines = fp.read().split(\"\\n\")\n    fp.close()\n    prog = re.compile(\n        \"#define[ \\t][ \\t]*([A-Z0-9][A-Z0-9_]*)[ \\t][ \\t]*([0-9][0-9]*)\",\n        re.IGNORECASE)\n    tokens = {}\n    for line in lines:\n        match = prog.match(line)\n        if match:\n            name, val = match.group(1, 2)\n            val = int(val)\n            tokens[val] = name          # reverse so we can sort them...\n    keys = tokens.keys()\n    keys.sort()\n    # load the output skeleton from the target:\n    try:\n        fp = open(outFileName)\n    except IOError, err:\n        sys.stderr.write(\"I/O error: %s\\n\" % str(err))\n        sys.exit(2)\n    format = fp.read().split(\"\\n\")\n    fp.close()\n    try:\n        start = format.index(\"#--start constants--\") + 1\n        end = format.index(\"#--end constants--\")\n    except ValueError:\n        sys.stderr.write(\"target does not contain format markers\")\n        sys.exit(3)\n    lines = []\n    for val in keys:\n        lines.append(\"%s = %d\" % (tokens[val], val))\n    format[start:end] = lines\n    try:\n        fp = open(outFileName, 'w')\n    except IOError, err:\n        sys.stderr.write(\"I/O error: %s\\n\" % str(err))\n        sys.exit(4)\n    fp.write(\"\\n\".join(format))\n    fp.close()\n\n\nif __name__ == \"__main__\":\n    main()\n", 
    "tokenize": "\"\"\"Tokenization help for Python programs.\n\ngenerate_tokens(readline) is a generator that breaks a stream of\ntext into Python tokens.  It accepts a readline-like method which is called\nrepeatedly to get the next line of input (or \"\" for EOF).  It generates\n5-tuples with these members:\n\n    the token type (see token.py)\n    the token (a string)\n    the starting (row, column) indices of the token (a 2-tuple of ints)\n    the ending (row, column) indices of the token (a 2-tuple of ints)\n    the original line (string)\n\nIt is designed to match the working of the Python tokenizer exactly, except\nthat it produces COMMENT tokens for comments and gives type OP for all\noperators\n\nOlder entry points\n    tokenize_loop(readline, tokeneater)\n    tokenize(readline, tokeneater=printtoken)\nare the same, except instead of generating tokens, tokeneater is a callback\nfunction to which the 5 fields described above are passed as 5 arguments,\neach time a new token is found.\"\"\"\n\n__author__ = 'Ka-Ping Yee <ping@lfw.org>'\n__credits__ = ('GvR, ESR, Tim Peters, Thomas Wouters, Fred Drake, '\n               'Skip Montanaro, Raymond Hettinger')\n\nfrom itertools import chain\nimport string, re\nfrom token import *\n\nimport token\n__all__ = [x for x in dir(token) if not x.startswith(\"_\")]\n__all__ += [\"COMMENT\", \"tokenize\", \"generate_tokens\", \"NL\", \"untokenize\"]\ndel x\ndel token\n\nCOMMENT = N_TOKENS\ntok_name[COMMENT] = 'COMMENT'\nNL = N_TOKENS + 1\ntok_name[NL] = 'NL'\nN_TOKENS += 2\n\ndef group(*choices): return '(' + '|'.join(choices) + ')'\ndef any(*choices): return group(*choices) + '*'\ndef maybe(*choices): return group(*choices) + '?'\n\nWhitespace = r'[ \\f\\t]*'\nComment = r'#[^\\r\\n]*'\nIgnore = Whitespace + any(r'\\\\\\r?\\n' + Whitespace) + maybe(Comment)\nName = r'[a-zA-Z_]\\w*'\n\nHexnumber = r'0[xX][\\da-fA-F]+[lL]?'\nOctnumber = r'(0[oO][0-7]+)|(0[0-7]*)[lL]?'\nBinnumber = r'0[bB][01]+[lL]?'\nDecnumber = r'[1-9]\\d*[lL]?'\nIntnumber = group(Hexnumber, Binnumber, Octnumber, Decnumber)\nExponent = r'[eE][-+]?\\d+'\nPointfloat = group(r'\\d+\\.\\d*', r'\\.\\d+') + maybe(Exponent)\nExpfloat = r'\\d+' + Exponent\nFloatnumber = group(Pointfloat, Expfloat)\nImagnumber = group(r'\\d+[jJ]', Floatnumber + r'[jJ]')\nNumber = group(Imagnumber, Floatnumber, Intnumber)\n\n# Tail end of ' string.\nSingle = r\"[^'\\\\]*(?:\\\\.[^'\\\\]*)*'\"\n# Tail end of \" string.\nDouble = r'[^\"\\\\]*(?:\\\\.[^\"\\\\]*)*\"'\n# Tail end of ''' string.\nSingle3 = r\"[^'\\\\]*(?:(?:\\\\.|'(?!''))[^'\\\\]*)*'''\"\n# Tail end of \"\"\" string.\nDouble3 = r'[^\"\\\\]*(?:(?:\\\\.|\"(?!\"\"))[^\"\\\\]*)*\"\"\"'\nTriple = group(\"[uUbB]?[rR]?'''\", '[uUbB]?[rR]?\"\"\"')\n# Single-line ' or \" string.\nString = group(r\"[uUbB]?[rR]?'[^\\n'\\\\]*(?:\\\\.[^\\n'\\\\]*)*'\",\n               r'[uUbB]?[rR]?\"[^\\n\"\\\\]*(?:\\\\.[^\\n\"\\\\]*)*\"')\n\n# Because of leftmost-then-longest match semantics, be sure to put the\n# longest operators first (e.g., if = came before ==, == would get\n# recognized as two instances of =).\nOperator = group(r\"\\*\\*=?\", r\">>=?\", r\"<<=?\", r\"<>\", r\"!=\",\n                 r\"//=?\",\n                 r\"[+\\-*/%&|^=<>]=?\",\n                 r\"~\")\n\nBracket = '[][(){}]'\nSpecial = group(r'\\r?\\n', r'[:;.,`@]')\nFunny = group(Operator, Bracket, Special)\n\nPlainToken = group(Number, Funny, String, Name)\nToken = Ignore + PlainToken\n\n# First (or only) line of ' or \" string.\nContStr = group(r\"[uUbB]?[rR]?'[^\\n'\\\\]*(?:\\\\.[^\\n'\\\\]*)*\" +\n                group(\"'\", r'\\\\\\r?\\n'),\n                r'[uUbB]?[rR]?\"[^\\n\"\\\\]*(?:\\\\.[^\\n\"\\\\]*)*' +\n                group('\"', r'\\\\\\r?\\n'))\nPseudoExtras = group(r'\\\\\\r?\\n|\\Z', Comment, Triple)\nPseudoToken = Whitespace + group(PseudoExtras, Number, Funny, ContStr, Name)\n\ntokenprog, pseudoprog, single3prog, double3prog = map(\n    re.compile, (Token, PseudoToken, Single3, Double3))\nendprogs = {\"'\": re.compile(Single), '\"': re.compile(Double),\n            \"'''\": single3prog, '\"\"\"': double3prog,\n            \"r'''\": single3prog, 'r\"\"\"': double3prog,\n            \"u'''\": single3prog, 'u\"\"\"': double3prog,\n            \"ur'''\": single3prog, 'ur\"\"\"': double3prog,\n            \"R'''\": single3prog, 'R\"\"\"': double3prog,\n            \"U'''\": single3prog, 'U\"\"\"': double3prog,\n            \"uR'''\": single3prog, 'uR\"\"\"': double3prog,\n            \"Ur'''\": single3prog, 'Ur\"\"\"': double3prog,\n            \"UR'''\": single3prog, 'UR\"\"\"': double3prog,\n            \"b'''\": single3prog, 'b\"\"\"': double3prog,\n            \"br'''\": single3prog, 'br\"\"\"': double3prog,\n            \"B'''\": single3prog, 'B\"\"\"': double3prog,\n            \"bR'''\": single3prog, 'bR\"\"\"': double3prog,\n            \"Br'''\": single3prog, 'Br\"\"\"': double3prog,\n            \"BR'''\": single3prog, 'BR\"\"\"': double3prog,\n            'r': None, 'R': None, 'u': None, 'U': None,\n            'b': None, 'B': None}\n\ntriple_quoted = {}\nfor t in (\"'''\", '\"\"\"',\n          \"r'''\", 'r\"\"\"', \"R'''\", 'R\"\"\"',\n          \"u'''\", 'u\"\"\"', \"U'''\", 'U\"\"\"',\n          \"ur'''\", 'ur\"\"\"', \"Ur'''\", 'Ur\"\"\"',\n          \"uR'''\", 'uR\"\"\"', \"UR'''\", 'UR\"\"\"',\n          \"b'''\", 'b\"\"\"', \"B'''\", 'B\"\"\"',\n          \"br'''\", 'br\"\"\"', \"Br'''\", 'Br\"\"\"',\n          \"bR'''\", 'bR\"\"\"', \"BR'''\", 'BR\"\"\"'):\n    triple_quoted[t] = t\nsingle_quoted = {}\nfor t in (\"'\", '\"',\n          \"r'\", 'r\"', \"R'\", 'R\"',\n          \"u'\", 'u\"', \"U'\", 'U\"',\n          \"ur'\", 'ur\"', \"Ur'\", 'Ur\"',\n          \"uR'\", 'uR\"', \"UR'\", 'UR\"',\n          \"b'\", 'b\"', \"B'\", 'B\"',\n          \"br'\", 'br\"', \"Br'\", 'Br\"',\n          \"bR'\", 'bR\"', \"BR'\", 'BR\"' ):\n    single_quoted[t] = t\n\ntabsize = 8\n\nclass TokenError(Exception): pass\n\nclass StopTokenizing(Exception): pass\n\ndef printtoken(type, token, srow_scol, erow_ecol, line): # for testing\n    srow, scol = srow_scol\n    erow, ecol = erow_ecol\n    print \"%d,%d-%d,%d:\\t%s\\t%s\" % \\\n        (srow, scol, erow, ecol, tok_name[type], repr(token))\n\ndef tokenize(readline, tokeneater=printtoken):\n    \"\"\"\n    The tokenize() function accepts two parameters: one representing the\n    input stream, and one providing an output mechanism for tokenize().\n\n    The first parameter, readline, must be a callable object which provides\n    the same interface as the readline() method of built-in file objects.\n    Each call to the function should return one line of input as a string.\n\n    The second parameter, tokeneater, must also be a callable object. It is\n    called once for each token, with five arguments, corresponding to the\n    tuples generated by generate_tokens().\n    \"\"\"\n    try:\n        tokenize_loop(readline, tokeneater)\n    except StopTokenizing:\n        pass\n\n# backwards compatible interface\ndef tokenize_loop(readline, tokeneater):\n    for token_info in generate_tokens(readline):\n        tokeneater(*token_info)\n\nclass Untokenizer:\n\n    def __init__(self):\n        self.tokens = []\n        self.prev_row = 1\n        self.prev_col = 0\n\n    def add_whitespace(self, start):\n        row, col = start\n        if row < self.prev_row or row == self.prev_row and col < self.prev_col:\n            raise ValueError(\"start ({},{}) precedes previous end ({},{})\"\n                             .format(row, col, self.prev_row, self.prev_col))\n        row_offset = row - self.prev_row\n        if row_offset:\n            self.tokens.append(\"\\\\\\n\" * row_offset)\n            self.prev_col = 0\n        col_offset = col - self.prev_col\n        if col_offset:\n            self.tokens.append(\" \" * col_offset)\n\n    def untokenize(self, iterable):\n        it = iter(iterable)\n        for t in it:\n            if len(t) == 2:\n                self.compat(t, it)\n                break\n            tok_type, token, start, end, line = t\n            if tok_type == ENDMARKER:\n                break\n            self.add_whitespace(start)\n            self.tokens.append(token)\n            self.prev_row, self.prev_col = end\n            if tok_type in (NEWLINE, NL):\n                self.prev_row += 1\n                self.prev_col = 0\n        return \"\".join(self.tokens)\n\n    def compat(self, token, iterable):\n        indents = []\n        toks_append = self.tokens.append\n        startline = token[0] in (NEWLINE, NL)\n        prevstring = False\n\n        for tok in chain([token], iterable):\n            toknum, tokval = tok[:2]\n\n            if toknum in (NAME, NUMBER):\n                tokval += ' '\n\n            # Insert a space between two consecutive strings\n            if toknum == STRING:\n                if prevstring:\n                    tokval = ' ' + tokval\n                prevstring = True\n            else:\n                prevstring = False\n\n            if toknum == INDENT:\n                indents.append(tokval)\n                continue\n            elif toknum == DEDENT:\n                indents.pop()\n                continue\n            elif toknum in (NEWLINE, NL):\n                startline = True\n            elif startline and indents:\n                toks_append(indents[-1])\n                startline = False\n            toks_append(tokval)\n\ndef untokenize(iterable):\n    \"\"\"Transform tokens back into Python source code.\n\n    Each element returned by the iterable must be a token sequence\n    with at least two elements, a token number and token value.  If\n    only two tokens are passed, the resulting output is poor.\n\n    Round-trip invariant for full input:\n        Untokenized source will match input source exactly\n\n    Round-trip invariant for limited intput:\n        # Output text will tokenize the back to the input\n        t1 = [tok[:2] for tok in generate_tokens(f.readline)]\n        newcode = untokenize(t1)\n        readline = iter(newcode.splitlines(1)).next\n        t2 = [tok[:2] for tok in generate_tokens(readline)]\n        assert t1 == t2\n    \"\"\"\n    ut = Untokenizer()\n    return ut.untokenize(iterable)\n\ndef generate_tokens(readline):\n    \"\"\"\n    The generate_tokens() generator requires one argument, readline, which\n    must be a callable object which provides the same interface as the\n    readline() method of built-in file objects. Each call to the function\n    should return one line of input as a string.  Alternately, readline\n    can be a callable function terminating with StopIteration:\n        readline = open(myfile).next    # Example of alternate readline\n\n    The generator produces 5-tuples with these members: the token type; the\n    token string; a 2-tuple (srow, scol) of ints specifying the row and\n    column where the token begins in the source; a 2-tuple (erow, ecol) of\n    ints specifying the row and column where the token ends in the source;\n    and the line on which the token was found. The line passed is the\n    logical line; continuation lines are included.\n    \"\"\"\n    lnum = parenlev = continued = 0\n    namechars, numchars = string.ascii_letters + '_', '0123456789'\n    contstr, needcont = '', 0\n    contline = None\n    indents = [0]\n\n    while 1:                                   # loop over lines in stream\n        try:\n            line = readline()\n        except StopIteration:\n            line = ''\n        lnum += 1\n        pos, max = 0, len(line)\n\n        if contstr:                            # continued string\n            if not line:\n                raise TokenError, (\"EOF in multi-line string\", strstart)\n            endmatch = endprog.match(line)\n            if endmatch:\n                pos = end = endmatch.end(0)\n                yield (STRING, contstr + line[:end],\n                       strstart, (lnum, end), contline + line)\n                contstr, needcont = '', 0\n                contline = None\n            elif needcont and line[-2:] != '\\\\\\n' and line[-3:] != '\\\\\\r\\n':\n                yield (ERRORTOKEN, contstr + line,\n                           strstart, (lnum, len(line)), contline)\n                contstr = ''\n                contline = None\n                continue\n            else:\n                contstr = contstr + line\n                contline = contline + line\n                continue\n\n        elif parenlev == 0 and not continued:  # new statement\n            if not line: break\n            column = 0\n            while pos < max:                   # measure leading whitespace\n                if line[pos] == ' ':\n                    column += 1\n                elif line[pos] == '\\t':\n                    column = (column//tabsize + 1)*tabsize\n                elif line[pos] == '\\f':\n                    column = 0\n                else:\n                    break\n                pos += 1\n            if pos == max:\n                break\n\n            if line[pos] in '#\\r\\n':           # skip comments or blank lines\n                if line[pos] == '#':\n                    comment_token = line[pos:].rstrip('\\r\\n')\n                    nl_pos = pos + len(comment_token)\n                    yield (COMMENT, comment_token,\n                           (lnum, pos), (lnum, pos + len(comment_token)), line)\n                    yield (NL, line[nl_pos:],\n                           (lnum, nl_pos), (lnum, len(line)), line)\n                else:\n                    yield ((NL, COMMENT)[line[pos] == '#'], line[pos:],\n                           (lnum, pos), (lnum, len(line)), line)\n                continue\n\n            if column > indents[-1]:           # count indents or dedents\n                indents.append(column)\n                yield (INDENT, line[:pos], (lnum, 0), (lnum, pos), line)\n            while column < indents[-1]:\n                if column not in indents:\n                    raise IndentationError(\n                        \"unindent does not match any outer indentation level\",\n                        (\"<tokenize>\", lnum, pos, line))\n                indents = indents[:-1]\n                yield (DEDENT, '', (lnum, pos), (lnum, pos), line)\n\n        else:                                  # continued statement\n            if not line:\n                raise TokenError, (\"EOF in multi-line statement\", (lnum, 0))\n            continued = 0\n\n        while pos < max:\n            pseudomatch = pseudoprog.match(line, pos)\n            if pseudomatch:                                # scan for tokens\n                start, end = pseudomatch.span(1)\n                spos, epos, pos = (lnum, start), (lnum, end), end\n                if start == end:\n                    continue\n                token, initial = line[start:end], line[start]\n\n                if initial in numchars or \\\n                   (initial == '.' and token != '.'):      # ordinary number\n                    yield (NUMBER, token, spos, epos, line)\n                elif initial in '\\r\\n':\n                    yield (NL if parenlev > 0 else NEWLINE,\n                           token, spos, epos, line)\n                elif initial == '#':\n                    assert not token.endswith(\"\\n\")\n                    yield (COMMENT, token, spos, epos, line)\n                elif token in triple_quoted:\n                    endprog = endprogs[token]\n                    endmatch = endprog.match(line, pos)\n                    if endmatch:                           # all on one line\n                        pos = endmatch.end(0)\n                        token = line[start:pos]\n                        yield (STRING, token, spos, (lnum, pos), line)\n                    else:\n                        strstart = (lnum, start)           # multiple lines\n                        contstr = line[start:]\n                        contline = line\n                        break\n                elif initial in single_quoted or \\\n                    token[:2] in single_quoted or \\\n                    token[:3] in single_quoted:\n                    if token[-1] == '\\n':                  # continued string\n                        strstart = (lnum, start)\n                        endprog = (endprogs[initial] or endprogs[token[1]] or\n                                   endprogs[token[2]])\n                        contstr, needcont = line[start:], 1\n                        contline = line\n                        break\n                    else:                                  # ordinary string\n                        yield (STRING, token, spos, epos, line)\n                elif initial in namechars:                 # ordinary name\n                    yield (NAME, token, spos, epos, line)\n                elif initial == '\\\\':                      # continued stmt\n                    continued = 1\n                else:\n                    if initial in '([{':\n                        parenlev += 1\n                    elif initial in ')]}':\n                        parenlev -= 1\n                    yield (OP, token, spos, epos, line)\n            else:\n                yield (ERRORTOKEN, line[pos],\n                           (lnum, pos), (lnum, pos+1), line)\n                pos += 1\n\n    for indent in indents[1:]:                 # pop remaining indent levels\n        yield (DEDENT, '', (lnum, 0), (lnum, 0), '')\n    yield (ENDMARKER, '', (lnum, 0), (lnum, 0), '')\n\nif __name__ == '__main__':                     # testing\n    import sys\n    if len(sys.argv) > 1:\n        tokenize(open(sys.argv[1]).readline)\n    else:\n        tokenize(sys.stdin.readline)\n", 
    "traceback": "\"\"\"Extract, format and print information about Python stack traces.\"\"\"\n\nimport linecache\nimport sys\nimport types\n\n__all__ = ['extract_stack', 'extract_tb', 'format_exception',\n           'format_exception_only', 'format_list', 'format_stack',\n           'format_tb', 'print_exc', 'format_exc', 'print_exception',\n           'print_last', 'print_stack', 'print_tb', 'tb_lineno']\n\ndef _print(file, str='', terminator='\\n'):\n    file.write(str+terminator)\n\n\ndef print_list(extracted_list, file=None):\n    \"\"\"Print the list of tuples as returned by extract_tb() or\n    extract_stack() as a formatted stack trace to the given file.\"\"\"\n    if file is None:\n        file = sys.stderr\n    for filename, lineno, name, line in extracted_list:\n        _print(file,\n               '  File \"%s\", line %d, in %s' % (filename,lineno,name))\n        if line:\n            _print(file, '    %s' % line.strip())\n\ndef format_list(extracted_list):\n    \"\"\"Format a list of traceback entry tuples for printing.\n\n    Given a list of tuples as returned by extract_tb() or\n    extract_stack(), return a list of strings ready for printing.\n    Each string in the resulting list corresponds to the item with the\n    same index in the argument list.  Each string ends in a newline;\n    the strings may contain internal newlines as well, for those items\n    whose source text line is not None.\n    \"\"\"\n    list = []\n    for filename, lineno, name, line in extracted_list:\n        item = '  File \"%s\", line %d, in %s\\n' % (filename,lineno,name)\n        if line:\n            item = item + '    %s\\n' % line.strip()\n        list.append(item)\n    return list\n\n\ndef print_tb(tb, limit=None, file=None):\n    \"\"\"Print up to 'limit' stack trace entries from the traceback 'tb'.\n\n    If 'limit' is omitted or None, all entries are printed.  If 'file'\n    is omitted or None, the output goes to sys.stderr; otherwise\n    'file' should be an open file or file-like object with a write()\n    method.\n    \"\"\"\n    if file is None:\n        file = sys.stderr\n    if limit is None:\n        if hasattr(sys, 'tracebacklimit'):\n            limit = sys.tracebacklimit\n    n = 0\n    while tb is not None and (limit is None or n < limit):\n        f = tb.tb_frame\n        lineno = tb.tb_lineno\n        co = f.f_code\n        filename = co.co_filename\n        name = co.co_name\n        _print(file,\n               '  File \"%s\", line %d, in %s' % (filename, lineno, name))\n        linecache.checkcache(filename)\n        line = linecache.getline(filename, lineno, f.f_globals)\n        if line: _print(file, '    ' + line.strip())\n        tb = tb.tb_next\n        n = n+1\n\ndef format_tb(tb, limit = None):\n    \"\"\"A shorthand for 'format_list(extract_tb(tb, limit))'.\"\"\"\n    return format_list(extract_tb(tb, limit))\n\ndef extract_tb(tb, limit = None):\n    \"\"\"Return list of up to limit pre-processed entries from traceback.\n\n    This is useful for alternate formatting of stack traces.  If\n    'limit' is omitted or None, all entries are extracted.  A\n    pre-processed stack trace entry is a quadruple (filename, line\n    number, function name, text) representing the information that is\n    usually printed for a stack trace.  The text is a string with\n    leading and trailing whitespace stripped; if the source is not\n    available it is None.\n    \"\"\"\n    if limit is None:\n        if hasattr(sys, 'tracebacklimit'):\n            limit = sys.tracebacklimit\n    list = []\n    n = 0\n    while tb is not None and (limit is None or n < limit):\n        f = tb.tb_frame\n        lineno = tb.tb_lineno\n        co = f.f_code\n        filename = co.co_filename\n        name = co.co_name\n        linecache.checkcache(filename)\n        line = linecache.getline(filename, lineno, f.f_globals)\n        if line: line = line.strip()\n        else: line = None\n        list.append((filename, lineno, name, line))\n        tb = tb.tb_next\n        n = n+1\n    return list\n\n\ndef print_exception(etype, value, tb, limit=None, file=None, _encoding=None):\n    \"\"\"Print exception up to 'limit' stack trace entries from 'tb' to 'file'.\n\n    This differs from print_tb() in the following ways: (1) if\n    traceback is not None, it prints a header \"Traceback (most recent\n    call last):\"; (2) it prints the exception type and value after the\n    stack trace; (3) if type is SyntaxError and value has the\n    appropriate format, it prints the line where the syntax error\n    occurred with a caret on the next line indicating the approximate\n    position of the error.\n    \"\"\"\n    if file is None:\n        file = sys.stderr\n    if tb:\n        _print(file, 'Traceback (most recent call last):')\n        print_tb(tb, limit, file)\n    lines = format_exception_only(etype, value, _encoding)\n    for line in lines:\n        _print(file, line, '')\n\ndef format_exception(etype, value, tb, limit = None):\n    \"\"\"Format a stack trace and the exception information.\n\n    The arguments have the same meaning as the corresponding arguments\n    to print_exception().  The return value is a list of strings, each\n    ending in a newline and some containing internal newlines.  When\n    these lines are concatenated and printed, exactly the same text is\n    printed as does print_exception().\n    \"\"\"\n    if tb:\n        list = ['Traceback (most recent call last):\\n']\n        list = list + format_tb(tb, limit)\n    else:\n        list = []\n    list = list + format_exception_only(etype, value)\n    return list\n\ndef format_exception_only(etype, value, _encoding=None):\n    \"\"\"Format the exception part of a traceback.\n\n    The arguments are the exception type and value such as given by\n    sys.last_type and sys.last_value. The return value is a list of\n    strings, each ending in a newline.\n\n    Normally, the list contains a single string; however, for\n    SyntaxError exceptions, it contains several lines that (when\n    printed) display detailed information about where the syntax\n    error occurred.\n\n    The message indicating which exception occurred is always the last\n    string in the list.\n\n    \"\"\"\n\n    # An instance should not have a meaningful value parameter, but\n    # sometimes does, particularly for string exceptions, such as\n    # >>> raise string1, string2  # deprecated\n    #\n    # Clear these out first because issubtype(string1, SyntaxError)\n    # would raise another exception and mask the original problem.\n    if (isinstance(etype, BaseException) or\n        isinstance(etype, types.InstanceType) or\n        etype is None or type(etype) is str):\n        return [_format_final_exc_line(etype, value, _encoding)]\n\n    stype = etype.__name__\n\n    if not issubclass(etype, SyntaxError):\n        return [_format_final_exc_line(stype, value, _encoding)]\n\n    # It was a syntax error; show exactly where the problem was found.\n    lines = []\n    try:\n        msg, (filename, lineno, offset, badline) = value.args\n    except Exception:\n        pass\n    else:\n        filename = filename or \"<string>\"\n        lines.append('  File \"%s\", line %d\\n' % (filename, lineno))\n        if badline is not None:\n            lines.append('    %s\\n' % badline.strip())\n            if offset is not None:\n                caretspace = badline.rstrip('\\n')\n                offset = min(len(caretspace), offset) - 1\n                caretspace = caretspace[:offset].lstrip()\n                # non-space whitespace (likes tabs) must be kept for alignment\n                caretspace = ((c.isspace() and c or ' ') for c in caretspace)\n                lines.append('    %s^\\n' % ''.join(caretspace))\n        value = msg\n\n    lines.append(_format_final_exc_line(stype, value, _encoding))\n    return lines\n\ndef _format_final_exc_line(etype, value, _encoding=None):\n    \"\"\"Return a list of a single line -- normal case for format_exception_only\"\"\"\n    valuestr = _some_str(value, _encoding)\n    if value is None or not valuestr:\n        line = \"%s\\n\" % etype\n    else:\n        line = \"%s: %s\\n\" % (etype, valuestr)\n    return line\n\ndef _some_str(value, _encoding=None):\n    try:\n        return str(value)\n    except Exception:\n        pass\n    try:\n        value = unicode(value)\n        return value.encode(_encoding or \"ascii\", \"backslashreplace\")\n    except Exception:\n        pass\n    return '<unprintable %s object>' % type(value).__name__\n\n\ndef print_exc(limit=None, file=None):\n    \"\"\"Shorthand for 'print_exception(sys.exc_type, sys.exc_value, sys.exc_traceback, limit, file)'.\n    (In fact, it uses sys.exc_info() to retrieve the same information\n    in a thread-safe way.)\"\"\"\n    if file is None:\n        file = sys.stderr\n    try:\n        etype, value, tb = sys.exc_info()\n        print_exception(etype, value, tb, limit, file)\n    finally:\n        etype = value = tb = None\n\n\ndef format_exc(limit=None):\n    \"\"\"Like print_exc() but return a string.\"\"\"\n    try:\n        etype, value, tb = sys.exc_info()\n        return ''.join(format_exception(etype, value, tb, limit))\n    finally:\n        etype = value = tb = None\n\n\ndef print_last(limit=None, file=None):\n    \"\"\"This is a shorthand for 'print_exception(sys.last_type,\n    sys.last_value, sys.last_traceback, limit, file)'.\"\"\"\n    if not hasattr(sys, \"last_type\"):\n        raise ValueError(\"no last exception\")\n    if file is None:\n        file = sys.stderr\n    print_exception(sys.last_type, sys.last_value, sys.last_traceback,\n                    limit, file)\n\n\ndef print_stack(f=None, limit=None, file=None):\n    \"\"\"Print a stack trace from its invocation point.\n\n    The optional 'f' argument can be used to specify an alternate\n    stack frame at which to start. The optional 'limit' and 'file'\n    arguments have the same meaning as for print_exception().\n    \"\"\"\n    if f is None:\n        try:\n            raise ZeroDivisionError\n        except ZeroDivisionError:\n            f = sys.exc_info()[2].tb_frame.f_back\n    print_list(extract_stack(f, limit), file)\n\ndef format_stack(f=None, limit=None):\n    \"\"\"Shorthand for 'format_list(extract_stack(f, limit))'.\"\"\"\n    if f is None:\n        try:\n            raise ZeroDivisionError\n        except ZeroDivisionError:\n            f = sys.exc_info()[2].tb_frame.f_back\n    return format_list(extract_stack(f, limit))\n\ndef extract_stack(f=None, limit = None):\n    \"\"\"Extract the raw traceback from the current stack frame.\n\n    The return value has the same format as for extract_tb().  The\n    optional 'f' and 'limit' arguments have the same meaning as for\n    print_stack().  Each item in the list is a quadruple (filename,\n    line number, function name, text), and the entries are in order\n    from oldest to newest stack frame.\n    \"\"\"\n    if f is None:\n        try:\n            raise ZeroDivisionError\n        except ZeroDivisionError:\n            f = sys.exc_info()[2].tb_frame.f_back\n    if limit is None:\n        if hasattr(sys, 'tracebacklimit'):\n            limit = sys.tracebacklimit\n    list = []\n    n = 0\n    while f is not None and (limit is None or n < limit):\n        lineno = f.f_lineno\n        co = f.f_code\n        filename = co.co_filename\n        name = co.co_name\n        linecache.checkcache(filename)\n        line = linecache.getline(filename, lineno, f.f_globals)\n        if line: line = line.strip()\n        else: line = None\n        list.append((filename, lineno, name, line))\n        f = f.f_back\n        n = n+1\n    list.reverse()\n    return list\n\ndef tb_lineno(tb):\n    \"\"\"Calculate correct line number of traceback given in tb.\n\n    Obsolete in 2.3.\n    \"\"\"\n    return tb.tb_lineno\n", 
    "types": "\"\"\"Define names for all type symbols known in the standard interpreter.\n\nTypes that are part of optional modules (e.g. array) are not listed.\n\"\"\"\nimport sys\n\n# Iterators in Python aren't a matter of type but of protocol.  A large\n# and changing number of builtin types implement *some* flavor of\n# iterator.  Don't check the type!  Use hasattr to check for both\n# \"__iter__\" and \"next\" attributes instead.\n\nNoneType = type(None)\nTypeType = type\nObjectType = object\n\nIntType = int\nLongType = long\nFloatType = float\nBooleanType = bool\ntry:\n    ComplexType = complex\nexcept NameError:\n    pass\n\nStringType = str\n\n# StringTypes is already outdated.  Instead of writing \"type(x) in\n# types.StringTypes\", you should use \"isinstance(x, basestring)\".  But\n# we keep around for compatibility with Python 2.2.\ntry:\n    UnicodeType = unicode\n    StringTypes = (StringType, UnicodeType)\nexcept NameError:\n    StringTypes = (StringType,)\n\nBufferType = buffer\n\nTupleType = tuple\nListType = list\nDictType = DictionaryType = dict\n\ndef _f(): pass\nFunctionType = type(_f)\nLambdaType = type(lambda: None)         # Same as FunctionType\nCodeType = type(_f.func_code)\n\ndef _g():\n    yield 1\nGeneratorType = type(_g())\n\nclass _C:\n    def _m(self): pass\nClassType = type(_C)\nUnboundMethodType = type(_C._m)         # Same as MethodType\n_x = _C()\nInstanceType = type(_x)\nMethodType = type(_x._m)\n\nBuiltinFunctionType = type(len)\nBuiltinMethodType = type([].append)     # Same as BuiltinFunctionType\n\nModuleType = type(sys)\nFileType = file\nXRangeType = xrange\n\ntry:\n    raise TypeError\nexcept TypeError:\n    tb = sys.exc_info()[2]\n    TracebackType = type(tb)\n    FrameType = type(tb.tb_frame)\n    del tb\n\nSliceType = slice\nEllipsisType = type(Ellipsis)\n\nDictProxyType = type(TypeType.__dict__)\nNotImplementedType = type(NotImplemented)\n\n# For Jython, the following two types are identical\nGetSetDescriptorType = type(FunctionType.func_code)\nMemberDescriptorType = type(FunctionType.func_globals)\n\ndel sys, _f, _g, _C, _x                           # Not for export\n", 
    "warnings": "\"\"\"Python part of the warnings subsystem.\"\"\"\n\n# Note: function level imports should *not* be used\n# in this module as it may cause import lock deadlock.\n# See bug 683658.\nimport linecache\nimport sys\nimport types\n\n__all__ = [\"warn\", \"warn_explicit\", \"showwarning\",\n           \"formatwarning\", \"filterwarnings\", \"simplefilter\",\n           \"resetwarnings\", \"catch_warnings\"]\n\n\ndef warnpy3k(message, category=None, stacklevel=1):\n    \"\"\"Issue a deprecation warning for Python 3.x related changes.\n\n    Warnings are omitted unless Python is started with the -3 option.\n    \"\"\"\n    if sys.py3kwarning:\n        if category is None:\n            category = DeprecationWarning\n        warn(message, category, stacklevel+1)\n\ndef _show_warning(message, category, filename, lineno, file=None, line=None):\n    \"\"\"Hook to write a warning to a file; replace if you like.\"\"\"\n    if file is None:\n        file = sys.stderr\n    try:\n        file.write(formatwarning(message, category, filename, lineno, line))\n    except IOError:\n        pass # the file (probably stderr) is invalid - this warning gets lost.\n# Keep a working version around in case the deprecation of the old API is\n# triggered.\nshowwarning = _show_warning\n\ndef formatwarning(message, category, filename, lineno, line=None):\n    \"\"\"Function to format a warning the standard way.\"\"\"\n    s =  \"%s:%s: %s: %s\\n\" % (filename, lineno, category.__name__, message)\n    line = linecache.getline(filename, lineno) if line is None else line\n    if line:\n        line = line.strip()\n        s += \"  %s\\n\" % line\n    return s\n\ndef filterwarnings(action, message=\"\", category=Warning, module=\"\", lineno=0,\n                   append=0):\n    \"\"\"Insert an entry into the list of warnings filters (at the front).\n\n    'action' -- one of \"error\", \"ignore\", \"always\", \"default\", \"module\",\n                or \"once\"\n    'message' -- a regex that the warning message must match\n    'category' -- a class that the warning must be a subclass of\n    'module' -- a regex that the module name must match\n    'lineno' -- an integer line number, 0 matches all warnings\n    'append' -- if true, append to the list of filters\n    \"\"\"\n    import re\n    assert action in (\"error\", \"ignore\", \"always\", \"default\", \"module\",\n                      \"once\"), \"invalid action: %r\" % (action,)\n    assert isinstance(message, basestring), \"message must be a string\"\n    assert isinstance(category, (type, types.ClassType)), \\\n           \"category must be a class\"\n    assert issubclass(category, Warning), \"category must be a Warning subclass\"\n    assert isinstance(module, basestring), \"module must be a string\"\n    assert isinstance(lineno, int) and lineno >= 0, \\\n           \"lineno must be an int >= 0\"\n    item = (action, re.compile(message, re.I), category,\n            re.compile(module), lineno)\n    if append:\n        filters.append(item)\n    else:\n        filters.insert(0, item)\n\ndef simplefilter(action, category=Warning, lineno=0, append=0):\n    \"\"\"Insert a simple entry into the list of warnings filters (at the front).\n\n    A simple filter matches all modules and messages.\n    'action' -- one of \"error\", \"ignore\", \"always\", \"default\", \"module\",\n                or \"once\"\n    'category' -- a class that the warning must be a subclass of\n    'lineno' -- an integer line number, 0 matches all warnings\n    'append' -- if true, append to the list of filters\n    \"\"\"\n    assert action in (\"error\", \"ignore\", \"always\", \"default\", \"module\",\n                      \"once\"), \"invalid action: %r\" % (action,)\n    assert isinstance(lineno, int) and lineno >= 0, \\\n           \"lineno must be an int >= 0\"\n    item = (action, None, category, None, lineno)\n    if append:\n        filters.append(item)\n    else:\n        filters.insert(0, item)\n\ndef resetwarnings():\n    \"\"\"Clear the list of warning filters, so that no filters are active.\"\"\"\n    filters[:] = []\n\nclass _OptionError(Exception):\n    \"\"\"Exception used by option processing helpers.\"\"\"\n    pass\n\n# Helper to process -W options passed via sys.warnoptions\ndef _processoptions(args):\n    for arg in args:\n        try:\n            _setoption(arg)\n        except _OptionError, msg:\n            print >>sys.stderr, \"Invalid -W option ignored:\", msg\n\n# Helper for _processoptions()\ndef _setoption(arg):\n    import re\n    parts = arg.split(':')\n    if len(parts) > 5:\n        raise _OptionError(\"too many fields (max 5): %r\" % (arg,))\n    while len(parts) < 5:\n        parts.append('')\n    action, message, category, module, lineno = [s.strip()\n                                                 for s in parts]\n    action = _getaction(action)\n    message = re.escape(message)\n    category = _getcategory(category)\n    module = re.escape(module)\n    if module:\n        module = module + '$'\n    if lineno:\n        try:\n            lineno = int(lineno)\n            if lineno < 0:\n                raise ValueError\n        except (ValueError, OverflowError):\n            raise _OptionError(\"invalid lineno %r\" % (lineno,))\n    else:\n        lineno = 0\n    filterwarnings(action, message, category, module, lineno)\n\n# Helper for _setoption()\ndef _getaction(action):\n    if not action:\n        return \"default\"\n    if action == \"all\": return \"always\" # Alias\n    for a in ('default', 'always', 'ignore', 'module', 'once', 'error'):\n        if a.startswith(action):\n            return a\n    raise _OptionError(\"invalid action: %r\" % (action,))\n\n# Helper for _setoption()\ndef _getcategory(category):\n    import re\n    if not category:\n        return Warning\n    if re.match(\"^[a-zA-Z0-9_]+$\", category):\n        try:\n            cat = eval(category)\n        except NameError:\n            raise _OptionError(\"unknown warning category: %r\" % (category,))\n    else:\n        i = category.rfind(\".\")\n        module = category[:i]\n        klass = category[i+1:]\n        try:\n            m = __import__(module, None, None, [klass])\n        except ImportError:\n            raise _OptionError(\"invalid module name: %r\" % (module,))\n        try:\n            cat = getattr(m, klass)\n        except AttributeError:\n            raise _OptionError(\"unknown warning category: %r\" % (category,))\n    if not issubclass(cat, Warning):\n        raise _OptionError(\"invalid warning category: %r\" % (category,))\n    return cat\n\n\n# Code typically replaced by _warnings\ndef warn(message, category=None, stacklevel=1):\n    \"\"\"Issue a warning, or maybe ignore it or raise an exception.\"\"\"\n    # Check if message is already a Warning object\n    if isinstance(message, Warning):\n        category = message.__class__\n    # Check category argument\n    if category is None:\n        category = UserWarning\n    assert issubclass(category, Warning)\n    # Get context information\n    try:\n        caller = sys._getframe(stacklevel)\n    except ValueError:\n        globals = sys.__dict__\n        lineno = 1\n    else:\n        globals = caller.f_globals\n        lineno = caller.f_lineno\n    if '__name__' in globals:\n        module = globals['__name__']\n    else:\n        module = \"<string>\"\n    filename = globals.get('__file__')\n    if filename:\n        fnl = filename.lower()\n        if fnl.endswith((\".pyc\", \".pyo\")):\n            filename = filename[:-1]\n    else:\n        if module == \"__main__\":\n            try:\n                filename = sys.argv[0]\n            except AttributeError:\n                # embedded interpreters don't have sys.argv, see bug #839151\n                filename = '__main__'\n        if not filename:\n            filename = module\n    registry = globals.setdefault(\"__warningregistry__\", {})\n    warn_explicit(message, category, filename, lineno, module, registry,\n                  globals)\n\ndef warn_explicit(message, category, filename, lineno,\n                  module=None, registry=None, module_globals=None):\n    lineno = int(lineno)\n    if module is None:\n        module = filename or \"<unknown>\"\n        if module[-3:].lower() == \".py\":\n            module = module[:-3] # XXX What about leading pathname?\n    if registry is None:\n        registry = {}\n    if isinstance(message, Warning):\n        text = str(message)\n        category = message.__class__\n    else:\n        text = message\n        message = category(message)\n    key = (text, category, lineno)\n    # Quick test for common case\n    if registry.get(key):\n        return\n    # Search the filters\n    for item in filters:\n        action, msg, cat, mod, ln = item\n        if ((msg is None or msg.match(text)) and\n            issubclass(category, cat) and\n            (mod is None or mod.match(module)) and\n            (ln == 0 or lineno == ln)):\n            break\n    else:\n        action = defaultaction\n    # Early exit actions\n    if action == \"ignore\":\n        registry[key] = 1\n        return\n\n    # Prime the linecache for formatting, in case the\n    # \"file\" is actually in a zipfile or something.\n    linecache.getlines(filename, module_globals)\n\n    if action == \"error\":\n        raise message\n    # Other actions\n    if action == \"once\":\n        registry[key] = 1\n        oncekey = (text, category)\n        if onceregistry.get(oncekey):\n            return\n        onceregistry[oncekey] = 1\n    elif action == \"always\":\n        pass\n    elif action == \"module\":\n        registry[key] = 1\n        altkey = (text, category, 0)\n        if registry.get(altkey):\n            return\n        registry[altkey] = 1\n    elif action == \"default\":\n        registry[key] = 1\n    else:\n        # Unrecognized actions are errors\n        raise RuntimeError(\n              \"Unrecognized action (%r) in warnings.filters:\\n %s\" %\n              (action, item))\n    # Print message and context\n    showwarning(message, category, filename, lineno)\n\n\nclass WarningMessage(object):\n\n    \"\"\"Holds the result of a single showwarning() call.\"\"\"\n\n    _WARNING_DETAILS = (\"message\", \"category\", \"filename\", \"lineno\", \"file\",\n                        \"line\")\n\n    def __init__(self, message, category, filename, lineno, file=None,\n                    line=None):\n        local_values = locals()\n        for attr in self._WARNING_DETAILS:\n            setattr(self, attr, local_values[attr])\n        self._category_name = category.__name__ if category else None\n\n    def __str__(self):\n        return (\"{message : %r, category : %r, filename : %r, lineno : %s, \"\n                    \"line : %r}\" % (self.message, self._category_name,\n                                    self.filename, self.lineno, self.line))\n\n\nclass catch_warnings(object):\n\n    \"\"\"A context manager that copies and restores the warnings filter upon\n    exiting the context.\n\n    The 'record' argument specifies whether warnings should be captured by a\n    custom implementation of warnings.showwarning() and be appended to a list\n    returned by the context manager. Otherwise None is returned by the context\n    manager. The objects appended to the list are arguments whose attributes\n    mirror the arguments to showwarning().\n\n    The 'module' argument is to specify an alternative module to the module\n    named 'warnings' and imported under that name. This argument is only useful\n    when testing the warnings module itself.\n\n    \"\"\"\n\n    def __init__(self, record=False, module=None):\n        \"\"\"Specify whether to record warnings and if an alternative module\n        should be used other than sys.modules['warnings'].\n\n        For compatibility with Python 3.0, please consider all arguments to be\n        keyword-only.\n\n        \"\"\"\n        self._record = record\n        self._module = sys.modules['warnings'] if module is None else module\n        self._entered = False\n\n    def __repr__(self):\n        args = []\n        if self._record:\n            args.append(\"record=True\")\n        if self._module is not sys.modules['warnings']:\n            args.append(\"module=%r\" % self._module)\n        name = type(self).__name__\n        return \"%s(%s)\" % (name, \", \".join(args))\n\n    def __enter__(self):\n        if self._entered:\n            raise RuntimeError(\"Cannot enter %r twice\" % self)\n        self._entered = True\n        self._filters = self._module.filters\n        self._module.filters = self._filters[:]\n        self._showwarning = self._module.showwarning\n        if self._record:\n            log = []\n            def showwarning(*args, **kwargs):\n                log.append(WarningMessage(*args, **kwargs))\n            self._module.showwarning = showwarning\n            return log\n        else:\n            return None\n\n    def __exit__(self, *exc_info):\n        if not self._entered:\n            raise RuntimeError(\"Cannot exit %r without entering first\" % self)\n        self._module.filters = self._filters\n        self._module.showwarning = self._showwarning\n\n\n# filters contains a sequence of filter 5-tuples\n# The components of the 5-tuple are:\n# - an action: error, ignore, always, default, module, or once\n# - a compiled regex that must match the warning message\n# - a class representing the warning category\n# - a compiled regex that must match the module that is being warned\n# - a line number for the line being warning, or 0 to mean any line\n# If either if the compiled regexs are None, match anything.\n_warnings_defaults = False\ntry:\n    from _warnings import (filters, default_action, once_registry,\n                            warn, warn_explicit)\n    defaultaction = default_action\n    onceregistry = once_registry\n    _warnings_defaults = True\nexcept ImportError:\n    filters = []\n    defaultaction = \"default\"\n    onceregistry = {}\n\n\n# Module initialization\n_processoptions(sys.warnoptions)\nif not _warnings_defaults:\n    silence = [ImportWarning, PendingDeprecationWarning]\n    # Don't silence DeprecationWarning if -3 or -Q was used.\n    if not sys.py3kwarning and not sys.flags.division_warning:\n        silence.append(DeprecationWarning)\n    for cls in silence:\n        simplefilter(\"ignore\", category=cls)\n    bytes_warning = sys.flags.bytes_warning\n    if bytes_warning > 1:\n        bytes_action = \"error\"\n    elif bytes_warning:\n        bytes_action = \"default\"\n    else:\n        bytes_action = \"ignore\"\n    simplefilter(bytes_action, category=BytesWarning, append=1)\ndel _warnings_defaults\n", 
    "weakref": "\"\"\"Weak reference support for Python.\n\nThis module is an implementation of PEP 205:\n\nhttp://www.python.org/dev/peps/pep-0205/\n\"\"\"\n\n# Naming convention: Variables named \"wr\" are weak reference objects;\n# they are called this instead of \"ref\" to avoid name collisions with\n# the module-global ref() function imported from _weakref.\n\nimport UserDict\n\nfrom _weakref import (\n     getweakrefcount,\n     getweakrefs,\n     ref,\n     proxy,\n     CallableProxyType,\n     ProxyType,\n     ReferenceType)\n\nfrom _weakrefset import WeakSet, _IterationGuard\n\nfrom exceptions import ReferenceError\n\n\nProxyTypes = (ProxyType, CallableProxyType)\n\n__all__ = [\"ref\", \"proxy\", \"getweakrefcount\", \"getweakrefs\",\n           \"WeakKeyDictionary\", \"ReferenceError\", \"ReferenceType\", \"ProxyType\",\n           \"CallableProxyType\", \"ProxyTypes\", \"WeakValueDictionary\", 'WeakSet']\n\n\nclass WeakValueDictionary(UserDict.UserDict):\n    \"\"\"Mapping class that references values weakly.\n\n    Entries in the dictionary will be discarded when no strong\n    reference to the value exists anymore\n    \"\"\"\n    # We inherit the constructor without worrying about the input\n    # dictionary; since it uses our .update() method, we get the right\n    # checks (if the other dictionary is a WeakValueDictionary,\n    # objects are unwrapped on the way out, and we always wrap on the\n    # way in).\n\n    def __init__(self, *args, **kw):\n        def remove(wr, selfref=ref(self)):\n            self = selfref()\n            if self is not None:\n                if self._iterating:\n                    self._pending_removals.append(wr.key)\n                else:\n                    # Changed this for PyPy: made more resistent.  The\n                    # issue is that in some corner cases, self.data\n                    # might already be changed or removed by the time\n                    # this weakref's callback is called.  If that is\n                    # the case, we don't want to randomly kill an\n                    # unrelated entry.\n                    if self.data.get(wr.key) is wr:\n                        del self.data[wr.key]\n        self._remove = remove\n        # A list of keys to be removed\n        self._pending_removals = []\n        self._iterating = set()\n        UserDict.UserDict.__init__(self, *args, **kw)\n\n    def _commit_removals(self):\n        l = self._pending_removals\n        d = self.data\n        # We shouldn't encounter any KeyError, because this method should\n        # always be called *before* mutating the dict.\n        while l:\n            del d[l.pop()]\n\n    def __getitem__(self, key):\n        o = self.data[key]()\n        if o is None:\n            raise KeyError, key\n        else:\n            return o\n\n    def __delitem__(self, key):\n        if self._pending_removals:\n            self._commit_removals()\n        del self.data[key]\n\n    def __contains__(self, key):\n        try:\n            o = self.data[key]()\n        except KeyError:\n            return False\n        return o is not None\n\n    def has_key(self, key):\n        try:\n            o = self.data[key]()\n        except KeyError:\n            return False\n        return o is not None\n\n    def __repr__(self):\n        return \"<WeakValueDictionary at %s>\" % id(self)\n\n    def __setitem__(self, key, value):\n        if self._pending_removals:\n            self._commit_removals()\n        self.data[key] = KeyedRef(value, self._remove, key)\n\n    def clear(self):\n        if self._pending_removals:\n            self._commit_removals()\n        self.data.clear()\n\n    def copy(self):\n        new = WeakValueDictionary()\n        for key, wr in self.data.items():\n            o = wr()\n            if o is not None:\n                new[key] = o\n        return new\n\n    __copy__ = copy\n\n    def __deepcopy__(self, memo):\n        from copy import deepcopy\n        new = self.__class__()\n        for key, wr in self.data.items():\n            o = wr()\n            if o is not None:\n                new[deepcopy(key, memo)] = o\n        return new\n\n    def get(self, key, default=None):\n        try:\n            wr = self.data[key]\n        except KeyError:\n            return default\n        else:\n            o = wr()\n            if o is None:\n                # This should only happen\n                return default\n            else:\n                return o\n\n    def items(self):\n        L = []\n        for key, wr in self.data.items():\n            o = wr()\n            if o is not None:\n                L.append((key, o))\n        return L\n\n    def iteritems(self):\n        with _IterationGuard(self):\n            for wr in self.data.itervalues():\n                value = wr()\n                if value is not None:\n                    yield wr.key, value\n\n    def iterkeys(self):\n        with _IterationGuard(self):\n            for k in self.data.iterkeys():\n                yield k\n\n    __iter__ = iterkeys\n\n    def itervaluerefs(self):\n        \"\"\"Return an iterator that yields the weak references to the values.\n\n        The references are not guaranteed to be 'live' at the time\n        they are used, so the result of calling the references needs\n        to be checked before being used.  This can be used to avoid\n        creating references that will cause the garbage collector to\n        keep the values around longer than needed.\n\n        \"\"\"\n        with _IterationGuard(self):\n            for wr in self.data.itervalues():\n                yield wr\n\n    def itervalues(self):\n        with _IterationGuard(self):\n            for wr in self.data.itervalues():\n                obj = wr()\n                if obj is not None:\n                    yield obj\n\n    def popitem(self):\n        if self._pending_removals:\n            self._commit_removals()\n        while 1:\n            key, wr = self.data.popitem()\n            o = wr()\n            if o is not None:\n                return key, o\n\n    def pop(self, key, *args):\n        if self._pending_removals:\n            self._commit_removals()\n        try:\n            o = self.data.pop(key)()\n        except KeyError:\n            o = None\n        if o is None:\n            if args:\n                return args[0]\n            raise KeyError, key\n        else:\n            return o\n        # The logic above was fixed in PyPy\n\n    def setdefault(self, key, default=None):\n        try:\n            o = self.data[key]()\n        except KeyError:\n            o = None\n        if o is None:\n            if self._pending_removals:\n                self._commit_removals()\n            self.data[key] = KeyedRef(default, self._remove, key)\n            return default\n        else:\n            return o\n        # The logic above was fixed in PyPy\n\n    def update(self, dict=None, **kwargs):\n        if self._pending_removals:\n            self._commit_removals()\n        d = self.data\n        if dict is not None:\n            if not hasattr(dict, \"items\"):\n                dict = type({})(dict)\n            for key, o in dict.items():\n                d[key] = KeyedRef(o, self._remove, key)\n        if len(kwargs):\n            self.update(kwargs)\n\n    def valuerefs(self):\n        \"\"\"Return a list of weak references to the values.\n\n        The references are not guaranteed to be 'live' at the time\n        they are used, so the result of calling the references needs\n        to be checked before being used.  This can be used to avoid\n        creating references that will cause the garbage collector to\n        keep the values around longer than needed.\n\n        \"\"\"\n        return self.data.values()\n\n    def values(self):\n        L = []\n        for wr in self.data.values():\n            o = wr()\n            if o is not None:\n                L.append(o)\n        return L\n\n\nclass KeyedRef(ref):\n    \"\"\"Specialized reference that includes a key corresponding to the value.\n\n    This is used in the WeakValueDictionary to avoid having to create\n    a function object for each key stored in the mapping.  A shared\n    callback object can use the 'key' attribute of a KeyedRef instead\n    of getting a reference to the key from an enclosing scope.\n\n    \"\"\"\n\n    __slots__ = \"key\",\n\n    def __new__(type, ob, callback, key):\n        self = ref.__new__(type, ob, callback)\n        self.key = key\n        return self\n\n    def __init__(self, ob, callback, key):\n        super(KeyedRef,  self).__init__(ob, callback)\n\n\nclass WeakKeyDictionary(UserDict.UserDict):\n    \"\"\" Mapping class that references keys weakly.\n\n    Entries in the dictionary will be discarded when there is no\n    longer a strong reference to the key. This can be used to\n    associate additional data with an object owned by other parts of\n    an application without adding attributes to those objects. This\n    can be especially useful with objects that override attribute\n    accesses.\n    \"\"\"\n\n    def __init__(self, dict=None):\n        self.data = {}\n        def remove(k, selfref=ref(self)):\n            self = selfref()\n            if self is not None:\n                if self._iterating:\n                    self._pending_removals.append(k)\n                else:\n                    del self.data[k]\n        self._remove = remove\n        # A list of dead weakrefs (keys to be removed)\n        self._pending_removals = []\n        self._iterating = set()\n        if dict is not None:\n            self.update(dict)\n\n    def _commit_removals(self):\n        # NOTE: We don't need to call this method before mutating the dict,\n        # because a dead weakref never compares equal to a live weakref,\n        # even if they happened to refer to equal objects.\n        # However, it means keys may already have been removed.\n        l = self._pending_removals\n        d = self.data\n        while l:\n            try:\n                del d[l.pop()]\n            except KeyError:\n                pass\n\n    def __delitem__(self, key):\n        del self.data[ref(key)]\n\n    def __getitem__(self, key):\n        return self.data[ref(key)]\n\n    def __repr__(self):\n        return \"<WeakKeyDictionary at %s>\" % id(self)\n\n    def __setitem__(self, key, value):\n        self.data[ref(key, self._remove)] = value\n\n    def copy(self):\n        new = WeakKeyDictionary()\n        for key, value in self.data.items():\n            o = key()\n            if o is not None:\n                new[o] = value\n        return new\n\n    __copy__ = copy\n\n    def __deepcopy__(self, memo):\n        from copy import deepcopy\n        new = self.__class__()\n        for key, value in self.data.items():\n            o = key()\n            if o is not None:\n                new[o] = deepcopy(value, memo)\n        return new\n\n    def get(self, key, default=None):\n        return self.data.get(ref(key),default)\n\n    def has_key(self, key):\n        try:\n            wr = ref(key)\n        except TypeError:\n            return 0\n        return wr in self.data\n\n    def __contains__(self, key):\n        try:\n            wr = ref(key)\n        except TypeError:\n            return 0\n        return wr in self.data\n\n    def items(self):\n        L = []\n        for key, value in self.data.items():\n            o = key()\n            if o is not None:\n                L.append((o, value))\n        return L\n\n    def iteritems(self):\n        with _IterationGuard(self):\n            for wr, value in self.data.iteritems():\n                key = wr()\n                if key is not None:\n                    yield key, value\n\n    def iterkeyrefs(self):\n        \"\"\"Return an iterator that yields the weak references to the keys.\n\n        The references are not guaranteed to be 'live' at the time\n        they are used, so the result of calling the references needs\n        to be checked before being used.  This can be used to avoid\n        creating references that will cause the garbage collector to\n        keep the keys around longer than needed.\n\n        \"\"\"\n        with _IterationGuard(self):\n            for wr in self.data.iterkeys():\n                yield wr\n\n    def iterkeys(self):\n        with _IterationGuard(self):\n            for wr in self.data.iterkeys():\n                obj = wr()\n                if obj is not None:\n                    yield obj\n\n    __iter__ = iterkeys\n\n    def itervalues(self):\n        with _IterationGuard(self):\n            for value in self.data.itervalues():\n                yield value\n\n    def keyrefs(self):\n        \"\"\"Return a list of weak references to the keys.\n\n        The references are not guaranteed to be 'live' at the time\n        they are used, so the result of calling the references needs\n        to be checked before being used.  This can be used to avoid\n        creating references that will cause the garbage collector to\n        keep the keys around longer than needed.\n\n        \"\"\"\n        return self.data.keys()\n\n    def keys(self):\n        L = []\n        for wr in self.data.keys():\n            o = wr()\n            if o is not None:\n                L.append(o)\n        return L\n\n    def popitem(self):\n        while 1:\n            key, value = self.data.popitem()\n            o = key()\n            if o is not None:\n                return o, value\n\n    def pop(self, key, *args):\n        return self.data.pop(ref(key), *args)\n\n    def setdefault(self, key, default=None):\n        return self.data.setdefault(ref(key, self._remove),default)\n\n    def update(self, dict=None, **kwargs):\n        d = self.data\n        if dict is not None:\n            if not hasattr(dict, \"items\"):\n                dict = type({})(dict)\n            for key, value in dict.items():\n                d[ref(key, self._remove)] = value\n        if len(kwargs):\n            self.update(kwargs)\n"
  }
}